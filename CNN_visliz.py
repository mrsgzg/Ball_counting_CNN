"""
Single Image CNN Model Feature Analysis Tool
é’ˆå¯¹SingleImageClassifierçš„ç‰¹å¾é™ç»´å¯è§†åŒ–åˆ†æå·¥å…·
"""

import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import matplotlib
matplotlib.use('Agg')  # è®¾ç½®éäº¤äº’å¼åç«¯ï¼Œé€‚åˆæœåŠ¡å™¨ç¯å¢ƒ
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics import silhouette_score, adjusted_rand_score
import umap
import os
import sys
import json
import argparse
import time
from collections import defaultdict
from datetime import datetime
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# ç¡®ä¿ä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


class SingleImageFeatureExtractor:
    """Single Image Modelç‰¹å¾æå–å™¨"""
    
    def __init__(self, model, device='cuda'):
        self.model = model
        self.device = device
        self.model.eval()
        self.features = {}
        self.hooks = []
        
    def inspect_model_structure(self):
        """æ£€æŸ¥SingleImageClassifieræ¨¡å‹ç»“æ„"""
        print("=== Single Image Model ç»“æ„æ£€æŸ¥ ===")
        available_layers = []
        
        def print_module_structure(module, prefix="", max_depth=3, current_depth=0):
            if current_depth > max_depth:
                return
            
            for name, child in module.named_children():
                full_name = f"{prefix}.{name}" if prefix else name
                print(f"{'  ' * current_depth}{full_name}: {child.__class__.__name__}")
                available_layers.append(full_name)
                
                # é€’å½’æ‰“å°å­æ¨¡å—
                if len(list(child.children())) > 0:
                    print_module_structure(child, full_name, max_depth, current_depth + 1)
        
        print_module_structure(self.model)
        print(f"\næ€»å…±å‘ç° {len(available_layers)} ä¸ªå±‚")
        
        return available_layers
    
    def auto_detect_key_layers(self):
        """è‡ªåŠ¨æ£€æµ‹SingleImageClassifierçš„å…³é”®å±‚"""
        available_layers = self.inspect_model_structure()
        
        # SingleImageClassifierçš„å…³é”®å±‚
        target_layers = [
            'visual_encoder',      # è§†è§‰ç¼–ç å™¨
            'visual_encoder.cnn',  # CNNç‰¹å¾æå–
            'spatial_attention',   # ç©ºé—´æ³¨æ„åŠ›
            'classifier'           # åˆ†ç±»å™¨
        ]
        
        detected_layers = []
        
        # ç²¾ç¡®åŒ¹é…
        for target in target_layers:
            if target in available_layers:
                detected_layers.append(target)
                print(f"âœ… æ£€æµ‹åˆ°å…³é”®å±‚: {target}")
        
        # æ¨¡ç³ŠåŒ¹é…CNNå­å±‚
        cnn_layers = []
        for layer_name in available_layers:
            if 'visual_encoder.cnn' in layer_name and any(x in layer_name.lower() for x in ['conv', 'relu', 'pool']):
                cnn_layers.append(layer_name)
        
        if cnn_layers:
            # é€‰æ‹©ä¸€äº›ä»£è¡¨æ€§çš„CNNå±‚
            selected_cnn = cnn_layers[::max(1, len(cnn_layers)//3)][:3]  # é€‰æ‹©3ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„å±‚
            detected_layers.extend(selected_cnn)
            print(f"ğŸ” æ£€æµ‹åˆ°CNNå­å±‚: {selected_cnn}")
        
        print(f"\nå»ºè®®æå–çš„å±‚: {detected_layers}")
        return detected_layers
        
    def register_hooks(self, layers_to_extract):
        """æ³¨å†Œé’©å­å‡½æ•°æ¥æå–ä¸­é—´å±‚ç‰¹å¾"""
        def get_activation(name):
            def hook(model, input, output):
                if isinstance(output, tuple):
                    # å¦‚æœè¾“å‡ºæ˜¯å…ƒç»„ï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ 
                    self.features[name] = output[0].detach().cpu()
                else:
                    self.features[name] = output.detach().cpu()
            return hook
        
        successful_hooks = []
        failed_hooks = []
        
        # æ³¨å†Œé’©å­
        for layer_name in layers_to_extract:
            try:
                # é€šè¿‡ç‚¹å·åˆ†å‰²çš„è·¯å¾„è®¿é—®åµŒå¥—æ¨¡å—
                module = self.model
                for part in layer_name.split('.'):
                    module = getattr(module, part)
                
                handle = module.register_forward_hook(get_activation(layer_name))
                self.hooks.append(handle)
                successful_hooks.append(layer_name)
                print(f"âœ… æˆåŠŸæ³¨å†Œé’©å­: {layer_name}")
                
            except AttributeError as e:
                failed_hooks.append(layer_name)
                print(f"âŒ æ³¨å†Œé’©å­å¤±è´¥: {layer_name} - {e}")
        
        print(f"\né’©å­æ³¨å†Œç»“æœ: æˆåŠŸ {len(successful_hooks)}, å¤±è´¥ {len(failed_hooks)}")
        
        if not successful_hooks and failed_hooks:
            print("âš ï¸ æ²¡æœ‰æˆåŠŸæ³¨å†Œä»»ä½•é’©å­ï¼Œå°è¯•è‡ªåŠ¨æ£€æµ‹...")
            auto_layers = self.auto_detect_key_layers()
            if auto_layers:
                print(f"ä½¿ç”¨è‡ªåŠ¨æ£€æµ‹çš„å±‚: {auto_layers}")
                return self.register_hooks(auto_layers[:4])  # åªå–å‰4ä¸ªé¿å…å¤ªå¤š
        
        return successful_hooks
    
    def remove_hooks(self):
        """ç§»é™¤æ‰€æœ‰é’©å­"""
        for handle in self.hooks:
            handle.remove()
        self.hooks = []
        
    def _process_feature_tensor(self, feature_tensor):
        """å¤„ç†ä¸åŒå½¢çŠ¶çš„ç‰¹å¾å¼ é‡"""
        if len(feature_tensor.shape) == 4:  # [batch, channel, h, w]
            # å…¨å±€å¹³å‡æ± åŒ–
            pooled = feature_tensor.mean(dim=(-2, -1))  # [batch, channel]
            return pooled.cpu().numpy()
        elif len(feature_tensor.shape) == 3:  # [batch, seq, dim] æˆ–å…¶ä»–3D
            # å¦‚æœæœ‰åºåˆ—ç»´åº¦ï¼Œå–æœ€åä¸€ä¸ª
            if feature_tensor.shape[1] > feature_tensor.shape[2]:
                # å‡è®¾æ˜¯ [batch, spatial, channel]
                pooled = feature_tensor.mean(dim=1)
            else:
                # å‡è®¾æ˜¯ [batch, channel, spatial]
                pooled = feature_tensor.mean(dim=-1)
            return pooled.cpu().numpy()
        elif len(feature_tensor.shape) == 2:  # [batch, dim]
            return feature_tensor.cpu().numpy()
        else:
            # å…¶ä»–æƒ…å†µï¼Œå±•å¹³
            return feature_tensor.view(feature_tensor.shape[0], -1).cpu().numpy()
    
    def extract_features(self, data_loader, max_samples=1000):
        """æå–ç‰¹å¾å¹¶æ”¶é›†é¢„æµ‹ç»“æœ - é€‚é…SingleImageDataset"""
        all_features = defaultdict(list)
        all_predictions = []
        all_labels = []
        all_sample_ids = []
        all_attention_weights = []
        
        sample_count = 0
        
        with torch.no_grad():
            for batch in tqdm(data_loader, desc="æå–å•å›¾åƒç‰¹å¾"):
                if sample_count >= max_samples:
                    break
                
                # å•å›¾åƒæ•°æ®æ ¼å¼
                images = batch['image'].to(self.device)
                labels = batch['label'].cpu().numpy()
                sample_ids = batch['sample_id']
                
                # è®¡ç®—å®é™…å¤„ç†çš„æ ·æœ¬æ•°
                remaining_samples = max_samples - sample_count
                actual_batch_size = min(len(labels), remaining_samples)
                
                # æˆªæ–­æ‰¹æ¬¡ï¼ˆå¦‚æœéœ€è¦ï¼‰
                if actual_batch_size < len(labels):
                    images = images[:actual_batch_size]
                    labels = labels[:actual_batch_size]
                    sample_ids = sample_ids[:actual_batch_size]
                
                # æ¸…ç©ºç‰¹å¾å­—å…¸
                self.features = {}
                
                # å‰å‘ä¼ æ’­
                if hasattr(self.model, 'forward') and 'return_attention' in self.model.forward.__code__.co_varnames:
                    # å¦‚æœæ¨¡å‹æ”¯æŒè¿”å›æ³¨æ„åŠ›æƒé‡
                    outputs = self.model(images, return_attention=True)
                    if isinstance(outputs, tuple):
                        logits = outputs[0]
                        if len(outputs) > 1:
                            attention_weights = outputs[1]
                            if attention_weights is not None:
                                all_attention_weights.extend(attention_weights.cpu().numpy())
                    else:
                        logits = outputs
                else:
                    logits = self.model(images)
                
                # æ”¶é›†é¢„æµ‹ç»“æœ
                pred_labels = torch.argmax(logits, dim=-1).cpu().numpy()
                
                all_predictions.extend(pred_labels)
                all_labels.extend(labels)
                all_sample_ids.extend(sample_ids)
                
                # æ”¶é›†ä¸­é—´å±‚ç‰¹å¾
                for feature_name, feature_tensor in self.features.items():
                    processed_features = self._process_feature_tensor(feature_tensor)
                    all_features[feature_name].append(processed_features)
                
                sample_count += actual_batch_size
                
                if sample_count >= max_samples:
                    break
        
        # åˆå¹¶æ‰€æœ‰ç‰¹å¾
        final_features = {}
        for feature_name, feature_list in all_features.items():
            if feature_list:
                final_features[feature_name] = np.vstack(feature_list)
        
        result = {
            'features': final_features,
            'predictions': np.array(all_predictions),
            'labels': np.array(all_labels),
            'sample_ids': all_sample_ids
        }
        
        # æ·»åŠ æ³¨æ„åŠ›æƒé‡ï¼ˆå¦‚æœæœ‰ï¼‰
        if all_attention_weights:
            result['attention_weights'] = np.array(all_attention_weights)
        
        # æ‰“å°è°ƒè¯•ä¿¡æ¯
        print(f"å•å›¾åƒç‰¹å¾æå–å®Œæˆ:")
        print(f"  å®é™…æ ·æœ¬æ•°: {len(result['labels'])}")
        print(f"  çœŸå®æ ‡ç­¾èŒƒå›´: {result['labels'].min()} - {result['labels'].max()}")
        print(f"  é¢„æµ‹æ ‡ç­¾èŒƒå›´: {result['predictions'].min()} - {result['predictions'].max()}")
        print(f"  çœŸå®æ ‡ç­¾å”¯ä¸€å€¼: {sorted(np.unique(result['labels']))}")
        print(f"  é¢„æµ‹æ ‡ç­¾å”¯ä¸€å€¼: {sorted(np.unique(result['predictions']))}")
        print(f"  æå–çš„ç‰¹å¾å±‚: {list(final_features.keys())}")
        
        return result


class SingleImageVisualizationEngine:
    """Single Image Modelå¯è§†åŒ–å¼•æ“"""
    
    def __init__(self, figsize=(15, 10)):
        self.figsize = figsize
        
    def reduce_dimensions(self, features, method='tsne', n_components=2):
        """é™ç»´"""
        if method == 'tsne':
            reducer = TSNE(n_components=n_components, random_state=42, perplexity=min(30, len(features)//4))
        elif method == 'pca':
            reducer = PCA(n_components=n_components)
        elif method == 'umap':
            reducer = umap.UMAP(n_components=n_components, random_state=42)
        else:
            raise ValueError(f"Unsupported method: {method}")
        
        reduced_features = reducer.fit_transform(features)
        return reduced_features
    
    def plot_scatter(self, features_2d, labels, title, save_path=None, alpha=0.7):
        """ç»˜åˆ¶æ•£ç‚¹å›¾"""
        plt.figure(figsize=self.figsize)
        
        unique_labels = np.unique(labels)
        colors = plt.cm.tab10(np.linspace(0, 1, len(unique_labels)))
        
        for i, label in enumerate(unique_labels):
            mask = labels == label
            plt.scatter(features_2d[mask, 0], features_2d[mask, 1], 
                       c=[colors[i]], label=f'Count {label}', alpha=alpha, s=50)
        
        plt.title(title, fontsize=16)
        plt.xlabel('Dimension 1', fontsize=12)
        plt.ylabel('Dimension 2', fontsize=12)
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        
        if save_path:
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
    
    def plot_clustering_comparison(self, features_2d, true_labels, cluster_results, save_dir=None):
        """æ¯”è¾ƒä¸åŒèšç±»æ–¹æ³•çš„ç»“æœ"""
        n_methods = len(cluster_results) + 1  # +1 for true labels
        fig, axes = plt.subplots(1, n_methods, figsize=(5*n_methods, 5))
        
        if n_methods == 1:
            axes = [axes]
        
        # ç»˜åˆ¶çœŸå®æ ‡ç­¾
        unique_labels = np.unique(true_labels)
        colors = plt.cm.tab10(np.linspace(0, 1, len(unique_labels)))
        
        for i, label in enumerate(unique_labels):
            mask = true_labels == label
            axes[0].scatter(features_2d[mask, 0], features_2d[mask, 1], 
                          c=[colors[i]], label=f'Count {label}', alpha=0.7, s=30)
        axes[0].set_title('True Labels')
        axes[0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        axes[0].grid(True, alpha=0.3)
        
        # ç»˜åˆ¶èšç±»ç»“æœ
        for idx, (method, result) in enumerate(cluster_results.items()):
            cluster_labels = result['labels']
            unique_clusters = np.unique(cluster_labels)
            colors = plt.cm.Set3(np.linspace(0, 1, len(unique_clusters)))
            
            for i, cluster in enumerate(unique_clusters):
                mask = cluster_labels == cluster
                if cluster == -1:  # DBSCANçš„å™ªå£°ç‚¹
                    axes[idx+1].scatter(features_2d[mask, 0], features_2d[mask, 1], 
                                      c='black', label='Noise', alpha=0.5, s=30, marker='x')
                else:
                    axes[idx+1].scatter(features_2d[mask, 0], features_2d[mask, 1], 
                                      c=[colors[i]], label=f'Cluster {cluster}', alpha=0.7, s=30)
            
            title = f'{method.upper()}\nSilhouette: {result["silhouette_score"]:.3f}'
            axes[idx+1].set_title(title)
            axes[idx+1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_dir:
            os.makedirs(save_dir, exist_ok=True)
            save_path = os.path.join(save_dir, 'clustering_comparison.png')
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
    
    def plot_attention_heatmap(self, attention_weights, save_path=None):
        """å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡çƒ­åŠ›å›¾"""
        if attention_weights is None or len(attention_weights) == 0:
            return
        
        plt.figure(figsize=(15, 10))
        
        # å¤„ç†æ³¨æ„åŠ›æƒé‡
        if len(attention_weights.shape) == 4:  # [batch, heads, H, W]
            # å–å¹³å‡
            avg_attention = np.mean(attention_weights, axis=(0, 1))  # [H, W]
            sns.heatmap(avg_attention, cmap='viridis', cbar=True, square=True)
            plt.title('Average Spatial Attention Map')
            plt.axis('off')
        elif len(attention_weights.shape) == 3:  # [batch, heads, spatial]
            # å–å¹³å‡å¹¶é‡å¡‘
            avg_attention = np.mean(attention_weights, axis=0)  # [heads, spatial]
            sns.heatmap(avg_attention, cmap='viridis', cbar=True)
            plt.title('Multi-Head Attention Weights')
            plt.xlabel('Spatial Location')
            plt.ylabel('Attention Head')
        elif len(attention_weights.shape) == 2:  # [batch, spatial]
            # å–å¹³å‡
            avg_attention = np.mean(attention_weights, axis=0)
            spatial_size = len(avg_attention)
            grid_size = int(np.sqrt(spatial_size))
            
            if grid_size * grid_size == spatial_size:
                # å¯ä»¥é‡å¡‘ä¸ºæ­£æ–¹å½¢
                attention_map = avg_attention.reshape(grid_size, grid_size)
                sns.heatmap(attention_map, cmap='viridis', cbar=True, square=True)
                plt.title('Average Attention Map')
                plt.axis('off')
            else:
                # ç»˜åˆ¶1Då›¾
                plt.bar(range(len(avg_attention)), avg_attention)
                plt.title('Attention Weights')
                plt.xlabel('Spatial Location')
                plt.ylabel('Attention Weight')
        
        plt.tight_layout()
        
        if save_path:
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
    
    def plot_feature_hierarchy(self, features_dict, labels, save_path=None):
        """å¯è§†åŒ–ç‰¹å¾å±‚æ¬¡ç»“æ„"""
        n_layers = len(features_dict)
        if n_layers == 0:
            return
        
        fig, axes = plt.subplots(2, (n_layers + 1) // 2, figsize=(6 * ((n_layers + 1) // 2), 12))
        if n_layers == 1:
            axes = np.array([axes]).flatten()
        elif (n_layers + 1) // 2 == 1:
            axes = axes.reshape(-1)
        else:
            axes = axes.flatten()
        
        unique_labels = np.unique(labels)
        colors = plt.cm.tab10(np.linspace(0, 1, len(unique_labels)))
        
        for idx, (layer_name, features) in enumerate(features_dict.items()):
            if idx >= len(axes):
                break
                
            # t-SNEé™ç»´
            features_2d = self.reduce_dimensions(features, 'tsne')
            
            for i, label in enumerate(unique_labels):
                mask = labels == label
                axes[idx].scatter(features_2d[mask, 0], features_2d[mask, 1], 
                                c=[colors[i]], label=f'Count {label}', alpha=0.7, s=30)
            
            axes[idx].set_title(f'{layer_name}', fontsize=12)
            axes[idx].grid(True, alpha=0.3)
            if idx == 0:
                axes[idx].legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        
        # éšè—å¤šä½™çš„å­å›¾
        for idx in range(len(features_dict), len(axes)):
            axes[idx].axis('off')
        
        plt.tight_layout()
        
        if save_path:
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
    
    def plot_layer_statistics(self, features_dict, save_path=None):
        """ç»˜åˆ¶å„å±‚ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯"""
        n_layers = len(features_dict)
        if n_layers == 0:
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        axes = axes.flatten()
        
        layer_names = list(features_dict.keys())
        
        # 1. ç‰¹å¾ç»´åº¦
        feature_dims = [features.shape[1] for features in features_dict.values()]
        axes[0].bar(range(len(layer_names)), feature_dims, color='skyblue')
        axes[0].set_title('Feature Dimensions by Layer')
        axes[0].set_xticks(range(len(layer_names)))
        axes[0].set_xticklabels(layer_names, rotation=45, ha='right')
        axes[0].set_ylabel('Dimension')
        axes[0].grid(True, alpha=0.3)
        
        # 2. ç‰¹å¾æ–¹å·®
        feature_vars = [np.var(features) for features in features_dict.values()]
        axes[1].bar(range(len(layer_names)), feature_vars, color='lightcoral')
        axes[1].set_title('Feature Variance by Layer')
        axes[1].set_xticks(range(len(layer_names)))
        axes[1].set_xticklabels(layer_names, rotation=45, ha='right')
        axes[1].set_ylabel('Variance')
        axes[1].grid(True, alpha=0.3)
        
        # 3. ç‰¹å¾èŒƒå›´
        feature_ranges = [(np.max(features) - np.min(features)) for features in features_dict.values()]
        axes[2].bar(range(len(layer_names)), feature_ranges, color='lightgreen')
        axes[2].set_title('Feature Range by Layer')
        axes[2].set_xticks(range(len(layer_names)))
        axes[2].set_xticklabels(layer_names, rotation=45, ha='right')
        axes[2].set_ylabel('Range')
        axes[2].grid(True, alpha=0.3)
        
        # 4. æœ‰æ•ˆç§©ï¼ˆç‰¹å¾å¤æ‚åº¦ï¼‰
        effective_ranks = []
        for features in features_dict.values():
            try:
                rank = np.linalg.matrix_rank(features)
                effective_ranks.append(rank)
            except:
                effective_ranks.append(0)
        
        axes[3].bar(range(len(layer_names)), effective_ranks, color='orange')
        axes[3].set_title('Effective Rank by Layer')
        axes[3].set_xticks(range(len(layer_names)))
        axes[3].set_xticklabels(layer_names, rotation=45, ha='right')
        axes[3].set_ylabel('Rank')
        axes[3].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()


class ClusterAnalyzer:
    """èšç±»åˆ†æå™¨"""
    
    def __init__(self):
        self.results = {}
    
    def perform_clustering(self, features, methods=['kmeans', 'dbscan'], n_clusters=None, true_labels=None):
        """æ‰§è¡Œå¤šç§èšç±»ç®—æ³•"""
        results = {}
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šèšç±»æ•°ï¼Œä»çœŸå®æ ‡ç­¾æ¨æ–­
        if n_clusters is None and true_labels is not None:
            n_clusters = len(np.unique(true_labels))
        elif n_clusters is None:
            n_clusters = min(10, int(np.sqrt(len(features))))
        
        for method in methods:
            if method == 'kmeans':
                clusterer = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
                cluster_labels = clusterer.fit_predict(features)
                if len(np.unique(cluster_labels)) > 1:
                    silhouette = silhouette_score(features, cluster_labels)
                else:
                    silhouette = -1
                results[method] = {
                    'labels': cluster_labels,
                    'silhouette_score': silhouette,
                    'n_clusters': n_clusters
                }
                
            elif method == 'dbscan':
                clusterer = DBSCAN(eps=0.5, min_samples=5)
                cluster_labels = clusterer.fit_predict(features)
                n_clusters_found = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)
                
                if n_clusters_found > 1:
                    mask = cluster_labels != -1
                    if np.sum(mask) > 1:
                        silhouette = silhouette_score(features[mask], cluster_labels[mask])
                    else:
                        silhouette = -1
                else:
                    silhouette = -1
                    
                results[method] = {
                    'labels': cluster_labels,
                    'silhouette_score': silhouette,
                    'n_clusters': n_clusters_found
                }
        
        return results


def load_single_image_model_and_data(checkpoint_path, val_csv, data_root, batch_size=16):
    """åŠ è½½å•å›¾åƒæ¨¡å‹å’Œæ•°æ®"""
    print("ğŸ“¥ åŠ è½½å•å›¾åƒæ¨¡å‹å’Œæ•°æ®...")
    
    # åŠ è½½æ£€æŸ¥ç‚¹
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    config = checkpoint['config']
    
    # å¯¼å…¥æ¨¡å‹ç±»
    try:
        from Model_single_image import create_single_image_model
        from DataLoader_single_image import get_single_image_data_loaders
    except ImportError:
        print("âŒ æ— æ³•å¯¼å…¥å•å›¾åƒæ¨¡å‹ç›¸å…³æ¨¡å—")
        raise
    
    # é‡å»ºæ¨¡å‹
    model = create_single_image_model(config)
    model.load_state_dict(checkpoint['model_state_dict'])
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    model.eval()
    
    image_mode = config.get('image_mode', 'rgb')
    print(f"âœ… å•å›¾åƒæ¨¡å‹åŠ è½½å®Œæˆ (å›¾åƒæ¨¡å¼: {image_mode}, è®¾å¤‡: {device})")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    _, val_loader = get_single_image_data_loaders(
        train_csv_path=config['train_csv'],
        val_csv_path=val_csv,
        data_root=data_root,
        batch_size=batch_size,
        num_workers=2,
        image_mode=image_mode,
        normalize_images=True
    )
    
    print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆï¼ŒéªŒè¯é›†å¤§å°: {len(val_loader.dataset)}")
    
    return model, val_loader, device, config


def analyze_single_image_model(checkpoint_path, val_csv, data_root, 
                              save_dir='./single_image_analysis', 
                              max_samples=500, specific_layers=None):
    """åˆ†æå•å›¾åƒCNNæ¨¡å‹"""
    
    print("ğŸ–¼ï¸ å¼€å§‹å•å›¾åƒCNNæ¨¡å‹ç‰¹å¾åˆ†æ...")
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(save_dir, exist_ok=True)
    
    try:
        # 1. åŠ è½½æ¨¡å‹å’Œæ•°æ®
        model, val_loader, device, config = load_single_image_model_and_data(
            checkpoint_path, val_csv, data_root
        )
        
        # 2. åˆ›å»ºç‰¹å¾æå–å™¨
        feature_extractor = SingleImageFeatureExtractor(model, device)
        
        # 3. ç¡®å®šè¦åˆ†æçš„å±‚
        if specific_layers is None:
            print("ğŸ” è‡ªåŠ¨æ£€æµ‹å…³é”®å±‚...")
            key_layers = feature_extractor.auto_detect_key_layers()
            if not key_layers:
                key_layers = ['visual_encoder', 'classifier']
        else:
            key_layers = specific_layers
        
        print(f"ğŸ“‹ å‡†å¤‡åˆ†æçš„å±‚: {key_layers}")
        
        # 4. æ³¨å†Œé’©å­å¹¶æå–ç‰¹å¾
        successful_layers = feature_extractor.register_hooks(key_layers)
        
        if not successful_layers:
            print("âŒ æ²¡æœ‰æˆåŠŸæ³¨å†Œä»»ä½•é’©å­ï¼")
            return None
        
        try:
            # 5. æå–ç‰¹å¾
            print("ğŸ¯ æå–ç‰¹å¾...")
            data = feature_extractor.extract_features(val_loader, max_samples)
            
            features = data['features']
            predictions = data['predictions']
            true_labels = data['labels']
            attention_weights = data.get('attention_weights', None)
            
            print(f"âœ… ç‰¹å¾æå–å®Œæˆ:")
            print(f"   æ ·æœ¬æ•°: {len(true_labels)}")
            print(f"   æå–å±‚: {list(features.keys())}")
            print(f"   æ³¨æ„åŠ›æƒé‡: {'æœ‰' if attention_weights is not None else 'æ— '}")
            
            # 6. åˆ›å»ºå¯è§†åŒ–å¼•æ“
            visualizer = SingleImageVisualizationEngine()
            cluster_analyzer = ClusterAnalyzer()
            
            # 7. ç‰¹å¾å±‚æ¬¡ç»“æ„å¯è§†åŒ–
            print("ğŸ¨ ç”Ÿæˆç‰¹å¾å±‚æ¬¡ç»“æ„å¯è§†åŒ–...")
            visualizer.plot_feature_hierarchy(
                features, true_labels,
                save_path=os.path.join(save_dir, 'feature_hierarchy.png')
            )
            
            # 8. å„å±‚ç»Ÿè®¡ä¿¡æ¯
            print("ğŸ“Š ç”Ÿæˆå„å±‚ç»Ÿè®¡ä¿¡æ¯...")
            visualizer.plot_layer_statistics(
                features,
                save_path=os.path.join(save_dir, 'layer_statistics.png')
            )
            
            # 9. æ³¨æ„åŠ›æœºåˆ¶å¯è§†åŒ–ï¼ˆå¦‚æœæœ‰ï¼‰
            if attention_weights is not None:
                print("ğŸ‘ï¸ å¯è§†åŒ–æ³¨æ„åŠ›æœºåˆ¶...")
                visualizer.plot_attention_heatmap(
                    attention_weights,
                    save_path=os.path.join(save_dir, 'attention_heatmap.png')
                )
            
            # 10. å„å±‚é™ç»´å’Œèšç±»åˆ†æ
            print("ğŸ”¬ æ‰§è¡Œé™ç»´å’Œèšç±»åˆ†æ...")
            
            analysis_results = {}
            
            for layer_name, layer_features in features.items():
                if layer_features is None:
                    continue
                    
                print(f"   åˆ†æ {layer_name} å±‚...")
                layer_results = {}
                
                # å¤šç§é™ç»´æ–¹æ³•
                for dim_method in ['tsne', 'pca', 'umap']:
                    try:
                        # é™ç»´
                        features_2d = visualizer.reduce_dimensions(layer_features, dim_method)
                        
                        # å¯è§†åŒ–çœŸå®æ ‡ç­¾
                        visualizer.plot_scatter(
                            features_2d, true_labels,
                            f'{layer_name} - True Labels ({dim_method.upper()})',
                            save_path=os.path.join(save_dir, f'{layer_name}_{dim_method}_true.png')
                        )
                        
                        # å¯è§†åŒ–é¢„æµ‹ç»“æœ
                        visualizer.plot_scatter(
                            features_2d, predictions,
                            f'{layer_name} - Predictions ({dim_method.upper()})',
                            save_path=os.path.join(save_dir, f'{layer_name}_{dim_method}_pred.png')
                        )
                        
                        # èšç±»åˆ†æ
                        cluster_results = cluster_analyzer.perform_clustering(
                            layer_features, methods=['kmeans', 'dbscan'], true_labels=true_labels
                        )
                        
                        # èšç±»å¯¹æ¯”å¯è§†åŒ–
                        visualizer.plot_clustering_comparison(
                            features_2d, true_labels, cluster_results,
                            save_dir=os.path.join(save_dir, f'{layer_name}_{dim_method}_clustering')
                        )
                        
                        layer_results[dim_method] = {
                            'clustering': cluster_results,
                            'features_2d': features_2d
                        }
                        
                    except Exception as e:
                        print(f"     {dim_method} åˆ†æå¤±è´¥: {e}")
                        continue
                
                analysis_results[layer_name] = layer_results
            
            # 11. ç”Ÿæˆåˆ†ææŠ¥å‘Š
            print("ğŸ“ ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
            generate_single_image_report(
                analysis_results, features, true_labels, predictions, 
                attention_weights, config, save_dir
            )
            
            print(f"ğŸ‰ å•å›¾åƒæ¨¡å‹åˆ†æå®Œæˆï¼ç»“æœä¿å­˜åœ¨: {save_dir}")
            return analysis_results
            
        finally:
            feature_extractor.remove_hooks()
            
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def generate_single_image_report(analysis_results, features, true_labels, predictions, 
                               attention_weights, config, save_dir):
    """ç”Ÿæˆå•å›¾åƒæ¨¡å‹åˆ†ææŠ¥å‘Š"""
    
    from sklearn.metrics import accuracy_score
    
    # åŸºç¡€ç»Ÿè®¡
    accuracy = accuracy_score(true_labels, predictions)
    unique_true = len(np.unique(true_labels))
    unique_pred = len(np.unique(predictions))
    
    # å±‚çº§ç‰¹å¾å¤æ‚åº¦
    layer_complexity = {}
    for layer_name, layer_features in features.items():
        if layer_features is not None:
            layer_complexity[layer_name] = {
                'feature_dim': int(layer_features.shape[1]),
                'feature_variance': float(np.var(layer_features)),
                'feature_range': [float(np.min(layer_features)), float(np.max(layer_features))],
                'feature_mean': float(np.mean(layer_features)),
                'feature_std': float(np.std(layer_features)),
                'effective_rank': float(np.linalg.matrix_rank(layer_features))
            }
    
    # æ³¨æ„åŠ›åˆ†æ
    attention_analysis = {}
    if attention_weights is not None:
        attention_analysis = {
            'mean_attention': float(np.mean(attention_weights)),
            'attention_std': float(np.std(attention_weights)),
            'attention_entropy': float(-np.sum(attention_weights * np.log(attention_weights + 1e-8), axis=-1).mean()) if attention_weights.ndim > 1 else 0,
            'attention_sparsity': float(np.sum(attention_weights < 0.1) / attention_weights.size)
        }
    
    # èšç±»è´¨é‡æ€»ç»“
    clustering_summary = {}
    for layer_name, layer_results in analysis_results.items():
        layer_clustering = {}
        for dim_method, method_results in layer_results.items():
            if 'clustering' in method_results:
                clustering_results = method_results['clustering']
                best_silhouette = max([r['silhouette_score'] for r in clustering_results.values() 
                                     if r['silhouette_score'] != -1], default=0)
                layer_clustering[dim_method] = {
                    'best_silhouette': float(best_silhouette),
                    'kmeans_silhouette': float(clustering_results.get('kmeans', {}).get('silhouette_score', 0)),
                    'dbscan_silhouette': float(clustering_results.get('dbscan', {}).get('silhouette_score', 0))
                }
        clustering_summary[layer_name] = layer_clustering
    
    # ç”Ÿæˆå®Œæ•´æŠ¥å‘Š
    report = {
        'analysis_type': 'Single Image CNN Model Feature Analysis',
        'timestamp': pd.Timestamp.now().isoformat(),
        'model_config': {
            'image_mode': config.get('image_mode', 'rgb'),
            'use_attention': config.get('use_attention', True),
            'num_classes': config.get('num_classes', 10)
        },
        'summary': {
            'total_samples': int(len(true_labels)),
            'overall_accuracy': float(accuracy),
            'unique_true_labels': int(unique_true),
            'unique_predictions': int(unique_pred),
            'analyzed_layers': list(features.keys())
        },
        'layer_complexity': layer_complexity,
        'attention_analysis': attention_analysis,
        'clustering_summary': clustering_summary,
        'recommendations': generate_single_image_recommendations(analysis_results, features, accuracy, config)
    }
    
    # ä¿å­˜JSONæŠ¥å‘Š
    try:
        with open(os.path.join(save_dir, 'single_image_analysis_report.json'), 'w') as f:
            json.dump(report, f, indent=2)
        print(f"âœ… JSONæŠ¥å‘Šå·²ä¿å­˜")
    except Exception as e:
        print(f"âš ï¸ JSONæŠ¥å‘Šä¿å­˜å¤±è´¥: {e}")
    
    # ç”Ÿæˆå¯è¯»æŠ¥å‘Š
    try:
        with open(os.path.join(save_dir, 'single_image_analysis_summary.txt'), 'w', encoding='utf-8') as f:
            f.write("=== å•å›¾åƒCNNæ¨¡å‹ç‰¹å¾åˆ†ææŠ¥å‘Š ===\n\n")
            f.write(f"åˆ†ææ—¶é—´: {report['timestamp']}\n")
            f.write(f"æ€»æ ·æœ¬æ•°: {report['summary']['total_samples']}\n")
            f.write(f"æ•´ä½“å‡†ç¡®ç‡: {report['summary']['overall_accuracy']:.4f}\n")
            f.write(f"åˆ†æå±‚æ•°: {len(report['summary']['analyzed_layers'])}\n\n")
            
            # æ¨¡å‹é…ç½®
            f.write("=== æ¨¡å‹é…ç½® ===\n")
            for key, value in report['model_config'].items():
                f.write(f"{key}: {value}\n")
            f.write("\n")
            
            # å±‚çº§å¤æ‚åº¦
            f.write("=== å±‚çº§ç‰¹å¾å¤æ‚åº¦ ===\n")
            for layer_name, complexity in layer_complexity.items():
                f.write(f"{layer_name}:\n")
                f.write(f"  ç‰¹å¾ç»´åº¦: {complexity['feature_dim']}\n")
                f.write(f"  ç‰¹å¾æ–¹å·®: {complexity['feature_variance']:.4f}\n")
                f.write(f"  ç‰¹å¾å‡å€¼: {complexity['feature_mean']:.4f}\n")
                f.write(f"  ç‰¹å¾æ ‡å‡†å·®: {complexity['feature_std']:.4f}\n")
                f.write(f"  æœ‰æ•ˆç§©: {complexity['effective_rank']:.1f}\n")
                f.write(f"  æ•°å€¼èŒƒå›´: [{complexity['feature_range'][0]:.4f}, {complexity['feature_range'][1]:.4f}]\n")
            f.write("\n")
            
            # æ³¨æ„åŠ›åˆ†æ
            if attention_analysis:
                f.write("=== æ³¨æ„åŠ›æœºåˆ¶åˆ†æ ===\n")
                f.write(f"å¹³å‡æ³¨æ„åŠ›å¼ºåº¦: {attention_analysis['mean_attention']:.4f}\n")
                f.write(f"æ³¨æ„åŠ›æ ‡å‡†å·®: {attention_analysis['attention_std']:.4f}\n")
                if attention_analysis['attention_entropy'] > 0:
                    f.write(f"æ³¨æ„åŠ›ç†µ: {attention_analysis['attention_entropy']:.4f}\n")
                f.write(f"æ³¨æ„åŠ›ç¨€ç–æ€§: {attention_analysis['attention_sparsity']:.4f}\n\n")
            
            # èšç±»è´¨é‡æ€»ç»“
            f.write("=== èšç±»è´¨é‡æ€»ç»“ ===\n")
            for layer_name, clustering in clustering_summary.items():
                f.write(f"{layer_name}:\n")
                for dim_method, scores in clustering.items():
                    f.write(f"  {dim_method}: æœ€ä½³è½®å»“ç³»æ•°={scores['best_silhouette']:.3f}\n")
                    f.write(f"    K-means: {scores['kmeans_silhouette']:.3f}\n")
                    f.write(f"    DBSCAN: {scores['dbscan_silhouette']:.3f}\n")
            f.write("\n")
            
            # å»ºè®®
            f.write("=== åˆ†æå»ºè®® ===\n")
            for rec in report['recommendations']:
                f.write(f"â€¢ {rec}\n")
        
        print(f"âœ… æ–‡æœ¬æŠ¥å‘Šå·²ä¿å­˜")
        
    except Exception as e:
        print(f"âš ï¸ æ–‡æœ¬æŠ¥å‘Šä¿å­˜å¤±è´¥: {e}")


def generate_single_image_recommendations(analysis_results, features, accuracy, config):
    """åŸºäºåˆ†æç»“æœç”Ÿæˆå»ºè®®"""
    recommendations = []
    
    # å‡†ç¡®ç‡å»ºè®®
    if accuracy < 0.6:
        recommendations.append("æ¨¡å‹å‡†ç¡®ç‡è¾ƒä½ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®è´¨é‡ã€å¢åŠ è®­ç»ƒè½®æ•°æˆ–è°ƒæ•´å­¦ä¹ ç‡")
    elif accuracy > 0.95:
        recommendations.append("æ¨¡å‹å‡†ç¡®ç‡å¾ˆé«˜ï¼Œå¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆï¼Œå»ºè®®åœ¨æµ‹è¯•é›†ä¸ŠéªŒè¯")
    elif accuracy > 0.85:
        recommendations.append("æ¨¡å‹å‡†ç¡®ç‡è‰¯å¥½ï¼Œå¯ä»¥è€ƒè™‘åœ¨æ›´å¤æ‚çš„ä»»åŠ¡ä¸Šæµ‹è¯•")
    
    # ç‰¹å¾ç»´åº¦å»ºè®®
    high_dim_layers = [name for name, feats in features.items() 
                      if feats is not None and feats.shape[1] > 1024]
    if high_dim_layers:
        recommendations.append(f"å±‚ {high_dim_layers} ç‰¹å¾ç»´åº¦å¾ˆé«˜ï¼Œå¯è€ƒè™‘é™ç»´æˆ–æ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆ")
    
    low_dim_layers = [name for name, feats in features.items() 
                     if feats is not None and feats.shape[1] < 64]
    if low_dim_layers:
        recommendations.append(f"å±‚ {low_dim_layers} ç‰¹å¾ç»´åº¦è¾ƒä½ï¼Œå¯èƒ½é™åˆ¶äº†è¡¨è¾¾èƒ½åŠ›")
    
    # ç‰¹å¾å¤æ‚åº¦å»ºè®®
    low_rank_layers = []
    for layer_name, feats in features.items():
        if feats is not None:
            rank = np.linalg.matrix_rank(feats)
            dim = feats.shape[1]
            if rank < dim * 0.5:  # æœ‰æ•ˆç§©å°äºç»´åº¦çš„50%
                low_rank_layers.append(layer_name)
    
    if low_rank_layers:
        recommendations.append(f"å±‚ {low_rank_layers} çš„æœ‰æ•ˆç§©è¾ƒä½ï¼Œç‰¹å¾å¯èƒ½å­˜åœ¨å†—ä½™")
    
    # èšç±»è´¨é‡å»ºè®®
    poor_clustering_layers = []
    for layer_name, layer_results in analysis_results.items():
        avg_silhouette = []
        for dim_method, method_results in layer_results.items():
            if 'clustering' in method_results:
                silhouettes = [r['silhouette_score'] for r in method_results['clustering'].values() 
                             if r['silhouette_score'] != -1]
                if silhouettes:
                    avg_silhouette.extend(silhouettes)
        
        if avg_silhouette and np.mean(avg_silhouette) < 0.2:
            poor_clustering_layers.append(layer_name)
    
    if poor_clustering_layers:
        recommendations.append(f"å±‚ {poor_clustering_layers} çš„èšç±»è´¨é‡è¾ƒå·®ï¼Œä¸åŒç±»åˆ«çš„ç‰¹å¾åˆ†ç¦»åº¦ä¸å¤Ÿ")
    
    # æ¨¡å‹æ¶æ„å»ºè®®
    if not config.get('use_attention', True):
        recommendations.append("æ¨¡å‹æœªä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œå»ºè®®å°è¯•æ·»åŠ æ³¨æ„åŠ›æ¨¡å—æå‡æ€§èƒ½")
    
    return recommendations


def inspect_single_image_model_structure(checkpoint_path):
    """æ£€æŸ¥å•å›¾åƒæ¨¡å‹ç»“æ„"""
    print("ğŸ”¬ æ£€æŸ¥å•å›¾åƒæ¨¡å‹ç»“æ„...")
    
    try:
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        config = checkpoint['config']
        
        from Model_single_image import create_single_image_model
        
        model = create_single_image_model(config)
        
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model = model.to(device)
        
        feature_extractor = SingleImageFeatureExtractor(model, device)
        available_layers = feature_extractor.inspect_model_structure()
        auto_layers = feature_extractor.auto_detect_key_layers()
        
        print(f"\nâœ¨ è‡ªåŠ¨æ£€æµ‹çš„å…³é”®å±‚: {auto_layers}")
        print(f"\nğŸ“‹ æ¨èçš„åˆ†æç­–ç•¥:")
        print("  --mode quick --max_samples 100              # å¿«é€Ÿåˆ†æ")
        print("  --mode full --max_samples 500               # å®Œæ•´åˆ†æ") 
        print("  --layers visual_encoder classifier --max_samples 300  # è‡ªå®šä¹‰å±‚åˆ†æ")
        
        return available_layers, auto_layers
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹ç»“æ„æ£€æŸ¥å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None


def main():
    """ä¸»å…¥å£å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å•å›¾åƒCNNæ¨¡å‹ç‰¹å¾åˆ†æå·¥å…·')
    
    # å¿…éœ€å‚æ•°
    parser.add_argument('--checkpoint', type=str, required=True,
                       help='å•å›¾åƒæ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--val_csv', type=str, default='scratch/Ball_counting_CNN/Tools_script/ball_counting_dataset_val.csv',
                       help='éªŒè¯é›†CSVæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--data_root', type=str, default='/mnt/iusers01/fatpou01/compsci01/k09562zs/scratch/Ball_counting_CNN/ball_data_collection',
                       help='æ•°æ®æ ¹ç›®å½•')
    
    # åˆ†æé€‰é¡¹
    parser.add_argument('--mode', type=str, default='full',
                       choices=['inspect', 'quick', 'full'],
                       help='åˆ†ææ¨¡å¼')
    parser.add_argument('--save_dir', type=str, default=None,
                       help='ç»“æœä¿å­˜ç›®å½•')
    parser.add_argument('--max_samples', type=int, default=1100,
                       help='æœ€å¤§åˆ†ææ ·æœ¬æ•°')
    parser.add_argument('--layers', nargs='+', default=None,
                       help='æŒ‡å®šè¦åˆ†æçš„å±‚åç§°')
    
    # å…¶ä»–é€‰é¡¹
    parser.add_argument('--batch_size', type=int, default=32,
                       help='æ•°æ®åŠ è½½æ‰¹æ¬¡å¤§å°')
    
    args = parser.parse_args()
    
    # è®¾ç½®é»˜è®¤ä¿å­˜ç›®å½•
    if args.save_dir is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        args.save_dir = f'./single_image_analysis_{args.mode}_{timestamp}'
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    for path, name in [(args.checkpoint, 'æ£€æŸ¥ç‚¹æ–‡ä»¶'), 
                       (args.val_csv, 'éªŒè¯CSVæ–‡ä»¶'), 
                       (args.data_root, 'æ•°æ®æ ¹ç›®å½•')]:
        if not os.path.exists(path):
            print(f"âŒ {name}ä¸å­˜åœ¨: {path}")
            return
    
    print("ğŸ–¼ï¸ å•å›¾åƒCNNæ¨¡å‹ç‰¹å¾åˆ†æå·¥å…·")
    print("="*50)
    print(f"æ¨¡å¼: {args.mode}")
    print(f"æ£€æŸ¥ç‚¹: {args.checkpoint}")
    print(f"éªŒè¯é›†: {args.val_csv}")
    print(f"æ•°æ®æ ¹ç›®å½•: {args.data_root}")
    print(f"ä¿å­˜ç›®å½•: {args.save_dir}")
    print("="*50)
    
    start_time = time.time()
    
    try:
        if args.mode == 'inspect':
            print("ğŸ”¬ æ£€æŸ¥æ¨¡å‹ç»“æ„...")
            inspect_single_image_model_structure(args.checkpoint)
        
        elif args.mode == 'quick':
            print("âš¡ å¿«é€Ÿåˆ†æ...")
            results = analyze_single_image_model(
                args.checkpoint, args.val_csv, args.data_root, 
                args.save_dir, max_samples=min(100, args.max_samples),
                specific_layers=args.layers
            )
        
        elif args.mode == 'full':
            print("ğŸ–¼ï¸ å®Œæ•´åˆ†æ...")
            results = analyze_single_image_model(
                args.checkpoint, args.val_csv, args.data_root, 
                args.save_dir, args.max_samples, args.layers
            )
        
        elapsed_time = time.time() - start_time
        print(f"\nğŸ‰ åˆ†æå®Œæˆï¼")
        print(f"â±ï¸ æ€»è€—æ—¶: {elapsed_time:.2f} ç§’")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {args.save_dir}")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­åˆ†æ")
    except Exception as e:
        print(f"\nâŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # å¦‚æœæ²¡æœ‰å‘½ä»¤è¡Œå‚æ•°ï¼Œæ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
    if len(sys.argv) == 1:
        print("ğŸ–¼ï¸ å•å›¾åƒCNNæ¨¡å‹ç‰¹å¾åˆ†æå·¥å…·")
        print("="*50)
        print("ä½¿ç”¨æ–¹æ³•:")
        print("python single_image_analysis.py --checkpoint MODEL.pth --val_csv VAL.csv --data_root DATA_DIR")
        print("\nå¯ç”¨æ¨¡å¼:")
        print("  --mode inspect      # æ£€æŸ¥æ¨¡å‹ç»“æ„")
        print("  --mode quick        # å¿«é€Ÿåˆ†æï¼ˆå°‘é‡æ ·æœ¬ï¼‰")
        print("  --mode full         # å®Œæ•´åˆ†æï¼ˆæ¨èï¼‰")
        print("\nåŸºæœ¬ç¤ºä¾‹:")
        print("python single_image_analysis.py \\")
        print("    --checkpoint ./best_single_image_model.pth \\")
        print("    --val_csv ./val.csv \\")
        print("    --data_root ./data \\")
        print("    --mode full")
        print("\né«˜çº§ç¤ºä¾‹:")
        print("python single_image_analysis.py \\")
        print("    --checkpoint ./best_single_image_model.pth \\")
        print("    --val_csv ./val.csv \\")
        print("    --data_root ./data \\")
        print("    --mode full \\")
        print("    --layers visual_encoder spatial_attention classifier \\")
        print("    --max_samples 1000 \\")
        print("    --save_dir ./my_single_image_analysis")
        print("\nğŸ“‹ æ¨èå·¥ä½œæµ:")
        print("1. é¦–å…ˆè¿è¡Œ: --mode inspect    # æŸ¥çœ‹æ¨¡å‹ç»“æ„")
        print("2. ç„¶åè¿è¡Œ: --mode quick      # å¿«é€ŸéªŒè¯")
        print("3. æœ€åè¿è¡Œ: --mode full       # å®Œæ•´åˆ†æ")
        print("\nğŸ’¡ æç¤º:")
        print("- è¿™ä¸ªå·¥å…·ä¸“é—¨é’ˆå¯¹SingleImageClassifieræ¨¡å‹")
        print("- ä¼šè‡ªåŠ¨æå–CNNå„å±‚ã€æ³¨æ„åŠ›å±‚ã€åˆ†ç±»å™¨çš„ç‰¹å¾")
        print("- ç”Ÿæˆt-SNE/PCA/UMAPé™ç»´å¯è§†åŒ–")
        print("- åŒ…å«èšç±»åˆ†æå’Œç‰¹å¾ç»Ÿè®¡")
        sys.exit(0)
    
    main()


# =============================================================================
# ä¾¿æ·å‡½æ•°ï¼Œä¾›å…¶ä»–è„šæœ¬è°ƒç”¨
# =============================================================================

def quick_single_image_analysis(checkpoint_path, val_csv, data_root, save_dir=None, max_samples=100):
    """å¿«é€Ÿå•å›¾åƒåˆ†ææ¥å£"""
    if save_dir is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        save_dir = f'./quick_single_image_analysis_{timestamp}'
    
    return analyze_single_image_model(
        checkpoint_path, val_csv, data_root, save_dir, max_samples
    )


def full_single_image_analysis(checkpoint_path, val_csv, data_root, save_dir=None, max_samples=500, layers=None):
    """å®Œæ•´å•å›¾åƒåˆ†ææ¥å£"""
    if save_dir is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        save_dir = f'./full_single_image_analysis_{timestamp}'
    
    return analyze_single_image_model(
        checkpoint_path, val_csv, data_root, save_dir, max_samples, layers
    )


# =============================================================================
# ä½¿ç”¨ç¤ºä¾‹
# =============================================================================

"""
ä½¿ç”¨ç¤ºä¾‹:

1. å‘½ä»¤è¡Œä½¿ç”¨:
   python single_image_analysis.py \\
       --checkpoint ./best_single_image_model.pth \\
       --val_csv ./val.csv \\
       --data_root ./data \\
       --mode full \\
       --max_samples 500

2. åœ¨Pythonè„šæœ¬ä¸­ä½¿ç”¨:
   from single_image_analysis import quick_single_image_analysis, full_single_image_analysis
   
   # å¿«é€Ÿåˆ†æ
   results = quick_single_image_analysis(
       checkpoint_path='./best_single_image_model.pth',
       val_csv='./val.csv', 
       data_root='./data'
   )
   
   # å®Œæ•´åˆ†æ
   results = full_single_image_analysis(
       checkpoint_path='./best_single_image_model.pth',
       val_csv='./val.csv',
       data_root='./data',
       max_samples=1000,
       layers=['visual_encoder', 'spatial_attention', 'classifier']
   )

3. å¯¹æ¯”å…·èº«æ¨¡å‹å’Œå•å›¾åƒæ¨¡å‹:
   # åˆ†åˆ«è¿è¡Œä¸¤ä¸ªåˆ†æå·¥å…·
   embodied_results = full_analysis('./embodied_model.pth', ...)
   single_results = full_single_image_analysis('./single_model.pth', ...)
   
   # æ‰‹åŠ¨å¯¹æ¯”åˆ†æç»“æœ
"""