"""
å®Œæ•´çš„å…·èº«è®¡æ•°æ¨¡å‹åˆ†æå·¥å…·
æ”¯æŒå¤šç§ç‰¹å¾æå–ã€èšç±»åˆ†æå’Œå¯è§†åŒ–æ–¹æ³•
ä¸“é—¨é’ˆå¯¹EmbodiedCountingModelä¼˜åŒ–
"""

import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import matplotlib
matplotlib.use('Agg')  # è®¾ç½®éäº¤äº’å¼åç«¯ï¼Œé€‚åˆæœåŠ¡å™¨ç¯å¢ƒ
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics import silhouette_score, adjusted_rand_score, confusion_matrix, accuracy_score
import umap
import os
import sys
import json
import argparse
import time
from collections import defaultdict
from datetime import datetime
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# ç¡®ä¿ä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


class FeatureExtractor:
    """ç‰¹å¾æå–å™¨ - é€‚é…EmbodiedCountingModelçš„æ–°æ•°æ®æ ¼å¼"""
    
    def __init__(self, model, device='cuda'):
        self.model = model
        self.device = device
        self.model.eval()
        self.features = {}
        self.hooks = []
        
    def inspect_model_structure(self):
        """æ£€æŸ¥æ¨¡å‹ç»“æ„ï¼Œè¿”å›æ‰€æœ‰å¯ç”¨çš„å±‚åç§°"""
        print("=== æ¨¡å‹ç»“æ„æ£€æŸ¥ ===")
        available_layers = []
        
        def print_module_structure(module, prefix="", max_depth=3, current_depth=0):
            if current_depth > max_depth:
                return
            
            for name, child in module.named_children():
                full_name = f"{prefix}.{name}" if prefix else name
                print(f"{'  ' * current_depth}{full_name}: {child.__class__.__name__}")
                available_layers.append(full_name)
                
                # é€’å½’æ‰“å°å­æ¨¡å—
                if len(list(child.children())) > 0:
                    print_module_structure(child, full_name, max_depth, current_depth + 1)
        
        print_module_structure(self.model)
        print(f"\næ€»å…±å‘ç° {len(available_layers)} ä¸ªå±‚")
        
        return available_layers
    
    def auto_detect_key_layers(self):
        """è‡ªåŠ¨æ£€æµ‹å…³é”®å±‚ - é’ˆå¯¹EmbodiedCountingModelä¼˜åŒ–"""
        available_layers = self.inspect_model_structure()
        
        # ç²¾ç¡®çš„å±‚åç§°åŒ¹é…
        target_layers = [
            'fusion',              # å¤šæ¨¡æ€èåˆå±‚
            'lstm',                # æ—¶åºå¤„ç†å±‚
            'counting_decoder',    # è®¡æ•°è§£ç å±‚
            'visual_encoder',      # è§†è§‰ç¼–ç å±‚
            'embodiment_encoder'   # å…·èº«ç¼–ç å±‚
        ]
        
        detected_layers = []
        
        # ç²¾ç¡®åŒ¹é…
        for target in target_layers:
            if target in available_layers:
                detected_layers.append(target)
                print(f"âœ… æ£€æµ‹åˆ°å…³é”®å±‚: {target}")
        
        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•å­æ¨¡å—åŒ¹é…
        if not detected_layers:
            print("ğŸ” å°è¯•æ¨¡ç³ŠåŒ¹é…...")
            patterns = {
                'fusion': ['fusion', 'multimodal', 'cross_modal'],
                'lstm': ['lstm', 'rnn', 'gru'],
                'counting': ['counting', 'decoder', 'classifier', 'head'],
                'visual': ['visual', 'cnn', 'encoder'],
                'embodiment': ['embodiment', 'joint', 'pose']
            }
            
            for category, pattern_list in patterns.items():
                for layer_name in available_layers:
                    for pattern in pattern_list:
                        if pattern.lower() in layer_name.lower():
                            detected_layers.append(layer_name)
                            print(f"ğŸ” æ¨¡ç³ŠåŒ¹é…åˆ°: {layer_name} (ç±»åˆ«: {category})")
                            break
                    if layer_name in detected_layers:
                        break
        
        print(f"\nå»ºè®®æå–çš„å±‚: {detected_layers}")
        return detected_layers
        
    def register_hooks(self, layers_to_extract):
        """æ³¨å†Œé’©å­å‡½æ•°æ¥æå–ä¸­é—´å±‚ç‰¹å¾"""
        def get_activation(name):
            def hook(model, input, output):
                if isinstance(output, tuple):
                    self.features[name] = output[0].detach().cpu()
                else:
                    self.features[name] = output.detach().cpu()
            return hook
        
        successful_hooks = []
        failed_hooks = []
        
        # æ³¨å†Œé’©å­
        for layer_name in layers_to_extract:
            try:
                # é€šè¿‡ç‚¹å·åˆ†å‰²çš„è·¯å¾„è®¿é—®åµŒå¥—æ¨¡å—
                module = self.model
                for part in layer_name.split('.'):
                    module = getattr(module, part)
                
                handle = module.register_forward_hook(get_activation(layer_name))
                self.hooks.append(handle)
                successful_hooks.append(layer_name)
                print(f"âœ… æˆåŠŸæ³¨å†Œé’©å­: {layer_name}")
                
            except AttributeError as e:
                failed_hooks.append(layer_name)
                print(f"âŒ æ³¨å†Œé’©å­å¤±è´¥: {layer_name} - {e}")
        
        print(f"\né’©å­æ³¨å†Œç»“æœ: æˆåŠŸ {len(successful_hooks)}, å¤±è´¥ {len(failed_hooks)}")
        
        if not successful_hooks and failed_hooks:
            print("âš ï¸ æ²¡æœ‰æˆåŠŸæ³¨å†Œä»»ä½•é’©å­ï¼Œå°è¯•è‡ªåŠ¨æ£€æµ‹...")
            auto_layers = self.auto_detect_key_layers()
            if auto_layers:
                print(f"ä½¿ç”¨è‡ªåŠ¨æ£€æµ‹çš„å±‚: {auto_layers}")
                return self.register_hooks(auto_layers[:3])  # åªå–å‰3ä¸ªé¿å…å¤ªå¤š
        
        return successful_hooks
    
    def remove_hooks(self):
        """ç§»é™¤æ‰€æœ‰é’©å­"""
        for handle in self.hooks:
            handle.remove()
        self.hooks = []
        
    def _process_feature_tensor(self, feature_tensor):
        """å¤„ç†ä¸åŒå½¢çŠ¶çš„ç‰¹å¾å¼ é‡"""
        if len(feature_tensor.shape) == 3:  # [batch, seq, dim]
            # å–æœ€åä¸€ä¸ªæ—¶åˆ»çš„ç‰¹å¾
            return feature_tensor[:, -1, :].cpu().numpy()
        elif len(feature_tensor.shape) == 4:  # [batch, seq, h, w] or [batch, channel, h, w]
            # å…¨å±€å¹³å‡æ± åŒ–
            pooled = feature_tensor.mean(dim=(-2, -1))
            if len(pooled.shape) == 3:  # å¦‚æœè¿˜æœ‰seqç»´åº¦
                pooled = pooled[:, -1, :]
            return pooled.cpu().numpy()
        elif len(feature_tensor.shape) == 2:  # [batch, dim]
            return feature_tensor.cpu().numpy()
        else:
            # å…¶ä»–æƒ…å†µï¼Œå±•å¹³
            return feature_tensor.view(feature_tensor.shape[0], -1).cpu().numpy()
    
    def extract_features(self, data_loader, max_samples=1000):
        """æå–ç‰¹å¾å¹¶æ”¶é›†é¢„æµ‹ç»“æœ - é€‚é…æ–°çš„æ•°æ®æ ¼å¼"""
        all_features = defaultdict(list)
        all_predictions = []
        all_labels = []
        all_sample_ids = []
        all_attention_weights = []
        
        sample_count = 0
        
        with torch.no_grad():
            for batch in tqdm(data_loader, desc="æå–ç‰¹å¾"):
                if sample_count >= max_samples:
                    break
                
                # é€‚é…æ–°çš„æ•°æ®æ ¼å¼ - ä»batch['sequence_data']ä¸­æå–
                sequence_data = {
                    'images': batch['sequence_data']['images'].to(self.device),
                    'joints': batch['sequence_data']['joints'].to(self.device),
                    'timestamps': batch['sequence_data']['timestamps'].to(self.device),
                    'labels': batch['sequence_data']['labels'].to(self.device)
                }
                
                # è·å–æ‰¹æ¬¡ä¿¡æ¯
                labels = batch['label'].cpu().numpy()  # CSVä¸­çš„æ ‡ç­¾
                sample_ids = batch['sample_id']
                
                # è®¡ç®—å®é™…å¤„ç†çš„æ ·æœ¬æ•°
                remaining_samples = max_samples - sample_count
                actual_batch_size = min(len(labels), remaining_samples)
                
                # æˆªæ–­æ‰¹æ¬¡ï¼ˆå¦‚æœéœ€è¦ï¼‰
                if actual_batch_size < len(labels):
                    for key in sequence_data:
                        sequence_data[key] = sequence_data[key][:actual_batch_size]
                    labels = labels[:actual_batch_size]
                    sample_ids = sample_ids[:actual_batch_size]
                
                # æ¸…ç©ºç‰¹å¾å­—å…¸
                self.features = {}
                
                # å‰å‘ä¼ æ’­ - ä½¿ç”¨æ–°çš„æ•°æ®æ ¼å¼
                outputs = self.model(
                    sequence_data=sequence_data,
                    use_teacher_forcing=False,
                    return_attention=True  # è¿”å›æ³¨æ„åŠ›æƒé‡
                )
                
                # æ”¶é›†é¢„æµ‹ç»“æœ
                count_logits = outputs['counts']  # [batch, seq_len, 11]
                pred_labels = torch.argmax(count_logits, dim=-1)  # [batch, seq_len]
                final_pred = pred_labels[:, -1].cpu().numpy()  # æœ€ç»ˆæ—¶åˆ»çš„é¢„æµ‹
                
                all_predictions.extend(final_pred)
                all_labels.extend(labels)
                all_sample_ids.extend(sample_ids)
                
                # æ”¶é›†æ³¨æ„åŠ›æƒé‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if 'attention_weights' in outputs:
                    attention_weights = outputs['attention_weights']  # [batch, seq_len, spatial_size]
                    # å–æœ€åä¸€ä¸ªæ—¶åˆ»çš„æ³¨æ„åŠ›æƒé‡
                    final_attention = attention_weights[:, -1, :].cpu().numpy()
                    all_attention_weights.extend(final_attention)
                
                # æ”¶é›†ä¸­é—´å±‚ç‰¹å¾
                for feature_name, feature_tensor in self.features.items():
                    processed_features = self._process_feature_tensor(feature_tensor)
                    all_features[feature_name].append(processed_features)
                
                sample_count += actual_batch_size
                
                if sample_count >= max_samples:
                    break
        
        # åˆå¹¶æ‰€æœ‰ç‰¹å¾
        final_features = {}
        for feature_name, feature_list in all_features.items():
            if feature_list:
                final_features[feature_name] = np.vstack(feature_list)
        
        result = {
            'features': final_features,
            'predictions': np.array(all_predictions),
            'labels': np.array(all_labels),
            'sample_ids': all_sample_ids
        }
        
        # æ·»åŠ æ³¨æ„åŠ›æƒé‡ï¼ˆå¦‚æœæœ‰ï¼‰
        if all_attention_weights:
            result['attention_weights'] = np.array(all_attention_weights)
        
        # æ‰“å°è°ƒè¯•ä¿¡æ¯
        print(f"ç‰¹å¾æå–å®Œæˆ:")
        print(f"  å®é™…æ ·æœ¬æ•°: {len(result['labels'])}")
        print(f"  çœŸå®æ ‡ç­¾èŒƒå›´: {result['labels'].min()} - {result['labels'].max()}")
        print(f"  é¢„æµ‹æ ‡ç­¾èŒƒå›´: {result['predictions'].min()} - {result['predictions'].max()}")
        print(f"  çœŸå®æ ‡ç­¾å”¯ä¸€å€¼: {sorted(np.unique(result['labels']))}")
        print(f"  é¢„æµ‹æ ‡ç­¾å”¯ä¸€å€¼: {sorted(np.unique(result['predictions']))}")
        print(f"  æå–çš„ç‰¹å¾å±‚: {list(final_features.keys())}")
        
        return result


class ClusterAnalyzer:
    """èšç±»åˆ†æå™¨"""
    
    def __init__(self):
        self.results = {}
    
    def perform_clustering(self, features, methods=['kmeans', 'dbscan'], n_clusters=None, true_labels=None):
        """æ‰§è¡Œå¤šç§èšç±»ç®—æ³•"""
        results = {}
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šèšç±»æ•°ï¼Œä»çœŸå®æ ‡ç­¾æ¨æ–­
        if n_clusters is None and true_labels is not None:
            n_clusters = len(np.unique(true_labels))
        elif n_clusters is None:
            n_clusters = min(10, int(np.sqrt(len(features))))  # é»˜è®¤å¯å‘å¼
        
        for method in methods:
            if method == 'kmeans':
                clusterer = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
                cluster_labels = clusterer.fit_predict(features)
                if len(np.unique(cluster_labels)) > 1:
                    silhouette = silhouette_score(features, cluster_labels)
                else:
                    silhouette = -1
                results[method] = {
                    'labels': cluster_labels,
                    'silhouette_score': silhouette,
                    'n_clusters': n_clusters
                }
                
            elif method == 'dbscan':
                clusterer = DBSCAN(eps=0.5, min_samples=5)
                cluster_labels = clusterer.fit_predict(features)
                n_clusters_found = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)
                
                if n_clusters_found > 1:
                    # åªå¯¹éå™ªå£°ç‚¹è®¡ç®—è½®å»“ç³»æ•°
                    mask = cluster_labels != -1
                    if np.sum(mask) > 1:
                        silhouette = silhouette_score(features[mask], cluster_labels[mask])
                    else:
                        silhouette = -1
                else:
                    silhouette = -1
                    
                results[method] = {
                    'labels': cluster_labels,
                    'silhouette_score': silhouette,
                    'n_clusters': n_clusters_found
                }
        
        return results
    
    def evaluate_clustering(self, cluster_labels, true_labels):
        """è¯„ä¼°èšç±»è´¨é‡"""
        try:
            # è®¡ç®—è°ƒæ•´å…°å¾·æŒ‡æ•°
            ari = adjusted_rand_score(true_labels, cluster_labels)
            
            # è®¡ç®—èšç±»çº¯åº¦
            def purity_score(y_true, y_pred):
                # å¤„ç†è´Ÿæ ‡ç­¾ï¼ˆDBSCANçš„å™ªå£°ç‚¹ï¼‰
                y_pred_clean = y_pred.copy()
                noise_mask = y_pred_clean == -1
                if np.any(noise_mask):
                    # å°†å™ªå£°ç‚¹åˆ†é…ç»™è‡ªå·±çš„ç±»åˆ«
                    max_cluster = np.max(y_pred_clean[~noise_mask]) if np.any(~noise_mask) else 0
                    y_pred_clean[noise_mask] = np.arange(max_cluster + 1, max_cluster + 1 + np.sum(noise_mask))
                
                unique_true = np.unique(y_true)
                unique_pred = np.unique(y_pred_clean)
                
                contingency_matrix = np.zeros((len(unique_true), len(unique_pred)))
                
                for i, true_label in enumerate(unique_true):
                    for j, pred_label in enumerate(unique_pred):
                        contingency_matrix[i, j] = np.sum((y_true == true_label) & (y_pred_clean == pred_label))
                
                return np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix)
            
            purity = purity_score(true_labels, cluster_labels)
            
            return {'ari': ari, 'purity': purity}
            
        except Exception as e:
            print(f"èšç±»è¯„ä¼°å¤±è´¥: {e}")
            return {'ari': -1, 'purity': -1}


class EnhancedVisualizationEngine:
    """å¢å¼ºçš„å¯è§†åŒ–å¼•æ“"""
    
    def __init__(self, figsize=(15, 10)):
        self.figsize = figsize
        
    def reduce_dimensions(self, features, method='tsne', n_components=2):
        """é™ç»´"""
        if method == 'tsne':
            reducer = TSNE(n_components=n_components, random_state=42, perplexity=min(30, len(features)//4))
        elif method == 'pca':
            reducer = PCA(n_components=n_components)
        elif method == 'umap':
            reducer = umap.UMAP(n_components=n_components, random_state=42)
        else:
            raise ValueError(f"Unsupported method: {method}")
        
        reduced_features = reducer.fit_transform(features)
        return reduced_features
    
    def plot_scatter(self, features_2d, labels, title, save_path=None, alpha=0.7):
        """ç»˜åˆ¶æ•£ç‚¹å›¾"""
        plt.figure(figsize=self.figsize)
        
        unique_labels = np.unique(labels)
        colors = plt.cm.tab10(np.linspace(0, 1, len(unique_labels)))
        
        for i, label in enumerate(unique_labels):
            mask = labels == label
            plt.scatter(features_2d[mask, 0], features_2d[mask, 1], 
                       c=[colors[i]], label=f'Class {label}', alpha=alpha, s=50)
        
        plt.title(title, fontsize=16)
        plt.xlabel('Dimension 1', fontsize=12)
        plt.ylabel('Dimension 2', fontsize=12)
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        
        if save_path:
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
    
    def plot_clustering_comparison(self, features_2d, true_labels, cluster_results, save_dir=None):
        """æ¯”è¾ƒä¸åŒèšç±»æ–¹æ³•çš„ç»“æœ"""
        n_methods = len(cluster_results) + 1  # +1 for true labels
        fig, axes = plt.subplots(1, n_methods, figsize=(5*n_methods, 5))
        
        if n_methods == 1:
            axes = [axes]
        
        # ç»˜åˆ¶çœŸå®æ ‡ç­¾
        unique_labels = np.unique(true_labels)
        colors = plt.cm.tab10(np.linspace(0, 1, len(unique_labels)))
        
        for i, label in enumerate(unique_labels):
            mask = true_labels == label
            axes[0].scatter(features_2d[mask, 0], features_2d[mask, 1], 
                          c=[colors[i]], label=f'Class {label}', alpha=0.7, s=30)
        axes[0].set_title('True Labels')
        axes[0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        axes[0].grid(True, alpha=0.3)
        
        # ç»˜åˆ¶èšç±»ç»“æœ
        for idx, (method, result) in enumerate(cluster_results.items()):
            cluster_labels = result['labels']
            unique_clusters = np.unique(cluster_labels)
            colors = plt.cm.Set3(np.linspace(0, 1, len(unique_clusters)))
            
            for i, cluster in enumerate(unique_clusters):
                mask = cluster_labels == cluster
                if cluster == -1:  # DBSCANçš„å™ªå£°ç‚¹
                    axes[idx+1].scatter(features_2d[mask, 0], features_2d[mask, 1], 
                                      c='black', label='Noise', alpha=0.5, s=30, marker='x')
                else:
                    axes[idx+1].scatter(features_2d[mask, 0], features_2d[mask, 1], 
                                      c=[colors[i]], label=f'Cluster {cluster}', alpha=0.7, s=30)
            
            title = f'{method.upper()}\nSilhouette: {result["silhouette_score"]:.3f}'
            axes[idx+1].set_title(title)
            axes[idx+1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_dir:
            os.makedirs(save_dir, exist_ok=True)
            save_path = os.path.join(save_dir, 'clustering_comparison.png')
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
    
    def plot_confusion_heatmap(self, true_labels, pred_labels, save_path=None):
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µçƒ­åŠ›å›¾"""
        # åŠ¨æ€ç¡®å®šæ ‡ç­¾èŒƒå›´
        all_labels = np.concatenate([true_labels, pred_labels])
        unique_labels = sorted(np.unique(all_labels))
        
        cm = confusion_matrix(true_labels, pred_labels, labels=unique_labels)
        
        plt.figure(figsize=(10, 8))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=unique_labels, yticklabels=unique_labels)
        plt.title('Prediction Confusion Matrix')
        plt.xlabel('Predicted Count')
        plt.ylabel('True Count')
        
        if save_path:
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
    
    def plot_error_analysis(self, features_2d, true_labels, pred_labels, save_path=None):
        """é”™è¯¯åˆ†æå¯è§†åŒ–"""
        plt.figure(figsize=self.figsize)
        
        # æ­£ç¡®å’Œé”™è¯¯é¢„æµ‹çš„mask
        correct_mask = true_labels == pred_labels
        error_mask = ~correct_mask
        
        # ç»˜åˆ¶æ­£ç¡®é¢„æµ‹ï¼ˆç°è‰²èƒŒæ™¯ï¼‰
        plt.scatter(features_2d[correct_mask, 0], features_2d[correct_mask, 1], 
                   c='lightgray', alpha=0.3, s=30, label='Correct')
        
        # ç»˜åˆ¶é”™è¯¯é¢„æµ‹ï¼ŒæŒ‰çœŸå®æ ‡ç­¾ç€è‰²
        if np.any(error_mask):
            unique_labels = np.unique(true_labels[error_mask])
            colors = plt.cm.Reds(np.linspace(0.3, 1, len(unique_labels)))
            
            for i, label in enumerate(unique_labels):
                mask = error_mask & (true_labels == label)
                plt.scatter(features_2d[mask, 0], features_2d[mask, 1], 
                           c=[colors[i]], label=f'Error Class {label}', alpha=0.8, s=60, 
                           edgecolors='black', linewidth=0.5)
        
        plt.title('Error Analysis: Misclassified Samples', fontsize=16)
        plt.xlabel('Dimension 1', fontsize=12)
        plt.ylabel('Dimension 2', fontsize=12)
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        
        if save_path:
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
    
    def plot_attention_heatmap(self, attention_weights, save_path=None):
        """å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡çƒ­åŠ›å›¾"""
        if attention_weights is None or len(attention_weights) == 0:
            return
        
        plt.figure(figsize=(15, 10))
        
        # å¦‚æœæ˜¯3Dæ•°æ® [samples, seq_len, spatial_size]ï¼Œå–å¹³å‡
        if len(attention_weights.shape) == 3:
            avg_attention = np.mean(attention_weights, axis=0)  # [seq_len, spatial_size]
        else:
            avg_attention = attention_weights
        
        # é‡æ–°æ•´å½¢ä¸º2Dç½‘æ ¼ï¼ˆå‡è®¾spatial_sizeæ˜¯æ­£æ–¹å½¢ï¼‰
        spatial_size = avg_attention.shape[-1]
        grid_size = int(np.sqrt(spatial_size))
        
        if grid_size * grid_size == spatial_size:
            # å¯ä»¥é‡æ–°æ•´å½¢ä¸ºæ­£æ–¹å½¢ç½‘æ ¼
            if len(avg_attention.shape) == 2:
                # å¯¹æ¯ä¸ªæ—¶åˆ»ç»˜åˆ¶æ³¨æ„åŠ›å›¾
                seq_len = avg_attention.shape[0]
                cols = min(4, seq_len)
                rows = (seq_len + cols - 1) // cols
                
                for t in range(min(seq_len, 12)):  # æœ€å¤šæ˜¾ç¤º12ä¸ªæ—¶åˆ»
                    plt.subplot(rows, cols, t+1)
                    attention_map = avg_attention[t].reshape(grid_size, grid_size)
                    sns.heatmap(attention_map, cmap='viridis', cbar=True, square=True)
                    plt.title(f'Attention at t={t}')
                    plt.axis('off')
            else:
                # å•ä¸ªæ³¨æ„åŠ›å›¾
                attention_map = avg_attention.reshape(grid_size, grid_size)
                sns.heatmap(attention_map, cmap='viridis', cbar=True, square=True)
                plt.title('Average Attention Map')
                plt.axis('off')
        else:
            # ä¸èƒ½é‡æ–°æ•´å½¢ï¼Œç»˜åˆ¶1Dæ¡å½¢å›¾
            if len(avg_attention.shape) == 1:
                plt.bar(range(len(avg_attention)), avg_attention)
                plt.title('Attention Weights')
                plt.xlabel('Spatial Location')
                plt.ylabel('Attention Weight')
            else:
                # å¤šä¸ªåºåˆ—ï¼Œç»˜åˆ¶çƒ­åŠ›å›¾
                sns.heatmap(avg_attention, cmap='viridis', cbar=True)
                plt.title('Attention Weights Heatmap')
                plt.xlabel('Spatial Location')
                plt.ylabel('Time Step')
        
        plt.tight_layout()
        
        if save_path:
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
    
    def plot_embodiment_analysis(self, visual_features, embodiment_features, 
                                fusion_features, labels, save_path=None):
        """å…·èº«å­¦ä¹ ç‰¹å®šçš„åˆ†æå¯è§†åŒ–"""
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        unique_labels = np.unique(labels)
        colors = plt.cm.tab10(np.linspace(0, 1, len(unique_labels)))
        
        # 1. è§†è§‰ç‰¹å¾åˆ†å¸ƒ
        if visual_features is not None:
            visual_2d = self.reduce_dimensions(visual_features, 'tsne')
            
            for i, label in enumerate(unique_labels):
                mask = labels == label
                axes[0, 0].scatter(visual_2d[mask, 0], visual_2d[mask, 1], 
                                 c=[colors[i]], label=f'Class {label}', alpha=0.7)
            axes[0, 0].set_title('Visual Features')
            axes[0, 0].legend()
            axes[0, 0].grid(True, alpha=0.3)
        
        # 2. å…·èº«ç‰¹å¾åˆ†å¸ƒ
        if embodiment_features is not None:
            embodiment_2d = self.reduce_dimensions(embodiment_features, 'tsne')
            for i, label in enumerate(unique_labels):
                mask = labels == label
                axes[0, 1].scatter(embodiment_2d[mask, 0], embodiment_2d[mask, 1], 
                                 c=[colors[i]], label=f'Class {label}', alpha=0.7)
            axes[0, 1].set_title('Embodiment Features')
            axes[0, 1].legend()
            axes[0, 1].grid(True, alpha=0.3)
        
        # 3. èåˆç‰¹å¾åˆ†å¸ƒ
        if fusion_features is not None:
            fusion_2d = self.reduce_dimensions(fusion_features, 'tsne')
            for i, label in enumerate(unique_labels):
                mask = labels == label
                axes[0, 2].scatter(fusion_2d[mask, 0], fusion_2d[mask, 1], 
                                 c=[colors[i]], label=f'Class {label}', alpha=0.7)
            axes[0, 2].set_title('Fusion Features')
            axes[0, 2].legend()
            axes[0, 2].grid(True, alpha=0.3)
        
        # 4. ç‰¹å¾ç›¸å…³æ€§åˆ†æ
        if visual_features is not None and embodiment_features is not None:
            visual_mean = np.mean(visual_features, axis=1)
            embodiment_mean = np.mean(embodiment_features, axis=1)
            
            scatter = axes[1, 0].scatter(visual_mean, embodiment_mean, c=labels, 
                             cmap='tab10', alpha=0.7)
            axes[1, 0].set_xlabel('Visual Feature Mean')
            axes[1, 0].set_ylabel('Embodiment Feature Mean')
            axes[1, 0].set_title('Visual vs Embodiment Correlation')
            axes[1, 0].grid(True, alpha=0.3)
            plt.colorbar(scatter, ax=axes[1, 0])
        
        # 5. æ¨¡æ€è´¡çŒ®åˆ†æ
        if all(f is not None for f in [visual_features, embodiment_features, fusion_features]):
            visual_contribution = np.corrcoef(
                np.mean(visual_features, axis=1), 
                np.mean(fusion_features, axis=1)
            )[0, 1]
            embodiment_contribution = np.corrcoef(
                np.mean(embodiment_features, axis=1), 
                np.mean(fusion_features, axis=1)
            )[0, 1]
            
            contributions = [visual_contribution, embodiment_contribution]
            modal_names = ['Visual', 'Embodiment']
            
            axes[1, 1].bar(modal_names, contributions, color=['skyblue', 'lightcoral'])
            axes[1, 1].set_title('Modal Contribution to Fusion')
            axes[1, 1].set_ylabel('Correlation with Fusion')
            axes[1, 1].grid(True, alpha=0.3)
        
        # 6. å†³ç­–ç½®ä¿¡åº¦åˆ†æ
        if fusion_features is not None:
            uncertainty = np.std(fusion_features, axis=1)
            
            for i, label in enumerate(unique_labels):
                mask = labels == label
                axes[1, 2].scatter(np.arange(np.sum(mask)), uncertainty[mask], 
                                 c=[colors[i]], label=f'Class {label}', alpha=0.7)
            
            axes[1, 2].set_title('Decision Uncertainty by Class')
            axes[1, 2].set_xlabel('Sample Index')
            axes[1, 2].set_ylabel('Feature Uncertainty (std)')
            axes[1, 2].legend()
            axes[1, 2].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()


class ModelAnalyzer:
    """ä¸»åˆ†æå™¨ - æ•´åˆæ‰€æœ‰åˆ†æåŠŸèƒ½"""
    
    def __init__(self, model, device='cuda'):
        self.model = model
        self.device = device
        self.feature_extractor = FeatureExtractor(model, device)
        self.cluster_analyzer = ClusterAnalyzer()
        self.visualizer = EnhancedVisualizationEngine()


def load_model_and_data(checkpoint_path, val_csv, data_root, batch_size=8):
    """åŠ è½½æ¨¡å‹å’Œæ•°æ®"""
    print("ğŸ“¥ åŠ è½½æ¨¡å‹å’Œæ•°æ®...")
    
    # åŠ è½½æ£€æŸ¥ç‚¹
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    config = checkpoint['config']
    
    # å¯¼å…¥æ¨¡å‹ç±»
    try:
        from Model_embodiment import EmbodiedCountingModel
    except ImportError:
        print("âŒ æ— æ³•å¯¼å…¥EmbodiedCountingModelï¼Œè¯·ç¡®ä¿Model_embodiment.pyåœ¨Pythonè·¯å¾„ä¸­")
        raise
    
    # ç¡®å®šå›¾åƒæ¨¡å¼
    image_mode = config.get('image_mode', 'rgb')
    input_channels = 3 if image_mode == 'rgb' else 1
    
    # é‡å»ºæ¨¡å‹
    model_config = config['model_config'].copy()
    model_config['input_channels'] = input_channels
    model = EmbodiedCountingModel(**model_config)
    model.load_state_dict(checkpoint['model_state_dict'])
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    model.eval()
    
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ (å›¾åƒæ¨¡å¼: {image_mode}, è®¾å¤‡: {device})")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    try:
        from DataLoader_embodiment import get_ball_counting_data_loaders
    except ImportError:
        print("âŒ æ— æ³•å¯¼å…¥get_ball_counting_data_loadersï¼Œè¯·ç¡®ä¿DataLoader_embodiment.pyåœ¨Pythonè·¯å¾„ä¸­")
        raise
    
    _, val_loader, _ = get_ball_counting_data_loaders(
        train_csv_path=config['train_csv'],
        val_csv_path=val_csv,
        data_root=data_root,
        batch_size=batch_size,
        sequence_length=config['sequence_length'],
        normalize=config['normalize'],
        num_workers=2,
        image_mode=image_mode
    )
    
    print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆï¼ŒéªŒè¯é›†å¤§å°: {len(val_loader.dataset)}")
    
    return model, val_loader, device, config


def debug_labels_and_model(checkpoint_path, val_csv, data_root):
    """è°ƒè¯•æ ‡ç­¾èŒƒå›´å’Œæ¨¡å‹è¾“å‡º"""
    print("=== è°ƒè¯•æ ‡ç­¾å’Œæ¨¡å‹è¾“å‡º ===")
    
    try:
        model, val_loader, device, config = load_model_and_data(
            checkpoint_path, val_csv, data_root, batch_size=4
        )
        
        print(f"æ¨¡å‹é…ç½®: {config['model_config']}")
        print(f"æ•°æ®é›†å¤§å°: {len(val_loader.dataset)}")
        
        # æ£€æŸ¥å‡ ä¸ªæ‰¹æ¬¡çš„æ•°æ®
        with torch.no_grad():
            for i, batch in enumerate(val_loader):
                if i >= 2:  # åªæ£€æŸ¥å‰ä¸¤ä¸ªæ‰¹æ¬¡
                    break
                
                print(f"\n--- æ‰¹æ¬¡ {i+1} ---")
                
                # è¾“å…¥æ•°æ®
                sequence_data = {
                    'images': batch['sequence_data']['images'].to(device),
                    'joints': batch['sequence_data']['joints'].to(device),
                    'timestamps': batch['sequence_data']['timestamps'].to(device),
                    'labels': batch['sequence_data']['labels'].to(device)
                }
                labels = batch['label'].cpu().numpy()
                
                print(f"æ‰¹æ¬¡å¤§å°: {len(labels)}")
                print(f"å›¾åƒå½¢çŠ¶: {sequence_data['images'].shape}")
                print(f"å…³èŠ‚å½¢çŠ¶: {sequence_data['joints'].shape}")
                print(f"åºåˆ—æ ‡ç­¾å½¢çŠ¶: {sequence_data['labels'].shape}")
                print(f"CSVæ ‡ç­¾: {labels}")
                print(f"CSVæ ‡ç­¾èŒƒå›´: {labels.min()} - {labels.max()}")
                
                # æ¨¡å‹å‰å‘ä¼ æ’­
                outputs = model(
                    sequence_data=sequence_data,
                    use_teacher_forcing=False
                )
                
                count_logits = outputs['counts']  # [batch, seq_len, num_classes]
                print(f"æ¨¡å‹è¾“å‡ºlogitså½¢çŠ¶: {count_logits.shape}")
                
                pred_labels = torch.argmax(count_logits, dim=-1)  # [batch, seq_len]
                final_pred = pred_labels[:, -1].cpu().numpy()  # æœ€ç»ˆæ—¶åˆ»çš„é¢„æµ‹
                
                print(f"é¢„æµ‹åºåˆ—å½¢çŠ¶: {pred_labels.shape}")
                print(f"æœ€ç»ˆé¢„æµ‹: {final_pred}")
                print(f"æœ€ç»ˆé¢„æµ‹èŒƒå›´: {final_pred.min()} - {final_pred.max()}")
                
                # å¯¹æ¯”æœ€åå‡ ä¸ªæ—¶åˆ»çš„ç›®æ ‡å’Œé¢„æµ‹
                print("æœ€å5ä¸ªæ—¶åˆ»çš„åºåˆ—æ ‡ç­¾:")
                for j in range(len(labels)):
                    print(f"  æ ·æœ¬{j}: {sequence_data['labels'][j, -5:].cpu().numpy()}")
                
                print("æœ€å5ä¸ªæ—¶åˆ»çš„é¢„æµ‹è®¡æ•°:")
                for j in range(len(labels)):
                    print(f"  æ ·æœ¬{j}: {pred_labels[j, -5:].cpu().numpy()}")
        
        return True
        
    except Exception as e:
        print(f"è°ƒè¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def inspect_model_structure(checkpoint_path):
    """æ£€æŸ¥æ¨¡å‹ç»“æ„"""
    print("ğŸ”¬ æ£€æŸ¥æ¨¡å‹ç»“æ„...")
    
    try:
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        config = checkpoint['config']
        
        from Model_embodiment import EmbodiedCountingModel
        
        # ç¡®å®šå›¾åƒæ¨¡å¼
        image_mode = config.get('image_mode', 'rgb')
        input_channels = 3 if image_mode == 'rgb' else 1
        
        model_config = config['model_config'].copy()
        model_config['input_channels'] = input_channels
        model = EmbodiedCountingModel(**model_config)
        
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model = model.to(device)
        
        feature_extractor = FeatureExtractor(model, device)
        available_layers = feature_extractor.inspect_model_structure()
        auto_layers = feature_extractor.auto_detect_key_layers()
        
        print(f"\nâœ¨ è‡ªåŠ¨æ£€æµ‹çš„å…³é”®å±‚: {auto_layers}")
        print(f"\nğŸ“‹ æ¨èçš„åˆ†æç­–ç•¥:")
        print("  --mode quick --max_samples 100              # æœ€å¿«åˆ†æ")
        print("  --mode enhanced --max_samples 500           # æ¨èçš„å®Œæ•´åˆ†æ")
        print("  --layers fusion lstm --max_samples 300      # è‡ªå®šä¹‰å±‚åˆ†æ")
        
        return available_layers, auto_layers
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹ç»“æ„æ£€æŸ¥å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None


def analyze_embodied_counting_model_enhanced(checkpoint_path, val_csv, data_root, 
                                           save_dir='./enhanced_analysis', 
                                           max_samples=500, specific_layers=None):
    """å¢å¼ºç‰ˆå…·èº«è®¡æ•°æ¨¡å‹åˆ†æ"""
    
    print("ğŸ¤– å¼€å§‹å¢å¼ºç‰ˆå…·èº«è®¡æ•°æ¨¡å‹åˆ†æ...")
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(save_dir, exist_ok=True)
    
    try:
        # 1. åŠ è½½æ¨¡å‹å’Œæ•°æ®
        model, val_loader, device, config = load_model_and_data(
            checkpoint_path, val_csv, data_root
        )
        
        # 2. åˆ›å»ºåˆ†æå™¨
        analyzer = ModelAnalyzer(model, device)
        
        # 3. ç¡®å®šè¦åˆ†æçš„å±‚
        if specific_layers is None:
            print("ğŸ” è‡ªåŠ¨æ£€æµ‹å…³é”®å±‚...")
            key_layers = analyzer.feature_extractor.auto_detect_key_layers()
            if not key_layers:
                key_layers = ['fusion', 'lstm', 'counting_decoder', 'visual_encoder', 'embodiment_encoder']
        else:
            key_layers = specific_layers
        
        print(f"ğŸ“‹ å‡†å¤‡åˆ†æçš„å±‚: {key_layers}")
        
        # 4. æ³¨å†Œé’©å­å¹¶æå–ç‰¹å¾
        successful_layers = analyzer.feature_extractor.register_hooks(key_layers)
        
        if not successful_layers:
            print("âŒ æ²¡æœ‰æˆåŠŸæ³¨å†Œä»»ä½•é’©å­ï¼")
            return None
        
        try:
            # 5. æå–ç‰¹å¾
            print("ğŸ¯ æå–å¤šå±‚ç‰¹å¾...")
            data = analyzer.feature_extractor.extract_features(val_loader, max_samples)
            
            features = data['features']
            predictions = data['predictions']
            true_labels = data['labels']
            attention_weights = data.get('attention_weights', None)
            
            print(f"âœ… ç‰¹å¾æå–å®Œæˆ:")
            print(f"   æ ·æœ¬æ•°: {len(true_labels)}")
            print(f"   æå–å±‚: {list(features.keys())}")
            print(f"   æ³¨æ„åŠ›æƒé‡: {'æœ‰' if attention_weights is not None else 'æ— '}")
            
            # 6. å…·èº«å­¦ä¹ ç‰¹å®šåˆ†æ
            print("ğŸ§  æ‰§è¡Œå…·èº«å­¦ä¹ ç‰¹å®šåˆ†æ...")
            
            # æå–ä¸åŒæ¨¡æ€çš„ç‰¹å¾
            visual_features = features.get('visual_encoder', None)
            embodiment_features = features.get('embodiment_encoder', None)
            fusion_features = features.get('fusion', None)
            lstm_features = features.get('lstm', None)
            
            # å…·èº«å­¦ä¹ åˆ†æå¯è§†åŒ–
            if any(f is not None for f in [visual_features, embodiment_features, fusion_features]):
                analyzer.visualizer.plot_embodiment_analysis(
                    visual_features, embodiment_features, fusion_features, true_labels,
                    save_path=os.path.join(save_dir, 'embodiment_analysis.png')
                )
                print("âœ… å…·èº«å­¦ä¹ åˆ†æå®Œæˆ")
            
            # 7. æ³¨æ„åŠ›æœºåˆ¶å¯è§†åŒ–
            if attention_weights is not None:
                print("ğŸ‘ï¸ å¯è§†åŒ–æ³¨æ„åŠ›æœºåˆ¶...")
                analyzer.visualizer.plot_attention_heatmap(
                    attention_weights,
                    save_path=os.path.join(save_dir, 'attention_heatmap.png')
                )
                print("âœ… æ³¨æ„åŠ›å¯è§†åŒ–å®Œæˆ")
            
            # 8. ä¼ ç»Ÿèšç±»å’Œé™ç»´åˆ†æ
            print("ğŸ“Š æ‰§è¡Œä¼ ç»Ÿèšç±»åˆ†æ...")
            
            analysis_results = {}
            
            for layer_name, layer_features in features.items():
                if layer_features is None:
                    continue
                    
                print(f"   åˆ†æ {layer_name} å±‚...")
                layer_results = {}
                
                # å¤šç§é™ç»´æ–¹æ³•
                for dim_method in ['tsne', 'pca', 'umap']:
                    try:
                        # é™ç»´
                        features_2d = analyzer.visualizer.reduce_dimensions(layer_features, dim_method)
                        
                        # å¯è§†åŒ–çœŸå®æ ‡ç­¾
                        analyzer.visualizer.plot_scatter(
                            features_2d, true_labels,
                            f'{layer_name} - True Labels ({dim_method.upper()})',
                            save_path=os.path.join(save_dir, f'{layer_name}_{dim_method}_true.png')
                        )
                        
                        # å¯è§†åŒ–é¢„æµ‹ç»“æœ
                        analyzer.visualizer.plot_scatter(
                            features_2d, predictions,
                            f'{layer_name} - Predictions ({dim_method.upper()})',
                            save_path=os.path.join(save_dir, f'{layer_name}_{dim_method}_pred.png')
                        )
                        
                        # é”™è¯¯åˆ†æ
                        analyzer.visualizer.plot_error_analysis(
                            features_2d, true_labels, predictions,
                            save_path=os.path.join(save_dir, f'{layer_name}_{dim_method}_errors.png')
                        )
                        
                        # èšç±»åˆ†æ
                        cluster_results = analyzer.cluster_analyzer.perform_clustering(
                            layer_features, methods=['kmeans', 'dbscan'], true_labels=true_labels
                        )
                        
                        # è¯„ä¼°èšç±»è´¨é‡
                        for method, result in cluster_results.items():
                            evaluation = analyzer.cluster_analyzer.evaluate_clustering(
                                result['labels'], true_labels
                            )
                            result.update(evaluation)
                        
                        # èšç±»å¯¹æ¯”å¯è§†åŒ–
                        analyzer.visualizer.plot_clustering_comparison(
                            features_2d, true_labels, cluster_results,
                            save_dir=os.path.join(save_dir, f'{layer_name}_{dim_method}_clustering')
                        )
                        
                        layer_results[dim_method] = {
                            'clustering': cluster_results,
                            'features_2d': features_2d
                        }
                        
                    except Exception as e:
                        print(f"     {dim_method} åˆ†æå¤±è´¥: {e}")
                        continue
                
                analysis_results[layer_name] = layer_results
            
            # 9. ç»˜åˆ¶æ··æ·†çŸ©é˜µ
            print("ğŸ“ˆ ç”Ÿæˆæ··æ·†çŸ©é˜µ...")
            analyzer.visualizer.plot_confusion_heatmap(
                true_labels, predictions,
                save_path=os.path.join(save_dir, 'confusion_matrix.png')
            )
            
            # 10. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
            print("ğŸ“ ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š...")
            generate_enhanced_report(
                analysis_results, features, true_labels, predictions, 
                attention_weights, save_dir
            )
            
            print(f"ğŸ‰ å¢å¼ºç‰ˆåˆ†æå®Œæˆï¼ç»“æœä¿å­˜åœ¨: {save_dir}")
            return analysis_results
            
        finally:
            analyzer.feature_extractor.remove_hooks()
            
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def generate_enhanced_report(analysis_results, features, true_labels, predictions, 
                           attention_weights, save_dir):
    """ç”Ÿæˆå¢å¼ºç‰ˆåˆ†ææŠ¥å‘Š"""
    
    # åŸºç¡€ç»Ÿè®¡
    accuracy = accuracy_score(true_labels, predictions)
    unique_true = len(np.unique(true_labels))
    unique_pred = len(np.unique(predictions))
    
    # æ¨¡æ€åˆ†æ
    modality_analysis = {}
    if 'visual_encoder' in features and 'embodiment_encoder' in features:
        visual_var = np.var(features['visual_encoder'])
        embodiment_var = np.var(features['embodiment_encoder'])
        modality_analysis = {
            'visual_variance': float(visual_var),
            'embodiment_variance': float(embodiment_var),
            'modality_balance': float(visual_var / (visual_var + embodiment_var))
        }
    
    # å±‚çº§ç‰¹å¾å¤æ‚åº¦
    layer_complexity = {}
    for layer_name, layer_features in features.items():
        if layer_features is not None:
            layer_complexity[layer_name] = {
                'feature_dim': int(layer_features.shape[1]),
                'feature_variance': float(np.var(layer_features)),
                'feature_range': [float(np.min(layer_features)), float(np.max(layer_features))],
                'effective_rank': float(np.linalg.matrix_rank(layer_features))
            }
    
    # æ³¨æ„åŠ›åˆ†æ
    attention_analysis = {}
    if attention_weights is not None:
        attention_analysis = {
            'mean_attention': float(np.mean(attention_weights)),
            'attention_entropy': float(-np.sum(attention_weights * np.log(attention_weights + 1e-8), axis=-1).mean()),
            'attention_sparsity': float(np.sum(attention_weights < 0.1) / attention_weights.size)
        }
    
    # èšç±»è´¨é‡æ€»ç»“
    clustering_summary = {}
    for layer_name, layer_results in analysis_results.items():
        layer_clustering = {}
        for dim_method, method_results in layer_results.items():
            if 'clustering' in method_results:
                clustering_results = method_results['clustering']
                best_silhouette = max([r['silhouette_score'] for r in clustering_results.values() 
                                     if r['silhouette_score'] != -1], default=0)
                best_ari = max([r.get('ari', 0) for r in clustering_results.values()], default=0)
                layer_clustering[dim_method] = {
                    'best_silhouette': float(best_silhouette),
                    'best_ari': float(best_ari)
                }
        clustering_summary[layer_name] = layer_clustering
    
    # ç”Ÿæˆå®Œæ•´æŠ¥å‘Š
    enhanced_report = {
        'analysis_type': 'Enhanced Embodied Counting Model Analysis',
        'timestamp': pd.Timestamp.now().isoformat(),
        'summary': {
            'total_samples': int(len(true_labels)),
            'overall_accuracy': float(accuracy),
            'unique_true_labels': int(unique_true),
            'unique_predictions': int(unique_pred),
            'analyzed_layers': list(features.keys())
        },
        'modality_analysis': modality_analysis,
        'layer_complexity': layer_complexity,
        'attention_analysis': attention_analysis,
        'clustering_summary': clustering_summary,
        'recommendations': generate_recommendations(analysis_results, features, accuracy)
    }
    
    # ä¿å­˜JSONæŠ¥å‘Š
    try:
        with open(os.path.join(save_dir, 'enhanced_analysis_report.json'), 'w') as f:
            json.dump(enhanced_report, f, indent=2)
        print(f"âœ… JSONæŠ¥å‘Šå·²ä¿å­˜")
    except Exception as e:
        print(f"âš ï¸ JSONæŠ¥å‘Šä¿å­˜å¤±è´¥: {e}")
    
    # ç”Ÿæˆå¯è¯»æŠ¥å‘Š
    try:
        with open(os.path.join(save_dir, 'enhanced_analysis_summary.txt'), 'w', encoding='utf-8') as f:
            f.write("=== å¢å¼ºç‰ˆå…·èº«è®¡æ•°æ¨¡å‹åˆ†ææŠ¥å‘Š ===\n\n")
            f.write(f"åˆ†ææ—¶é—´: {enhanced_report['timestamp']}\n")
            f.write(f"æ€»æ ·æœ¬æ•°: {enhanced_report['summary']['total_samples']}\n")
            f.write(f"æ•´ä½“å‡†ç¡®ç‡: {enhanced_report['summary']['overall_accuracy']:.4f}\n")
            f.write(f"åˆ†æå±‚æ•°: {len(enhanced_report['summary']['analyzed_layers'])}\n\n")
            
            # æ¨¡æ€åˆ†æ
            if modality_analysis:
                f.write("=== å¤šæ¨¡æ€åˆ†æ ===\n")
                f.write(f"è§†è§‰ç‰¹å¾æ–¹å·®: {modality_analysis['visual_variance']:.4f}\n")
                f.write(f"å…·èº«ç‰¹å¾æ–¹å·®: {modality_analysis['embodiment_variance']:.4f}\n")
                f.write(f"æ¨¡æ€å¹³è¡¡åº¦: {modality_analysis['modality_balance']:.4f}\n\n")
            
            # å±‚çº§å¤æ‚åº¦
            f.write("=== å±‚çº§ç‰¹å¾å¤æ‚åº¦ ===\n")
            for layer_name, complexity in layer_complexity.items():
                f.write(f"{layer_name}:\n")
                f.write(f"  ç‰¹å¾ç»´åº¦: {complexity['feature_dim']}\n")
                f.write(f"  ç‰¹å¾æ–¹å·®: {complexity['feature_variance']:.4f}\n")
                f.write(f"  æœ‰æ•ˆç§©: {complexity['effective_rank']:.1f}\n")
            f.write("\n")
            
            # æ³¨æ„åŠ›åˆ†æ
            if attention_analysis:
                f.write("=== æ³¨æ„åŠ›æœºåˆ¶åˆ†æ ===\n")
                f.write(f"å¹³å‡æ³¨æ„åŠ›å¼ºåº¦: {attention_analysis['mean_attention']:.4f}\n")
                f.write(f"æ³¨æ„åŠ›ç†µ: {attention_analysis['attention_entropy']:.4f}\n")
                f.write(f"æ³¨æ„åŠ›ç¨€ç–æ€§: {attention_analysis['attention_sparsity']:.4f}\n\n")
            
            # èšç±»è´¨é‡æ€»ç»“
            f.write("=== èšç±»è´¨é‡æ€»ç»“ ===\n")
            for layer_name, clustering in clustering_summary.items():
                f.write(f"{layer_name}:\n")
                for dim_method, scores in clustering.items():
                    f.write(f"  {dim_method}: è½®å»“ç³»æ•°={scores['best_silhouette']:.3f}, ARI={scores['best_ari']:.3f}\n")
            f.write("\n")
            
            # å»ºè®®
            f.write("=== åˆ†æå»ºè®® ===\n")
            for rec in enhanced_report['recommendations']:
                f.write(f"â€¢ {rec}\n")
        
        print(f"âœ… æ–‡æœ¬æŠ¥å‘Šå·²ä¿å­˜")
        
    except Exception as e:
        print(f"âš ï¸ æ–‡æœ¬æŠ¥å‘Šä¿å­˜å¤±è´¥: {e}")


def generate_recommendations(analysis_results, features, accuracy):
    """åŸºäºåˆ†æç»“æœç”Ÿæˆå»ºè®®"""
    recommendations = []
    
    # å‡†ç¡®ç‡å»ºè®®
    if accuracy < 0.7:
        recommendations.append("æ¨¡å‹å‡†ç¡®ç‡è¾ƒä½ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®è´¨é‡æˆ–è°ƒæ•´æ¨¡å‹æ¶æ„")
    elif accuracy > 0.9:
        recommendations.append("æ¨¡å‹å‡†ç¡®ç‡å¾ˆé«˜ï¼Œå¯ä»¥è€ƒè™‘åœ¨æ›´å¤æ‚çš„ä»»åŠ¡ä¸Šæµ‹è¯•")
    
    # ç‰¹å¾ç»´åº¦å»ºè®®
    high_dim_layers = [name for name, feats in features.items() 
                      if feats is not None and feats.shape[1] > 512]
    if high_dim_layers:
        recommendations.append(f"å±‚ {high_dim_layers} ç‰¹å¾ç»´åº¦è¾ƒé«˜ï¼Œå¯è€ƒè™‘é™ç»´ä¼˜åŒ–")
    
    # èšç±»è´¨é‡å»ºè®®
    poor_clustering_layers = []
    for layer_name, layer_results in analysis_results.items():
        avg_silhouette = []
        for dim_method, method_results in layer_results.items():
            if 'clustering' in method_results:
                silhouettes = [r['silhouette_score'] for r in method_results['clustering'].values() 
                             if r['silhouette_score'] != -1]
                if silhouettes:
                    avg_silhouette.extend(silhouettes)
        
        if avg_silhouette and np.mean(avg_silhouette) < 0.3:
            poor_clustering_layers.append(layer_name)
    
    if poor_clustering_layers:
        recommendations.append(f"å±‚ {poor_clustering_layers} çš„èšç±»è´¨é‡è¾ƒå·®ï¼Œç‰¹å¾å¯åˆ†ç¦»æ€§ä¸å¼º")
    
    # æ¨¡æ€å¹³è¡¡å»ºè®®
    if 'visual_encoder' in features and 'embodiment_encoder' in features:
        visual_var = np.var(features['visual_encoder'])
        embodiment_var = np.var(features['embodiment_encoder'])
        ratio = visual_var / (embodiment_var + 1e-8)
        
        if ratio > 10:
            recommendations.append("è§†è§‰ç‰¹å¾å ä¸»å¯¼åœ°ä½ï¼Œå»ºè®®åŠ å¼ºå…·èº«ç‰¹å¾çš„è¡¨è¾¾èƒ½åŠ›")
        elif ratio < 0.1:
            recommendations.append("å…·èº«ç‰¹å¾å ä¸»å¯¼åœ°ä½ï¼Œå»ºè®®å¹³è¡¡å¤šæ¨¡æ€ç‰¹å¾")
    
    return recommendations


def main():
    """ä¸»å…¥å£å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å…·èº«è®¡æ•°æ¨¡å‹åˆ†æå·¥å…·')
    
    # å¿…éœ€å‚æ•°
    parser.add_argument('--checkpoint', type=str, required=True,
                       help='æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--val_csv', type=str, default='scratch/Ball_counting_CNN/Tools_script/ball_counting_dataset_val.csv',
                       help='éªŒè¯é›†CSVæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--data_root', type=str, default='/mnt/iusers01/fatpou01/compsci01/k09562zs/scratch/Ball_counting_CNN/ball_data_collection',
                       help='æ•°æ®æ ¹ç›®å½•')
    
    # åˆ†æé€‰é¡¹
    parser.add_argument('--mode', type=str, default='enhanced',
                       choices=['debug', 'inspect', 'quick', 'enhanced'],
                       help='åˆ†ææ¨¡å¼')
    parser.add_argument('--save_dir', type=str, default='/mnt/iusers01/fatpou01/compsci01/k09562zs/scratch/Ball_counting_CNN/analysis_results',
                       help='ç»“æœä¿å­˜ç›®å½•')
    parser.add_argument('--max_samples', type=int, default=500,
                       help='æœ€å¤§åˆ†ææ ·æœ¬æ•°')
    parser.add_argument('--layers', nargs='+', default=None,
                       help='æŒ‡å®šè¦åˆ†æçš„å±‚åç§°')
    
    # å…¶ä»–é€‰é¡¹
    parser.add_argument('--batch_size', type=int, default=8,
                       help='æ•°æ®åŠ è½½æ‰¹æ¬¡å¤§å°')
    
    args = parser.parse_args()
    
    # è®¾ç½®é»˜è®¤ä¿å­˜ç›®å½•
    if args.save_dir is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        args.save_dir = f'./analysis_results_{args.mode}_{timestamp}'
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    for path, name in [(args.checkpoint, 'æ£€æŸ¥ç‚¹æ–‡ä»¶'), 
                       (args.val_csv, 'éªŒè¯CSVæ–‡ä»¶'), 
                       (args.data_root, 'æ•°æ®æ ¹ç›®å½•')]:
        if not os.path.exists(path):
            print(f"âŒ {name}ä¸å­˜åœ¨: {path}")
            return
    
    print("ğŸš€ å…·èº«è®¡æ•°æ¨¡å‹åˆ†æå·¥å…·")
    print("="*50)
    print(f"æ¨¡å¼: {args.mode}")
    print(f"æ£€æŸ¥ç‚¹: {args.checkpoint}")
    print(f"éªŒè¯é›†: {args.val_csv}")
    print(f"æ•°æ®æ ¹ç›®å½•: {args.data_root}")
    print(f"ä¿å­˜ç›®å½•: {args.save_dir}")
    print("="*50)
    
    start_time = time.time()
    
    try:
        if args.mode == 'debug':
            print("ğŸ” è¿è¡Œè°ƒè¯•æ¨¡å¼...")
            success = debug_labels_and_model(args.checkpoint, args.val_csv, args.data_root)
            if success:
                print("âœ… è°ƒè¯•å®Œæˆ")
            else:
                print("âŒ è°ƒè¯•å¤±è´¥")
        
        elif args.mode == 'inspect':
            print("ğŸ”¬ æ£€æŸ¥æ¨¡å‹ç»“æ„...")
            inspect_model_structure(args.checkpoint)
        
        elif args.mode == 'quick':
            print("âš¡ å¿«é€Ÿåˆ†æ...")
            results = analyze_embodied_counting_model_enhanced(
                args.checkpoint, args.val_csv, args.data_root, 
                args.save_dir, max_samples=min(100, args.max_samples),
                specific_layers=args.layers
            )
        
        elif args.mode == 'enhanced':
            print("ğŸ¤– å¢å¼ºç‰ˆåˆ†æ...")
            results = analyze_embodied_counting_model_enhanced(
                args.checkpoint, args.val_csv, args.data_root, 
                args.save_dir, args.max_samples, args.layers
            )
        
        elapsed_time = time.time() - start_time
        print(f"\nğŸ‰ åˆ†æå®Œæˆï¼")
        print(f"â±ï¸ æ€»è€—æ—¶: {elapsed_time:.2f} ç§’")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {args.save_dir}")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­åˆ†æ")
    except Exception as e:
        print(f"\nâŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # å¦‚æœæ²¡æœ‰å‘½ä»¤è¡Œå‚æ•°ï¼Œæ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
    if len(sys.argv) == 1:
        print("ğŸ¤– å…·èº«è®¡æ•°æ¨¡å‹åˆ†æå·¥å…·")
        print("="*50)
        print("ä½¿ç”¨æ–¹æ³•:")
        print("python embodied_analysis.py --checkpoint MODEL.pth --val_csv VAL.csv --data_root DATA_DIR")
        print("\nå¯ç”¨æ¨¡å¼:")
        print("  --mode debug        # è°ƒè¯•æ¨¡å‹å’Œæ•°æ®")
        print("  --mode inspect      # æ£€æŸ¥æ¨¡å‹ç»“æ„")
        print("  --mode quick        # å¿«é€Ÿåˆ†æï¼ˆå°‘é‡æ ·æœ¬ï¼‰")
        print("  --mode enhanced     # å¢å¼ºåˆ†æï¼ˆæ¨èï¼‰")
        print("\nåŸºæœ¬ç¤ºä¾‹:")
        print("python embodied_analysis.py \\")
        print("    --checkpoint ./best_model.pth \\")
        print("    --val_csv ./val.csv \\")
        print("    --data_root ./data \\")
        print("    --mode enhanced")
        print("\né«˜çº§ç¤ºä¾‹:")
        print("python embodied_analysis.py \\")
        print("    --checkpoint ./best_model.pth \\")
        print("    --val_csv ./val.csv \\")
        print("    --data_root ./data \\")
        print("    --mode enhanced \\")
        print("    --layers fusion lstm counting_decoder \\")
        print("    --max_samples 1000 \\")
        print("    --save_dir ./my_analysis")
        print("\nğŸ“‹ æ¨èå·¥ä½œæµ:")
        print("1. é¦–å…ˆè¿è¡Œ: --mode inspect    # æŸ¥çœ‹æ¨¡å‹ç»“æ„")
        print("2. ç„¶åè¿è¡Œ: --mode debug      # éªŒè¯æ•°æ®åŠ è½½")
        print("3. æœ€åè¿è¡Œ: --mode enhanced   # å®Œæ•´åˆ†æ")
        print("\nğŸ’¡ æç¤º:")
        print("- å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡ä½¿ç”¨ï¼Œå»ºè®®å…ˆç”¨ --mode quick æµ‹è¯•")
        print("- enhanced æ¨¡å¼æä¾›æœ€å…¨é¢çš„åˆ†æ")
        print("- å¯ä»¥ç”¨ --layers å‚æ•°æŒ‡å®šç‰¹å®šå±‚è¿›è¡Œåˆ†æ")
        sys.exit(0)
    
    main()


# =============================================================================
# ä¾¿æ·å‡½æ•°ï¼Œä¾›å…¶ä»–è„šæœ¬è°ƒç”¨
# =============================================================================

def quick_analysis(checkpoint_path, val_csv, data_root, save_dir=None, max_samples=100):
    """å¿«é€Ÿåˆ†ææ¥å£"""
    if save_dir is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        save_dir = f'./quick_analysis_{timestamp}'
    
    return analyze_embodied_counting_model_enhanced(
        checkpoint_path, val_csv, data_root, save_dir, max_samples
    )


def full_analysis(checkpoint_path, val_csv, data_root, save_dir=None, max_samples=500, layers=None):
    """å®Œæ•´åˆ†ææ¥å£"""
    if save_dir is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        save_dir = f'./full_analysis_{timestamp}'
    
    return analyze_embodied_counting_model_enhanced(
        checkpoint_path, val_csv, data_root, save_dir, max_samples, layers
    )


def batch_analysis(checkpoint_paths, val_csv, data_root, base_save_dir='./batch_analysis'):
    """æ‰¹é‡åˆ†æå¤šä¸ªæ¨¡å‹"""
    results = {}
    
    for i, checkpoint_path in enumerate(checkpoint_paths):
        print(f"\n{'='*60}")
        print(f"åˆ†ææ¨¡å‹ {i+1}/{len(checkpoint_paths)}: {checkpoint_path}")
        print(f"{'='*60}")
        
        model_name = os.path.basename(checkpoint_path).replace('.pth', '')
        save_dir = os.path.join(base_save_dir, f'model_{i+1}_{model_name}')
        
        try:
            result = analyze_embodied_counting_model_enhanced(
                checkpoint_path, val_csv, data_root, save_dir, max_samples=300
            )
            results[model_name] = result
            print(f"âœ… æ¨¡å‹ {model_name} åˆ†æå®Œæˆ")
        except Exception as e:
            print(f"âŒ æ¨¡å‹ {model_name} åˆ†æå¤±è´¥: {e}")
            results[model_name] = None
    
    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    generate_comparison_report(results, base_save_dir)
    
    return results


def generate_comparison_report(results, save_dir):
    """ç”Ÿæˆå¤šæ¨¡å‹å¯¹æ¯”æŠ¥å‘Š"""
    print("\nğŸ“Š ç”Ÿæˆå¤šæ¨¡å‹å¯¹æ¯”æŠ¥å‘Š...")
    
    comparison_data = []
    
    for model_name, result in results.items():
        if result is None:
            continue
        
        # è¿™é‡Œå¯ä»¥æå–æ¯ä¸ªæ¨¡å‹çš„å…³é”®æŒ‡æ ‡è¿›è¡Œå¯¹æ¯”
        # ç”±äºç»“æœç»“æ„æ¯”è¾ƒå¤æ‚ï¼Œè¿™é‡Œæä¾›ä¸€ä¸ªæ¡†æ¶
        model_summary = {
            'model_name': model_name,
            'status': 'success' if result else 'failed'
        }
        comparison_data.append(model_summary)
    
    # ä¿å­˜å¯¹æ¯”ç»“æœ
    comparison_df = pd.DataFrame(comparison_data)
    comparison_df.to_csv(os.path.join(save_dir, 'model_comparison.csv'), index=False)
    
    print(f"âœ… å¯¹æ¯”æŠ¥å‘Šä¿å­˜åœ¨: {os.path.join(save_dir, 'model_comparison.csv')}")


# =============================================================================
# ä½¿ç”¨ç¤ºä¾‹
# =============================================================================

"""
ä½¿ç”¨ç¤ºä¾‹:

1. å‘½ä»¤è¡Œä½¿ç”¨:
   python embodied_analysis.py \\
       --checkpoint ./best_model.pth \\
       --val_csv ./val.csv \\
       --data_root ./data \\
       --mode enhanced \\
       --max_samples 500

2. åœ¨Pythonè„šæœ¬ä¸­ä½¿ç”¨:
   from embodied_analysis import quick_analysis, full_analysis
   
   # å¿«é€Ÿåˆ†æ
   results = quick_analysis(
       checkpoint_path='./best_model.pth',
       val_csv='./val.csv', 
       data_root='./data'
   )
   
   # å®Œæ•´åˆ†æ
   results = full_analysis(
       checkpoint_path='./best_model.pth',
       val_csv='./val.csv',
       data_root='./data',
       max_samples=1000,
       layers=['fusion', 'lstm', 'counting_decoder']
   )

3. æ‰¹é‡åˆ†æå¤šä¸ªæ¨¡å‹:
   from embodied_analysis import batch_analysis
   
   checkpoint_paths = [
       './model1.pth',
       './model2.pth', 
       './model3.pth'
   ]
   
   results = batch_analysis(
       checkpoint_paths=checkpoint_paths,
       val_csv='./val.csv',
       data_root='./data'
   )
"""