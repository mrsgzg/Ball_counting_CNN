"""
ä¿®å¤åçš„æ³¨æ„åŠ›æœºåˆ¶æ·±åº¦åˆ†æå·¥å…·
æ”¯æŒå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
"""

import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import matplotlib
matplotlib.use('Agg')  # éäº¤äº’å¼åç«¯
import matplotlib.pyplot as plt
import seaborn as sns
import os
import sys
import json
import argparse
import time
from collections import defaultdict
from datetime import datetime
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# ç¡®ä¿ä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


def load_model_and_data(checkpoint_path, val_csv, data_root, batch_size=8):
    """åŠ è½½æ¨¡å‹å’Œæ•°æ®"""
    print("ğŸ“¥ åŠ è½½æ¨¡å‹å’Œæ•°æ®...")
    
    # åŠ è½½æ£€æŸ¥ç‚¹
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    config = checkpoint['config']
    
    # å¯¼å…¥æ¨¡å‹ç±»
    try:
        from Model_embodiment import EmbodiedCountingModel
    except ImportError:
        print("âŒ æ— æ³•å¯¼å…¥EmbodiedCountingModelï¼Œè¯·ç¡®ä¿Model_embodiment.pyåœ¨Pythonè·¯å¾„ä¸­")
        raise
    
    # ç¡®å®šå›¾åƒæ¨¡å¼
    image_mode = config.get('image_mode', 'rgb')
    input_channels = 3 if image_mode == 'rgb' else 1
    
    # é‡å»ºæ¨¡å‹
    model_config = config['model_config'].copy()
    model_config['input_channels'] = input_channels
    model = EmbodiedCountingModel(**model_config)
    model.load_state_dict(checkpoint['model_state_dict'])
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    model.eval()
    
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ (å›¾åƒæ¨¡å¼: {image_mode}, è®¾å¤‡: {device})")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    try:
        from DataLoader_embodiment import get_ball_counting_data_loaders
    except ImportError:
        print("âŒ æ— æ³•å¯¼å…¥get_ball_counting_data_loadersï¼Œè¯·ç¡®ä¿DataLoader_embodiment.pyåœ¨Pythonè·¯å¾„ä¸­")
        raise
    
    _, val_loader, _ = get_ball_counting_data_loaders(
        train_csv_path=config['train_csv'],
        val_csv_path=val_csv,
        data_root=data_root,
        batch_size=batch_size,
        sequence_length=config['sequence_length'],
        normalize=config['normalize'],
        num_workers=2,
        image_mode=image_mode
    )
    
    print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆï¼ŒéªŒè¯é›†å¤§å°: {len(val_loader.dataset)}")
    
    return model, val_loader, device, config


class AttentionAnalyzer:
    """ä¸“é—¨çš„æ³¨æ„åŠ›æœºåˆ¶åˆ†æå™¨ - æ”¯æŒå¤šå¤´æ³¨æ„åŠ›"""
    
    def __init__(self, figsize=(20, 15)):
        self.figsize = figsize
    
    def extract_attention_data(self, model, data_loader, max_samples=300, device='cuda'):
        """æå–å®Œæ•´çš„æ³¨æ„åŠ›æ•°æ®"""
        print("ğŸ¯ æå–æ³¨æ„åŠ›æœºåˆ¶æ•°æ®...")
        
        all_attention_sequences = []  # [sample, seq_len, heads, spatial_size]
        all_labels = []
        all_predictions = []
        all_sample_ids = []
        all_counting_sequences = []  # [sample, seq_len] - æ¯ä¸ªæ—¶åˆ»çš„è®¡æ•°é¢„æµ‹
        all_true_sequences = []     # [sample, seq_len] - æ¯ä¸ªæ—¶åˆ»çš„çœŸå®æ ‡ç­¾
        
        model.eval()
        sample_count = 0
        
        with torch.no_grad():
            for batch in tqdm(data_loader, desc="æå–æ³¨æ„åŠ›æ•°æ®"):
                if sample_count >= max_samples:
                    break
                
                # å‡†å¤‡æ•°æ®
                sequence_data = {
                    'images': batch['sequence_data']['images'].to(device),
                    'joints': batch['sequence_data']['joints'].to(device),
                    'timestamps': batch['sequence_data']['timestamps'].to(device),
                    'labels': batch['sequence_data']['labels'].to(device)
                }
                
                labels = batch['label'].cpu().numpy()  # CSVä¸­çš„æœ€ç»ˆæ ‡ç­¾
                sample_ids = batch['sample_id']
                true_sequence_labels = sequence_data['labels'].cpu().numpy()  # æ¯ä¸ªæ—¶åˆ»çš„çœŸå®æ ‡ç­¾
                
                # é™åˆ¶æ‰¹æ¬¡å¤§å°
                remaining_samples = max_samples - sample_count
                actual_batch_size = min(len(labels), remaining_samples)
                
                if actual_batch_size < len(labels):
                    for key in sequence_data:
                        sequence_data[key] = sequence_data[key][:actual_batch_size]
                    labels = labels[:actual_batch_size]
                    sample_ids = sample_ids[:actual_batch_size]
                    true_sequence_labels = true_sequence_labels[:actual_batch_size]
                
                # å‰å‘ä¼ æ’­ï¼Œè·å–æ³¨æ„åŠ›æƒé‡
                try:
                    outputs = model(
                        sequence_data=sequence_data,
                        use_teacher_forcing=False,
                        return_attention=True
                    )
                except Exception as e:
                    print(f"âš ï¸ æ¨¡å‹å‰å‘ä¼ æ’­å¤±è´¥ï¼Œå¯èƒ½ä¸æ”¯æŒreturn_attention: {e}")
                    # å°è¯•ä¸è¿”å›æ³¨æ„åŠ›æƒé‡
                    outputs = model(
                        sequence_data=sequence_data,
                        use_teacher_forcing=False
                    )
                
                # æå–è®¡æ•°é¢„æµ‹
                count_logits = outputs['counts']  # [batch, seq_len, 11]
                pred_sequence = torch.argmax(count_logits, dim=-1).cpu().numpy()  # [batch, seq_len]
                final_pred = pred_sequence[:, -1]  # æœ€ç»ˆé¢„æµ‹
                
                all_predictions.extend(final_pred)
                all_labels.extend(labels)
                all_sample_ids.extend(sample_ids)
                all_counting_sequences.extend(pred_sequence)
                all_true_sequences.extend(true_sequence_labels)
                
                # æå–æ³¨æ„åŠ›æƒé‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if 'attention_weights' in outputs:
                    attention_weights = outputs['attention_weights'].cpu().numpy()  # [batch, seq_len, heads, spatial]
                    all_attention_sequences.extend(attention_weights)
                else:
                    print("âš ï¸ æ¨¡å‹è¾“å‡ºä¸­æ²¡æœ‰æ³¨æ„åŠ›æƒé‡")
                
                sample_count += actual_batch_size
                
                if sample_count >= max_samples:
                    break
        
        # æ„å»ºç»“æœ
        result = {
            'attention_sequences': np.array(all_attention_sequences) if all_attention_sequences else None,
            'counting_sequences': np.array(all_counting_sequences),
            'true_sequences': np.array(all_true_sequences),
            'final_predictions': np.array(all_predictions),
            'true_labels': np.array(all_labels),
            'sample_ids': all_sample_ids
        }
        
        print(f"âœ… æ•°æ®æå–å®Œæˆ:")
        print(f"   æ ·æœ¬æ•°: {len(result['true_labels'])}")
        print(f"   æ³¨æ„åŠ›æ•°æ®: {'æœ‰' if result['attention_sequences'] is not None else 'æ— '}")
        if result['attention_sequences'] is not None:
            print(f"   æ³¨æ„åŠ›åºåˆ—å½¢çŠ¶: {result['attention_sequences'].shape}")
            if len(result['attention_sequences'].shape) == 4:
                n_samples, seq_len, n_heads, spatial_size = result['attention_sequences'].shape
                print(f"   æ£€æµ‹åˆ°å¤šå¤´æ³¨æ„åŠ›: {n_heads} ä¸ªå¤´ï¼Œ{spatial_size} ä¸ªç©ºé—´ä½ç½®")
        print(f"   è®¡æ•°åºåˆ—å½¢çŠ¶: {result['counting_sequences'].shape}")
        
        return result
    
    def _process_multi_head_attention(self, attention_seq):
        """å¤„ç†å¤šå¤´æ³¨æ„åŠ›ï¼Œè¿”å›å¹³å‡æ³¨æ„åŠ›å’Œæ¯ä¸ªå¤´çš„æ³¨æ„åŠ›"""
        if len(attention_seq.shape) == 3:
            # å½¢çŠ¶: (seq_len, heads, spatial) -> å¹³å‡æ‰€æœ‰å¤´
            avg_attention = np.mean(attention_seq, axis=1)  # (seq_len, spatial)
            individual_heads = attention_seq  # (seq_len, heads, spatial)
            return avg_attention, individual_heads
        elif len(attention_seq.shape) == 2:
            # å½¢çŠ¶: (seq_len, spatial) -> å•å¤´æ³¨æ„åŠ›
            return attention_seq, attention_seq.unsqueeze(1)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ³¨æ„åŠ›å½¢çŠ¶: {attention_seq.shape}")
    
    def plot_attention_evolution_detailed(self, attention_data, save_path=None):
        """è¯¦ç»†çš„æ³¨æ„åŠ›æ¼”åŒ–åˆ†æ - æ”¯æŒå¤šå¤´"""
        if attention_data['attention_sequences'] is None:
            print("âš ï¸ æ²¡æœ‰æ³¨æ„åŠ›æ•°æ®ï¼Œè·³è¿‡æ¼”åŒ–åˆ†æ")
            return
        
        attention_sequences = attention_data['attention_sequences']
        true_labels = attention_data['true_labels']
        counting_sequences = attention_data['counting_sequences']
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºå¤šå¤´æ³¨æ„åŠ›
        is_multi_head = len(attention_sequences.shape) == 4
        
        if is_multi_head:
            n_samples, seq_len, n_heads, spatial_size = attention_sequences.shape
            print(f"ğŸ” æ£€æµ‹åˆ°å¤šå¤´æ³¨æ„åŠ›: {n_heads} ä¸ªå¤´")
        else:
            n_samples, seq_len, spatial_size = attention_sequences.shape
            n_heads = 1
        
        # é€‰æ‹©ä¸åŒç±»åˆ«çš„ä»£è¡¨æ€§æ ·æœ¬
        unique_labels = np.unique(true_labels)
        n_classes = min(len(unique_labels), 10)  # æœ€å¤šæ˜¾ç¤º3ä¸ªç±»åˆ«
        samples_per_class = 2
        
        # è®¡ç®—å­å›¾æ•°é‡
        cols_per_sample = 4  # æ¯ä¸ªæ ·æœ¬4åˆ—: å¹³å‡æ³¨æ„åŠ›, æ³¨æ„åŠ›é›†ä¸­åº¦, è®¡æ•°vsæ³¨æ„åŠ›, å¤šå¤´å¯¹æ¯”
        total_cols = samples_per_class * cols_per_sample
        
        fig, axes = plt.subplots(n_classes, total_cols, 
                                figsize=(total_cols * 3, n_classes * 4))
        
        if n_classes == 1:
            axes = axes.reshape(1, -1)
        elif total_cols == 1:
            axes = axes.reshape(-1, 1)
        
        for class_idx, label in enumerate(unique_labels[:n_classes]):
            class_mask = true_labels == label
            class_indices = np.where(class_mask)[0]
            
            # é€‰æ‹©è¯¥ç±»åˆ«çš„æ ·æœ¬
            if len(class_indices) >= samples_per_class:
                selected_samples = np.random.choice(class_indices, samples_per_class, replace=False)
            else:
                selected_samples = class_indices
            
            for sample_idx, sample_id in enumerate(selected_samples):
                if sample_idx >= samples_per_class:
                    break
                    
                attention_seq = attention_sequences[sample_id]  # (seq_len, heads, spatial) æˆ– (seq_len, spatial)
                counting_seq = counting_sequences[sample_id]    # (seq_len,)
                
                # å¤„ç†å¤šå¤´æ³¨æ„åŠ›
                if is_multi_head:
                    avg_attention, individual_heads = self._process_multi_head_attention(attention_seq)
                else:
                    avg_attention = attention_seq
                    individual_heads = attention_seq
                
                seq_len, spatial_size = avg_attention.shape
                
                # è®¡ç®—åˆ—çš„åŸºç¡€ç´¢å¼•
                col_base = sample_idx * cols_per_sample
                
                # 1. å¹³å‡æ³¨æ„åŠ›æ—¶åºæ¼”åŒ–çƒ­åŠ›å›¾
                ax1 = axes[class_idx, col_base] if n_classes > 1 else axes[col_base]
                
                im1 = ax1.imshow(avg_attention.T, cmap='viridis', aspect='auto')
                ax1.set_title(f'Class {label} Sample {sample_idx+1}\nAvg Attention Evolution')
                ax1.set_xlabel('Time Step')
                ax1.set_ylabel('Spatial Location')
                plt.colorbar(im1, ax=ax1, shrink=0.8)
                
                # 2. æ³¨æ„åŠ›é›†ä¸­åº¦éšæ—¶é—´å˜åŒ–
                ax2 = axes[class_idx, col_base + 1] if n_classes > 1 else axes[col_base + 1]
                
                # è®¡ç®—æ¯ä¸ªæ—¶åˆ»çš„æ³¨æ„åŠ›é›†ä¸­åº¦ï¼ˆç†µçš„å€’æ•°ï¼‰
                concentration = []
                for t in range(seq_len):
                    attention_t = avg_attention[t] + 1e-8
                    entropy = -np.sum(attention_t * np.log(attention_t))
                    concentration.append(1.0 / (entropy + 1e-8))
                
                time_steps = range(seq_len)
                ax2.plot(time_steps, concentration, 'b-', linewidth=2, marker='o')
                ax2.set_title('Attention Concentration')
                ax2.set_xlabel('Time Step')
                ax2.set_ylabel('Concentration (1/Entropy)')
                ax2.grid(True, alpha=0.3)
                
                # 3. è®¡æ•°é¢„æµ‹ä¸æ³¨æ„åŠ›å…³è”
                ax3 = axes[class_idx, col_base + 2] if n_classes > 1 else axes[col_base + 2]
                
                # åŒè½´å›¾ï¼šè®¡æ•°é¢„æµ‹å’Œæ³¨æ„åŠ›é›†ä¸­åº¦
                ax3_twin = ax3.twinx()
                
                line1 = ax3.plot(time_steps, counting_seq, 'r-', linewidth=2, marker='s', 
                               label='Count Prediction')
                line2 = ax3_twin.plot(time_steps, concentration, 'b--', linewidth=2, marker='o', 
                                    label='Attention Concentration')
                
                ax3.set_xlabel('Time Step')
                ax3.set_ylabel('Predicted Count', color='red')
                ax3_twin.set_ylabel('Attention Concentration', color='blue')
                ax3.set_title('Count vs Attention')
                ax3.grid(True, alpha=0.3)
                
                # æ·»åŠ çœŸå®æ ‡ç­¾çº¿
                true_label = true_labels[sample_id]
                ax3.axhline(y=true_label, color='green', linestyle=':', 
                          linewidth=2, label=f'True Count ({true_label})')
                
                # åˆå¹¶å›¾ä¾‹
                lines = line1 + line2
                labels = [l.get_label() for l in lines]
                ax3.legend(lines + [ax3.lines[-1]], labels + ['True Count'], loc='upper left')
                
                # 4. å¤šå¤´æ³¨æ„åŠ›å¯¹æ¯” (å¦‚æœæ˜¯å¤šå¤´)
                ax4 = axes[class_idx, col_base + 3] if n_classes > 1 else axes[col_base + 3]
                
                if is_multi_head and n_heads > 1:
                    # æ˜¾ç¤ºæ¯ä¸ªå¤´çš„æ³¨æ„åŠ›é›†ä¸­åº¦
                    colors = plt.cm.tab10(np.linspace(0, 1, n_heads))
                    
                    for head_idx in range(n_heads):
                        head_attention = individual_heads[:, head_idx, :]  # (seq_len, spatial)
                        head_concentration = []
                        
                        for t in range(seq_len):
                            attention_t = head_attention[t] + 1e-8
                            entropy = -np.sum(attention_t * np.log(attention_t))
                            head_concentration.append(1.0 / (entropy + 1e-8))
                        
                        ax4.plot(time_steps, head_concentration, 
                               color=colors[head_idx], linewidth=2, 
                               marker='o', label=f'Head {head_idx+1}')
                    
                    ax4.plot(time_steps, concentration, 'k--', linewidth=3, 
                           label='Average', alpha=0.8)
                    ax4.set_title('Multi-Head Attention Comparison')
                    ax4.set_xlabel('Time Step')
                    ax4.set_ylabel('Concentration')
                    ax4.legend()
                    ax4.grid(True, alpha=0.3)
                else:
                    # å•å¤´æˆ–å¹³å‡æ³¨æ„åŠ›çš„ç©ºé—´åˆ†å¸ƒ
                    final_attention = avg_attention[-1]  # æœ€åæ—¶åˆ»çš„æ³¨æ„åŠ›
                    ax4.bar(range(min(20, len(final_attention))), final_attention[:20])
                    ax4.set_title('Final Attention Distribution\n(Top 20 locations)')
                    ax4.set_xlabel('Spatial Location')
                    ax4.set_ylabel('Attention Weight')
                    ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
    
    def plot_attention_accuracy_analysis(self, attention_data, save_path=None):
        """æ³¨æ„åŠ›ä¸å‡†ç¡®æ€§çš„è¯¦ç»†åˆ†æ - æ”¯æŒå¤šå¤´"""
        if attention_data['attention_sequences'] is None:
            print("âš ï¸ æ²¡æœ‰æ³¨æ„åŠ›æ•°æ®ï¼Œè·³è¿‡å‡†ç¡®æ€§åˆ†æ")
            return
        
        attention_sequences = attention_data['attention_sequences']
        true_labels = attention_data['true_labels']
        final_predictions = attention_data['final_predictions']
        counting_sequences = attention_data['counting_sequences']
        
        # å¤„ç†å¤šå¤´æ³¨æ„åŠ› - å–å¹³å‡
        if len(attention_sequences.shape) == 4:
            # (n_samples, seq_len, n_heads, spatial) -> (n_samples, seq_len, spatial)
            attention_sequences = np.mean(attention_sequences, axis=2)
            print("ğŸ” å¤šå¤´æ³¨æ„åŠ›å·²å¹³å‡å¤„ç†")
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        # 1. è®¡ç®—æ³¨æ„åŠ›é›†ä¸­åº¦
        attention_concentration = []
        for seq in attention_sequences:
            concentration = []
            for t in range(seq.shape[0]):
                attention_t = seq[t] + 1e-8
                entropy = -np.sum(attention_t * np.log(attention_t))
                concentration.append(1.0 / (entropy + 1e-8))
            attention_concentration.append(concentration)
        
        attention_concentration = np.array(attention_concentration)
        
        # 1. æ•´ä½“æ³¨æ„åŠ›é›†ä¸­åº¦æ¼”åŒ–
        mean_concentration = np.mean(attention_concentration, axis=0)
        std_concentration = np.std(attention_concentration, axis=0)
        time_steps = range(len(mean_concentration))
        
        axes[0, 0].plot(time_steps, mean_concentration, 'b-', linewidth=2, label='Mean')
        axes[0, 0].fill_between(time_steps, 
                               mean_concentration - std_concentration,
                               mean_concentration + std_concentration,
                               alpha=0.3, color='blue')
        axes[0, 0].set_title('Overall Attention Concentration Evolution')
        axes[0, 0].set_xlabel('Time Step')
        axes[0, 0].set_ylabel('Concentration')
        axes[0, 0].grid(True, alpha=0.3)
        axes[0, 0].legend()
        
        # 2. æ­£ç¡®vsé”™è¯¯é¢„æµ‹çš„æ³¨æ„åŠ›å¯¹æ¯”
        correct_mask = final_predictions == true_labels
        error_mask = ~correct_mask
        
        if np.any(correct_mask) and np.any(error_mask):
            correct_concentration = np.mean(attention_concentration[correct_mask], axis=0)
            error_concentration = np.mean(attention_concentration[error_mask], axis=0)
            
            axes[0, 1].plot(time_steps, correct_concentration, 'g-', linewidth=2, 
                           label=f'Correct ({np.sum(correct_mask)} samples)')
            axes[0, 1].plot(time_steps, error_concentration, 'r-', linewidth=2, 
                           label=f'Error ({np.sum(error_mask)} samples)')
            axes[0, 1].set_title('Attention: Correct vs Error Predictions')
            axes[0, 1].set_xlabel('Time Step')
            axes[0, 1].set_ylabel('Mean Concentration')
            axes[0, 1].grid(True, alpha=0.3)
            axes[0, 1].legend()
        else:
            axes[0, 1].text(0.5, 0.5, 'No comparison available\n(all correct or all error)', 
                           ha='center', va='center', transform=axes[0, 1].transAxes)
            axes[0, 1].set_title('Attention: Correct vs Error Predictions')
        
        # 3. ä¸åŒç±»åˆ«çš„æ³¨æ„åŠ›æ¨¡å¼
        unique_labels = np.unique(true_labels)
        colors = plt.cm.tab10(np.linspace(0, 1, len(unique_labels)))
        
        for i, label in enumerate(unique_labels):
            class_mask = true_labels == label
            if np.any(class_mask):
                class_concentration = np.mean(attention_concentration[class_mask], axis=0)
                axes[0, 2].plot(time_steps, class_concentration, 
                               color=colors[i], linewidth=2, 
                               label=f'Class {label} ({np.sum(class_mask)} samples)')
        
        axes[0, 2].set_title('Attention Patterns by True Class')
        axes[0, 2].set_xlabel('Time Step')
        axes[0, 2].set_ylabel('Mean Concentration')
        axes[0, 2].grid(True, alpha=0.3)
        axes[0, 2].legend()
        
        # 4. æœ€ç»ˆæ³¨æ„åŠ›é›†ä¸­åº¦åˆ†å¸ƒ
        final_concentration = attention_concentration[:, -1]
        
        if np.any(correct_mask):
            axes[1, 0].hist(final_concentration[correct_mask], bins=20, alpha=0.7, 
                           color='green', label='Correct Predictions', density=True)
        if np.any(error_mask):
            axes[1, 0].hist(final_concentration[error_mask], bins=20, alpha=0.7, 
                           color='red', label='Error Predictions', density=True)
        axes[1, 0].set_title('Final Attention Concentration Distribution')
        axes[1, 0].set_xlabel('Final Concentration')
        axes[1, 0].set_ylabel('Density')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # 5. æ³¨æ„åŠ›é›†ä¸­åº¦ä¸å‡†ç¡®æ€§çš„å…³ç³»
        # å°†æœ€ç»ˆé›†ä¸­åº¦åˆ†æ¡¶ï¼Œè®¡ç®—æ¯æ¡¶çš„å‡†ç¡®ç‡
        n_bins = 8
        concentration_bins = np.linspace(np.min(final_concentration), 
                                       np.max(final_concentration), n_bins + 1)
        
        bin_accuracies = []
        bin_centers = []
        bin_counts = []
        
        for i in range(n_bins):
            bin_mask = (final_concentration >= concentration_bins[i]) & \
                      (final_concentration < concentration_bins[i + 1])
            if i == n_bins - 1:  # æœ€åä¸€ä¸ªbinåŒ…å«å³è¾¹ç•Œ
                bin_mask = (final_concentration >= concentration_bins[i]) & \
                          (final_concentration <= concentration_bins[i + 1])
            
            if np.any(bin_mask):
                bin_accuracy = np.mean(final_predictions[bin_mask] == true_labels[bin_mask])
                bin_accuracies.append(bin_accuracy)
                bin_centers.append((concentration_bins[i] + concentration_bins[i + 1]) / 2)
                bin_counts.append(np.sum(bin_mask))
            
        if bin_centers:
            axes[1, 1].bar(bin_centers, bin_accuracies, 
                          width=np.diff(concentration_bins)[0] * 0.8, alpha=0.7,
                          color='purple')
            axes[1, 1].set_title('Accuracy vs Final Attention Concentration')
            axes[1, 1].set_xlabel('Final Attention Concentration')
            axes[1, 1].set_ylabel('Accuracy')
            axes[1, 1].grid(True, alpha=0.3)
            
            # æ·»åŠ æ ·æœ¬æ•°é‡æ ‡æ³¨
            for center, accuracy, count in zip(bin_centers, bin_accuracies, bin_counts):
                axes[1, 1].text(center, accuracy + 0.02, f'n={count}', 
                               ha='center', va='bottom', fontsize=8)
        
        # 6. è®¡æ•°å‡†ç¡®æ€§éšæ—¶é—´æ¼”åŒ–
        step_accuracies = []
        for t in range(counting_sequences.shape[1]):
            step_accuracy = np.mean(counting_sequences[:, t] == true_labels)
            step_accuracies.append(step_accuracy)
        
        # åŒè½´ï¼šè®¡æ•°å‡†ç¡®æ€§å’Œæ³¨æ„åŠ›é›†ä¸­åº¦
        ax_acc = axes[1, 2]
        ax_att = ax_acc.twinx()
        
        line1 = ax_acc.plot(time_steps, step_accuracies, 'g-', linewidth=2, 
                           marker='o', label='Count Accuracy')
        line2 = ax_att.plot(time_steps, mean_concentration, 'b--', linewidth=2, 
                           marker='s', label='Attention Concentration')
        
        ax_acc.set_xlabel('Time Step')
        ax_acc.set_ylabel('Count Accuracy', color='green')
        ax_att.set_ylabel('Attention Concentration', color='blue')
        ax_acc.set_title('Count Accuracy vs Attention Over Time')
        ax_acc.grid(True, alpha=0.3)
        
        # åˆå¹¶å›¾ä¾‹
        lines = line1 + line2
        labels = [l.get_label() for l in lines]
        ax_acc.legend(lines, labels, loc='upper left')
        
        plt.tight_layout()
        
        if save_path:
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
    
    def plot_spatial_attention_analysis(self, attention_data, save_path=None):
        """ç©ºé—´æ³¨æ„åŠ›åˆ†æ - æ”¯æŒå¤šå¤´"""
        if attention_data['attention_sequences'] is None:
            print("âš ï¸ æ²¡æœ‰æ³¨æ„åŠ›æ•°æ®ï¼Œè·³è¿‡ç©ºé—´åˆ†æ")
            return
        
        attention_sequences = attention_data['attention_sequences']
        true_labels = attention_data['true_labels']
        
        # å¤„ç†å¤šå¤´æ³¨æ„åŠ›
        if len(attention_sequences.shape) == 4:
            n_samples, seq_len, n_heads, spatial_size = attention_sequences.shape
            # å–æ‰€æœ‰å¤´çš„å¹³å‡
            attention_sequences_avg = np.mean(attention_sequences, axis=2)
            print(f"ğŸ” å¤šå¤´æ³¨æ„åŠ› ({n_heads} ä¸ªå¤´) å·²å¹³å‡å¤„ç†")
        else:
            n_samples, seq_len, spatial_size = attention_sequences.shape
            attention_sequences_avg = attention_sequences
            n_heads = 1
        
        grid_size = int(np.sqrt(spatial_size))
        
        if grid_size * grid_size != spatial_size:
            print(f"âš ï¸ ç©ºé—´å¤§å° {spatial_size} ä¸æ˜¯å®Œå…¨å¹³æ–¹æ•°ï¼Œä½¿ç”¨1Då¯è§†åŒ–")
            self._plot_1d_spatial_analysis(attention_data, save_path)
            return
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        # 1. æ•´ä½“å¹³å‡ç©ºé—´æ³¨æ„åŠ›
        overall_avg = np.mean(attention_sequences_avg, axis=(0, 1))  # å¹³å‡æ‰€æœ‰æ ·æœ¬å’Œæ—¶åˆ»
        overall_heatmap = overall_avg.reshape(grid_size, grid_size)
        
        im1 = axes[0, 0].imshow(overall_heatmap, cmap='viridis')
        axes[0, 0].set_title('Overall Average Spatial Attention')
        axes[0, 0].axis('off')
        plt.colorbar(im1, ax=axes[0, 0])
        
        # 2. åˆå§‹vsæœ€ç»ˆæ—¶åˆ»å¯¹æ¯”
        initial_avg = np.mean(attention_sequences_avg[:, 0, :], axis=0).reshape(grid_size, grid_size)
        final_avg = np.mean(attention_sequences_avg[:, -1, :], axis=0).reshape(grid_size, grid_size)
        
        im2 = axes[0, 1].imshow(initial_avg, cmap='viridis')
        axes[0, 1].set_title('Initial Time Step Attention')
        axes[0, 1].axis('off')
        plt.colorbar(im2, ax=axes[0, 1])
        
        im3 = axes[0, 2].imshow(final_avg, cmap='viridis')
        axes[0, 2].set_title('Final Time Step Attention')
        axes[0, 2].axis('off')
        plt.colorbar(im3, ax=axes[0, 2])
        
        # 3. ä¸åŒç±»åˆ«çš„ç©ºé—´æ³¨æ„åŠ›å¯¹æ¯”
        unique_labels = np.unique(true_labels)
        if len(unique_labels) >= 2:
            label1, label2 = unique_labels[0], unique_labels[-1]
            
            class1_attention = np.mean(attention_sequences_avg[true_labels == label1], axis=(0, 1))
            class2_attention = np.mean(attention_sequences_avg[true_labels == label2], axis=(0, 1))
            
            class1_heatmap = class1_attention.reshape(grid_size, grid_size)
            
            im4 = axes[1, 0].imshow(class1_heatmap, cmap='viridis')
            axes[1, 0].set_title(f'Class {label1} Average Attention')
            axes[1, 0].axis('off')
            plt.colorbar(im4, ax=axes[1, 0])
            
            # è®¡ç®—å¹¶æ˜¾ç¤ºç±»åˆ«å·®å¼‚
            diff_heatmap = class2_attention.reshape(grid_size, grid_size) - class1_heatmap
            
            im5 = axes[1, 1].imshow(diff_heatmap, cmap='RdBu_r')
            axes[1, 1].set_title(f'Attention Difference\n(Class {label2} - Class {label1})')
            axes[1, 1].axis('off')
            plt.colorbar(im5, ax=axes[1, 1])
        
        # 4. ç©ºé—´æ³¨æ„åŠ›çš„æ—¶åºå˜åŒ–
        temporal_change = np.std(attention_sequences_avg, axis=1)  # [samples, spatial]
        mean_temporal_change = np.mean(temporal_change, axis=0).reshape(grid_size, grid_size)
        
        im6 = axes[1, 2].imshow(mean_temporal_change, cmap='plasma')
        axes[1, 2].set_title('Temporal Variability in Spatial Attention')
        axes[1, 2].axis('off')
        plt.colorbar(im6, ax=axes[1, 2])
        
        plt.tight_layout()
        
        if save_path:
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_1d_spatial_analysis(self, attention_data, save_path=None):
        """1Dç©ºé—´æ³¨æ„åŠ›åˆ†æï¼ˆå½“ç©ºé—´ç»´åº¦ä¸æ˜¯å®Œå…¨å¹³æ–¹æ•°æ—¶ï¼‰ - æ”¯æŒå¤šå¤´"""
        attention_sequences = attention_data['attention_sequences']
        true_labels = attention_data['true_labels']
        
        # å¤„ç†å¤šå¤´æ³¨æ„åŠ›
        if len(attention_sequences.shape) == 4:
            # å–å¹³å‡
            attention_sequences = np.mean(attention_sequences, axis=2)
            print("ğŸ” 1Dåˆ†æ: å¤šå¤´æ³¨æ„åŠ›å·²å¹³å‡å¤„ç†")
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 10))
        
        # 1. æ•´ä½“ç©ºé—´æ³¨æ„åŠ›åˆ†å¸ƒ
        overall_avg = np.mean(attention_sequences, axis=(0, 1))
        
        axes[0, 0].bar(range(len(overall_avg)), overall_avg, alpha=0.7)
        axes[0, 0].set_title('Overall Average Spatial Attention')
        axes[0, 0].set_xlabel('Spatial Dimension')
        axes[0, 0].set_ylabel('Attention Weight')
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. æ—¶åºæ¼”åŒ–çƒ­åŠ›å›¾
        mean_attention_over_time = np.mean(attention_sequences, axis=0)  # [seq_len, spatial]
        
        im = axes[0, 1].imshow(mean_attention_over_time.T, cmap='viridis', aspect='auto')
        axes[0, 1].set_title('Spatial Attention Evolution Over Time')
        axes[0, 1].set_xlabel('Time Step')
        axes[0, 1].set_ylabel('Spatial Dimension')
        plt.colorbar(im, ax=axes[0, 1])
        
        # 3. ä¸åŒç±»åˆ«çš„ç©ºé—´æ³¨æ„åŠ›
        unique_labels = np.unique(true_labels)
        colors = plt.cm.tab10(np.linspace(0, 1, len(unique_labels)))
        
        for i, label in enumerate(unique_labels):
            class_mask = true_labels == label
            if np.any(class_mask):
                class_attention = np.mean(attention_sequences[class_mask], axis=(0, 1))
                axes[1, 0].plot(class_attention, color=colors[i], linewidth=2, 
                               label=f'Class {label}')
        
        axes[1, 0].set_title('Spatial Attention by Class')
        axes[1, 0].set_xlabel('Spatial Dimension')
        axes[1, 0].set_ylabel('Attention Weight')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # 4. ç©ºé—´æ³¨æ„åŠ›çš„æ–¹å·®
        spatial_variance = np.var(attention_sequences, axis=(0, 1))
        
        axes[1, 1].bar(range(len(spatial_variance)), spatial_variance, 
                      alpha=0.7, color='orange')
        axes[1, 1].set_title('Spatial Attention Variance')
        axes[1, 1].set_xlabel('Spatial Dimension')
        axes[1, 1].set_ylabel('Variance')
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
    
    def plot_multi_head_analysis(self, attention_data, save_path=None):
        """ä¸“é—¨çš„å¤šå¤´æ³¨æ„åŠ›åˆ†æ"""
        if attention_data['attention_sequences'] is None:
            print("âš ï¸ æ²¡æœ‰æ³¨æ„åŠ›æ•°æ®ï¼Œè·³è¿‡å¤šå¤´åˆ†æ")
            return
        
        attention_sequences = attention_data['attention_sequences']
        
        if len(attention_sequences.shape) != 4:
            print("âš ï¸ ä¸æ˜¯å¤šå¤´æ³¨æ„åŠ›ï¼Œè·³è¿‡å¤šå¤´åˆ†æ")
            return
        
        n_samples, seq_len, n_heads, spatial_size = attention_sequences.shape
        true_labels = attention_data['true_labels']
        
        print(f"ğŸ” å¤šå¤´æ³¨æ„åŠ›åˆ†æ: {n_heads} ä¸ªå¤´")
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        # 1. æ¯ä¸ªå¤´çš„å¹³å‡æ³¨æ„åŠ›é›†ä¸­åº¦
        head_concentrations = []
        for head_idx in range(n_heads):
            head_attention = attention_sequences[:, :, head_idx, :]  # [samples, seq_len, spatial]
            
            head_conc = []
            for sample_idx in range(n_samples):
                sample_conc = []
                for t in range(seq_len):
                    attention_t = head_attention[sample_idx, t] + 1e-8
                    entropy = -np.sum(attention_t * np.log(attention_t))
                    sample_conc.append(1.0 / (entropy + 1e-8))
                head_conc.append(sample_conc)
            head_concentrations.append(np.array(head_conc))
        
        # ç»˜åˆ¶æ¯ä¸ªå¤´çš„å¹³å‡é›†ä¸­åº¦
        time_steps = range(seq_len)
        colors = plt.cm.tab10(np.linspace(0, 1, n_heads))
        
        for head_idx in range(n_heads):
            mean_conc = np.mean(head_concentrations[head_idx], axis=0)
            axes[0, 0].plot(time_steps, mean_conc, color=colors[head_idx], 
                           linewidth=2, marker='o', label=f'Head {head_idx+1}')
        
        # å¹³å‡æ‰€æœ‰å¤´
        overall_mean = np.mean([np.mean(hc, axis=0) for hc in head_concentrations], axis=0)
        axes[0, 0].plot(time_steps, overall_mean, 'k--', linewidth=3, 
                       label='Average All Heads', alpha=0.8)
        
        axes[0, 0].set_title('Multi-Head Attention Concentration')
        axes[0, 0].set_xlabel('Time Step')
        axes[0, 0].set_ylabel('Concentration')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. å¤´ä¹‹é—´çš„ç›¸å…³æ€§çŸ©é˜µ
        head_correlations = np.zeros((n_heads, n_heads))
        
        for i in range(n_heads):
            for j in range(n_heads):
                # è®¡ç®—ä¸¤ä¸ªå¤´åœ¨æ‰€æœ‰æ ·æœ¬å’Œæ—¶åˆ»çš„ç›¸å…³æ€§
                head_i_flat = attention_sequences[:, :, i, :].flatten()
                head_j_flat = attention_sequences[:, :, j, :].flatten()
                correlation = np.corrcoef(head_i_flat, head_j_flat)[0, 1]
                head_correlations[i, j] = correlation
        
        im_corr = axes[0, 1].imshow(head_correlations, cmap='RdBu_r', vmin=-1, vmax=1)
        axes[0, 1].set_title('Head-to-Head Correlation Matrix')
        axes[0, 1].set_xlabel('Head Index')
        axes[0, 1].set_ylabel('Head Index')
        
        # æ·»åŠ æ•°å€¼æ ‡æ³¨
        for i in range(n_heads):
            for j in range(n_heads):
                text = axes[0, 1].text(j, i, f'{head_correlations[i, j]:.2f}',
                                     ha="center", va="center", color="black" if abs(head_correlations[i, j]) < 0.5 else "white")
        
        plt.colorbar(im_corr, ax=axes[0, 1])
        
        # 3. æ¯ä¸ªå¤´çš„ç©ºé—´æ³¨æ„åŠ›åˆ†å¸ƒæ–¹å·®
        head_spatial_vars = []
        for head_idx in range(n_heads):
            head_attention = attention_sequences[:, :, head_idx, :]
            # è®¡ç®—æ¯ä¸ªç©ºé—´ä½ç½®çš„æ–¹å·®
            spatial_var = np.var(head_attention, axis=(0, 1))  # [spatial]
            head_spatial_vars.append(np.mean(spatial_var))  # å¹³å‡æ–¹å·®
        
        axes[0, 2].bar(range(n_heads), head_spatial_vars, color=colors, alpha=0.7)
        axes[0, 2].set_title('Spatial Attention Variance by Head')
        axes[0, 2].set_xlabel('Head Index')
        axes[0, 2].set_ylabel('Mean Spatial Variance')
        axes[0, 2].set_xticks(range(n_heads))
        axes[0, 2].set_xticklabels([f'H{i+1}' for i in range(n_heads)])
        axes[0, 2].grid(True, alpha=0.3)
        
        # 4. ä¸åŒç±»åˆ«ä¸‹å„å¤´çš„è¡¨ç°
        unique_labels = np.unique(true_labels)
        if len(unique_labels) >= 2:
            label1, label2 = unique_labels[0], unique_labels[-1]
            
            class1_mask = true_labels == label1
            class2_mask = true_labels == label2
            
            class1_head_conc = []
            class2_head_conc = []
            
            for head_idx in range(n_heads):
                class1_conc = np.mean(head_concentrations[head_idx][class1_mask])
                class2_conc = np.mean(head_concentrations[head_idx][class2_mask])
                class1_head_conc.append(class1_conc)
                class2_head_conc.append(class2_conc)
            
            x = np.arange(n_heads)
            width = 0.35
            
            axes[1, 0].bar(x - width/2, class1_head_conc, width, label=f'Class {label1}', alpha=0.7)
            axes[1, 0].bar(x + width/2, class2_head_conc, width, label=f'Class {label2}', alpha=0.7)
            
            axes[1, 0].set_title('Head Performance by Class')
            axes[1, 0].set_xlabel('Head Index')
            axes[1, 0].set_ylabel('Mean Concentration')
            axes[1, 0].set_xticks(x)
            axes[1, 0].set_xticklabels([f'H{i+1}' for i in range(n_heads)])
            axes[1, 0].legend()
            axes[1, 0].grid(True, alpha=0.3)
        
        # 5. å¤´çš„å¤šæ ·æ€§åˆ†æ
        # è®¡ç®—æ¯ä¸ªæ—¶åˆ»å„å¤´æ³¨æ„åŠ›çš„æ ‡å‡†å·®ï¼ˆå¤šæ ·æ€§æŒ‡æ ‡ï¼‰
        diversity_over_time = []
        for t in range(seq_len):
            time_diversity = []
            for sample_idx in range(n_samples):
                # å½“å‰æ ·æœ¬å½“å‰æ—¶åˆ»æ‰€æœ‰å¤´çš„æ³¨æ„åŠ›
                heads_attention = attention_sequences[sample_idx, t, :, :]  # [n_heads, spatial]
                # è®¡ç®—å¤´ä¹‹é—´çš„æ ‡å‡†å·®
                head_std = np.std(heads_attention, axis=0)  # [spatial]
                time_diversity.append(np.mean(head_std))
            diversity_over_time.append(np.mean(time_diversity))
        
        axes[1, 1].plot(time_steps, diversity_over_time, 'purple', linewidth=2, marker='o')
        axes[1, 1].set_title('Head Diversity Over Time')
        axes[1, 1].set_xlabel('Time Step')
        axes[1, 1].set_ylabel('Mean Head Diversity')
        axes[1, 1].grid(True, alpha=0.3)
        
        # 6. æœ€æœ‰æ•ˆçš„å¤´è¯†åˆ«
        # åŸºäºä¸æœ€ç»ˆé¢„æµ‹å‡†ç¡®æ€§çš„ç›¸å…³æ€§
        final_predictions = attention_data['final_predictions']
        correct_mask = final_predictions == true_labels
        
        head_accuracy_correlation = []
        for head_idx in range(n_heads):
            head_final_conc = np.mean(head_concentrations[head_idx], axis=1)  # æ¯ä¸ªæ ·æœ¬çš„å¹³å‡é›†ä¸­åº¦
            
            # è®¡ç®—ä¸å‡†ç¡®æ€§çš„ç›¸å…³æ€§
            if len(np.unique(correct_mask)) > 1:  # ç¡®ä¿æœ‰æ­£ç¡®å’Œé”™è¯¯çš„æ ·æœ¬
                correlation = np.corrcoef(head_final_conc, correct_mask.astype(float))[0, 1]
            else:
                correlation = 0.0
            head_accuracy_correlation.append(correlation)
        
        axes[1, 2].bar(range(n_heads), head_accuracy_correlation, 
                      color=['green' if c > 0 else 'red' for c in head_accuracy_correlation], 
                      alpha=0.7)
        axes[1, 2].set_title('Head-Accuracy Correlation')
        axes[1, 2].set_xlabel('Head Index')
        axes[1, 2].set_ylabel('Correlation with Accuracy')
        axes[1, 2].set_xticks(range(n_heads))
        axes[1, 2].set_xticklabels([f'H{i+1}' for i in range(n_heads)])
        axes[1, 2].axhline(y=0, color='black', linestyle='-', alpha=0.3)
        axes[1, 2].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
    
    def generate_attention_report(self, attention_data, save_path=None):
        """ç”Ÿæˆè¯¦ç»†çš„æ³¨æ„åŠ›åˆ†ææŠ¥å‘Š - æ”¯æŒå¤šå¤´"""
        
        if attention_data['attention_sequences'] is None:
            print("âš ï¸ æ²¡æœ‰æ³¨æ„åŠ›æ•°æ®ï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Š")
            return None
        
        attention_sequences = attention_data['attention_sequences']
        true_labels = attention_data['true_labels']
        final_predictions = attention_data['final_predictions']
        counting_sequences = attention_data['counting_sequences']
        
        # æ£€æŸ¥å¤šå¤´æ³¨æ„åŠ›
        is_multi_head = len(attention_sequences.shape) == 4
        if is_multi_head:
            n_samples, seq_len, n_heads, spatial_size = attention_sequences.shape
            # ä¸ºæŠ¥å‘Šä½¿ç”¨å¹³å‡æ³¨æ„åŠ›
            attention_sequences_avg = np.mean(attention_sequences, axis=2)
        else:
            n_samples, seq_len, spatial_size = attention_sequences.shape
            attention_sequences_avg = attention_sequences
            n_heads = 1
        
        # è®¡ç®—å„ç§ç»Ÿè®¡æŒ‡æ ‡
        overall_accuracy = np.mean(final_predictions == true_labels)
        
        # æ³¨æ„åŠ›é›†ä¸­åº¦åˆ†æ
        attention_concentration = []
        for seq in attention_sequences_avg:
            concentration = []
            for t in range(seq.shape[0]):
                attention_t = seq[t] + 1e-8
                entropy = -np.sum(attention_t * np.log(attention_t))
                concentration.append(1.0 / (entropy + 1e-8))
            attention_concentration.append(concentration)
        
        attention_concentration = np.array(attention_concentration)
        
        # æ„å»ºæŠ¥å‘Š
        report = {
            'analysis_summary': {
                'total_samples': int(n_samples),
                'sequence_length': int(seq_len),
                'spatial_dimension': int(spatial_size),
                'is_multi_head': is_multi_head,
                'num_heads': int(n_heads),
                'overall_accuracy': float(overall_accuracy),
                'unique_classes': len(np.unique(true_labels))
            },
            'attention_concentration_stats': {
                'initial_mean': float(np.mean(attention_concentration[:, 0])),
                'initial_std': float(np.std(attention_concentration[:, 0])),
                'final_mean': float(np.mean(attention_concentration[:, -1])),
                'final_std': float(np.std(attention_concentration[:, -1])),
                'mean_change': float(np.mean(attention_concentration[:, -1] - attention_concentration[:, 0])),
                'max_concentration': float(np.max(attention_concentration)),
                'min_concentration': float(np.min(attention_concentration))
            }
        }
        
        # å¤šå¤´æ³¨æ„åŠ›ç‰¹æœ‰åˆ†æ
        if is_multi_head:
            # è®¡ç®—å¤´ä¹‹é—´çš„ç›¸å…³æ€§
            head_correlations = []
            for i in range(n_heads):
                for j in range(i+1, n_heads):
                    head_i_flat = attention_sequences[:, :, i, :].flatten()
                    head_j_flat = attention_sequences[:, :, j, :].flatten()
                    correlation = np.corrcoef(head_i_flat, head_j_flat)[0, 1]
                    head_correlations.append(correlation)
            
            report['multi_head_analysis'] = {
                'mean_inter_head_correlation': float(np.mean(head_correlations)),
                'std_inter_head_correlation': float(np.std(head_correlations)),
                'max_correlation': float(np.max(head_correlations)),
                'min_correlation': float(np.min(head_correlations))
            }
        
        # å‡†ç¡®æ€§ç›¸å…³åˆ†æ
        correct_mask = final_predictions == true_labels
        if np.any(correct_mask) and np.any(~correct_mask):
            correct_final_conc = attention_concentration[correct_mask, -1]
            error_final_conc = attention_concentration[~correct_mask, -1]
            
            report['accuracy_correlation'] = {
                'correct_samples': int(np.sum(correct_mask)),
                'error_samples': int(np.sum(~correct_mask)),
                'correct_mean_concentration': float(np.mean(correct_final_conc)),
                'error_mean_concentration': float(np.mean(error_final_conc)),
                'concentration_difference': float(np.mean(correct_final_conc) - np.mean(error_final_conc)),
                'statistical_significance': self._compute_significance(correct_final_conc, error_final_conc)
            }
        
        # ç±»åˆ«ç‰¹å¼‚æ€§åˆ†æ
        unique_labels = np.unique(true_labels)
        class_stats = {}
        
        for label in unique_labels:
            class_mask = true_labels == label
            if np.any(class_mask):
                class_concentration = attention_concentration[class_mask]
                class_accuracy = np.mean(final_predictions[class_mask] == label)
                
                class_stats[f'class_{label}'] = {
                    'sample_count': int(np.sum(class_mask)),
                    'accuracy': float(class_accuracy),
                    'initial_concentration_mean': float(np.mean(class_concentration[:, 0])),
                    'final_concentration_mean': float(np.mean(class_concentration[:, -1])),
                    'concentration_change_mean': float(np.mean(class_concentration[:, -1] - class_concentration[:, 0])),
                    'concentration_stability': float(np.std(np.std(class_concentration, axis=1)))
                }
        
        report['class_specific_attention'] = class_stats
        
        # æ—¶åºåˆ†æ
        step_accuracies = []
        for t in range(seq_len):
            step_accuracy = np.mean(counting_sequences[:, t] == true_labels)
            step_accuracies.append(step_accuracy)
        
        report['temporal_analysis'] = {
            'initial_accuracy': float(step_accuracies[0]),
            'final_accuracy': float(step_accuracies[-1]),
            'max_accuracy': float(np.max(step_accuracies)),
            'accuracy_improvement': float(step_accuracies[-1] - step_accuracies[0]),
            'convergence_step': int(np.argmax(step_accuracies))
        }
        
        # ä¿å­˜æŠ¥å‘Š
        if save_path:
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            
            # JSONæ ¼å¼
            json_path = save_path.replace('.txt', '.json')
            with open(json_path, 'w') as f:
                json.dump(report, f, indent=2)
            
            # æ–‡æœ¬æ ¼å¼
            with open(save_path, 'w', encoding='utf-8') as f:
                f.write("=== æ·±åº¦æ³¨æ„åŠ›æœºåˆ¶åˆ†ææŠ¥å‘Š ===\n\n")
                
                # åŸºç¡€ç»Ÿè®¡
                summary = report['analysis_summary']
                f.write("åŸºç¡€ç»Ÿè®¡ä¿¡æ¯:\n")
                f.write(f"  æ€»æ ·æœ¬æ•°: {summary['total_samples']}\n")
                f.write(f"  åºåˆ—é•¿åº¦: {summary['sequence_length']}\n")
                f.write(f"  ç©ºé—´ç»´åº¦: {summary['spatial_dimension']}\n")
                f.write(f"  å¤šå¤´æ³¨æ„åŠ›: {'æ˜¯' if summary['is_multi_head'] else 'å¦'}\n")
                if summary['is_multi_head']:
                    f.write(f"  æ³¨æ„åŠ›å¤´æ•°: {summary['num_heads']}\n")
                f.write(f"  æ•´ä½“å‡†ç¡®ç‡: {summary['overall_accuracy']:.4f}\n")
                f.write(f"  ç±»åˆ«æ•°é‡: {summary['unique_classes']}\n\n")
                
                # å¤šå¤´æ³¨æ„åŠ›åˆ†æ
                if 'multi_head_analysis' in report:
                    mha = report['multi_head_analysis']
                    f.write("å¤šå¤´æ³¨æ„åŠ›åˆ†æ:\n")
                    f.write(f"  å¤´é—´å¹³å‡ç›¸å…³æ€§: {mha['mean_inter_head_correlation']:.4f} Â± {mha['std_inter_head_correlation']:.4f}\n")
                    f.write(f"  æœ€å¤§å¤´é—´ç›¸å…³æ€§: {mha['max_correlation']:.4f}\n")
                    f.write(f"  æœ€å°å¤´é—´ç›¸å…³æ€§: {mha['min_correlation']:.4f}\n\n")
                
                # æ³¨æ„åŠ›é›†ä¸­åº¦åˆ†æ
                conc_stats = report['attention_concentration_stats']
                f.write("æ³¨æ„åŠ›é›†ä¸­åº¦åˆ†æ:\n")
                f.write(f"  åˆå§‹é›†ä¸­åº¦: {conc_stats['initial_mean']:.4f} Â± {conc_stats['initial_std']:.4f}\n")
                f.write(f"  æœ€ç»ˆé›†ä¸­åº¦: {conc_stats['final_mean']:.4f} Â± {conc_stats['final_std']:.4f}\n")
                f.write(f"  å¹³å‡å˜åŒ–é‡: {conc_stats['mean_change']:.4f}\n")
                f.write(f"  é›†ä¸­åº¦èŒƒå›´: [{conc_stats['min_concentration']:.4f}, {conc_stats['max_concentration']:.4f}]\n\n")
                
                # å‡†ç¡®æ€§å…³è”åˆ†æ
                if 'accuracy_correlation' in report:
                    acc_corr = report['accuracy_correlation']
                    f.write("å‡†ç¡®æ€§å…³è”åˆ†æ:\n")
                    f.write(f"  æ­£ç¡®é¢„æµ‹æ ·æœ¬: {acc_corr['correct_samples']}\n")
                    f.write(f"  é”™è¯¯é¢„æµ‹æ ·æœ¬: {acc_corr['error_samples']}\n")
                    f.write(f"  æ­£ç¡®é¢„æµ‹çš„å¹³å‡æ³¨æ„åŠ›é›†ä¸­åº¦: {acc_corr['correct_mean_concentration']:.4f}\n")
                    f.write(f"  é”™è¯¯é¢„æµ‹çš„å¹³å‡æ³¨æ„åŠ›é›†ä¸­åº¦: {acc_corr['error_mean_concentration']:.4f}\n")
                    f.write(f"  é›†ä¸­åº¦å·®å¼‚: {acc_corr['concentration_difference']:.4f}\n")
                    f.write(f"  ç»Ÿè®¡æ˜¾è‘—æ€§: {acc_corr['statistical_significance']}\n\n")
                
                # ç±»åˆ«ç‰¹å¼‚æ€§åˆ†æ
                f.write("ç±»åˆ«ç‰¹å¼‚æ€§æ³¨æ„åŠ›åˆ†æ:\n")
                for class_name, stats in report['class_specific_attention'].items():
                    f.write(f"  {class_name}:\n")
                    f.write(f"    æ ·æœ¬æ•°: {stats['sample_count']}\n")
                    f.write(f"    å‡†ç¡®ç‡: {stats['accuracy']:.4f}\n")
                    f.write(f"    åˆå§‹é›†ä¸­åº¦: {stats['initial_concentration_mean']:.4f}\n")
                    f.write(f"    æœ€ç»ˆé›†ä¸­åº¦: {stats['final_concentration_mean']:.4f}\n")
                    f.write(f"    é›†ä¸­åº¦å˜åŒ–: {stats['concentration_change_mean']:.4f}\n")
                    f.write(f"    æ³¨æ„åŠ›ç¨³å®šæ€§: {stats['concentration_stability']:.4f}\n")
                f.write("\n")
                
                # æ—¶åºåˆ†æ
                temporal = report['temporal_analysis']
                f.write("æ—¶åºæ¼”åŒ–åˆ†æ:\n")
                f.write(f"  åˆå§‹å‡†ç¡®ç‡: {temporal['initial_accuracy']:.4f}\n")
                f.write(f"  æœ€ç»ˆå‡†ç¡®ç‡: {temporal['final_accuracy']:.4f}\n")
                f.write(f"  æœ€é«˜å‡†ç¡®ç‡: {temporal['max_accuracy']:.4f}\n")
                f.write(f"  å‡†ç¡®ç‡æå‡: {temporal['accuracy_improvement']:.4f}\n")
                f.write(f"  æ”¶æ•›æ­¥éª¤: {temporal['convergence_step']}\n\n")
                
                # åˆ†æå»ºè®®
                f.write("åˆ†æå»ºè®®:\n")
                recommendations = self._generate_recommendations(report)
                for rec in recommendations:
                    f.write(f"  â€¢ {rec}\n")
            
            print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜: {save_path}")
            print(f"âœ… JSONæ•°æ®å·²ä¿å­˜: {json_path}")
        
        return report
    
    def _compute_significance(self, correct_data, error_data):
        """è®¡ç®—ç»Ÿè®¡æ˜¾è‘—æ€§"""
        try:
            from scipy import stats
            t_stat, p_value = stats.ttest_ind(correct_data, error_data)
            if p_value < 0.001:
                return "é«˜åº¦æ˜¾è‘— (p < 0.001)"
            elif p_value < 0.01:
                return "æ˜¾è‘— (p < 0.01)"
            elif p_value < 0.05:
                return "è¾¹é™…æ˜¾è‘— (p < 0.05)"
            else:
                return "ä¸æ˜¾è‘— (p >= 0.05)"
        except:
            return "æ— æ³•è®¡ç®—"
    
    def _generate_recommendations(self, report):
        """åŸºäºåˆ†æç»“æœç”Ÿæˆå»ºè®®"""
        recommendations = []
        
        # åŸºäºå‡†ç¡®ç‡çš„å»ºè®®
        accuracy = report['analysis_summary']['overall_accuracy']
        if accuracy < 0.7:
            recommendations.append("æ•´ä½“å‡†ç¡®ç‡è¾ƒä½ï¼Œæ³¨æ„åŠ›æœºåˆ¶å¯èƒ½éœ€è¦æ”¹è¿›")
        elif accuracy > 0.9:
            recommendations.append("å‡†ç¡®ç‡å¾ˆé«˜ï¼Œæ³¨æ„åŠ›æœºåˆ¶è¿ä½œè‰¯å¥½")
        
        # å¤šå¤´æ³¨æ„åŠ›å»ºè®®
        if report['analysis_summary']['is_multi_head']:
            if 'multi_head_analysis' in report:
                mha = report['multi_head_analysis']
                if mha['mean_inter_head_correlation'] > 0.8:
                    recommendations.append("æ³¨æ„åŠ›å¤´ä¹‹é—´ç›¸å…³æ€§è¿‡é«˜ï¼Œå¯èƒ½å­˜åœ¨å†—ä½™ï¼Œå»ºè®®å‡å°‘å¤´æ•°æˆ–å¢åŠ å¤šæ ·æ€§")
                elif mha['mean_inter_head_correlation'] < 0.3:
                    recommendations.append("æ³¨æ„åŠ›å¤´ä¹‹é—´ç›¸å…³æ€§è¾ƒä½ï¼Œæ˜¾ç¤ºäº†è‰¯å¥½çš„å¤šæ ·æ€§")
        
        # åŸºäºæ³¨æ„åŠ›é›†ä¸­åº¦çš„å»ºè®®
        conc_stats = report['attention_concentration_stats']
        if conc_stats['mean_change'] > 0:
            recommendations.append("æ³¨æ„åŠ›é›†ä¸­åº¦éšæ—¶é—´å¢åŠ ï¼Œæ˜¾ç¤ºäº†è‰¯å¥½çš„å­¦ä¹ è¿‡ç¨‹")
        else:
            recommendations.append("æ³¨æ„åŠ›é›†ä¸­åº¦éšæ—¶é—´å‡å°‘ï¼Œå¯èƒ½å­˜åœ¨æ³¨æ„åŠ›åˆ†æ•£é—®é¢˜")
        
        # åŸºäºå‡†ç¡®æ€§å…³è”çš„å»ºè®®
        if 'accuracy_correlation' in report:
            acc_corr = report['accuracy_correlation']
            if acc_corr['concentration_difference'] > 0:
                recommendations.append("æ­£ç¡®é¢„æµ‹å…·æœ‰æ›´é«˜çš„æ³¨æ„åŠ›é›†ä¸­åº¦ï¼Œæ³¨æ„åŠ›è´¨é‡ä¸å‡†ç¡®æ€§æ­£ç›¸å…³")
            else:
                recommendations.append("é”™è¯¯é¢„æµ‹çš„æ³¨æ„åŠ›é›†ä¸­åº¦æ›´é«˜ï¼Œå¯èƒ½å­˜åœ¨è¿‡åº¦å…³æ³¨é”™è¯¯ç‰¹å¾çš„é—®é¢˜")
        
        # åŸºäºæ—¶åºåˆ†æçš„å»ºè®®
        temporal = report['temporal_analysis']
        if temporal['accuracy_improvement'] > 0.1:
            recommendations.append("éšæ—¶é—´å‡†ç¡®ç‡æ˜¾è‘—æå‡ï¼Œæ—¶åºå»ºæ¨¡æ•ˆæœè‰¯å¥½")
        elif temporal['accuracy_improvement'] < 0:
            recommendations.append("å‡†ç¡®ç‡éšæ—¶é—´ä¸‹é™ï¼Œå¯èƒ½å­˜åœ¨æ¢¯åº¦æ¶ˆå¤±æˆ–è¿‡æ‹Ÿåˆé—®é¢˜")
        
        return recommendations


def comprehensive_attention_analysis(checkpoint_path, val_csv, data_root, 
                                   save_dir='./attention_analysis', 
                                   max_samples=300):
    """å®Œæ•´çš„æ³¨æ„åŠ›æœºåˆ¶åˆ†ææµæ°´çº¿ - æ”¯æŒå¤šå¤´æ³¨æ„åŠ›"""
    
    print("ğŸ” å¼€å§‹æ·±åº¦æ³¨æ„åŠ›æœºåˆ¶åˆ†æ...")
    print("="*60)
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(save_dir, exist_ok=True)
    
    try:
        # 1. åŠ è½½æ¨¡å‹å’Œæ•°æ®
        model, val_loader, device, config = load_model_and_data(
            checkpoint_path, val_csv, data_root
        )
        
        # 2. åˆ›å»ºæ³¨æ„åŠ›åˆ†æå™¨
        analyzer = AttentionAnalyzer()
        
        # 3. æå–æ³¨æ„åŠ›æ•°æ®
        print("\nğŸ“Š ç¬¬1æ­¥: æå–æ³¨æ„åŠ›æœºåˆ¶æ•°æ®")
        attention_data = analyzer.extract_attention_data(
            model, val_loader, max_samples, device
        )
        
        if attention_data['attention_sequences'] is None:
            print("âŒ æ¨¡å‹ä¸æ”¯æŒæ³¨æ„åŠ›æƒé‡æå–ï¼Œåˆ†æç»ˆæ­¢")
            print("ğŸ’¡ è¯·ç¡®ä¿æ¨¡å‹çš„forwardæ–¹æ³•æ”¯æŒreturn_attention=Trueå‚æ•°")
            return None
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºå¤šå¤´æ³¨æ„åŠ›
        is_multi_head = len(attention_data['attention_sequences'].shape) == 4
        if is_multi_head:
            n_heads = attention_data['attention_sequences'].shape[2]
            print(f"ğŸ¯ æ£€æµ‹åˆ°å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶: {n_heads} ä¸ªæ³¨æ„åŠ›å¤´")
        
        # 4. æ³¨æ„åŠ›æ¼”åŒ–åˆ†æ
        print("\nğŸ“ˆ ç¬¬2æ­¥: æ³¨æ„åŠ›æ¼”åŒ–åˆ†æ")
        analyzer.plot_attention_evolution_detailed(
            attention_data,
            save_path=os.path.join(save_dir, 'attention_evolution_detailed.png')
        )
        
        # 5. æ³¨æ„åŠ›ä¸å‡†ç¡®æ€§å…³è”åˆ†æ
        print("\nğŸ¯ ç¬¬3æ­¥: æ³¨æ„åŠ›ä¸å‡†ç¡®æ€§å…³è”åˆ†æ")
        analyzer.plot_attention_accuracy_analysis(
            attention_data,
            save_path=os.path.join(save_dir, 'attention_accuracy_analysis.png')
        )
        
        # 6. ç©ºé—´æ³¨æ„åŠ›åˆ†æ
        print("\nğŸ—ºï¸ ç¬¬4æ­¥: ç©ºé—´æ³¨æ„åŠ›åˆ†æ")
        analyzer.plot_spatial_attention_analysis(
            attention_data,
            save_path=os.path.join(save_dir, 'spatial_attention_analysis.png')
        )
        
        # 7. å¤šå¤´æ³¨æ„åŠ›ä¸“é—¨åˆ†æï¼ˆå¦‚æœé€‚ç”¨ï¼‰
        if is_multi_head:
            print("\nğŸ§  ç¬¬5æ­¥: å¤šå¤´æ³¨æ„åŠ›ä¸“é—¨åˆ†æ")
            analyzer.plot_multi_head_analysis(
                attention_data,
                save_path=os.path.join(save_dir, 'multi_head_attention_analysis.png')
            )
        
        # 8. ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        print(f"\nğŸ“ ç¬¬{'6' if is_multi_head else '5'}æ­¥: ç”Ÿæˆåˆ†ææŠ¥å‘Š")
        report = analyzer.generate_attention_report(
            attention_data,
            save_path=os.path.join(save_dir, 'attention_analysis_report.txt')
        )
        
        # 9. è¾“å‡ºç»“æœæ€»ç»“
        print("\nğŸ‰ æ³¨æ„åŠ›åˆ†æå®Œæˆï¼")
        print("="*60)
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {save_dir}")
        print("\nç”Ÿæˆçš„æ–‡ä»¶:")
        print("  ğŸ“ˆ attention_evolution_detailed.png     - è¯¦ç»†çš„æ³¨æ„åŠ›æ—¶åºæ¼”åŒ–åˆ†æ")
        print("  ğŸ¯ attention_accuracy_analysis.png      - æ³¨æ„åŠ›ä¸å‡†ç¡®æ€§å…³è”åˆ†æ")
        print("  ğŸ—ºï¸ spatial_attention_analysis.png       - ç©ºé—´æ³¨æ„åŠ›åˆ†å¸ƒåˆ†æ")
        if is_multi_head:
            print("  ğŸ§  multi_head_attention_analysis.png    - å¤šå¤´æ³¨æ„åŠ›ä¸“é—¨åˆ†æ")
        print("  ğŸ“ attention_analysis_report.txt        - è¯¦ç»†æ–‡å­—æŠ¥å‘Š")
        print("  ğŸ“Š attention_analysis_report.json       - ç»“æ„åŒ–æ•°æ®æŠ¥å‘Š")
        
        if report:
            print(f"\nğŸ“Š åˆ†ææ‘˜è¦:")
            summary = report['analysis_summary']
            print(f"   æ€»æ ·æœ¬æ•°: {summary['total_samples']}")
            print(f"   åºåˆ—é•¿åº¦: {summary['sequence_length']}")
            print(f"   ç©ºé—´ç»´åº¦: {summary['spatial_dimension']}")
            if summary['is_multi_head']:
                print(f"   æ³¨æ„åŠ›å¤´æ•°: {summary['num_heads']}")
            print(f"   æ•´ä½“å‡†ç¡®ç‡: {summary['overall_accuracy']:.4f}")
            
            if 'accuracy_correlation' in report:
                acc_corr = report['accuracy_correlation']
                print(f"   æ­£ç¡®é¢„æµ‹æ³¨æ„åŠ›é›†ä¸­åº¦: {acc_corr['correct_mean_concentration']:.4f}")
                print(f"   é”™è¯¯é¢„æµ‹æ³¨æ„åŠ›é›†ä¸­åº¦: {acc_corr['error_mean_concentration']:.4f}")
            
            if 'multi_head_analysis' in report:
                mha = report['multi_head_analysis']
                print(f"   å¤´é—´å¹³å‡ç›¸å…³æ€§: {mha['mean_inter_head_correlation']:.4f}")
        
        return attention_data, report
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def main():
    """ä¸»å…¥å£å‡½æ•°"""
    parser = argparse.ArgumentParser(description='ç‹¬ç«‹çš„æ³¨æ„åŠ›æœºåˆ¶æ·±åº¦åˆ†æå·¥å…·')
    
    # å¿…éœ€å‚æ•°
    parser.add_argument('--checkpoint', type=str, required=True,
                       help='æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--val_csv', type=str, default='scratch/Ball_counting_CNN/Tools_script/ball_counting_dataset_val.csv',
                       help='éªŒè¯é›†CSVæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--data_root', type=str, default='/mnt/iusers01/fatpou01/compsci01/k09562zs/scratch/Ball_counting_CNN/ball_data_collection',
                       help='æ•°æ®æ ¹ç›®å½•')
    
    # å¯é€‰å‚æ•°
    parser.add_argument('--save_dir', type=str, default=None,
                       help='ç»“æœä¿å­˜ç›®å½•')
    parser.add_argument('--max_samples', type=int, default=500,
                       help='æœ€å¤§åˆ†ææ ·æœ¬æ•°')
    parser.add_argument('--batch_size', type=int, default=8,
                       help='æ•°æ®åŠ è½½æ‰¹æ¬¡å¤§å°')
    
    args = parser.parse_args()
    
    # è®¾ç½®é»˜è®¤ä¿å­˜ç›®å½•
    if args.save_dir is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        args.save_dir = f'./attention_analysis_{timestamp}'
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    for path, name in [(args.checkpoint, 'æ£€æŸ¥ç‚¹æ–‡ä»¶'), 
                       (args.val_csv, 'éªŒè¯CSVæ–‡ä»¶'), 
                       (args.data_root, 'æ•°æ®æ ¹ç›®å½•')]:
        if not os.path.exists(path):
            print(f"âŒ {name}ä¸å­˜åœ¨: {path}")
            return
    
    print("ğŸ” ç‹¬ç«‹æ³¨æ„åŠ›æœºåˆ¶åˆ†æå·¥å…·")
    print("="*60)
    print(f"æ£€æŸ¥ç‚¹: {args.checkpoint}")
    print(f"éªŒè¯é›†: {args.val_csv}")
    print(f"æ•°æ®æ ¹ç›®å½•: {args.data_root}")
    print(f"ä¿å­˜ç›®å½•: {args.save_dir}")
    print(f"æœ€å¤§æ ·æœ¬æ•°: {args.max_samples}")
    print("="*60)
    
    start_time = time.time()
    
    try:
        # æ‰§è¡Œå®Œæ•´çš„æ³¨æ„åŠ›åˆ†æ
        results = comprehensive_attention_analysis(
            args.checkpoint, args.val_csv, args.data_root, 
            args.save_dir, args.max_samples
        )
        
        elapsed_time = time.time() - start_time
        
        if results:
            print(f"\nğŸ‰ åˆ†ææˆåŠŸå®Œæˆï¼")
            print(f"â±ï¸ æ€»è€—æ—¶: {elapsed_time:.2f} ç§’")
            print(f"ğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {args.save_dir}")
        else:
            print(f"\nâŒ åˆ†æå¤±è´¥")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­åˆ†æ")
    except Exception as e:
        print(f"\nâŒ åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # å¦‚æœæ²¡æœ‰å‘½ä»¤è¡Œå‚æ•°ï¼Œæ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
    if len(sys.argv) == 1:
        print("ğŸ” ç‹¬ç«‹æ³¨æ„åŠ›æœºåˆ¶æ·±åº¦åˆ†æå·¥å…·")
        print("="*60)
        print("ä¸“é—¨åˆ†æå…·èº«è®¡æ•°æ¨¡å‹çš„æ³¨æ„åŠ›æœºåˆ¶")
        print("æ”¯æŒå•å¤´å’Œå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶")
        print("æä¾›å¤šç»´åº¦ã€å¤šæ—¶åˆ»çš„æ·±åº¦æ³¨æ„åŠ›åˆ†æ")
        print()
        print("ä½¿ç”¨æ–¹æ³•:")
        print("python attention_analyzer.py \\")
        print("    --checkpoint MODEL.pth \\")
        print("    --val_csv VAL.csv \\")
        print("    --data_root DATA_DIR")
        print()
        print("å¯é€‰å‚æ•°:")
        print("  --save_dir DIR        # ç»“æœä¿å­˜ç›®å½•")
        print("  --max_samples N       # æœ€å¤§åˆ†ææ ·æœ¬æ•° (é»˜è®¤500)")
        print("  --batch_size N        # æ‰¹æ¬¡å¤§å° (é»˜è®¤8)")
        print()
        print("ç¤ºä¾‹:")
        print("python attention_analyzer.py \\")
        print("    --checkpoint ./best_model.pth \\")
        print("    --val_csv ./val.csv \\")
        print("    --data_root ./data \\")
        print("    --max_samples 500 \\")
        print("    --save_dir ./my_attention_analysis")
        print()
        print("ç”Ÿæˆçš„åˆ†æåŒ…æ‹¬:")
        print("  ğŸ“ˆ æ³¨æ„åŠ›æ—¶åºæ¼”åŒ–åˆ†æ")
        print("  ğŸ¯ æ³¨æ„åŠ›ä¸é¢„æµ‹å‡†ç¡®æ€§å…³è”")
        print("  ğŸ—ºï¸ ç©ºé—´æ³¨æ„åŠ›åˆ†å¸ƒåˆ†æ")
        print("  ğŸ§  å¤šå¤´æ³¨æ„åŠ›ä¸“é—¨åˆ†æ (å¦‚é€‚ç”¨)")
        print("  ğŸ“ è¯¦ç»†é‡åŒ–åˆ†ææŠ¥å‘Š")
        print()
        print("ğŸ’¡ æ³¨æ„: æ¨¡å‹å¿…é¡»æ”¯æŒreturn_attention=Trueå‚æ•°")
        print("ğŸ”§ æ”¯æŒå½¢çŠ¶: (batch, seq_len, spatial) æˆ– (batch, seq_len, heads, spatial)")
        sys.exit(0)
    
    main()


# =============================================================================
# ä¾¿æ·è°ƒç”¨å‡½æ•°
# =============================================================================

def quick_attention_analysis(checkpoint_path, val_csv, data_root, save_dir=None):
    """å¿«é€Ÿæ³¨æ„åŠ›åˆ†ææ¥å£"""
    if save_dir is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        save_dir = f'./quick_attention_{timestamp}'
    
    return comprehensive_attention_analysis(
        checkpoint_path, val_csv, data_root, save_dir, max_samples=100
    )


def detailed_attention_analysis(checkpoint_path, val_csv, data_root, save_dir=None, max_samples=500):
    """è¯¦ç»†æ³¨æ„åŠ›åˆ†ææ¥å£"""
    if save_dir is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        save_dir = f'./detailed_attention_{timestamp}'
    
    return comprehensive_attention_analysis(
        checkpoint_path, val_csv, data_root, save_dir, max_samples
    )


"""
=============================================================================
ä½¿ç”¨è¯´æ˜
=============================================================================

è¿™æ˜¯ä¸€ä¸ªä¿®å¤åçš„ç‹¬ç«‹æ³¨æ„åŠ›æœºåˆ¶åˆ†æå·¥å…·ï¼Œç°åœ¨æ”¯æŒå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ã€‚

ä¸»è¦æ”¹è¿›:
1. æ”¯æŒå¤šå¤´æ³¨æ„åŠ›: è‡ªåŠ¨æ£€æµ‹å½¢çŠ¶ (batch, seq_len, heads, spatial)
2. å¤šå¤´ä¸“é—¨åˆ†æ: å¤´é—´ç›¸å…³æ€§ã€å¤šæ ·æ€§ã€æ•ˆæœå¯¹æ¯”ç­‰
3. æ™ºèƒ½å½¢çŠ¶å¤„ç†: è‡ªåŠ¨é€‚é…å•å¤´å’Œå¤šå¤´æ³¨æ„åŠ›
4. å¢å¼ºçš„å¯è§†åŒ–: åŒ…å«å¤šå¤´å¯¹æ¯”å’Œåˆ†æ

æ³¨æ„åŠ›æ•°æ®å½¢çŠ¶æ”¯æŒ:
- å•å¤´: (n_samples, seq_len, spatial_size) 
- å¤šå¤´: (n_samples, seq_len, n_heads, spatial_size)

å‘½ä»¤è¡Œä½¿ç”¨:
python attention_analyzer.py \\
    --checkpoint your_model.pth \\
    --val_csv your_val.csv \\
    --data_root your_data \\
    --max_samples 300

Pythonè„šæœ¬ä½¿ç”¨:
from attention_analyzer import quick_attention_analysis, detailed_attention_analysis

# å¿«é€Ÿåˆ†æ
results = quick_attention_analysis(
    'model.pth', 'val.csv', 'data_dir'
)

# è¯¦ç»†åˆ†æ
results = detailed_attention_analysis(
    'model.pth', 'val.csv', 'data_dir', max_samples=500
)

æ³¨æ„äº‹é¡¹:
- æ¨¡å‹å¿…é¡»æ”¯æŒreturn_attention=Trueå‚æ•°
- è‡ªåŠ¨æ£€æµ‹å¹¶å¤„ç†å¤šå¤´æ³¨æ„åŠ›
- ç”Ÿæˆçš„åˆ†æåŒ…å«å¤šå¤´ç‰¹æœ‰çš„åˆ†æå›¾è¡¨
- æŠ¥å‘ŠåŒ…å«å¤šå¤´æ³¨æ„åŠ›çš„è¯¦ç»†ç»Ÿè®¡
=============================================================================
"""