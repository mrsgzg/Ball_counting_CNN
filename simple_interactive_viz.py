"""
ç®€æ´çš„äº¤äº’å¼PCA/t-SNEå¯è§†åŒ–å·¥å…·
ç‚¹å‡»æ•£ç‚¹å›¾ä¸Šçš„ç‚¹å¯ä»¥æŸ¥çœ‹å¯¹åº”çš„åŸå§‹å›¾åƒ
ä¸“é—¨é’ˆå¯¹EmbodiedCountingModelä¼˜åŒ–
"""

import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import matplotlib
matplotlib.use('Agg')  # æœåŠ¡å™¨ç¯å¢ƒçš„éäº¤äº’å¼åç«¯
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
import os
import sys
import json
import argparse
import time
from collections import defaultdict
from datetime import datetime
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')
from PIL import Image

# ç¡®ä¿ä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


class SimpleFeatureExtractor:
    """ç®€åŒ–çš„ç‰¹å¾æå–å™¨ - åªå…³æ³¨æ ¸å¿ƒåŠŸèƒ½"""
    
    def __init__(self, model, device='cuda'):
        self.model = model
        self.device = device
        self.model.eval()
        self.features = {}
        self.hooks = []
        
    def auto_detect_key_layers(self):
        """è‡ªåŠ¨æ£€æµ‹å…³é”®å±‚"""
        # ç²¾ç¡®çš„å±‚åç§°åŒ¹é…
        target_layers = [
            'fusion',              # å¤šæ¨¡æ€èåˆå±‚
            'lstm',                # æ—¶åºå¤„ç†å±‚
            'counting_decoder',    # è®¡æ•°è§£ç å±‚
            'visual_encoder',      # è§†è§‰ç¼–ç å±‚
            'embodiment_encoder'   # å…·èº«ç¼–ç å±‚
        ]
        
        detected_layers = []
        
        # æ£€æŸ¥å“ªäº›å±‚çœŸå®å­˜åœ¨
        for target in target_layers:
            try:
                module = self.model
                for part in target.split('.'):
                    module = getattr(module, part)
                detected_layers.append(target)
                print(f"âœ… æ£€æµ‹åˆ°å…³é”®å±‚: {target}")
            except AttributeError:
                print(f"âŒ æœªæ‰¾åˆ°å±‚: {target}")
        
        return detected_layers
        
    def register_hooks(self, layers_to_extract):
        """æ³¨å†Œé’©å­å‡½æ•°æ¥æå–ä¸­é—´å±‚ç‰¹å¾"""
        def get_activation(name):
            def hook(model, input, output):
                if isinstance(output, tuple):
                    self.features[name] = output[0].detach().cpu()
                else:
                    self.features[name] = output.detach().cpu()
            return hook
        
        successful_hooks = []
        
        # æ³¨å†Œé’©å­
        for layer_name in layers_to_extract:
            try:
                module = self.model
                for part in layer_name.split('.'):
                    module = getattr(module, part)
                
                handle = module.register_forward_hook(get_activation(layer_name))
                self.hooks.append(handle)
                successful_hooks.append(layer_name)
                print(f"âœ… æˆåŠŸæ³¨å†Œé’©å­: {layer_name}")
                
            except AttributeError as e:
                print(f"âŒ æ³¨å†Œé’©å­å¤±è´¥: {layer_name} - {e}")
        
        return successful_hooks
    
    def remove_hooks(self):
        """ç§»é™¤æ‰€æœ‰é’©å­"""
        for handle in self.hooks:
            handle.remove()
        self.hooks = []
        
    def _process_feature_tensor(self, feature_tensor):
        """å¤„ç†ä¸åŒå½¢çŠ¶çš„ç‰¹å¾å¼ é‡"""
        if len(feature_tensor.shape) == 3:  # [batch, seq, dim]
            # å–æœ€åä¸€ä¸ªæ—¶åˆ»çš„ç‰¹å¾
            return feature_tensor[:, -1, :].cpu().numpy()
        elif len(feature_tensor.shape) == 4:  # [batch, seq, h, w] or [batch, channel, h, w]
            # å…¨å±€å¹³å‡æ± åŒ–
            pooled = feature_tensor.mean(dim=(-2, -1))
            if len(pooled.shape) == 3:  # å¦‚æœè¿˜æœ‰seqç»´åº¦
                pooled = pooled[:, -1, :]
            return pooled.cpu().numpy()
        elif len(feature_tensor.shape) == 2:  # [batch, dim]
            return feature_tensor.cpu().numpy()
        else:
            # å…¶ä»–æƒ…å†µï¼Œå±•å¹³
            return feature_tensor.view(feature_tensor.shape[0], -1).cpu().numpy()
    
    def extract_features(self, data_loader, max_samples=1000):
        """æå–ç‰¹å¾ - ç®€åŒ–ç‰ˆ"""
        all_features = defaultdict(list)
        all_predictions = []
        all_labels = []
        all_sample_ids = []
        all_image_info = []  # æ–°å¢ï¼šä¿å­˜å›¾åƒä¿¡æ¯
        
        sample_count = 0
        
        with torch.no_grad():
            for batch in tqdm(data_loader, desc="æå–ç‰¹å¾"):
                if sample_count >= max_samples:
                    break
                
                # é€‚é…æ–°çš„æ•°æ®æ ¼å¼
                sequence_data = {
                    'images': batch['sequence_data']['images'].to(self.device),
                    'joints': batch['sequence_data']['joints'].to(self.device),
                    'timestamps': batch['sequence_data']['timestamps'].to(self.device),
                    'labels': batch['sequence_data']['labels'].to(self.device)
                }
                
                # è·å–æ‰¹æ¬¡ä¿¡æ¯
                labels = batch['label'].cpu().numpy()  # CSVä¸­çš„æ ‡ç­¾
                sample_ids = batch['sample_id']
                
                # è®¡ç®—å®é™…å¤„ç†çš„æ ·æœ¬æ•°
                remaining_samples = max_samples - sample_count
                actual_batch_size = min(len(labels), remaining_samples)
                
                # æˆªæ–­æ‰¹æ¬¡ï¼ˆå¦‚æœéœ€è¦ï¼‰
                if actual_batch_size < len(labels):
                    for key in sequence_data:
                        sequence_data[key] = sequence_data[key][:actual_batch_size]
                    labels = labels[:actual_batch_size]
                    sample_ids = sample_ids[:actual_batch_size]
                
                # æå–å›¾åƒä¿¡æ¯ - ä¿å­˜åŸå§‹å›¾åƒå¼ é‡ç”¨äºåç»­æ˜¾ç¤º
                original_images = sequence_data['images'][:, 0].cpu()  # å–ç¬¬ä¸€å¸§
                
                # æ¸…ç©ºç‰¹å¾å­—å…¸
                self.features = {}
                
                # å‰å‘ä¼ æ’­
                outputs = self.model(
                    sequence_data=sequence_data,
                    use_teacher_forcing=False
                )
                
                # æ”¶é›†é¢„æµ‹ç»“æœ
                count_logits = outputs['counts']  # [batch, seq_len, 11]
                pred_labels = torch.argmax(count_logits, dim=-1)  # [batch, seq_len]
                final_pred = pred_labels[:, -1].cpu().numpy()  # æœ€ç»ˆæ—¶åˆ»çš„é¢„æµ‹
                
                all_predictions.extend(final_pred)
                all_labels.extend(labels)
                all_sample_ids.extend(sample_ids)
                
                # ä¿å­˜å›¾åƒä¿¡æ¯
                for i in range(actual_batch_size):
                    image_info = {
                        'image_tensor': original_images[i],  # [C, H, W]
                        'sample_id': sample_ids[i],
                        'true_label': labels[i],
                        'pred_label': final_pred[i],
                        'global_index': sample_count + i  # å…¨å±€ç´¢å¼•ç”¨äºæ˜ å°„
                    }
                    all_image_info.append(image_info)
                
                # æ”¶é›†ä¸­é—´å±‚ç‰¹å¾
                for feature_name, feature_tensor in self.features.items():
                    processed_features = self._process_feature_tensor(feature_tensor)
                    all_features[feature_name].append(processed_features)
                
                sample_count += actual_batch_size
                
                if sample_count >= max_samples:
                    break
        
        # åˆå¹¶æ‰€æœ‰ç‰¹å¾
        final_features = {}
        for feature_name, feature_list in all_features.items():
            if feature_list:
                final_features[feature_name] = np.vstack(feature_list)
        
        result = {
            'features': final_features,
            'predictions': np.array(all_predictions),
            'labels': np.array(all_labels),
            'sample_ids': all_sample_ids,
            'image_info': all_image_info  # æ–°å¢
        }
        
        print(f"ç‰¹å¾æå–å®Œæˆ:")
        print(f"  å®é™…æ ·æœ¬æ•°: {len(result['labels'])}")
        print(f"  æå–çš„ç‰¹å¾å±‚: {list(final_features.keys())}")
        print(f"  å›¾åƒä¿¡æ¯æ•°: {len(all_image_info)}")
        
        return result


class InteractiveVisualizer:
    """äº¤äº’å¼å¯è§†åŒ–å™¨ - ç®€åŒ–ç‰ˆæœ¬ï¼Œä¸“æ³¨äºPCAå’Œt-SNE"""
    
    def __init__(self, image_info, figsize=(15, 10)):
        self.image_info = image_info
        self.figsize = figsize
        self.current_fig = None
        self.current_ax = None
        
    def denormalize_image(self, tensor, mode='rgb'):
        """åå½’ä¸€åŒ–å›¾åƒå¼ é‡ä»¥ä¾¿æ˜¾ç¤º"""
        if mode == 'rgb':
            # ImageNetæ ‡å‡†åŒ–å‚æ•°
            mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
            std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
        else:
            # ç°åº¦æ ‡å‡†åŒ–å‚æ•°
            mean = torch.tensor([0.5]).view(1, 1, 1)
            std = torch.tensor([0.5]).view(1, 1, 1)
        
        # åå½’ä¸€åŒ–
        denorm_tensor = tensor * std + mean
        # é™åˆ¶åœ¨[0, 1]èŒƒå›´å†…
        denorm_tensor = torch.clamp(denorm_tensor, 0, 1)
        return denorm_tensor
    
    def tensor_to_image(self, tensor):
        """å°†å¼ é‡è½¬æ¢ä¸ºå¯æ˜¾ç¤ºçš„å›¾åƒ"""
        if tensor.dim() == 3:
            if tensor.shape[0] == 1:
                # ç°åº¦å›¾åƒ
                return tensor.squeeze(0).numpy()
            elif tensor.shape[0] == 3:
                # RGBå›¾åƒï¼Œéœ€è¦è½¬ç½®ä¸º (H, W, C)
                return tensor.permute(1, 2, 0).numpy()
        return tensor.numpy()
    
    def reduce_dimensions(self, features, method='pca', n_components=2):
        """é™ç»´"""
        if method == 'pca':
            reducer = PCA(n_components=n_components, random_state=42)
        elif method == 'tsne':
            reducer = TSNE(n_components=n_components, random_state=42, perplexity=min(30, len(features)//4))
        else:
            raise ValueError(f"Unsupported method: {method}")
        
        reduced_features = reducer.fit_transform(features)
        return reduced_features
    
    def create_interactive_plot(self, features_2d, labels, layer_name, method, save_dir):
        """åˆ›å»ºäº¤äº’å¼æ•£ç‚¹å›¾ - ä¿å­˜ç‰ˆæœ¬ï¼Œå¸¦ç‚¹å‡»ä¿¡æ¯"""
        
        # åˆ›å»ºä¸»æ•£ç‚¹å›¾
        fig, ax = plt.subplots(figsize=self.figsize)
        
        unique_labels = np.unique(labels)
        colors = plt.cm.tab10(np.linspace(0, 1, len(unique_labels)))
        
        # ç»˜åˆ¶æ•£ç‚¹å›¾
        scatter_plots = []
        for i, label in enumerate(unique_labels):
            mask = labels == label
            scatter = ax.scatter(features_2d[mask, 0], features_2d[mask, 1], 
                               c=[colors[i]], label=f'Class {label}', 
                               alpha=0.7, s=60, picker=True)
            scatter_plots.append((scatter, mask, label))
        
        ax.set_title(f'{layer_name} - {method.upper()} Visualization\n(Check point_info.txt for detailed sample information)', fontsize=14)
        ax.set_xlabel('Dimension 1', fontsize=12)
        ax.set_ylabel('Dimension 2', fontsize=12)
        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        ax.grid(True, alpha=0.3)
        
        # ä¿å­˜ä¸»å›¾
        plt.tight_layout()
        main_plot_path = os.path.join(save_dir, f'{layer_name}_{method}_interactive.png')
        plt.savefig(main_plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        # ç”Ÿæˆç‚¹ä¿¡æ¯æ–‡ä»¶
        self.generate_point_info_file(features_2d, labels, layer_name, method, save_dir)
        
        # åˆ›å»ºç¤ºä¾‹å›¾åƒç½‘æ ¼ - æ˜¾ç¤ºæ¯ä¸ªç±»åˆ«çš„ä»£è¡¨æ€§æ ·æœ¬
        self.create_sample_grid(labels, layer_name, method, save_dir)
        
        print(f"âœ… äº¤äº’å¼å¯è§†åŒ–å·²ä¿å­˜: {main_plot_path}")
        print(f"âœ… ç‚¹ä¿¡æ¯æ–‡ä»¶å·²ç”Ÿæˆ: æŸ¥çœ‹ point_info.txt")
        print(f"âœ… ç¤ºä¾‹å›¾åƒç½‘æ ¼å·²ç”Ÿæˆ")
        
        return main_plot_path
    
    def generate_point_info_file(self, features_2d, labels, layer_name, method, save_dir):
        """ç”Ÿæˆç‚¹ä¿¡æ¯æ–‡ä»¶ï¼ŒåŒ…å«æ¯ä¸ªç‚¹çš„è¯¦ç»†ä¿¡æ¯"""
        
        info_file_path = os.path.join(save_dir, f'{layer_name}_{method}_point_info.txt')
        
        with open(info_file_path, 'w', encoding='utf-8') as f:
            f.write(f"=== {layer_name} - {method.upper()} å¯è§†åŒ–ç‚¹ä¿¡æ¯ ===\n\n")
            f.write("æ ¼å¼: ç‚¹ç´¢å¼• | 2Dåæ ‡ | æ ·æœ¬ID | çœŸå®æ ‡ç­¾ | é¢„æµ‹æ ‡ç­¾\n")
            f.write("-" * 80 + "\n")
            
            for i, (x, y) in enumerate(features_2d):
                if i < len(self.image_info):
                    img_info = self.image_info[i]
                    f.write(f"ç‚¹{i:3d} | ({x:8.3f}, {y:8.3f}) | {img_info['sample_id']:>10} | "
                           f"çœŸå®:{img_info['true_label']:2d} | é¢„æµ‹:{img_info['pred_label']:2d}\n")
                else:
                    f.write(f"ç‚¹{i:3d} | ({x:8.3f}, {y:8.3f}) | ä¿¡æ¯ç¼ºå¤±\n")
        
        print(f"âœ… ç‚¹ä¿¡æ¯æ–‡ä»¶å·²ä¿å­˜: {info_file_path}")
    
    def create_sample_grid(self, labels, layer_name, method, save_dir, samples_per_class=3):
        """åˆ›å»ºæ¯ä¸ªç±»åˆ«çš„ä»£è¡¨æ€§æ ·æœ¬ç½‘æ ¼"""
        
        unique_labels = np.unique(labels)
        n_classes = len(unique_labels)
        
        # è®¡ç®—ç½‘æ ¼å¤§å°
        grid_cols = samples_per_class
        grid_rows = n_classes
        
        fig, axes = plt.subplots(grid_rows, grid_cols, 
                                figsize=(4*grid_cols, 3*grid_rows))
        
        if grid_rows == 1:
            axes = axes.reshape(1, -1)
        if grid_cols == 1:
            axes = axes.reshape(-1, 1)
        
        fig.suptitle(f'{layer_name} - {method.upper()} Sample Grid\n'
                     f'Representative samples from each class', fontsize=16)
        
        for class_idx, label in enumerate(unique_labels):
            # æ‰¾åˆ°è¯¥ç±»åˆ«çš„æ‰€æœ‰æ ·æœ¬
            class_mask = labels == label
            class_indices = np.where(class_mask)[0]
            
            # éšæœºé€‰æ‹©å‡ ä¸ªæ ·æœ¬ï¼ˆæˆ–å–å‰å‡ ä¸ªï¼‰
            selected_indices = class_indices[:samples_per_class] if len(class_indices) >= samples_per_class else class_indices
            
            for sample_idx in range(samples_per_class):
                ax = axes[class_idx, sample_idx]
                
                if sample_idx < len(selected_indices):
                    # è·å–å¯¹åº”çš„å›¾åƒä¿¡æ¯
                    global_idx = selected_indices[sample_idx]
                    if global_idx < len(self.image_info):
                        img_info = self.image_info[global_idx]
                        
                        # åå½’ä¸€åŒ–å¹¶æ˜¾ç¤ºå›¾åƒ
                        image_tensor = img_info['image_tensor']
                        
                        # åˆ¤æ–­å›¾åƒæ¨¡å¼
                        if image_tensor.shape[0] == 3:
                            denorm_image = self.denormalize_image(image_tensor, 'rgb')
                            display_image = self.tensor_to_image(denorm_image)
                            ax.imshow(display_image)
                        elif image_tensor.shape[0] == 1:
                            denorm_image = self.denormalize_image(image_tensor, 'grayscale')
                            display_image = self.tensor_to_image(denorm_image)
                            ax.imshow(display_image, cmap='gray')
                        else:
                            ax.text(0.5, 0.5, 'Unknown\nFormat', ha='center', va='center',
                                   transform=ax.transAxes, fontsize=10)
                        
                        # æ·»åŠ æ ‡é¢˜ä¿¡æ¯
                        ax.set_title(f'ID:{img_info["sample_id"]}\n'
                                   f'T:{img_info["true_label"]} P:{img_info["pred_label"]}', 
                                   fontsize=10)
                    else:
                        ax.text(0.5, 0.5, 'No Image', ha='center', va='center',
                               transform=ax.transAxes, fontsize=12)
                        ax.set_title(f'Class {label} - Sample {sample_idx+1}', fontsize=10)
                else:
                    ax.text(0.5, 0.5, 'No More\nSamples', ha='center', va='center',
                           transform=ax.transAxes, fontsize=10)
                    ax.set_title(f'Class {label} - N/A', fontsize=10)
                
                ax.axis('off')
        
        plt.tight_layout()
        
        # ä¿å­˜ç½‘æ ¼å›¾
        grid_path = os.path.join(save_dir, f'{layer_name}_{method}_sample_grid.png')
        plt.savefig(grid_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… æ ·æœ¬ç½‘æ ¼å·²ä¿å­˜: {grid_path}")
    
    def create_detailed_sample_view(self, selected_indices, features_2d, labels, 
                                   layer_name, method, save_dir, max_samples=20):
        """åˆ›å»ºé€‰å®šæ ·æœ¬çš„è¯¦ç»†è§†å›¾"""
        
        # é™åˆ¶æ ·æœ¬æ•°é‡
        if len(selected_indices) > max_samples:
            selected_indices = selected_indices[:max_samples]
        
        n_samples = len(selected_indices)
        cols = min(5, n_samples)
        rows = (n_samples + cols - 1) // cols
        
        fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 4*rows))
        
        if rows == 1 and cols == 1:
            axes = [axes]
        elif rows == 1:
            axes = axes.reshape(1, -1)
        elif cols == 1:
            axes = axes.reshape(-1, 1)
        
        fig.suptitle(f'Detailed View - {layer_name} {method.upper()}\n'
                     f'Selected {n_samples} samples', fontsize=16)
        
        for i, idx in enumerate(selected_indices):
            row = i // cols
            col = i % cols
            
            if rows == 1:
                ax = axes[col]
            else:
                ax = axes[row, col]
            
            # è·å–å›¾åƒä¿¡æ¯
            if idx < len(self.image_info):
                img_info = self.image_info[idx]
                
                # æ˜¾ç¤ºå›¾åƒ
                image_tensor = img_info['image_tensor']
                
                if image_tensor.shape[0] == 3:
                    denorm_image = self.denormalize_image(image_tensor, 'rgb')
                    display_image = self.tensor_to_image(denorm_image)
                    ax.imshow(display_image)
                elif image_tensor.shape[0] == 1:
                    denorm_image = self.denormalize_image(image_tensor, 'grayscale')
                    display_image = self.tensor_to_image(denorm_image)
                    ax.imshow(display_image, cmap='gray')
                
                # æ·»åŠ è¯¦ç»†ä¿¡æ¯
                x, y = features_2d[idx]
                title = (f'Point {idx}\n'
                        f'ID: {img_info["sample_id"]}\n'
                        f'2D: ({x:.2f}, {y:.2f})\n'
                        f'True: {img_info["true_label"]}, Pred: {img_info["pred_label"]}')
                ax.set_title(title, fontsize=10)
            else:
                ax.text(0.5, 0.5, f'Point {idx}\nNo Image Data', 
                       ha='center', va='center', transform=ax.transAxes)
            
            ax.axis('off')
        
        # éšè—å¤šä½™çš„å­å›¾
        for i in range(n_samples, rows * cols):
            row = i // cols
            col = i % cols
            if rows == 1:
                axes[col].axis('off')
            else:
                axes[row, col].axis('off')
        
        plt.tight_layout()
        
        # ä¿å­˜è¯¦ç»†è§†å›¾
        detail_path = os.path.join(save_dir, f'{layer_name}_{method}_selected_samples.png')
        plt.savefig(detail_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… è¯¦ç»†æ ·æœ¬è§†å›¾å·²ä¿å­˜: {detail_path}")
        return detail_path


def load_model_and_data(checkpoint_path, val_csv, data_root, batch_size=8):
    """åŠ è½½æ¨¡å‹å’Œæ•°æ®"""
    print("ğŸ“¥ åŠ è½½æ¨¡å‹å’Œæ•°æ®...")
    
    # åŠ è½½æ£€æŸ¥ç‚¹
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    config = checkpoint['config']
    
    # å¯¼å…¥æ¨¡å‹ç±»
    try:
        from Model_embodiment import EmbodiedCountingModel
    except ImportError:
        print("âŒ æ— æ³•å¯¼å…¥EmbodiedCountingModelï¼Œè¯·ç¡®ä¿Model_embodiment.pyåœ¨Pythonè·¯å¾„ä¸­")
        raise
    
    # ç¡®å®šå›¾åƒæ¨¡å¼
    image_mode = config.get('image_mode', 'rgb')
    input_channels = 3 if image_mode == 'rgb' else 1
    
    # é‡å»ºæ¨¡å‹
    model_config = config['model_config'].copy()
    model_config['input_channels'] = input_channels
    model = EmbodiedCountingModel(**model_config)
    model.load_state_dict(checkpoint['model_state_dict'])
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    model.eval()
    
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ (å›¾åƒæ¨¡å¼: {image_mode}, è®¾å¤‡: {device})")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    try:
        from DataLoader_embodiment import get_ball_counting_data_loaders
    except ImportError:
        print("âŒ æ— æ³•å¯¼å…¥get_ball_counting_data_loadersï¼Œè¯·ç¡®ä¿DataLoader_embodiment.pyåœ¨Pythonè·¯å¾„ä¸­")
        raise
    
    _, val_loader, _ = get_ball_counting_data_loaders(
        train_csv_path=config['train_csv'],
        val_csv_path=val_csv,
        data_root=data_root,
        batch_size=batch_size,
        sequence_length=config['sequence_length'],
        normalize=config['normalize'],
        num_workers=2,
        image_mode=image_mode
    )
    
    print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆï¼ŒéªŒè¯é›†å¤§å°: {len(val_loader.dataset)}")
    
    return model, val_loader, device, config


def simple_interactive_analysis(checkpoint_path, val_csv, data_root, 
                               save_dir='./simple_analysis', 
                               max_samples=300, specific_layers=None):
    """ç®€æ´çš„äº¤äº’å¼åˆ†æä¸»å‡½æ•°"""
    
    print("ğŸ¯ å¼€å§‹ç®€æ´äº¤äº’å¼åˆ†æ...")
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(save_dir, exist_ok=True)
    
    try:
        # 1. åŠ è½½æ¨¡å‹å’Œæ•°æ®
        model, val_loader, device, config = load_model_and_data(
            checkpoint_path, val_csv, data_root
        )
        
        # 2. åˆ›å»ºç‰¹å¾æå–å™¨
        extractor = SimpleFeatureExtractor(model, device)
        
        # 3. ç¡®å®šè¦åˆ†æçš„å±‚
        if specific_layers is None:
            print("ğŸ” è‡ªåŠ¨æ£€æµ‹å…³é”®å±‚...")
            key_layers = extractor.auto_detect_key_layers()
            if not key_layers:
                print("âŒ æœªæ£€æµ‹åˆ°ä»»ä½•å±‚ï¼Œä½¿ç”¨é»˜è®¤å±‚")
                key_layers = ['fusion']
        else:
            key_layers = specific_layers
        
        print(f"ğŸ“‹ å‡†å¤‡åˆ†æçš„å±‚: {key_layers}")
        
        # 4. æ³¨å†Œé’©å­å¹¶æå–ç‰¹å¾
        successful_layers = extractor.register_hooks(key_layers)
        
        if not successful_layers:
            print("âŒ æ²¡æœ‰æˆåŠŸæ³¨å†Œä»»ä½•é’©å­ï¼")
            return None
        
        try:
            # 5. æå–ç‰¹å¾
            print("ğŸ¯ æå–ç‰¹å¾...")
            data = extractor.extract_features(val_loader, max_samples)
            
            features = data['features']
            predictions = data['predictions']
            true_labels = data['labels']
            image_info = data['image_info']
            
            print(f"âœ… ç‰¹å¾æå–å®Œæˆ:")
            print(f"   æ ·æœ¬æ•°: {len(true_labels)}")
            print(f"   æå–å±‚: {list(features.keys())}")
            print(f"   å›¾åƒä¿¡æ¯æ•°: {len(image_info)}")
            
            # 6. åˆ›å»ºå¯è§†åŒ–å™¨
            visualizer = InteractiveVisualizer(image_info)
            
            # 7. å¯¹æ¯ä¸ªå±‚è¿›è¡ŒPCAå’Œt-SNEåˆ†æ
            print("ğŸ“Š å¼€å§‹å¯è§†åŒ–åˆ†æ...")
            
            analysis_results = {}
            
            for layer_name, layer_features in features.items():
                if layer_features is None:
                    continue
                    
                print(f"   åˆ†æ {layer_name} å±‚...")
                layer_results = {}
                
                # PCAåˆ†æ
                try:
                    print(f"     æ‰§è¡ŒPCA...")
                    pca_features = visualizer.reduce_dimensions(layer_features, 'pca')
                    
                    pca_plot_path = visualizer.create_interactive_plot(
                        pca_features, true_labels, layer_name, 'pca', save_dir
                    )
                    
                    layer_results['pca'] = {
                        'features_2d': pca_features,
                        'plot_path': pca_plot_path
                    }
                    
                except Exception as e:
                    print(f"     PCAåˆ†æå¤±è´¥: {e}")
                
                # t-SNEåˆ†æ
                try:
                    print(f"     æ‰§è¡Œt-SNE...")
                    tsne_features = visualizer.reduce_dimensions(layer_features, 'tsne')
                    
                    tsne_plot_path = visualizer.create_interactive_plot(
                        tsne_features, true_labels, layer_name, 'tsne', save_dir
                    )
                    
                    layer_results['tsne'] = {
                        'features_2d': tsne_features,
                        'plot_path': tsne_plot_path
                    }
                    
                except Exception as e:
                    print(f"     t-SNEåˆ†æå¤±è´¥: {e}")
                
                analysis_results[layer_name] = layer_results
            
            # 8. ç”Ÿæˆä½¿ç”¨è¯´æ˜
            generate_usage_guide(analysis_results, save_dir, image_info)
            
            print(f"ğŸ‰ ç®€æ´åˆ†æå®Œæˆï¼ç»“æœä¿å­˜åœ¨: {save_dir}")
            
            return analysis_results
            
        finally:
            extractor.remove_hooks()
            
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def generate_usage_guide(analysis_results, save_dir, image_info):
    """ç”Ÿæˆä½¿ç”¨è¯´æ˜æ–‡ä»¶"""
    
    guide_path = os.path.join(save_dir, 'README.md')
    
    with open(guide_path, 'w', encoding='utf-8') as f:
        f.write("# ç®€æ´äº¤äº’å¼å¯è§†åŒ–åˆ†æç»“æœ\n\n")
        f.write("## ğŸ¯ åŠŸèƒ½è¯´æ˜\n\n")
        f.write("æœ¬å·¥å…·æä¾›äº†ç®€æ´çš„PCAå’Œt-SNEå¯è§†åŒ–ï¼Œè®©ä½ å¯ä»¥äº†è§£é™ç»´å›¾ä¸Šæ¯ä¸ªç‚¹å¯¹åº”çš„åŸå§‹æ ·æœ¬ä¿¡æ¯ã€‚\n\n")
        
        f.write("## ğŸ“ æ–‡ä»¶è¯´æ˜\n\n")
        
        for layer_name, layer_results in analysis_results.items():
            f.write(f"### {layer_name} å±‚åˆ†æç»“æœ\n\n")
            
            for method in ['pca', 'tsne']:
                if method in layer_results:
                    f.write(f"- `{layer_name}_{method}_interactive.png`: {method.upper()}é™ç»´æ•£ç‚¹å›¾\n")
                    f.write(f"- `{layer_name}_{method}_point_info.txt`: æ¯ä¸ªç‚¹çš„è¯¦ç»†ä¿¡æ¯\n")
                    f.write(f"- `{layer_name}_{method}_sample_grid.png`: æ¯ä¸ªç±»åˆ«çš„ä»£è¡¨æ€§æ ·æœ¬\n")
            f.write("\n")
        
        f.write("## ğŸ” å¦‚ä½•æŸ¥çœ‹ç‚¹çš„è¯¦ç»†ä¿¡æ¯\n\n")
        f.write("1. **æŸ¥çœ‹æ•£ç‚¹å›¾**: æ‰“å¼€ `*_interactive.png` æ–‡ä»¶æŸ¥çœ‹é™ç»´ç»“æœ\n")
        f.write("2. **æŸ¥æ‰¾ç‚¹ä¿¡æ¯**: æ‰“å¼€å¯¹åº”çš„ `*_point_info.txt` æ–‡ä»¶\n")
        f.write("3. **å®šä½å…·ä½“ç‚¹**: åœ¨æ–‡ä»¶ä¸­æœç´¢ä½ æ„Ÿå…´è¶£çš„åŒºåŸŸåæ ‡æˆ–æ ·æœ¬ID\n")
        f.write("4. **æŸ¥çœ‹æ ·æœ¬å›¾åƒ**: å‚è€ƒ `*_sample_grid.png` äº†è§£å„ç±»åˆ«çš„å…¸å‹æ ·æœ¬\n\n")
        
        f.write("## ğŸ“Š ç‚¹ä¿¡æ¯æ–‡ä»¶æ ¼å¼\n\n")
        f.write("```\n")
        f.write("ç‚¹ç´¢å¼• | 2Dåæ ‡ | æ ·æœ¬ID | çœŸå®æ ‡ç­¾ | é¢„æµ‹æ ‡ç­¾\n")
        f.write("ç‚¹  0 | ( -2.156,   1.423) |    sample_1 | çœŸå®: 3 | é¢„æµ‹: 3\n")
        f.write("ç‚¹  1 | (  0.892,  -0.756) |    sample_2 | çœŸå®: 5 | é¢„æµ‹: 4\n")
        f.write("...\n")
        f.write("```\n\n")
        
        f.write("## ğŸ¯ ä½¿ç”¨æŠ€å·§\n\n")
        f.write("1. **æŸ¥æ‰¾å¼‚å¸¸ç‚¹**: åœ¨æ•£ç‚¹å›¾ä¸­æ‰¾åˆ°ç¦»ç¾¤æˆ–é”™è¯¯åˆ†ç±»çš„ç‚¹\n")
        f.write("2. **æŸ¥æ‰¾åæ ‡**: è®°å½•å¼‚å¸¸ç‚¹çš„å¤§è‡´åæ ‡ä½ç½®\n")
        f.write("3. **å®šä½æ ·æœ¬**: åœ¨point_info.txtä¸­æœç´¢ç›¸è¿‘çš„åæ ‡\n")
        f.write("4. **åˆ†ææ ·æœ¬**: æ ¹æ®æ ·æœ¬IDè¿›ä¸€æ­¥åˆ†æå…·ä½“åŸå› \n\n")
        
        f.write("## ğŸ“ˆ åˆ†æå»ºè®®\n\n")
        f.write("- **PCA**: æ˜¾ç¤ºçº¿æ€§ä¸»æˆåˆ†ï¼Œé€‚åˆç†è§£ç‰¹å¾çš„ä¸»è¦å˜åŒ–æ–¹å‘\n")
        f.write("- **t-SNE**: æ˜¾ç¤ºéçº¿æ€§ç»“æ„ï¼Œé€‚åˆå‘ç°èšç±»å’Œå±€éƒ¨é‚»åŸŸå…³ç³»\n")
        f.write("- **å¯¹æ¯”åˆ†æ**: åŒæ—¶æŸ¥çœ‹PCAå’Œt-SNEç»“æœï¼Œè·å¾—æ›´å…¨é¢çš„ç†è§£\n")
        f.write("- **å¤šå±‚åˆ†æ**: æ¯”è¾ƒä¸åŒå±‚çš„å¯è§†åŒ–ç»“æœï¼Œç†è§£ç‰¹å¾æ¼”åŒ–è¿‡ç¨‹\n\n")
        
        f.write(f"## ğŸ“‹ æœ¬æ¬¡åˆ†æç»Ÿè®¡\n\n")
        f.write(f"- æ€»æ ·æœ¬æ•°: {len(image_info)}\n")
        f.write(f"- åˆ†æå±‚æ•°: {len(analysis_results)}\n")
        f.write(f"- ç”Ÿæˆæ–‡ä»¶æ•°: {sum(len(lr) * 3 for lr in analysis_results.values())}\n")
        
    print(f"âœ… ä½¿ç”¨è¯´æ˜å·²ä¿å­˜: {guide_path}")


def main():
    """ä¸»å…¥å£å‡½æ•°"""
    parser = argparse.ArgumentParser(description='ç®€æ´äº¤äº’å¼å¯è§†åŒ–å·¥å…·')
    
    # å¿…éœ€å‚æ•°
    parser.add_argument('--checkpoint', type=str, required=True,
                       help='æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--val_csv', type=str, 
                       default='scratch/Ball_counting_CNN/Tools_script/ball_counting_dataset_val.csv',
                       help='éªŒè¯é›†CSVæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--data_root', type=str, 
                       default='/mnt/iusers01/fatpou01/compsci01/k09562zs/scratch/Ball_counting_CNN/ball_data_collection',
                       help='æ•°æ®æ ¹ç›®å½•')
    
    # åˆ†æé€‰é¡¹
    parser.add_argument('--save_dir', type=str, default=None,
                       help='ç»“æœä¿å­˜ç›®å½•')
    parser.add_argument('--max_samples', type=int, default=300,
                       help='æœ€å¤§åˆ†ææ ·æœ¬æ•°')
    parser.add_argument('--layers', nargs='+', default=None,
                       help='æŒ‡å®šè¦åˆ†æçš„å±‚åç§°')
    
    # å…¶ä»–é€‰é¡¹
    parser.add_argument('--batch_size', type=int, default=8,
                       help='æ•°æ®åŠ è½½æ‰¹æ¬¡å¤§å°')
    
    args = parser.parse_args()
    
    # è®¾ç½®é»˜è®¤ä¿å­˜ç›®å½•
    if args.save_dir is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        args.save_dir = f'./simple_analysis_{timestamp}'
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    for path, name in [(args.checkpoint, 'æ£€æŸ¥ç‚¹æ–‡ä»¶'), 
                       (args.val_csv, 'éªŒè¯CSVæ–‡ä»¶'), 
                       (args.data_root, 'æ•°æ®æ ¹ç›®å½•')]:
        if not os.path.exists(path):
            print(f"âŒ {name}ä¸å­˜åœ¨: {path}")
            return
    
    print("ğŸ¯ ç®€æ´äº¤äº’å¼å¯è§†åŒ–å·¥å…·")
    print("="*50)
    print(f"æ£€æŸ¥ç‚¹: {args.checkpoint}")
    print(f"éªŒè¯é›†: {args.val_csv}")
    print(f"æ•°æ®æ ¹ç›®å½•: {args.data_root}")
    print(f"ä¿å­˜ç›®å½•: {args.save_dir}")
    print(f"æœ€å¤§æ ·æœ¬æ•°: {args.max_samples}")
    if args.layers:
        print(f"æŒ‡å®šå±‚: {args.layers}")
    print("="*50)
    
    start_time = time.time()
    
    try:
        results = simple_interactive_analysis(
            args.checkpoint, args.val_csv, args.data_root, 
            args.save_dir, args.max_samples, args.layers
        )
        
        if results:
            elapsed_time = time.time() - start_time
            print(f"\nğŸ‰ åˆ†æå®Œæˆï¼")
            print(f"â±ï¸ æ€»è€—æ—¶: {elapsed_time:.2f} ç§’")
            print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {args.save_dir}")
            print(f"ğŸ“– æŸ¥çœ‹ README.md äº†è§£å¦‚ä½•ä½¿ç”¨ç»“æœ")
        else:
            print(f"\nâŒ åˆ†æå¤±è´¥")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­åˆ†æ")
    except Exception as e:
        print(f"\nâŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # å¦‚æœæ²¡æœ‰å‘½ä»¤è¡Œå‚æ•°ï¼Œæ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
    if len(sys.argv) == 1:
        print("ğŸ¯ ç®€æ´äº¤äº’å¼å¯è§†åŒ–å·¥å…·")
        print("="*50)
        print("åŠŸèƒ½: PCAå’Œt-SNEé™ç»´å¯è§†åŒ–ï¼Œå¯æŸ¥çœ‹æ¯ä¸ªç‚¹çš„åŸå§‹å›¾åƒä¿¡æ¯")
        print("ä¼˜åŠ¿: ç®€æ´ã€ä¸“æ³¨ã€æ˜“ç”¨")
        print("\nä½¿ç”¨æ–¹æ³•:")
        print("python simple_interactive_viz.py --checkpoint MODEL.pth --val_csv VAL.csv --data_root DATA_DIR")
        print("\nåŸºæœ¬ç¤ºä¾‹:")
        print("python simple_interactive_viz.py \\")
        print("    --checkpoint ./best_model.pth \\")
        print("    --val_csv ./val.csv \\")
        print("    --data_root ./data")
        print("\né«˜çº§ç¤ºä¾‹:")
        print("python simple_interactive_viz.py \\")
        print("    --checkpoint ./best_model.pth \\")
        print("    --val_csv ./val.csv \\")
        print("    --data_root ./data \\")
        print("    --layers fusion lstm \\")
        print("    --max_samples 500 \\")
        print("    --save_dir ./my_viz")
        print("\nè¾“å‡ºæ–‡ä»¶:")
        print("- *_pca_interactive.png     # PCAé™ç»´æ•£ç‚¹å›¾")
        print("- *_tsne_interactive.png    # t-SNEé™ç»´æ•£ç‚¹å›¾")
        print("- *_point_info.txt          # æ¯ä¸ªç‚¹çš„è¯¦ç»†ä¿¡æ¯")
        print("- *_sample_grid.png         # æ¯ç±»çš„ä»£è¡¨æ€§æ ·æœ¬")
        print("- README.md                 # è¯¦ç»†ä½¿ç”¨è¯´æ˜")
        print("\nğŸ’¡ å¦‚ä½•æŸ¥çœ‹ç‚¹çš„åŸå§‹å›¾åƒ:")
        print("1. åœ¨æ•£ç‚¹å›¾ä¸­æ‰¾åˆ°æ„Ÿå…´è¶£çš„ç‚¹")
        print("2. è®°å½•è¯¥ç‚¹çš„å¤§è‡´åæ ‡")
        print("3. åœ¨å¯¹åº”çš„ point_info.txt ä¸­æœç´¢ç›¸è¿‘åæ ‡")
        print("4. è·å¾—æ ·æœ¬IDå’Œæ ‡ç­¾ä¿¡æ¯")
        print("5. å‚è€ƒ sample_grid.png æŸ¥çœ‹è¯¥ç±»çš„å…¸å‹æ ·æœ¬")
        sys.exit(0)
    
    main()


# =============================================================================
# ä¾¿æ·å‡½æ•°ï¼Œä¾›å…¶ä»–è„šæœ¬è°ƒç”¨
# =============================================================================

def quick_viz(checkpoint_path, val_csv, data_root, save_dir=None, max_samples=200):
    """å¿«é€Ÿå¯è§†åŒ–æ¥å£"""
    if save_dir is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        save_dir = f'./quick_viz_{timestamp}'
    
    return simple_interactive_analysis(
        checkpoint_path, val_csv, data_root, save_dir, max_samples
    )


def custom_viz(checkpoint_path, val_csv, data_root, layers, save_dir=None, max_samples=300):
    """è‡ªå®šä¹‰å±‚å¯è§†åŒ–æ¥å£"""
    if save_dir is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        save_dir = f'./custom_viz_{timestamp}'
    
    return simple_interactive_analysis(
        checkpoint_path, val_csv, data_root, save_dir, max_samples, layers
    )


# =============================================================================
# ä½¿ç”¨ç¤ºä¾‹
# =============================================================================

"""
ä½¿ç”¨ç¤ºä¾‹:

1. å‘½ä»¤è¡Œä½¿ç”¨:
   python simple_interactive_viz.py \\
       --checkpoint ./best_model.pth \\
       --val_csv ./val.csv \\
       --data_root ./data \\
       --max_samples 300

2. åœ¨Pythonè„šæœ¬ä¸­ä½¿ç”¨:
   from simple_interactive_viz import quick_viz, custom_viz
   
   # å¿«é€Ÿå¯è§†åŒ–
   results = quick_viz(
       checkpoint_path='./best_model.pth',
       val_csv='./val.csv', 
       data_root='./data'
   )
   
   # è‡ªå®šä¹‰å±‚å¯è§†åŒ–
   results = custom_viz(
       checkpoint_path='./best_model.pth',
       val_csv='./val.csv',
       data_root='./data',
       layers=['fusion', 'lstm']
   )

3. æŸ¥çœ‹ç»“æœ:
   - æ‰“å¼€ç”Ÿæˆçš„ *_interactive.png æŸ¥çœ‹æ•£ç‚¹å›¾
   - æ‰“å¼€å¯¹åº”çš„ *_point_info.txt æŸ¥çœ‹ç‚¹çš„è¯¦ç»†ä¿¡æ¯
   - å‚è€ƒ README.md äº†è§£å…·ä½“ä½¿ç”¨æ–¹æ³•

4. å®šä½ç‰¹å®šç‚¹çš„å›¾åƒ:
   - åœ¨æ•£ç‚¹å›¾ä¸­æ‰¾åˆ°æ„Ÿå…´è¶£çš„åŒºåŸŸæˆ–å¼‚å¸¸ç‚¹
   - è®°å½•è¯¥ç‚¹çš„åæ ‡ (x, y)
   - åœ¨ point_info.txt ä¸­æœç´¢ç›¸è¿‘çš„åæ ‡
   - è·å¾—å¯¹åº”çš„æ ·æœ¬IDå’Œæ ‡ç­¾ä¿¡æ¯
   - åœ¨ sample_grid.png ä¸­æŸ¥çœ‹è¯¥ç±»çš„å…¸å‹æ ·æœ¬
"""