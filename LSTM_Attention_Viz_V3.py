"""
é€šç”¨åŠ¨æ€æ¨¡å‹å¯è§†åŒ–å·¥å…·
æ”¯æŒåŸå§‹Embodimentæ¨¡å‹å’ŒAblationæ¨¡å‹çš„å¯è§†åŒ–
å¯è§†åŒ–æ¯ä¸ªæ ·æœ¬æ¯ä¸€å¸§çš„:
1. Attentionçƒ­åŠ›å›¾ (å¦‚æœæ¨¡å‹æ”¯æŒ)
2. Softmaxè¾“å‡ºåˆ†å¸ƒ
3. LSTMéšçŠ¶æ€å˜åŒ–
"""

import torch
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import json
import argparse
from datetime import datetime
from PIL import Image
import cv2

# è®¾ç½®matplotlibåç«¯
import matplotlib
matplotlib.use('Agg')

# è®¾ç½®å­—ä½“
plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


class UniversalModelVisualizer:
    """é€šç”¨æ¨¡å‹å¯è§†åŒ–å™¨ - æ”¯æŒæ‰€æœ‰æ¨¡å‹ç±»å‹"""
    
    def __init__(self, model, device='cuda'):
        self.model = model
        self.device = device
        self.model.eval()
        
        # æ£€æµ‹æ¨¡å‹ç±»å‹å’Œèƒ½åŠ›
        self.model_info = self._detect_model_capabilities()
        print(f"âœ… æ£€æµ‹åˆ°æ¨¡å‹ç±»å‹: {self.model_info['model_type']}")
        print(f"   æ”¯æŒçš„åŠŸèƒ½: {', '.join(self.model_info['capabilities'])}")
        
    def _detect_model_capabilities(self):
        """æ£€æµ‹æ¨¡å‹ç±»å‹å’Œæ”¯æŒçš„åŠŸèƒ½"""
        model_info = {
            'model_type': 'unknown',
            'capabilities': [],
            'has_attention': False,
            'has_embodiment': False,
            'has_motion_decoder': False
        }
        
        # æ£€æŸ¥æ¨¡å‹ç±»å‹
        model_class_name = self.model.__class__.__name__
        
        if hasattr(self.model, 'get_model_info'):
            # æ–°çš„ablationæ¨¡å‹æœ‰get_model_infoæ–¹æ³•
            info = self.model.get_model_info()
            model_info.update(info)
            model_info['model_type'] = info.get('model_type', model_class_name)
            model_info['has_embodiment'] = info.get('has_embodiment', False)
            model_info['has_motion_decoder'] = info.get('has_motion_decoder', False)
        else:
            # åŸå§‹Embodimentæ¨¡å‹
            model_info['model_type'] = 'EmbodiedCountingModel'
            model_info['has_embodiment'] = True
            model_info['has_motion_decoder'] = True
        
        # æ£€æŸ¥æ˜¯å¦æœ‰attentionæœºåˆ¶
        if hasattr(self.model, 'fusion') or hasattr(self.model, 'attention_weights_history'):
            model_info['has_attention'] = True
        
        # ç¡®å®šæ”¯æŒçš„åŠŸèƒ½
        capabilities = ['counting', 'lstm_states']
        if model_info['has_attention']:
            capabilities.append('attention')
        if model_info['has_embodiment']:
            capabilities.append('embodiment')
        if model_info['has_motion_decoder']:
            capabilities.append('motion_prediction')
            
        model_info['capabilities'] = capabilities
        
        return model_info
    
    def _prepare_model_for_visualization(self):
        """ä¸ºå¯è§†åŒ–å‡†å¤‡æ¨¡å‹"""
        # æ¸…ç©ºå†å²è®°å½•
        if hasattr(self.model, 'lstm_hidden_states'):
            self.model.lstm_hidden_states = []
        if hasattr(self.model, 'attention_weights_history'):
            self.model.attention_weights_history = []
    
    def _get_model_outputs(self, sequence_data):
        """è·å–æ¨¡å‹è¾“å‡º"""
        self._prepare_model_for_visualization()
        
        with torch.no_grad():
            # æ ¹æ®æ¨¡å‹ç±»å‹è°ƒç”¨forward
            if self.model_info['has_attention']:
                outputs = self.model(
                    sequence_data=sequence_data,
                    use_teacher_forcing=False,
                    return_attention=True
                )
            else:
                outputs = self.model(
                    sequence_data=sequence_data,
                    use_teacher_forcing=False
                )
        
        return outputs
    
    def _extract_visualization_data(self, outputs):
        """æå–å¯è§†åŒ–æ•°æ®"""
        viz_data = {}
        
        # è®¡æ•°ç›¸å…³æ•°æ®
        count_logits = outputs['counts'][0]  # [seq_len, 11]
        
        viz_data['softmax_probs'] = F.softmax(count_logits, dim=-1)  # [seq_len, 11]
        viz_data['predictions'] = torch.argmax(count_logits, dim=-1)  # [seq_len]
        
        # LSTMçŠ¶æ€
        if hasattr(self.model, 'lstm_hidden_states') and self.model.lstm_hidden_states:
            viz_data['lstm_states'] = self.model.lstm_hidden_states
        else:
            viz_data['lstm_states'] = []
        
        # Attentionæƒé‡
        if (self.model_info['has_attention'] and 
            hasattr(self.model, 'attention_weights_history') and 
            self.model.attention_weights_history):
            viz_data['attention_weights'] = self.model.attention_weights_history
        else:
            viz_data['attention_weights'] = []
        
        # å…³èŠ‚é¢„æµ‹ï¼ˆå¦‚æœæœ‰ï¼‰
        if 'joints' in outputs:
            viz_data['joint_predictions'] = outputs['joints'][0]
        else:
            viz_data['joint_predictions'] = None
            
        return viz_data
    
    def visualize_sample_sequence(self, sample_data, sample_id, save_dir):
        """å¯è§†åŒ–å•ä¸ªæ ·æœ¬çš„å®Œæ•´åºåˆ—"""
        
        # åˆ›å»ºæ ·æœ¬ä¿å­˜ç›®å½•
        sample_dir = os.path.join(save_dir, f'sample_{sample_id}')
        os.makedirs(sample_dir, exist_ok=True)
        
        # å‡†å¤‡æ•°æ®
        sequence_data = {
            'images': sample_data['sequence_data']['images'].unsqueeze(0).to(self.device),
            'joints': sample_data['sequence_data']['joints'].unsqueeze(0).to(self.device),
            'timestamps': sample_data['sequence_data']['timestamps'].unsqueeze(0).to(self.device),
            'labels': sample_data['sequence_data']['labels'].unsqueeze(0).to(self.device)
        }
        
        seq_len = sequence_data['images'].shape[1]
        true_label = sample_data['label'].item()
        
        print(f"\nå¤„ç†æ ·æœ¬ {sample_id} (çœŸå®æ ‡ç­¾: {true_label}, åºåˆ—é•¿åº¦: {seq_len})")
        
        # è·å–æ¨¡å‹è¾“å‡ºå’Œå¯è§†åŒ–æ•°æ®
        outputs = self._get_model_outputs(sequence_data)
        viz_data = self._extract_visualization_data(outputs)
        
        # ç”Ÿæˆæ¯ä¸€å¸§çš„å¯è§†åŒ–
        self._generate_frame_visualizations(
            sequence_data, viz_data, true_label, sample_id, sample_dir, seq_len
        )
        
        # ç”Ÿæˆæ±‡æ€»å›¾
        self._generate_summary_plot(
            viz_data, sequence_data, true_label, sample_id, sample_dir
        )
        
        # ä¿å­˜æ•°å€¼æ•°æ®
        self._save_numerical_data(
            viz_data, true_label, sample_id, sample_dir
        )
        
        print(f"âœ… æ ·æœ¬ {sample_id} å¯è§†åŒ–å®Œæˆ")
    
    def _generate_frame_visualizations(self, sequence_data, viz_data, true_label, 
                                     sample_id, sample_dir, seq_len):
        """ç”Ÿæˆæ¯ä¸€å¸§çš„å¯è§†åŒ–"""
        
        softmax_probs = viz_data['softmax_probs']
        predictions = viz_data['predictions']
        lstm_states = viz_data['lstm_states']
        attention_weights = viz_data['attention_weights']
        
        for t in range(seq_len):
            fig = self._create_frame_figure(
                sequence_data, viz_data, t, true_label, sample_id, seq_len
            )
            
            # ä¿å­˜å½“å‰å¸§
            frame_path = os.path.join(sample_dir, f'frame_{t:03d}.png')
            plt.savefig(frame_path, dpi=150, bbox_inches='tight')
            plt.close()
    
    def _create_frame_figure(self, sequence_data, viz_data, t, true_label, sample_id, seq_len):
        """åˆ›å»ºå•å¸§å¯è§†åŒ–å›¾"""
        
        # æ ¹æ®æ¨¡å‹èƒ½åŠ›ç¡®å®šå­å›¾å¸ƒå±€
        if self.model_info['has_attention']:
            fig = plt.figure(figsize=(20, 12))
            subplot_layout = (2, 3)
        else:
            fig = plt.figure(figsize=(16, 10))
            subplot_layout = (2, 2)
        
        subplot_idx = 1
        
        # 1. åŸå§‹å›¾åƒ
        ax1 = plt.subplot(subplot_layout[0], subplot_layout[1], subplot_idx)
        self._plot_original_image(ax1, sequence_data['images'][0, t], t, seq_len)
        subplot_idx += 1
        
        # 2. Attentionçƒ­åŠ›å›¾ (å¦‚æœæ”¯æŒ)
        if self.model_info['has_attention']:
            ax2 = plt.subplot(subplot_layout[0], subplot_layout[1], subplot_idx)
            self._plot_attention_map(ax2, viz_data['attention_weights'], 
                                   sequence_data['images'][0, t], t)
            subplot_idx += 1
        
        # 3. Softmaxè¾“å‡ºåˆ†å¸ƒ
        ax3 = plt.subplot(subplot_layout[0], subplot_layout[1], subplot_idx)
        self._plot_softmax_distribution(ax3, viz_data['softmax_probs'], 
                                      viz_data['predictions'], t)
        subplot_idx += 1
        
        # 4. Softmaxåˆ†å¸ƒæ—¶åºå˜åŒ–
        ax4 = plt.subplot(subplot_layout[0], subplot_layout[1], subplot_idx)
        self._plot_softmax_evolution(ax4, viz_data['softmax_probs'], 
                                   viz_data['predictions'], t, seq_len)
        subplot_idx += 1
        
        # 5. LSTMéšçŠ¶æ€å˜åŒ–
        if subplot_idx <= subplot_layout[0] * subplot_layout[1]:
            ax5 = plt.subplot(subplot_layout[0], subplot_layout[1], subplot_idx)
            self._plot_lstm_states(ax5, viz_data['lstm_states'], t)
            subplot_idx += 1
        
        # 6. é¢„æµ‹åºåˆ—å’Œä¿¡æ¯
        if subplot_idx <= subplot_layout[0] * subplot_layout[1]:
            ax6 = plt.subplot(subplot_layout[0], subplot_layout[1], subplot_idx)
            self._plot_prediction_sequence(ax6, viz_data['predictions'], 
                                         sequence_data['labels'][0], t, 
                                         true_label, sample_id)
        
        plt.tight_layout()
        return fig
    
    def _plot_original_image(self, ax, img_tensor, t, seq_len):
        """ç»˜åˆ¶åŸå§‹å›¾åƒ"""
        img = img_tensor.cpu()
        
        if img.shape[0] == 1:  # ç°åº¦å›¾
            img = img.squeeze(0)
            ax.imshow(img, cmap='gray')
        else:  # RGB
            img = img.permute(1, 2, 0)
            # åå½’ä¸€åŒ–
            mean = torch.tensor([0.485, 0.456, 0.406])
            std = torch.tensor([0.229, 0.224, 0.225])
            img = img * std + mean
            img = torch.clamp(img, 0, 1)
            ax.imshow(img)
        
        ax.set_title(f'Frame {t+1}/{seq_len}')
        ax.axis('off')
    
    def _plot_attention_map(self, ax, attention_weights, img_tensor, t):
        """ç»˜åˆ¶attentionçƒ­åŠ›å›¾"""
        if t < len(attention_weights):
            att_weights = attention_weights[t].cpu().numpy().squeeze()
            spatial_size = len(att_weights)
            grid_size = int(np.sqrt(spatial_size))
            
            if grid_size * grid_size == spatial_size:
                att_map = att_weights.reshape(grid_size, grid_size)
                att_map_resized = cv2.resize(att_map, (224, 224))
                
                # æ˜¾ç¤ºå›¾åƒèƒŒæ™¯
                img = img_tensor.cpu()
                if img.shape[0] == 1:
                    img = img.squeeze(0)
                    ax.imshow(img, cmap='gray', alpha=0.5)
                else:
                    img = img.permute(1, 2, 0)
                    mean = torch.tensor([0.485, 0.456, 0.406])
                    std = torch.tensor([0.229, 0.224, 0.225])
                    img = img * std + mean
                    img = torch.clamp(img, 0, 1)
                    ax.imshow(img, alpha=0.5)
                
                # å åŠ attention
                im = ax.imshow(att_map_resized, cmap='viridis', alpha=0.5)
                plt.colorbar(im, ax=ax, fraction=0.046)
            else:
                ax.bar(range(len(att_weights)), att_weights)
                ax.set_xlabel('Spatial Position')
                ax.set_ylabel('Attention Weight')
            
            ax.set_title(f'Attention Map (t={t+1})')
        else:
            ax.text(0.5, 0.5, 'No attention data', ha='center', va='center')
            ax.set_title(f'Attention Map (t={t+1})')
    
    def _plot_softmax_distribution(self, ax, softmax_probs, predictions, t):
        """ç»˜åˆ¶Softmaxåˆ†å¸ƒ"""
        probs_t = softmax_probs[t].cpu().numpy()
        pred_t = predictions[t].item()
        
        bars = ax.bar(range(11), probs_t, color='skyblue', alpha=0.7)
        bars[pred_t].set_color('red')
        
        # æ·»åŠ æ¦‚ç‡å€¼æ ‡ç­¾
        for i, prob in enumerate(probs_t):
            if prob > 0.01:
                ax.text(i, prob + 0.01, f'{prob:.2f}', ha='center', fontsize=8)
        
        ax.set_xlabel('Count Class')
        ax.set_ylabel('Probability')
        ax.set_title(f'Softmax Output (Pred: {pred_t})')
        ax.set_xticks(range(11))
        ax.set_ylim(0, 1.1)
        ax.grid(True, alpha=0.3)
    
    def _plot_softmax_evolution(self, ax, softmax_probs, predictions, t, seq_len):
        """ç»˜åˆ¶Softmaxæ¼”åŒ–"""
        pred_t = predictions[t].item()
        
        for class_idx in range(11):
            probs_history = softmax_probs[:t+1, class_idx].cpu().numpy()
            if np.max(probs_history) > 0.1:
                ax.plot(range(t+1), probs_history, label=f'Class {class_idx}', 
                       linewidth=2 if class_idx == pred_t else 1,
                       alpha=1.0 if class_idx == pred_t else 0.5)
        
        ax.set_xlabel('Frame')
        ax.set_ylabel('Probability')
        ax.set_title('Softmax Evolution')
        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        ax.grid(True, alpha=0.3)
        ax.set_xlim(-0.5, seq_len-0.5)
    
    def _plot_lstm_states(self, ax, lstm_states, t):
        """ç»˜åˆ¶LSTMçŠ¶æ€ - æ”¯æŒ2Då’Œ3D PCAå¯è§†åŒ–"""
        if t < len(lstm_states):
            states_so_far = torch.stack(lstm_states[:t+1]).cpu().numpy()
            if len(states_so_far.shape) > 2:
                states_so_far = states_so_far.squeeze(1)
            
            if states_so_far.shape[0] > 3:  # è‡³å°‘éœ€è¦4ä¸ªæ—¶é—´æ­¥æ‰èƒ½åšæœ‰æ„ä¹‰çš„PCA
                try:
                    from sklearn.decomposition import PCA
                    
                    # é€‰æ‹©ä½¿ç”¨2Dè¿˜æ˜¯3D PCA
                    if states_so_far.shape[1] >= 3 and t >= 5:  # å¦‚æœç»´åº¦å¤Ÿé«˜ä¸”æ—¶é—´æ­¥å¤Ÿå¤šï¼Œä½¿ç”¨3D
                        try:
                            from mpl_toolkits.mplot3d import Axes3D
                            
                            pca = PCA(n_components=3)
                            states_pca = pca.fit_transform(states_so_far)
                            
                            # æ¸…é™¤å½“å‰axesçš„å†…å®¹
                            ax.clear()
                            
                            # å°†å½“å‰axesè½¬æ¢ä¸º3D
                            ax.remove()
                            fig = plt.gcf()  # è·å–å½“å‰figure
                            
                            # æ ¹æ®æ¨¡å‹æ˜¯å¦æ”¯æŒattentionç¡®å®šsubplotä½ç½®
                            if self.model_info['has_attention']:
                                ax = fig.add_subplot(2, 3, 5, projection='3d')
                            else:
                                ax = fig.add_subplot(2, 2, 4, projection='3d')
                            
                            # ç»˜åˆ¶3Dè½¨è¿¹
                            ax.plot(states_pca[:, 0], states_pca[:, 1], states_pca[:, 2], 
                                   'b-', alpha=0.6, linewidth=2)
                            
                            # ç»˜åˆ¶æ—¶é—´æ­¥ç‚¹
                            scatter = ax.scatter(states_pca[:, 0], states_pca[:, 1], states_pca[:, 2],
                                               c=range(t+1), cmap='viridis', s=60, alpha=0.8)
                            
                            # çªå‡ºæ˜¾ç¤ºå½“å‰ç‚¹
                            ax.scatter(states_pca[-1, 0], states_pca[-1, 1], states_pca[-1, 2],
                                      color='red', s=120, marker='*', 
                                      edgecolor='black', linewidth=1, label='Current')
                            
                            ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%})')
                            ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%})')
                            ax.set_zlabel(f'PC3 ({pca.explained_variance_ratio_[2]:.2%})')
                            ax.set_title('LSTM State Trajectory (3D PCA)')
                            
                            # è®¾ç½®è§†è§’
                            ax.view_init(elev=20, azim=45)
                            
                            total_var = pca.explained_variance_ratio_.sum()
                            ax.text2D(0.02, 0.98, f'Total Var: {total_var:.1%}', 
                                     transform=ax.transAxes, fontsize=9,
                                     bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.8))
                            
                        except Exception as e3d:
                            print(f"3D PCAå¤±è´¥ï¼Œå›é€€åˆ°2D: {e3d}")
                            # å›é€€åˆ°2D PCA
                            self._plot_2d_pca(ax, states_so_far, t)
                    
                    else:
                        # ä½¿ç”¨2D PCA
                        self._plot_2d_pca(ax, states_so_far, t)
                        
                except ImportError:
                    # å¦‚æœæ²¡æœ‰sklearnï¼Œæ˜¾ç¤ºå‰ä¸¤ä¸ªç»´åº¦çš„åŸå§‹å€¼
                    self._plot_raw_states(ax, states_so_far)
                    
            else:
                # æ—¶é—´æ­¥ä¸å¤Ÿï¼Œæ˜¾ç¤ºåŸå§‹ç»´åº¦
                self._plot_raw_states(ax, states_so_far)
        else:
            ax.text(0.5, 0.5, 'No LSTM state data', ha='center', va='center')
            ax.set_title('LSTM States (Not Available)')
        
        # åªæœ‰2Då›¾æ‰æœ‰gridæ–¹æ³•
        if hasattr(ax, 'grid') and not hasattr(ax, 'zaxis'):
            ax.grid(True, alpha=0.3)
    
    def _plot_2d_pca(self, ax, states_so_far, t):
        """ç»˜åˆ¶2D PCA"""
        from sklearn.decomposition import PCA
        
        pca = PCA(n_components=2)
        states_2d = pca.fit_transform(states_so_far)
        
        # ç»˜åˆ¶2Dè½¨è¿¹
        ax.plot(states_2d[:, 0], states_2d[:, 1], 'b-', alpha=0.6, linewidth=2)
        ax.scatter(states_2d[:, 0], states_2d[:, 1], 
                  c=range(t+1), cmap='viridis', s=60, alpha=0.8)
        ax.scatter(states_2d[-1, 0], states_2d[-1, 1], 
                  color='red', s=100, marker='*', 
                  edgecolor='black', linewidth=1, label='Current')
        
        ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%})')
        ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%})')
        ax.set_title('LSTM State Trajectory (2D PCA)')
        ax.legend()
        
        total_var = pca.explained_variance_ratio_.sum()
        ax.text(0.02, 0.98, f'Total Var: {total_var:.1%}', 
               transform=ax.transAxes, fontsize=9,
               bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.8))
    
    def _plot_raw_states(self, ax, states_so_far):
        """ç»˜åˆ¶åŸå§‹çŠ¶æ€å€¼"""
        ax.plot(states_so_far[:, 0], 'b-', label='Dim 0', linewidth=2)
        if states_so_far.shape[1] > 1:
            ax.plot(states_so_far[:, 1], 'r-', label='Dim 1', linewidth=2)
        if states_so_far.shape[1] > 2:
            ax.plot(states_so_far[:, 2], 'g-', label='Dim 2', linewidth=2)
        ax.set_xlabel('Frame')
        ax.set_ylabel('State Value')
        ax.set_title('LSTM State Evolution (Raw)')
        ax.legend()
        ax.text(0.02, 0.98, 'Raw dimensions', 
               transform=ax.transAxes, fontsize=9,
               bbox=dict(boxstyle="round,pad=0.3", facecolor="yellow", alpha=0.8))
    
    def _plot_prediction_sequence(self, ax, predictions, true_labels, t, 
                                true_label, sample_id):
        """ç»˜åˆ¶é¢„æµ‹åºåˆ—"""
        pred_seq = predictions[:t+1].cpu().numpy()
        true_seq = true_labels[:t+1].cpu().numpy()
        
        ax.plot(range(t+1), pred_seq, 'ro-', label='Predictions', markersize=8)
        ax.plot(range(t+1), true_seq, 'bs--', label='True Labels', markersize=6)
        
        ax.set_xlabel('Frame')
        ax.set_ylabel('Count')
        ax.set_title('Prediction Sequence')
        ax.legend()
        ax.grid(True, alpha=0.3)
        ax.set_ylim(-0.5, 10.5)
        ax.set_yticks(range(11))
        
        # æ·»åŠ ä¿¡æ¯æ–‡æœ¬
        pred_t = predictions[t].item()
        
        # å®‰å…¨åœ°è®¡ç®—ç½®ä¿¡åº¦
        try:
            # å¯¹å½“å‰æ—¶åˆ»çš„logitsè®¡ç®—softmax
            current_logits = predictions[t].float()
            if current_logits.dim() == 0:  # å¦‚æœæ˜¯æ ‡é‡
                confidence = 1.0  # æˆ–è€…è®¾ä¸ºåˆç†çš„é»˜è®¤å€¼
            else:
                current_probs = F.softmax(current_logits.unsqueeze(0), dim=-1)
                confidence = current_probs[pred_t].item()
        except:
            confidence = 0.0  # å¦‚æœè®¡ç®—å¤±è´¥ï¼Œè®¾ä¸º0
        
        info_text = f"Sample ID: {sample_id}\n"
        info_text += f"True Label: {true_label}\n"
        info_text += f"Current Pred: {pred_t}\n"
        info_text += f"Final Pred: {predictions[-1].item()}\n"
        info_text += f"Model: {self.model_info['model_type']}"
        
        ax.text(0.02, 0.98, info_text, transform=ax.transAxes,
               bbox=dict(boxstyle="round,pad=0.3", facecolor="wheat", alpha=0.5),
               verticalalignment='top', fontsize=10)
    
    def _generate_summary_plot(self, viz_data, sequence_data, true_label, 
                              sample_id, sample_dir):
        """ç”Ÿæˆæ±‡æ€»å¯è§†åŒ–å›¾"""
        
        fig = plt.figure(figsize=(16, 10))
        
        # 1. Softmaxçƒ­åŠ›å›¾
        ax1 = plt.subplot(2, 2, 1)
        sns.heatmap(viz_data['softmax_probs'].cpu().numpy().T, 
                   cmap='YlOrRd', cbar=True, 
                   xticklabels=range(len(viz_data['softmax_probs'])),
                   yticklabels=range(11))
        ax1.set_xlabel('Frame')
        ax1.set_ylabel('Count Class')
        ax1.set_title('Softmax Probability Heatmap')
        
        # 2. é¢„æµ‹å‡†ç¡®åº¦æ—¶åºå›¾
        ax2 = plt.subplot(2, 2, 2)
        pred_np = viz_data['predictions'].cpu().numpy()
        true_np = sequence_data['labels'][0].cpu().numpy()
        correct = pred_np == true_np
        
        ax2.plot(range(len(pred_np)), pred_np, 'ro-', label='Predictions')
        ax2.plot(range(len(true_np)), true_np, 'bs--', label='True Labels')
        
        for i, is_correct in enumerate(correct):
            color = 'green' if is_correct else 'red'
            ax2.axvspan(i-0.3, i+0.3, alpha=0.2, color=color)
        
        ax2.set_xlabel('Frame')
        ax2.set_ylabel('Count')
        ax2.set_title('Prediction vs Ground Truth')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. LSTMçŠ¶æ€çƒ­åŠ›å›¾
        ax3 = plt.subplot(2, 2, 3)
        if viz_data['lstm_states']:
            lstm_array = torch.stack(viz_data['lstm_states']).cpu().numpy()
            if len(lstm_array.shape) > 2:
                lstm_array = lstm_array.squeeze(1)
            
            n_dims_to_show = min(50, lstm_array.shape[1])
            sns.heatmap(lstm_array[:, :n_dims_to_show].T,
                       cmap='coolwarm', center=0,
                       xticklabels=range(len(viz_data['lstm_states'])),
                       yticklabels=False)
            ax3.set_xlabel('Frame')
            ax3.set_ylabel(f'LSTM Hidden Dims (first {n_dims_to_show})')
            ax3.set_title('LSTM Hidden State Evolution')
        else:
            ax3.text(0.5, 0.5, 'No LSTM state data', ha='center', va='center')
            ax3.set_title('LSTM States (Not Available)')
        
        # 4. ç½®ä¿¡åº¦å˜åŒ–
        ax4 = plt.subplot(2, 2, 4)
        
        max_probs = torch.max(viz_data['softmax_probs'], dim=1)[0].cpu().numpy()
        
        correct_class_probs = []
        for t in range(len(viz_data['predictions'])):
            true_class = int(true_np[t])
            prob = viz_data['softmax_probs'][t, true_class].item()
            correct_class_probs.append(prob)
        
        ax4.plot(range(len(max_probs)), max_probs, 'b-', 
                label='Max Confidence', linewidth=2)
        ax4.plot(range(len(correct_class_probs)), correct_class_probs, 'g--', 
                label='True Class Prob', linewidth=2)
        
        ax4.fill_between(range(len(max_probs)), 0, max_probs, alpha=0.3)
        ax4.set_xlabel('Frame')
        ax4.set_ylabel('Probability')
        ax4.set_title('Model Confidence Over Time')
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        ax4.set_ylim(0, 1.1)
        
        # æ·»åŠ æ¨¡å‹ä¿¡æ¯
        model_info_text = f"Model: {self.model_info['model_type']}\n"
        model_info_text += f"Capabilities: {', '.join(self.model_info['capabilities'])}"
        
        fig.suptitle(f'Sample {sample_id} Summary - True Label: {true_label}, '
                    f'Final Prediction: {viz_data["predictions"][-1].item()}\n'
                    f'{model_info_text}', fontsize=14)
        
        plt.tight_layout()
        
        summary_path = os.path.join(sample_dir, 'summary.png')
        plt.savefig(summary_path, dpi=200, bbox_inches='tight')
        plt.close()
    
    def _save_numerical_data(self, viz_data, true_label, sample_id, sample_dir):
        """ä¿å­˜æ•°å€¼æ•°æ®"""
        
        data = {
            'sample_id': sample_id,
            'true_label': int(true_label),
            'predictions': viz_data['predictions'].cpu().tolist(),
            'softmax_probs': viz_data['softmax_probs'].cpu().tolist(),
            'final_prediction': int(viz_data['predictions'][-1].item()),
            'sequence_length': len(viz_data['predictions']),
            'accuracy': float((viz_data['predictions'].cpu().numpy() == true_label).mean()),
            'model_info': self.model_info
        }
        
        # LSTMçŠ¶æ€ç»Ÿè®¡
        if viz_data['lstm_states']:
            lstm_array = torch.stack(viz_data['lstm_states']).cpu().numpy()
            if len(lstm_array.shape) > 2:
                lstm_array = lstm_array.squeeze(1)
            
            data['lstm_stats'] = {
                'mean': float(lstm_array.mean()),
                'std': float(lstm_array.std()),
                'shape': list(lstm_array.shape)
            }
        
        # Attentionç»Ÿè®¡
        if viz_data['attention_weights']:
            att_array = torch.cat(viz_data['attention_weights']).cpu().numpy()
            data['attention_stats'] = {
                'mean': float(att_array.mean()),
                'std': float(att_array.std()),
                'max': float(att_array.max()),
                'min': float(att_array.min())
            }
        
        json_path = os.path.join(sample_dir, 'data.json')
        with open(json_path, 'w') as f:
            json.dump(data, f, indent=2)


def load_model_and_data(checkpoint_path, val_csv, data_root, device='cuda'):
    """é€šç”¨æ¨¡å‹å’Œæ•°æ®åŠ è½½å‡½æ•°"""
    print("ğŸ“¥ åŠ è½½æ¨¡å‹å’Œæ•°æ®...")
    
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    config = checkpoint['config']
    
    # å¯¼å…¥æ¨¡å—
    from DataLoader_embodiment import BallCountingDataset
    
    # ç¡®å®šå›¾åƒæ¨¡å¼
    image_mode = config.get('image_mode', 'rgb')
    input_channels = 3 if image_mode == 'rgb' else 1
    
    # æ£€æŸ¥æ¨¡å‹ç±»å‹
    model_type = checkpoint.get('model_type', 'embodied')
    
    if model_type in ['counting_only', 'visual_only']:
        # Ablationæ¨¡å‹
        from Model_embodiment_ablation import create_ablation_model
        model = create_ablation_model(model_type, config)
        print(f"âœ… åŠ è½½æ¶ˆèå®éªŒæ¨¡å‹: {model_type}")
    else:
        # åŸå§‹Embodimentæ¨¡å‹
        from Model_embodiment import EmbodiedCountingModel
        model_config = config['model_config'].copy()
        model_config['input_channels'] = input_channels
        model = EmbodiedCountingModel(**model_config)
        print("âœ… åŠ è½½åŸå§‹å…·èº«è®¡æ•°æ¨¡å‹")
    
    model.load_state_dict(checkpoint['model_state_dict'])
    
    device = torch.device(device if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    model.eval()
    
    print(f"   å›¾åƒæ¨¡å¼: {image_mode}, è®¾å¤‡: {device}")
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = BallCountingDataset(
        csv_path=val_csv,
        data_root=data_root,
        sequence_length=config['sequence_length'],
        normalize=config['normalize'],
        image_mode=image_mode,
        normalize_images=True
    )
    
    print(f"âœ… æ•°æ®é›†åŠ è½½å®Œæˆï¼Œæ ·æœ¬æ•°: {len(dataset)}")
    
    return model, dataset, device, config


def main():
    parser = argparse.ArgumentParser(description='é€šç”¨åŠ¨æ€æ¨¡å‹å¯è§†åŒ–å·¥å…·')
    
    parser.add_argument('--checkpoint', type=str, required=True,
                       help='æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--val_csv', type=str, 
                       default='scratch/Ball_counting_CNN/Tools_script/ball_counting_dataset_val_single_per_label_v1.csv',
                       help='éªŒè¯é›†CSVæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--data_root', type=str, 
                       default='/mnt/iusers01/fatpou01/compsci01/k09562zs/scratch/Ball_counting_CNN/ball_data_collection',
                       help='æ•°æ®æ ¹ç›®å½•')
    parser.add_argument('--save_dir', type=str, default='./universal_visualizations',
                       help='ç»“æœä¿å­˜ç›®å½•')
    parser.add_argument('--device', type=str, default='cuda',
                       help='è®¾å¤‡ (cuda/cpu)')
    parser.add_argument('--sample_indices', type=int, nargs='+', default=None,
                       help='è¦å¯è§†åŒ–çš„æ ·æœ¬ç´¢å¼•ï¼ˆé»˜è®¤å…¨éƒ¨ï¼‰')
    parser.add_argument('--max_samples', type=int, default=10,
                       help='æœ€å¤§å¯è§†åŒ–æ ·æœ¬æ•°')
    
    args = parser.parse_args()
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    save_dir = os.path.join(args.save_dir, f'viz_{timestamp}')
    os.makedirs(save_dir, exist_ok=True)
    
    print("ğŸ¬ é€šç”¨åŠ¨æ€æ¨¡å‹å¯è§†åŒ–å·¥å…·")
    print("="*50)
    print(f"æ£€æŸ¥ç‚¹: {args.checkpoint}")
    print(f"æ•°æ®é›†: {args.val_csv}")
    print(f"ä¿å­˜ç›®å½•: {save_dir}")
    print("="*50)
    
    try:
        # åŠ è½½æ¨¡å‹å’Œæ•°æ®
        model, dataset, device, config = load_model_and_data(
            args.checkpoint, args.val_csv, args.data_root, args.device
        )
        
        # åˆ›å»ºå¯è§†åŒ–å™¨
        visualizer = UniversalModelVisualizer(model, device)
        
        # ç¡®å®šè¦å¯è§†åŒ–çš„æ ·æœ¬
        if args.sample_indices is not None:
            sample_indices = args.sample_indices
        else:
            sample_indices = list(range(min(len(dataset), args.max_samples)))
        
        print(f"\nå‡†å¤‡å¯è§†åŒ– {len(sample_indices)} ä¸ªæ ·æœ¬: {sample_indices}")
        
        # å¯è§†åŒ–æ¯ä¸ªæ ·æœ¬
        for idx in sample_indices:
            if idx >= len(dataset):
                print(f"âš ï¸ æ ·æœ¬ç´¢å¼• {idx} è¶…å‡ºèŒƒå›´ï¼Œè·³è¿‡")
                continue
            
            try:
                sample_data = dataset[idx]
                visualizer.visualize_sample_sequence(sample_data, idx, save_dir)
            except Exception as e:
                print(f"âŒ æ ·æœ¬ {idx} å¯è§†åŒ–å¤±è´¥: {e}")
                continue
        
        print(f"\nğŸ‰ å¯è§†åŒ–å®Œæˆï¼")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {save_dir}")
        print(f"ğŸ“Š æ¯ä¸ªæ ·æœ¬çš„å¸§åºåˆ—å’Œæ±‡æ€»å›¾å·²ç”Ÿæˆ")
        
    except Exception as e:
        print(f"âŒ å¯è§†åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


def generate_index_html(sample_indices, save_dir, model_info):
    """ç”ŸæˆHTMLç´¢å¼•é¡µé¢ - å·²ç§»é™¤ï¼Œä¸å†ä½¿ç”¨"""
    pass


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) == 1:
        print("ğŸ¬ é€šç”¨åŠ¨æ€æ¨¡å‹å¯è§†åŒ–å·¥å…·")
        print("="*50)
        print("æ”¯æŒæ‰€æœ‰æ¨¡å‹ç±»å‹çš„å¯è§†åŒ–:")
        print("  â€¢ åŸå§‹Embodimentæ¨¡å‹ (å®Œæ•´åŠŸèƒ½)")
        print("  â€¢ Counting-Onlyæ¨¡å‹ (æ— å…³èŠ‚é¢„æµ‹)")
        print("  â€¢ Visual-Onlyæ¨¡å‹ (æ— å…·èº«ä¿¡æ¯)")
        print()
        print("è‡ªåŠ¨æ£€æµ‹æ¨¡å‹èƒ½åŠ›å¹¶è°ƒæ•´å¯è§†åŒ–å†…å®¹:")
        print("  â€¢ Attentionçƒ­åŠ›å›¾ (å¦‚æœæ”¯æŒ)")
        print("  â€¢ Softmaxè¾“å‡ºåˆ†å¸ƒ")
        print("  â€¢ LSTMéšçŠ¶æ€æ¼”åŒ–")
        print("  â€¢ é¢„æµ‹åºåˆ—å¯¹æ¯”")
        print()
        print("ä½¿ç”¨æ–¹æ³•:")
        print("python Universal_LSTM_Viz.py --checkpoint MODEL.pth --val_csv VAL.csv")
        print()
        print("ç¤ºä¾‹:")
        print("# å¯è§†åŒ–åŸå§‹Embodimentæ¨¡å‹")
        print("python Universal_LSTM_Viz.py \\")
        print("    --checkpoint ./best_embodied_model.pth \\")
        print("    --val_csv ./small_val_subset.csv")
        print()
        print("# å¯è§†åŒ–Counting-Onlyæ¶ˆèæ¨¡å‹")
        print("python Universal_LSTM_Viz.py \\")
        print("    --checkpoint ./best_counting_only_model.pth \\")
        print("    --val_csv ./small_val_subset.csv")
        print()
        print("# å¯è§†åŒ–Visual-Onlyæ¶ˆèæ¨¡å‹")
        print("python Universal_LSTM_Viz.py \\")
        print("    --checkpoint ./best_visual_only_model.pth \\")
        print("    --val_csv ./small_val_subset.csv")
        print()
        print("å¯é€‰å‚æ•°:")
        print("  --save_dir DIR          ä¿å­˜ç›®å½•")
        print("  --device DEVICE         è®¾å¤‡ (cuda/cpu)")
        print("  --sample_indices 0 1 2  æŒ‡å®šæ ·æœ¬ç´¢å¼•")
        print("  --max_samples N         æœ€å¤§æ ·æœ¬æ•°")
        print()
        print("ğŸ’¡ æ–°ç‰¹æ€§:")
        print("  â€¢ è‡ªåŠ¨æ£€æµ‹æ¨¡å‹ç±»å‹å’Œèƒ½åŠ›")
        print("  â€¢ æ ¹æ®æ¨¡å‹åŠŸèƒ½è°ƒæ•´å¯è§†åŒ–å¸ƒå±€")
        print("  â€¢ ç»Ÿä¸€çš„æ¥å£æ”¯æŒæ‰€æœ‰æ¨¡å‹")
        print("  â€¢ 3D PCAå¯è§†åŒ–LSTMçŠ¶æ€è½¨è¿¹")
        print("  â€¢ ä¼˜åŒ–çš„ä»£ç ç»“æ„å’Œé”™è¯¯å¤„ç†")
        print("  â€¢ è‡ªåŠ¨é€‰æ‹©2D/3D PCAæ˜¾ç¤º")
        sys.exit(0)
    
    main()