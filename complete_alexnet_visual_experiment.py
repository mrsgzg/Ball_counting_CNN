"""
AlexNetçº¯è§†è§‰æ¨¡å‹è®­ç»ƒè„šæœ¬ - å¯¹æ¯”é¢„è®­ç»ƒä¸éé¢„è®­ç»ƒæ•ˆæœ
ä½¿ç”¨å•å›¾åƒæ•°æ®åŠ è½½å™¨ï¼Œä¿æŒä¸å…·èº«æ¨¡å‹å®éªŒä¸€è‡´çš„è®­ç»ƒæµç¨‹
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
import numpy as np
import pandas as pd
import os
import time
import csv
from datetime import datetime
from sklearn.metrics import confusion_matrix
from tqdm import tqdm
import argparse
import json
import warnings
warnings.filterwarnings('ignore')
import matplotlib.pyplot as plt
import seaborn as sns

# è®¾ç½®matplotlibä¸ºéäº¤äº’å¼åç«¯
plt.switch_backend('Agg')

# å¯¼å…¥æˆ‘ä»¬çš„æ¨¡å‹å’Œæ•°æ®åŠ è½½å™¨
from Model_alexnet_visual import create_visual_model
from DataLoader_single_image import get_single_image_data_loaders


class VisualOnlyTrainer:
    """çº¯è§†è§‰æ¨¡å‹è®­ç»ƒå™¨ - ä¸å…·èº«æ¨¡å‹ä¿æŒä¸€è‡´çš„è®­ç»ƒæµç¨‹"""
    
    def __init__(self, model, train_loader, val_loader, config, device, log_dir, checkpoint_dir):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.config = config
        self.device = device
        self.checkpoint_dir = checkpoint_dir
        
        # åˆ›å»ºcheckpointç›®å½•
        os.makedirs(self.checkpoint_dir, exist_ok=True)
        
        # TensorBoardè®°å½•å™¨
        self.writer = SummaryWriter(log_dir)
        
        # ä¼˜åŒ–å™¨ - ä¸å…·èº«æ¨¡å‹ä¿æŒä¸€è‡´
        self.optimizer = optim.Adam(
            model.parameters(),
            lr=config['learning_rate'],
            betas=config.get('adam_betas', (0.9, 0.999)),
            weight_decay=config.get('weight_decay', 1e-5)
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = self._create_scheduler()
        
        # æ¢¯åº¦è£å‰ªé˜ˆå€¼
        self.grad_clip_norm = config.get('grad_clip_norm', 1.0)
        
        # è®­ç»ƒçŠ¶æ€è®°å½•
        self.best_val_accuracy = 0.0
        self.best_val_loss = float('inf')
        self.training_history = []
        
        # è®°å½•é…ç½®åˆ°TensorBoard
        config_text = f"Model: Visual Only - {'Pretrained' if config['use_pretrain'] else 'No Pretrain'}\n"
        config_text += f"Learning Rate: {config['learning_rate']}\n"
        config_text += f"Batch Size: {config['batch_size']}\n"
        config_text += f"Image Mode: {config.get('image_mode', 'rgb')}\n"
        self.writer.add_text('Config', config_text, 0)
        
    def _create_scheduler(self):
        """åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨ - ä¸å…·èº«æ¨¡å‹ä¿æŒä¸€è‡´"""
        scheduler_type = self.config.get('scheduler_type', 'none')
        
        if scheduler_type == 'cosine':
            return optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer, T_max=self.config.get('total_epochs', 1000)
            )
        elif scheduler_type == 'plateau':
            return optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer, mode='min', factor=0.5,
                patience=self.config.get('scheduler_patience', 5)
            )
        else:
            return None
    
    def compute_loss(self, logits, labels):
        """è®¡ç®—æŸå¤± - ç®€å•çš„åˆ†ç±»æŸå¤±"""
        loss = F.cross_entropy(logits, labels)
        return loss
    
    def compute_metrics(self, logits, labels):
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        metrics = {}
        
        # é¢„æµ‹æ ‡ç­¾
        pred_labels = torch.argmax(logits, dim=-1)
        
        # å‡†ç¡®ç‡
        metrics['accuracy'] = (pred_labels == labels).float().mean().item()
        
        # Top-3å‡†ç¡®ç‡
        top3_pred = torch.topk(logits, k=min(3, logits.size(1)), dim=-1)[1]
        top3_correct = (top3_pred == labels.unsqueeze(1)).any(dim=1)
        metrics['top3_accuracy'] = top3_correct.float().mean().item()
        
        return metrics
    
    def compute_per_digit_accuracy(self, all_preds, all_labels):
        """è®¡ç®—æ¯ä¸ªæ•°å­—çš„å‡†ç¡®ç‡"""
        per_digit_acc = {}
        unique_labels = torch.unique(all_labels)
        
        for digit in range(11):  # 0-10çš„çƒæ•°
            mask = all_labels == digit
            if mask.sum() > 0:
                digit_acc = (all_preds[mask] == all_labels[mask]).float().mean().item()
                per_digit_acc[f'digit_{digit}_accuracy'] = digit_acc
            else:
                per_digit_acc[f'digit_{digit}_accuracy'] = 0.0
        
        return per_digit_acc
    
    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        total_correct = 0
        total_samples = 0
        
        # æ”¶é›†æ‰€æœ‰é¢„æµ‹ç”¨äºè®¡ç®—per-digit accuracy
        all_preds = []
        all_labels = []
        
        # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦æ¡
        pbar = tqdm(self.train_loader, desc=f'Epoch {epoch}', leave=False)
        
        for batch_idx, batch in enumerate(pbar):
            # æ•°æ®å‡†å¤‡
            images = batch['image'].to(self.device)
            labels = batch['label'].to(self.device)
            
            # å‰å‘ä¼ æ’­
            logits = self.model(images)
            
            # è®¡ç®—æŸå¤±
            loss = self.compute_loss(logits, labels)
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip_norm)
            
            self.optimizer.step()
            
            # ç»Ÿè®¡
            total_loss += loss.item() * images.size(0)
            pred_labels = torch.argmax(logits, dim=-1)
            total_correct += (pred_labels == labels).sum().item()
            total_samples += images.size(0)
            
            # æ”¶é›†é¢„æµ‹
            all_preds.append(pred_labels.cpu())
            all_labels.append(labels.cpu())
            
            # æ›´æ–°è¿›åº¦æ¡
            current_acc = total_correct / total_samples
            pbar.set_postfix({'loss': loss.item(), 'acc': f'{current_acc:.4f}'})
            
            # è®°å½•batchçº§åˆ«çš„æŸå¤±åˆ°TensorBoard
            global_step = epoch * len(self.train_loader) + batch_idx
            self.writer.add_scalar('Batch/Train_Loss', loss.item(), global_step)
        
        # è®¡ç®—epochçº§åˆ«çš„æŒ‡æ ‡
        avg_loss = total_loss / total_samples
        avg_accuracy = total_correct / total_samples
        
        # åˆå¹¶æ‰€æœ‰é¢„æµ‹
        all_preds = torch.cat(all_preds, dim=0)
        all_labels = torch.cat(all_labels, dim=0)
        
        # è®¡ç®—per-digit accuracy
        per_digit_metrics = self.compute_per_digit_accuracy(all_preds, all_labels)
        
        # è®°å½•åˆ°TensorBoard
        self.writer.add_scalar('Epoch/Train_Loss', avg_loss, epoch)
        self.writer.add_scalar('Epoch/Train_Accuracy', avg_accuracy, epoch)
        
        for key, value in per_digit_metrics.items():
            self.writer.add_scalar(f'Train/{key}', value, epoch)
        
        return avg_loss, avg_accuracy, per_digit_metrics
    
    @torch.no_grad()
    def validate(self, epoch):
        """éªŒè¯ - åŒ…å«æ¯ä¸ªæ•°å­—çš„å‡†ç¡®ç‡"""
        self.model.eval()
        total_loss = 0
        total_correct = 0
        total_samples = 0
        
        # æ”¶é›†æ‰€æœ‰é¢„æµ‹å’ŒçœŸå®æ ‡ç­¾
        all_preds = []
        all_labels = []
        
        for batch in self.val_loader:
            # æ•°æ®å‡†å¤‡
            images = batch['image'].to(self.device)
            labels = batch['label'].to(self.device)
            
            # å‰å‘ä¼ æ’­
            logits = self.model(images)
            
            # è®¡ç®—æŸå¤±
            loss = self.compute_loss(logits, labels)
            total_loss += loss.item() * images.size(0)
            
            # è®¡ç®—å‡†ç¡®ç‡
            pred_labels = torch.argmax(logits, dim=-1)
            total_correct += (pred_labels == labels).sum().item()
            total_samples += images.size(0)
            
            # æ”¶é›†é¢„æµ‹
            all_preds.append(pred_labels.cpu())
            all_labels.append(labels.cpu())
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_loss = total_loss / total_samples
        avg_accuracy = total_correct / total_samples
        
        # åˆå¹¶æ‰€æœ‰é¢„æµ‹
        all_preds = torch.cat(all_preds, dim=0)
        all_labels = torch.cat(all_labels, dim=0)
        
        # è®¡ç®—per-digit accuracy
        per_digit_metrics = self.compute_per_digit_accuracy(all_preds, all_labels)
        
        # è®¡ç®—æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(all_labels.numpy(), all_preds.numpy(), labels=list(range(11)))
        
        # ç»˜åˆ¶å¹¶ä¿å­˜æ··æ·†çŸ©é˜µ
        plt.figure(figsize=(12, 10))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                    xticklabels=range(11), yticklabels=range(11))
        plt.title(f'Confusion Matrix - Epoch {epoch}')
        plt.ylabel('True Label')
        plt.xlabel('Predicted Label')
        
        # ä¿å­˜å›¾åƒåˆ°TensorBoard
        self.writer.add_figure('Confusion_Matrix', plt.gcf(), epoch)
        plt.close()
        
        # è®°å½•åˆ°TensorBoard
        self.writer.add_scalar('Epoch/Val_Loss', avg_loss, epoch)
        self.writer.add_scalar('Epoch/Val_Accuracy', avg_accuracy, epoch)
        
        for key, value in per_digit_metrics.items():
            self.writer.add_scalar(f'Val/{key}', value, epoch)
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
        is_best = avg_accuracy > self.best_val_accuracy
        if is_best:
            self.best_val_accuracy = avg_accuracy
            self.best_val_loss = avg_loss
        
        return avg_loss, avg_accuracy, per_digit_metrics, is_best
    
    def save_checkpoint(self, epoch, val_loss, val_accuracy, is_best=False):
        """ä¿å­˜æ¨¡å‹checkpoint"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'val_loss': val_loss,
            'val_accuracy': val_accuracy,
            'best_val_accuracy': self.best_val_accuracy,
            'config': self.config,
            'model_info': self.model.get_model_info()
        }
        
        # ä¿å­˜æœ€æ–°çš„checkpoint
        latest_path = os.path.join(self.checkpoint_dir, 'latest_checkpoint.pth')
        torch.save(checkpoint, latest_path)
        
        # å¦‚æœæ˜¯æœ€ä½³æ¨¡å‹ï¼Œé¢å¤–ä¿å­˜
        if is_best:
            best_path = os.path.join(self.checkpoint_dir, 'best_model.pth')
            torch.save(checkpoint, best_path)
            print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (å‡†ç¡®ç‡: {val_accuracy:.4f})")
        
        # å®šæœŸä¿å­˜checkpoint
        if epoch % self.config.get('save_every', 100) == 0:
            epoch_path = os.path.join(self.checkpoint_dir, f'checkpoint_epoch_{epoch}.pth')
            torch.save(checkpoint, epoch_path)
    
    def train(self, num_epochs):
        """å®Œæ•´çš„è®­ç»ƒæµç¨‹"""
        print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ - {'é¢„è®­ç»ƒ' if self.config['use_pretrain'] else 'æ— é¢„è®­ç»ƒ'} AlexNet")
        print(f"è®¾å¤‡: {self.device}")
        print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(self.train_loader.dataset)}")
        print(f"éªŒè¯æ ·æœ¬æ•°: {len(self.val_loader.dataset)}")
        
        start_time = time.time()
        
        # ğŸ’¾ ä¿å­˜åˆå§‹æ¨¡å‹ï¼ˆepoch 0ï¼‰
        if self.config.get('save_checkpoints', True):
            print("ğŸ’¾ ä¿å­˜åˆå§‹æ¨¡å‹ (epoch 0)...")
            
            # å…ˆè¿›è¡Œä¸€æ¬¡éªŒè¯ï¼Œè·å–åˆå§‹æ€§èƒ½
            print("ğŸ“Š è¯„ä¼°åˆå§‹æ¨¡å‹æ€§èƒ½...")
            initial_val_loss, initial_val_acc, initial_per_digit, _ = self.validate(0)
            
            # ä¿å­˜åˆå§‹checkpoint
            initial_checkpoint = {
                'epoch': 0,
                'model_state_dict': self.model.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'val_loss': initial_val_loss,
                'val_accuracy': initial_val_acc,
                'best_val_accuracy': 0.0,
                'config': self.config,
                'model_info': self.model.get_model_info()
            }
            
            # ä¿å­˜ä¸º checkpoint_epoch_0.pth
            epoch0_path = os.path.join(self.checkpoint_dir, 'checkpoint_epoch_0.pth')
            torch.save(initial_checkpoint, epoch0_path)
            
            # åŒæ—¶æ›´æ–° latest_checkpoint.pth
            latest_path = os.path.join(self.checkpoint_dir, 'latest_checkpoint.pth')
            torch.save(initial_checkpoint, latest_path)
            
            print(f"âœ… åˆå§‹æ¨¡å‹å·²ä¿å­˜")
            print(f"   åˆå§‹éªŒè¯æŸå¤±: {initial_val_loss:.4f}")
            print(f"   åˆå§‹éªŒè¯å‡†ç¡®ç‡: {initial_val_acc:.4f}")
            
            # è®°å½•åˆå§‹æ€§èƒ½åˆ°è®­ç»ƒå†å²
            initial_history = {
                'epoch': 0,
                'train_loss': float('inf'),
                'train_acc': 0.0,
                'val_loss': initial_val_loss,
                'val_acc': initial_val_acc,
                'learning_rate': self.config['learning_rate'],
                'epoch_time': 0.0,
                **initial_per_digit,
                **{f'val_{k}': v for k, v in initial_per_digit.items()}
            }
            self.training_history.append(initial_history)

        for epoch in range(1, num_epochs + 1):
            epoch_start_time = time.time()
            
            # è®­ç»ƒ
            train_loss, train_acc, train_per_digit = self.train_epoch(epoch)
            
            # éªŒè¯
            val_loss, val_acc, val_per_digit, is_best = self.validate(epoch)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            if self.scheduler:
                if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):
                    self.scheduler.step(val_loss)
                else:
                    self.scheduler.step()
            
            # è·å–å½“å‰å­¦ä¹ ç‡
            current_lr = self.optimizer.param_groups[0]['lr']
            self.writer.add_scalar('Learning_Rate', current_lr, epoch)
            
            # è®°å½•epochæ—¶é—´
            epoch_time = time.time() - epoch_start_time
            
            # ä¿å­˜è®­ç»ƒå†å²
            history_entry = {
                'epoch': epoch,
                'train_loss': train_loss,
                'train_acc': train_acc,
                'val_loss': val_loss,
                'val_acc': val_acc,
                'learning_rate': current_lr,
                'epoch_time': epoch_time,
                **train_per_digit,
                **{f'val_{k}': v for k, v in val_per_digit.items()}
            }
            self.training_history.append(history_entry)
            
            # ä¿å­˜checkpoint
            if self.config.get('save_checkpoints', True):
                self.save_checkpoint(epoch, val_loss, val_acc, is_best)
            
            # æ‰“å°è¿›åº¦
            if epoch % self.config.get('print_every', 10) == 0:
                elapsed_time = time.time() - start_time
                avg_epoch_time = elapsed_time / epoch
                remaining_epochs = num_epochs - epoch
                eta = avg_epoch_time * remaining_epochs
                
                print(f"\nEpoch [{epoch}/{num_epochs}] "
                      f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | "
                      f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f} | "
                      f"LR: {current_lr:.6f} | "
                      f"Time: {epoch_time:.1f}s | ETA: {eta/60:.1f}min")
                
                # æ‰“å°éƒ¨åˆ†per-digitå‡†ç¡®ç‡
                print("Per-digit Val Accuracy:", end=" ")
                for digit in [0, 1, 5, 10]:  # æ‰“å°å‡ ä¸ªå…³é”®æ•°å­—
                    key = f'digit_{digit}_accuracy'
                    if key in val_per_digit:
                        print(f"[{digit}]: {val_per_digit[key]:.3f}", end=" ")
                print()
        
        # è®­ç»ƒå®Œæˆ
        total_time = time.time() - start_time
        print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
        print(f"æ€»è€—æ—¶: {total_time/3600:.2f} å°æ—¶")
        print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {self.best_val_accuracy:.4f}")
        
        # ä¿å­˜è®­ç»ƒå†å²
        history_df = pd.DataFrame(self.training_history)
        history_path = os.path.join(self.checkpoint_dir, 'training_history.csv')
        history_df.to_csv(history_path, index=False)
        print(f"è®­ç»ƒå†å²å·²ä¿å­˜: {history_path}")
        
        # å…³é—­TensorBoard writer
        self.writer.close()
        
        return self.training_history


def run_single_experiment(config, data_config, save_dir):
    """è¿è¡Œå•ä¸ªå®éªŒ"""
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # è®¾ç½®éšæœºç§å­
    seed = config.get('seed', 42)
    torch.manual_seed(seed)
    np.random.seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    model_name = f"alexnet_{'pretrain' if config['use_pretrain'] else 'no_pretrain'}_seed_{seed}"
    experiment_dir = os.path.join(save_dir, model_name)
    log_dir = os.path.join(experiment_dir, 'tensorboard_logs')
    checkpoint_dir = os.path.join(experiment_dir, 'checkpoints')
    
    os.makedirs(experiment_dir, exist_ok=True)
    os.makedirs(log_dir, exist_ok=True)
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    # ä¿å­˜é…ç½®
    config_path = os.path.join(experiment_dir, 'config.json')
    with open(config_path, 'w') as f:
        json.dump(config, f, indent=4)
    
    print(f"\n{'='*60}")
    print(f"å®éªŒ: {model_name}")
    print(f"ä¿å­˜ç›®å½•: {experiment_dir}")
    print(f"{'='*60}")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader, val_loader = get_single_image_data_loaders(
        train_csv_path=data_config['train_csv'],
        val_csv_path=data_config['val_csv'],
        data_root=data_config['data_root'],
        batch_size=config['batch_size'],
        num_workers=config.get('num_workers', 4),
        image_mode=config.get('image_mode', 'rgb'),
        normalize_images=config.get('normalize_images', True)
    )
    
    # åˆ›å»ºæ¨¡å‹
    model = create_visual_model(config, use_pretrain=config['use_pretrain'])
    model = model.to(device)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = VisualOnlyTrainer(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        config=config,
        device=device,
        log_dir=log_dir,
        checkpoint_dir=checkpoint_dir
    )
    
    # å¼€å§‹è®­ç»ƒ
    start_time = time.time()
    history = trainer.train(num_epochs=config['total_epochs'])
    training_time = time.time() - start_time
    
    # è¿”å›å®éªŒç»“æœ
    result = {
        'model_type': model_name,
        'use_pretrain': config['use_pretrain'],
        'seed': seed,
        'best_val_accuracy': trainer.best_val_accuracy,
        'best_val_loss': trainer.best_val_loss,
        'final_val_accuracy': history[-1]['val_acc'] if history else 0.0,
        'total_epochs': config['total_epochs'],
        'training_time_hours': training_time / 3600,
        'experiment_dir': experiment_dir
    }
    
    return result


def main():
    parser = argparse.ArgumentParser(description='AlexNetçº¯è§†è§‰æ¨¡å‹è®­ç»ƒ - é¢„è®­ç»ƒå¯¹æ¯”å®éªŒ')
    
    # æ•°æ®è·¯å¾„ - ä¸åŸå®éªŒä¿æŒä¸€è‡´
    parser.add_argument('--data_root', type=str, 
                       default='/mnt/iusers01/fatpou01/compsci01/k09562zs/scratch/Ball_counting_CNN/ball_data_collection')
    parser.add_argument('--train_csv', type=str,
                       default='scratch/Ball_counting_CNN/Tools_script/ball_counting_dataset_train.csv')
    parser.add_argument('--val_csv', type=str,
                       default='scratch/Ball_counting_CNN/Tools_script/ball_counting_dataset_val.csv')
    
    # å®éªŒå‚æ•°
    parser.add_argument('--total_epochs', type=int, default=1000,
                       help='è®­ç»ƒæ€»epochæ•°')
    parser.add_argument('--batch_size', type=int, default=16,
                       help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--learning_rate', type=float, default=1e-4,
                       help='å­¦ä¹ ç‡')
    parser.add_argument('--seeds', nargs='+', type=int,
                       default=[2048, 4096, 9999],
                       help='éšæœºç§å­åˆ—è¡¨')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--image_mode', type=str, default='rgb',
                       choices=['rgb', 'grayscale'],
                       help='å›¾åƒæ¨¡å¼')
    parser.add_argument('--run_both', action='store_true', default=True,
                       help='åŒæ—¶è¿è¡Œé¢„è®­ç»ƒå’Œéé¢„è®­ç»ƒå®éªŒ')
    parser.add_argument('--use_pretrain', action='store_true',
                       help='åªè¿è¡Œé¢„è®­ç»ƒæ¨¡å‹')
    parser.add_argument('--no_pretrain', action='store_true',
                       help='åªè¿è¡Œéé¢„è®­ç»ƒæ¨¡å‹')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--save_dir', type=str, default='./alexnet_visual_only_experiments',
                       help='ç»“æœä¿å­˜ç›®å½•')
    parser.add_argument('--num_workers', type=int, default=4,
                       help='æ•°æ®åŠ è½½å™¨è¿›ç¨‹æ•°')
    parser.add_argument('--save_checkpoints', action='store_true', default=True,
                       help='æ˜¯å¦ä¿å­˜æ¨¡å‹checkpoints')
    parser.add_argument('--save_every', type=int, default=100,
                       help='æ¯å¤šå°‘ä¸ªepochä¿å­˜ä¸€æ¬¡checkpoint')
    parser.add_argument('--print_every', type=int, default=10,
                       help='æ¯å¤šå°‘ä¸ªepochæ‰“å°ä¸€æ¬¡è¿›åº¦')
    
    args = parser.parse_args()
    
    # ç¡®å®šè¦è¿è¡Œçš„å®éªŒ
    experiments = []
    if args.run_both or (not args.use_pretrain and not args.no_pretrain):
        experiments = [True, False]  # åŒæ—¶è¿è¡Œé¢„è®­ç»ƒå’Œéé¢„è®­ç»ƒ
    elif args.use_pretrain:
        experiments = [True]  # åªè¿è¡Œé¢„è®­ç»ƒ
    elif args.no_pretrain:
        experiments = [False]  # åªè¿è¡Œéé¢„è®­ç»ƒ
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    save_dir = os.path.join(args.save_dir, f'alexnet_visual_comparison_{timestamp}')
    os.makedirs(save_dir, exist_ok=True)
    
    # æ•°æ®é…ç½®
    data_config = {
        'data_root': args.data_root,
        'train_csv': args.train_csv,
        'val_csv': args.val_csv
    }
    
    # åŸºç¡€é…ç½®
    base_config = {
        'total_epochs': args.total_epochs,
        'batch_size': args.batch_size,
        'learning_rate': args.learning_rate,
        'image_mode': args.image_mode,
        'num_workers': args.num_workers,
        'save_checkpoints': args.save_checkpoints,
        'save_every': args.save_every,
        'print_every': args.print_every,
        'model_config': {
            'feature_dim': 256,
            'dropout': 0.5
        },
        # ä¸å…·èº«æ¨¡å‹ä¿æŒä¸€è‡´çš„è®­ç»ƒå‚æ•°
        'adam_betas': (0.9, 0.999),
        'weight_decay': 1e-5,
        'grad_clip_norm': 1.0,
        'scheduler_type': 'cosine',
        'normalize_images': True
    }
    
    # è®°å½•æ‰€æœ‰å®éªŒç»“æœ
    all_results = []
    results_file = os.path.join(save_dir, 'experiment_results.csv')
    
    print(f"ğŸš€ å¼€å§‹AlexNetçº¯è§†è§‰å¯¹æ¯”å®éªŒ")
    print(f"å®éªŒç±»å‹: {['é¢„è®­ç»ƒ', 'éé¢„è®­ç»ƒ'] if len(experiments) == 2 else ['é¢„è®­ç»ƒ' if experiments[0] else 'éé¢„è®­ç»ƒ']}")
    print(f"éšæœºç§å­: {args.seeds}")
    print(f"æ€»å®éªŒæ•°: {len(experiments) * len(args.seeds)}")
    print(f"æ¯ä¸ªå®éªŒè®­ç»ƒepochs: {args.total_epochs}")
    print(f"ç»“æœä¿å­˜: {save_dir}")
    
    # è¿è¡Œæ‰€æœ‰å®éªŒ
    total_experiments = len(experiments) * len(args.seeds)
    current_exp = 0
    start_time = time.time()
    
    for use_pretrain in experiments:
        for seed in args.seeds:
            current_exp += 1
            
            # æ›´æ–°é…ç½®
            config = base_config.copy()
            config['use_pretrain'] = use_pretrain
            config['seed'] = seed
            
            # æ˜¾ç¤ºè¿›åº¦
            elapsed_time = time.time() - start_time
            avg_time_per_exp = elapsed_time / current_exp if current_exp > 0 else 0
            remaining_time = avg_time_per_exp * (total_experiments - current_exp)
            
            print(f"\nğŸ“Š è¿›åº¦: {current_exp}/{total_experiments}")
            print(f"â±ï¸  å·²ç”¨æ—¶: {elapsed_time/3600:.1f}h, é¢„è®¡å‰©ä½™: {remaining_time/3600:.1f}h")
            
            # è¿è¡Œå®éªŒ
            result = run_single_experiment(config, data_config, save_dir)
            all_results.append(result)
            
            # ä¿å­˜ä¸­é—´ç»“æœ
            results_df = pd.DataFrame(all_results)
            results_df.to_csv(results_file, index=False)
            print(f"ğŸ’¾ ä¿å­˜ä¸­é—´ç»“æœ: {results_file}")
    
    # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
    print(f"\nğŸ“Š ç”Ÿæˆå®éªŒæŠ¥å‘Š...")
    results_df = pd.DataFrame(all_results)
    
    # è®¡ç®—ç»Ÿè®¡æ‘˜è¦
    summary = results_df.groupby('use_pretrain').agg({
        'best_val_accuracy': ['mean', 'std', 'max'],
        'final_val_accuracy': ['mean', 'std'],
        'training_time_hours': ['mean', 'sum']
    }).round(4)
    
    # ä¿å­˜æ‘˜è¦
    summary_file = os.path.join(save_dir, 'summary_stats.csv')
    summary.to_csv(summary_file)
    
    # æ‰“å°æ‘˜è¦
    print("\nğŸ“ˆ å®éªŒç»“æœæ‘˜è¦:")
    print("="*60)
    print(summary)
    print("="*60)
    
    # ç”ŸæˆMarkdownæŠ¥å‘Š
    report_content = f"""# AlexNetçº¯è§†è§‰æ¨¡å‹å¯¹æ¯”å®éªŒæŠ¥å‘Š

ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## å®éªŒæ¦‚è¿°

- **æ¨¡å‹ç±»å‹**: AlexNetçº¯è§†è§‰åˆ†ç±»
- **å¯¹æ¯”å†…å®¹**: é¢„è®­ç»ƒ vs éé¢„è®­ç»ƒ
- **ä»»åŠ¡**: å•å›¾åƒçƒæ•°åˆ†ç±» (0-10)
- **è®­ç»ƒepochs**: {args.total_epochs}
- **éšæœºç§å­**: {args.seeds}
- **æ‰¹æ¬¡å¤§å°**: {args.batch_size}
- **å­¦ä¹ ç‡**: {args.learning_rate}
- **å›¾åƒæ¨¡å¼**: {args.image_mode}

## å®éªŒç»“æœ

### å‡†ç¡®ç‡å¯¹æ¯”

| æ¨¡å‹ç±»å‹ | æœ€ä½³éªŒè¯å‡†ç¡®ç‡ (meanÂ±std) | æœ€é«˜å‡†ç¡®ç‡ | æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡ (meanÂ±std) |
|---------|-------------------------|----------|-------------------------|
"""
    
    for use_pretrain in [True, False]:
        model_type = "é¢„è®­ç»ƒAlexNet" if use_pretrain else "æ— é¢„è®­ç»ƒAlexNet"
        pretrain_results = results_df[results_df['use_pretrain'] == use_pretrain]
        if len(pretrain_results) > 0:
            best_mean = pretrain_results['best_val_accuracy'].mean()
            best_std = pretrain_results['best_val_accuracy'].std()
            best_max = pretrain_results['best_val_accuracy'].max()
            final_mean = pretrain_results['final_val_accuracy'].mean()
            final_std = pretrain_results['final_val_accuracy'].std()
            
            report_content += f"| {model_type} | {best_mean:.4f}Â±{best_std:.4f} | {best_max:.4f} | {final_mean:.4f}Â±{final_std:.4f} |\n"
    
    report_content += f"""

### è®­ç»ƒæ•ˆç‡

| æ¨¡å‹ç±»å‹ | å¹³å‡è®­ç»ƒæ—¶é—´ (å°æ—¶) | æ€»è®­ç»ƒæ—¶é—´ (å°æ—¶) |
|---------|------------------|----------------|
"""
    
    for use_pretrain in [True, False]:
        model_type = "é¢„è®­ç»ƒAlexNet" if use_pretrain else "æ— é¢„è®­ç»ƒAlexNet"
        pretrain_results = results_df[results_df['use_pretrain'] == use_pretrain]
        if len(pretrain_results) > 0:
            avg_time = pretrain_results['training_time_hours'].mean()
            total_time = pretrain_results['training_time_hours'].sum()
            report_content += f"| {model_type} | {avg_time:.2f} | {total_time:.2f} |\n"
    
    report_content += f"""

## ç»“è®º

åŸºäºå®éªŒç»“æœï¼š
1. é¢„è®­ç»ƒæ¨¡å‹ç›¸æ¯”éé¢„è®­ç»ƒæ¨¡å‹çš„æ€§èƒ½æå‡: {((results_df[results_df['use_pretrain']==True]['best_val_accuracy'].mean() / results_df[results_df['use_pretrain']==False]['best_val_accuracy'].mean() - 1) * 100):.1f}%
2. ä¸¤ç§æ¨¡å‹çš„è®­ç»ƒç¨³å®šæ€§ï¼ˆæ ‡å‡†å·®ï¼‰å¯¹æ¯”
3. è®­ç»ƒæ•ˆç‡å·®å¼‚

## æ–‡ä»¶è¯´æ˜

- è¯¦ç»†ç»“æœ: `experiment_results.csv`
- ç»Ÿè®¡æ‘˜è¦: `summary_stats.csv`
- TensorBoardæ—¥å¿—: å„æ¨¡å‹çš„ `tensorboard_logs/` ç›®å½•
- æ¨¡å‹checkpoints: å„æ¨¡å‹çš„ `checkpoints/` ç›®å½•

## æŸ¥çœ‹TensorBoard

```bash
tensorboard --logdir {save_dir}
```

## ä¸å…·èº«æ¨¡å‹å¯¹æ¯”

æ­¤å®éªŒå¯ä¸å…·èº«æ¨¡å‹å®éªŒç»“æœè¿›è¡Œå¯¹æ¯”ï¼Œä»¥è¯„ä¼°å…·èº«ä¿¡æ¯å¯¹è®¡æ•°ä»»åŠ¡çš„è´¡çŒ®ã€‚
"""
    
    report_file = os.path.join(save_dir, 'experiment_report.md')
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report_content)
    
    total_time = time.time() - start_time
    print(f"\nğŸ‰ æ‰€æœ‰å®éªŒå®Œæˆ!")
    print(f"â±ï¸  æ€»è€—æ—¶: {total_time/3600:.1f} å°æ—¶")
    print(f"ğŸ“Š è¯¦ç»†ç»“æœ: {results_file}")
    print(f"ğŸ“ˆ ç»Ÿè®¡æ‘˜è¦: {summary_file}")
    print(f"ğŸ“‹ å®éªŒæŠ¥å‘Š: {report_file}")
    print(f"ğŸ’¾ æ‰€æœ‰æ–‡ä»¶ä¿å­˜åœ¨: {save_dir}")


if __name__ == "__main__":
    main()