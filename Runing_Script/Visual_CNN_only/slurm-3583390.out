=== 作业信息 ===
节点: node802
GPU: 1 (ID: 0)
CPU核心: 4
内存: 20480 MB
开始时间: Sun 20 Jul 00:15:23 BST 2025
=== 激活环境 ===
/var/spool/slurmd/job3583390/slurm_script: line 25: --version: command not found
Python版本: 
当前环境: cgtest
=== 开始训练 ===
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:279: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(test_path, map_location=self.device)
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')
============================================================
单图像分类模型训练配置
============================================================
基础配置:
  device: cuda
  batch_size: 16
  learning_rate: 0.0001
  total_epochs: 350
  image_mode: rgb

数据配置:
  data_root: /mnt/iusers01/fatpou01/compsci01/k09562zs/scratch/Ball_counting_CNN/ball_data_collection
  train_csv: scratch/Ball_counting_CNN/Tools_script/ball_counting_dataset_train_20.csv
  val_csv: scratch/Ball_counting_CNN/Tools_script/ball_counting_dataset_val.csv

模型配置:
  use_attention: True
  cnn_layers: 3
  cnn_channels: [64, 128, 256]
  feature_dim: 256
  attention_heads: 1
  dropout: 0.1

训练配置:
  scheduler_type: none
  label_smoothing: 0.0
  grad_clip_norm: 1.0

保存配置:
  save_dir: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points
  log_dir: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/logs
  save_every: 10
============================================================
所有路径验证通过
配置保存到: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/single_image_config.json

正在初始化单图像分类模型训练器...
图像模式: RGB
使用注意力机制: True
标签范围: 1-10 (对应10个类别)
SingleImageClassifier初始化:
  CNN层数: 3
  CNN通道: [64, 128, 256]
  输入通道: 3
  输出类别: 10 (对应标签1-10)
  特征维度: 256
  隐藏维度: 256
  使用注意力: True
  注意力头数: 1
创建带注意力机制的单图像分类模型 (标签1-10)
✓ Model initialization validation passed
=== 创建单图像数据加载器 - 图像模式: RGB ===
标签: 直接使用ball_count
单图像数据集构建完成:
  原始序列数: 185
  提取的单图像样本数: 810
  图像模式: rgb
  标签: 直接使用ball_count
单图像数据集构建完成:
  原始序列数: 242
  提取的单图像样本数: 1074
  图像模式: rgb
  标签: 直接使用ball_count

训练集类别分布:
  球数 1: 128 样本
  球数 2: 96 样本
  球数 3: 84 样本
  球数 4: 80 样本
  球数 5: 72 样本
  球数 6: 70 样本
  球数 7: 72 样本
  球数 8: 72 样本
  球数 9: 70 样本
  球数 10: 66 样本

验证集类别分布:
  球数 1: 162 样本
  球数 2: 126 样本
  球数 3: 108 样本
  球数 4: 105 样本
  球数 5: 102 样本
  球数 6: 98 样本
  球数 7: 96 样本
  球数 8: 99 样本
  球数 9: 90 样本
  球数 10: 88 样本
SingleImageTrainer initialized:
  Model parameters: 734,858
  Training samples: 810
  Validation samples: 1,074
  Image mode: RGB
  Use attention: True
  Label mapping: 1-10 -> 0-9 (for loss calculation)

开始训练单图像分类模型...
目标: 作为具身计数模型的对比基线
使用所有帧图像，标签为对应序列的ball_count

开始训练单图像分类模型
总计 350 个epoch
设备: cuda
图像模式: rgb
Testing model save/load functionality...
✓ Model save/load test passed
Saving initial model state...
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/initial_single_image_model.pth

Performing initial validation...
Initial validation - Loss: 2.3060, Accuracy: 0.1006
Epoch [0] Batch [0/51] Loss: 2.2920 LR: 0.000100
Epoch [0] Batch [10/51] Loss: 2.2696 LR: 0.000100
Epoch [0] Batch [20/51] Loss: 2.2414 LR: 0.000100
Epoch [0] Batch [30/51] Loss: 2.2223 LR: 0.000100
Epoch [0] Batch [40/51] Loss: 2.1775 LR: 0.000100
Epoch [0] Batch [50/51] Loss: 2.0126 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [1/350] - Time: 5.32s
Train Loss: 2.2157 | Val Loss: 2.1044
Train Acc: 0.2099 | Val Acc: 0.2356
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.000    Count 3: 0.000    Count 4: 0.000    Count 5: 0.000  
  Count 6: 0.000    Count 7: 0.031    Count 8: 0.000    Count 9: 0.000    Count 10: 1.000  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/best_single_image_model.pth
  → New best model! Validation accuracy: 0.2356
新的最佳模型! 验证准确率: 0.2356
Epoch [1] Batch [0/51] Loss: 2.1158 LR: 0.000100
Epoch [1] Batch [10/51] Loss: 2.0956 LR: 0.000100
Epoch [1] Batch [20/51] Loss: 1.7768 LR: 0.000100
Epoch [1] Batch [30/51] Loss: 1.6416 LR: 0.000100
Epoch [1] Batch [40/51] Loss: 1.4670 LR: 0.000100
Epoch [1] Batch [50/51] Loss: 1.4567 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [2/350] - Time: 5.35s
Train Loss: 1.8605 | Val Loss: 1.6643
Train Acc: 0.2716 | Val Acc: 0.2914
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.000    Count 3: 0.000    Count 4: 0.133    Count 5: 0.000  
  Count 6: 0.000    Count 7: 0.521    Count 8: 0.000    Count 9: 0.000    Count 10: 1.000  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/best_single_image_model.pth
  → New best model! Validation accuracy: 0.2914
新的最佳模型! 验证准确率: 0.2914
Epoch [2] Batch [0/51] Loss: 1.5180 LR: 0.000100
Epoch [2] Batch [10/51] Loss: 1.8723 LR: 0.000100
Epoch [2] Batch [20/51] Loss: 1.5677 LR: 0.000100
Epoch [2] Batch [30/51] Loss: 1.4600 LR: 0.000100
Epoch [2] Batch [40/51] Loss: 1.8585 LR: 0.000100
Epoch [2] Batch [50/51] Loss: 1.6087 LR: 0.000100

Epoch [3/350] - Time: 5.01s
Train Loss: 1.5859 | Val Loss: 1.7803
Train Acc: 0.3358 | Val Acc: 0.2291
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.016    Count 3: 0.000    Count 4: 0.010    Count 5: 0.010  
  Count 6: 0.122    Count 7: 0.000    Count 8: 0.000    Count 9: 0.289    Count 10: 0.489  
Epoch [3] Batch [0/51] Loss: 1.6493 LR: 0.000100
Epoch [3] Batch [10/51] Loss: 2.0494 LR: 0.000100
Epoch [3] Batch [20/51] Loss: 1.2928 LR: 0.000100
Epoch [3] Batch [30/51] Loss: 1.3339 LR: 0.000100
Epoch [3] Batch [40/51] Loss: 1.2204 LR: 0.000100
Epoch [3] Batch [50/51] Loss: 1.0908 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [4/350] - Time: 5.08s
Train Loss: 1.4503 | Val Loss: 1.3794
Train Acc: 0.3963 | Val Acc: 0.3585
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.214    Count 3: 0.130    Count 4: 0.181    Count 5: 0.157  
  Count 6: 0.561    Count 7: 0.083    Count 8: 0.152    Count 9: 0.000    Count 10: 0.795  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/best_single_image_model.pth
  → New best model! Validation accuracy: 0.3585
新的最佳模型! 验证准确率: 0.3585
Epoch [4] Batch [0/51] Loss: 1.2488 LR: 0.000100
Epoch [4] Batch [10/51] Loss: 1.2917 LR: 0.000100
Epoch [4] Batch [20/51] Loss: 1.3397 LR: 0.000100
Epoch [4] Batch [30/51] Loss: 1.6008 LR: 0.000100
Epoch [4] Batch [40/51] Loss: 1.2736 LR: 0.000100
Epoch [4] Batch [50/51] Loss: 1.2832 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [5/350] - Time: 5.01s
Train Loss: 1.3632 | Val Loss: 1.2564
Train Acc: 0.4037 | Val Acc: 0.4767
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.969    Count 2: 0.770    Count 3: 0.213    Count 4: 0.629    Count 5: 0.500  
  Count 6: 0.020    Count 7: 0.375    Count 8: 0.424    Count 9: 0.000    Count 10: 0.432  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/best_single_image_model.pth
  → New best model! Validation accuracy: 0.4767
新的最佳模型! 验证准确率: 0.4767
Epoch [5] Batch [0/51] Loss: 1.1899 LR: 0.000100
Epoch [5] Batch [10/51] Loss: 1.4720 LR: 0.000100
Epoch [5] Batch [20/51] Loss: 1.4815 LR: 0.000100
Epoch [5] Batch [30/51] Loss: 1.1338 LR: 0.000100
Epoch [5] Batch [40/51] Loss: 0.9486 LR: 0.000100
Epoch [5] Batch [50/51] Loss: 0.9159 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [6/350] - Time: 5.10s
Train Loss: 1.1889 | Val Loss: 1.1879
Train Acc: 0.4728 | Val Acc: 0.5354
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.349    Count 3: 0.926    Count 4: 0.371    Count 5: 0.745  
  Count 6: 0.000    Count 7: 0.365    Count 8: 0.000    Count 9: 0.489    Count 10: 0.864  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/best_single_image_model.pth
  → New best model! Validation accuracy: 0.5354
新的最佳模型! 验证准确率: 0.5354
Epoch [6] Batch [0/51] Loss: 1.0488 LR: 0.000100
Epoch [6] Batch [10/51] Loss: 1.0091 LR: 0.000100
Epoch [6] Batch [20/51] Loss: 0.8663 LR: 0.000100
Epoch [6] Batch [30/51] Loss: 1.0664 LR: 0.000100
Epoch [6] Batch [40/51] Loss: 0.9790 LR: 0.000100
Epoch [6] Batch [50/51] Loss: 1.0033 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [7/350] - Time: 5.10s
Train Loss: 1.1220 | Val Loss: 1.1676
Train Acc: 0.5160 | Val Acc: 0.5950
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.975    Count 2: 0.810    Count 3: 0.750    Count 4: 0.790    Count 5: 0.206  
  Count 6: 0.235    Count 7: 0.677    Count 8: 0.646    Count 9: 0.000    Count 10: 0.477  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/best_single_image_model.pth
  → New best model! Validation accuracy: 0.5950
新的最佳模型! 验证准确率: 0.5950
Epoch [7] Batch [0/51] Loss: 0.7058 LR: 0.000100
Epoch [7] Batch [10/51] Loss: 0.8596 LR: 0.000100
Epoch [7] Batch [20/51] Loss: 1.5219 LR: 0.000100
Epoch [7] Batch [30/51] Loss: 1.0144 LR: 0.000100
Epoch [7] Batch [40/51] Loss: 0.9283 LR: 0.000100
Epoch [7] Batch [50/51] Loss: 1.7087 LR: 0.000100

Epoch [8/350] - Time: 4.96s
Train Loss: 1.1649 | Val Loss: 1.1451
Train Acc: 0.5086 | Val Acc: 0.4767
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.988    Count 2: 0.492    Count 3: 0.417    Count 4: 0.210    Count 5: 0.559  
  Count 6: 0.367    Count 7: 0.177    Count 8: 0.000    Count 9: 0.867    Count 10: 0.398  
Epoch [8] Batch [0/51] Loss: 1.2976 LR: 0.000100
Epoch [8] Batch [10/51] Loss: 0.7149 LR: 0.000100
Epoch [8] Batch [20/51] Loss: 1.1034 LR: 0.000100
Epoch [8] Batch [30/51] Loss: 1.7326 LR: 0.000100
Epoch [8] Batch [40/51] Loss: 1.0118 LR: 0.000100
Epoch [8] Batch [50/51] Loss: 1.3571 LR: 0.000100

Epoch [9/350] - Time: 5.13s
Train Loss: 0.9995 | Val Loss: 1.2648
Train Acc: 0.5827 | Val Acc: 0.4488
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.690    Count 3: 0.102    Count 4: 0.229    Count 5: 0.598  
  Count 6: 0.337    Count 7: 0.125    Count 8: 0.000    Count 9: 0.711    Count 10: 0.330  
Epoch [9] Batch [0/51] Loss: 0.8856 LR: 0.000100
Epoch [9] Batch [10/51] Loss: 0.8339 LR: 0.000100
Epoch [9] Batch [20/51] Loss: 1.0125 LR: 0.000100
Epoch [9] Batch [30/51] Loss: 0.9510 LR: 0.000100
Epoch [9] Batch [40/51] Loss: 0.7183 LR: 0.000100
Epoch [9] Batch [50/51] Loss: 1.0964 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [10/350] - Time: 5.36s
Train Loss: 0.9637 | Val Loss: 1.1770
Train Acc: 0.6012 | Val Acc: 0.6024
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.969    Count 2: 0.825    Count 3: 0.852    Count 4: 0.771    Count 5: 0.529  
  Count 6: 0.418    Count 7: 0.312    Count 8: 0.030    Count 9: 0.533    Count 10: 0.420  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/best_single_image_model.pth
  → New best model! Validation accuracy: 0.6024
新的最佳模型! 验证准确率: 0.6024
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/single_image_checkpoint_epoch_9.pth
Epoch [10] Batch [0/51] Loss: 0.9401 LR: 0.000100
Epoch [10] Batch [10/51] Loss: 0.4897 LR: 0.000100
Epoch [10] Batch [20/51] Loss: 0.7033 LR: 0.000100
Epoch [10] Batch [30/51] Loss: 0.6483 LR: 0.000100
Epoch [10] Batch [40/51] Loss: 0.8917 LR: 0.000100
Epoch [10] Batch [50/51] Loss: 0.9836 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [11/350] - Time: 4.96s
Train Loss: 0.9294 | Val Loss: 1.0892
Train Acc: 0.5914 | Val Acc: 0.6201
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.981    Count 2: 0.810    Count 3: 0.722    Count 4: 0.800    Count 5: 0.598  
  Count 6: 0.276    Count 7: 0.594    Count 8: 0.535    Count 9: 0.000    Count 10: 0.511  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/best_single_image_model.pth
  → New best model! Validation accuracy: 0.6201
新的最佳模型! 验证准确率: 0.6201
Epoch [11] Batch [0/51] Loss: 0.8104 LR: 0.000100
Epoch [11] Batch [10/51] Loss: 0.9963 LR: 0.000100
Epoch [11] Batch [20/51] Loss: 0.7729 LR: 0.000100
Epoch [11] Batch [30/51] Loss: 1.0407 LR: 0.000100
Epoch [11] Batch [40/51] Loss: 0.8213 LR: 0.000100
Epoch [11] Batch [50/51] Loss: 1.5610 LR: 0.000100

Epoch [12/350] - Time: 5.01s
Train Loss: 0.8661 | Val Loss: 1.1751
Train Acc: 0.6173 | Val Acc: 0.6108
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.957    Count 2: 0.944    Count 3: 0.870    Count 4: 0.810    Count 5: 0.137  
  Count 6: 0.633    Count 7: 0.417    Count 8: 0.040    Count 9: 0.367    Count 10: 0.568  
Epoch [12] Batch [0/51] Loss: 0.7904 LR: 0.000100
Epoch [12] Batch [10/51] Loss: 0.5723 LR: 0.000100
Epoch [12] Batch [20/51] Loss: 0.6719 LR: 0.000100
Epoch [12] Batch [30/51] Loss: 0.7632 LR: 0.000100
Epoch [12] Batch [40/51] Loss: 0.8301 LR: 0.000100
Epoch [12] Batch [50/51] Loss: 1.6221 LR: 0.000100

Epoch [13/350] - Time: 5.12s
Train Loss: 0.8296 | Val Loss: 1.3273
Train Acc: 0.6617 | Val Acc: 0.4702
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.929    Count 3: 0.361    Count 4: 0.438    Count 5: 0.412  
  Count 6: 0.388    Count 7: 0.104    Count 8: 0.141    Count 9: 0.067    Count 10: 0.364  
Epoch [13] Batch [0/51] Loss: 0.8491 LR: 0.000100
Epoch [13] Batch [10/51] Loss: 0.7504 LR: 0.000100
Epoch [13] Batch [20/51] Loss: 1.1617 LR: 0.000100
Epoch [13] Batch [30/51] Loss: 0.7236 LR: 0.000100
Epoch [13] Batch [40/51] Loss: 0.7505 LR: 0.000100
Epoch [13] Batch [50/51] Loss: 0.3787 LR: 0.000100

Epoch [14/350] - Time: 5.03s
Train Loss: 0.7501 | Val Loss: 1.1922
Train Acc: 0.6802 | Val Acc: 0.5782
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.988    Count 2: 0.881    Count 3: 0.667    Count 4: 0.581    Count 5: 0.725  
  Count 6: 0.408    Count 7: 0.406    Count 8: 0.172    Count 9: 0.033    Count 10: 0.500  
Epoch [14] Batch [0/51] Loss: 0.9013 LR: 0.000100
Epoch [14] Batch [10/51] Loss: 0.4631 LR: 0.000100
Epoch [14] Batch [20/51] Loss: 1.1751 LR: 0.000100
Epoch [14] Batch [30/51] Loss: 0.8844 LR: 0.000100
Epoch [14] Batch [40/51] Loss: 0.7970 LR: 0.000100
Epoch [14] Batch [50/51] Loss: 0.7214 LR: 0.000100

Epoch [15/350] - Time: 5.10s
Train Loss: 0.7659 | Val Loss: 1.4619
Train Acc: 0.6691 | Val Acc: 0.5065
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.833    Count 3: 0.731    Count 4: 0.438    Count 5: 0.480  
  Count 6: 0.500    Count 7: 0.062    Count 8: 0.091    Count 9: 0.167    Count 10: 0.273  
Epoch [15] Batch [0/51] Loss: 1.0052 LR: 0.000100
Epoch [15] Batch [10/51] Loss: 0.6120 LR: 0.000100
Epoch [15] Batch [20/51] Loss: 0.5449 LR: 0.000100
Epoch [15] Batch [30/51] Loss: 0.7360 LR: 0.000100
Epoch [15] Batch [40/51] Loss: 0.7773 LR: 0.000100
Epoch [15] Batch [50/51] Loss: 1.2411 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [16/350] - Time: 5.02s
Train Loss: 0.8093 | Val Loss: 1.2451
Train Acc: 0.6617 | Val Acc: 0.6229
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.988    Count 2: 0.921    Count 3: 0.806    Count 4: 0.867    Count 5: 0.186  
  Count 6: 0.633    Count 7: 0.375    Count 8: 0.061    Count 9: 0.511    Count 10: 0.523  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/best_single_image_model.pth
  → New best model! Validation accuracy: 0.6229
新的最佳模型! 验证准确率: 0.6229
Epoch [16] Batch [0/51] Loss: 0.4764 LR: 0.000100
Epoch [16] Batch [10/51] Loss: 0.5215 LR: 0.000100
Epoch [16] Batch [20/51] Loss: 0.7989 LR: 0.000100
Epoch [16] Batch [30/51] Loss: 0.4663 LR: 0.000100
Epoch [16] Batch [40/51] Loss: 0.6066 LR: 0.000100
Epoch [16] Batch [50/51] Loss: 0.9942 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [17/350] - Time: 5.04s
Train Loss: 0.5773 | Val Loss: 1.2965
Train Acc: 0.7556 | Val Acc: 0.6601
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.975    Count 2: 0.968    Count 3: 0.898    Count 4: 0.771    Count 5: 0.539  
  Count 6: 0.469    Count 7: 0.542    Count 8: 0.061    Count 9: 0.433    Count 10: 0.602  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/best_single_image_model.pth
  → New best model! Validation accuracy: 0.6601
新的最佳模型! 验证准确率: 0.6601
Epoch [17] Batch [0/51] Loss: 0.5252 LR: 0.000100
Epoch [17] Batch [10/51] Loss: 0.3715 LR: 0.000100
Epoch [17] Batch [20/51] Loss: 0.9741 LR: 0.000100
Epoch [17] Batch [30/51] Loss: 0.7919 LR: 0.000100
Epoch [17] Batch [40/51] Loss: 0.6162 LR: 0.000100
Epoch [17] Batch [50/51] Loss: 0.7372 LR: 0.000100

Epoch [18/350] - Time: 4.99s
Train Loss: 0.6216 | Val Loss: 1.3794
Train Acc: 0.7370 | Val Acc: 0.6397
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.873    Count 3: 0.870    Count 4: 0.724    Count 5: 0.598  
  Count 6: 0.520    Count 7: 0.521    Count 8: 0.141    Count 9: 0.322    Count 10: 0.466  
Epoch [18] Batch [0/51] Loss: 0.5436 LR: 0.000100
Epoch [18] Batch [10/51] Loss: 0.9687 LR: 0.000100
Epoch [18] Batch [20/51] Loss: 1.2432 LR: 0.000100
Epoch [18] Batch [30/51] Loss: 0.8328 LR: 0.000100
Epoch [18] Batch [40/51] Loss: 0.3800 LR: 0.000100
Epoch [18] Batch [50/51] Loss: 0.4752 LR: 0.000100

Epoch [19/350] - Time: 5.31s
Train Loss: 0.6132 | Val Loss: 1.3126
Train Acc: 0.7346 | Val Acc: 0.6229
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.841    Count 3: 0.657    Count 4: 0.810    Count 5: 0.676  
  Count 6: 0.622    Count 7: 0.542    Count 8: 0.081    Count 9: 0.233    Count 10: 0.398  
Epoch [19] Batch [0/51] Loss: 0.7375 LR: 0.000100
Epoch [19] Batch [10/51] Loss: 0.5297 LR: 0.000100
Epoch [19] Batch [20/51] Loss: 0.4234 LR: 0.000100
Epoch [19] Batch [30/51] Loss: 0.4495 LR: 0.000100
Epoch [19] Batch [40/51] Loss: 0.4647 LR: 0.000100
Epoch [19] Batch [50/51] Loss: 0.5990 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [20/350] - Time: 5.04s
Train Loss: 0.5321 | Val Loss: 1.3837
Train Acc: 0.7778 | Val Acc: 0.6387
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.960    Count 3: 0.870    Count 4: 0.800    Count 5: 0.461  
  Count 6: 0.582    Count 7: 0.438    Count 8: 0.121    Count 9: 0.200    Count 10: 0.557  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/single_image_checkpoint_epoch_19.pth
Epoch [20] Batch [0/51] Loss: 0.2894 LR: 0.000100
Epoch [20] Batch [10/51] Loss: 0.2323 LR: 0.000100
Epoch [20] Batch [20/51] Loss: 0.5128 LR: 0.000100
Epoch [20] Batch [30/51] Loss: 0.5305 LR: 0.000100
Epoch [20] Batch [40/51] Loss: 0.2950 LR: 0.000100
Epoch [20] Batch [50/51] Loss: 0.8147 LR: 0.000100

Epoch [21/350] - Time: 5.23s
Train Loss: 0.4781 | Val Loss: 1.4216
Train Acc: 0.8025 | Val Acc: 0.6313
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.881    Count 3: 0.722    Count 4: 0.800    Count 5: 0.647  
  Count 6: 0.571    Count 7: 0.490    Count 8: 0.101    Count 9: 0.278    Count 10: 0.455  
Epoch [21] Batch [0/51] Loss: 0.4851 LR: 0.000100
Epoch [21] Batch [10/51] Loss: 0.2001 LR: 0.000100
Epoch [21] Batch [20/51] Loss: 0.2810 LR: 0.000100
Epoch [21] Batch [30/51] Loss: 0.3418 LR: 0.000100
Epoch [21] Batch [40/51] Loss: 0.3008 LR: 0.000100
Epoch [21] Batch [50/51] Loss: 0.3929 LR: 0.000100

Epoch [22/350] - Time: 5.00s
Train Loss: 0.3831 | Val Loss: 1.6360
Train Acc: 0.8556 | Val Acc: 0.6378
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.988    Count 2: 0.984    Count 3: 0.713    Count 4: 0.819    Count 5: 0.480  
  Count 6: 0.551    Count 7: 0.604    Count 8: 0.162    Count 9: 0.133    Count 10: 0.557  
Epoch [22] Batch [0/51] Loss: 0.3553 LR: 0.000100
Epoch [22] Batch [10/51] Loss: 0.8744 LR: 0.000100
Epoch [22] Batch [20/51] Loss: 0.2342 LR: 0.000100
Epoch [22] Batch [30/51] Loss: 0.3125 LR: 0.000100
Epoch [22] Batch [40/51] Loss: 0.5739 LR: 0.000100
Epoch [22] Batch [50/51] Loss: 0.2758 LR: 0.000100

Epoch [23/350] - Time: 5.12s
Train Loss: 0.4414 | Val Loss: 1.5463
Train Acc: 0.8136 | Val Acc: 0.6434
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.873    Count 3: 0.898    Count 4: 0.771    Count 5: 0.627  
  Count 6: 0.612    Count 7: 0.438    Count 8: 0.162    Count 9: 0.167    Count 10: 0.511  
Epoch [23] Batch [0/51] Loss: 0.3475 LR: 0.000100
Epoch [23] Batch [10/51] Loss: 0.5341 LR: 0.000100
Epoch [23] Batch [20/51] Loss: 0.3051 LR: 0.000100
Epoch [23] Batch [30/51] Loss: 0.7802 LR: 0.000100
Epoch [23] Batch [40/51] Loss: 0.3286 LR: 0.000100
Epoch [23] Batch [50/51] Loss: 0.3951 LR: 0.000100

Epoch [24/350] - Time: 5.15s
Train Loss: 0.5190 | Val Loss: 1.7146
Train Acc: 0.7864 | Val Acc: 0.6285
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.960    Count 3: 0.889    Count 4: 0.848    Count 5: 0.490  
  Count 6: 0.337    Count 7: 0.521    Count 8: 0.162    Count 9: 0.122    Count 10: 0.545  
Epoch [24] Batch [0/51] Loss: 0.1139 LR: 0.000100
Epoch [24] Batch [10/51] Loss: 0.2585 LR: 0.000100
Epoch [24] Batch [20/51] Loss: 0.4912 LR: 0.000100
Epoch [24] Batch [30/51] Loss: 0.4961 LR: 0.000100
Epoch [24] Batch [40/51] Loss: 0.2112 LR: 0.000100
Epoch [24] Batch [50/51] Loss: 0.1232 LR: 0.000100

Epoch [25/350] - Time: 5.14s
Train Loss: 0.3592 | Val Loss: 1.8539
Train Acc: 0.8630 | Val Acc: 0.6406
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.957    Count 2: 0.960    Count 3: 0.898    Count 4: 0.771    Count 5: 0.578  
  Count 6: 0.551    Count 7: 0.333    Count 8: 0.121    Count 9: 0.300    Count 10: 0.568  
Epoch [25] Batch [0/51] Loss: 0.3215 LR: 0.000100
Epoch [25] Batch [10/51] Loss: 0.2525 LR: 0.000100
Epoch [25] Batch [20/51] Loss: 0.2366 LR: 0.000100
Epoch [25] Batch [30/51] Loss: 0.1260 LR: 0.000100
Epoch [25] Batch [40/51] Loss: 0.3429 LR: 0.000100
Epoch [25] Batch [50/51] Loss: 0.6847 LR: 0.000100

Epoch [26/350] - Time: 5.04s
Train Loss: 0.4358 | Val Loss: 1.7719
Train Acc: 0.8346 | Val Acc: 0.6480
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.975    Count 2: 0.960    Count 3: 0.796    Count 4: 0.714    Count 5: 0.441  
  Count 6: 0.561    Count 7: 0.635    Count 8: 0.091    Count 9: 0.444    Count 10: 0.523  
Epoch [26] Batch [0/51] Loss: 0.5102 LR: 0.000100
Epoch [26] Batch [10/51] Loss: 0.3007 LR: 0.000100
Epoch [26] Batch [20/51] Loss: 0.1916 LR: 0.000100
Epoch [26] Batch [30/51] Loss: 0.1497 LR: 0.000100
Epoch [26] Batch [40/51] Loss: 0.4708 LR: 0.000100
Epoch [26] Batch [50/51] Loss: 1.1543 LR: 0.000100

Epoch [27/350] - Time: 5.47s
Train Loss: 0.3354 | Val Loss: 2.0702
Train Acc: 0.8691 | Val Acc: 0.6294
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.988    Count 2: 0.960    Count 3: 0.880    Count 4: 0.800    Count 5: 0.373  
  Count 6: 0.561    Count 7: 0.365    Count 8: 0.172    Count 9: 0.189    Count 10: 0.614  
Epoch [27] Batch [0/51] Loss: 0.3533 LR: 0.000100
Epoch [27] Batch [10/51] Loss: 0.2055 LR: 0.000100
Epoch [27] Batch [20/51] Loss: 0.1533 LR: 0.000100
Epoch [27] Batch [30/51] Loss: 0.0899 LR: 0.000100
Epoch [27] Batch [40/51] Loss: 0.4544 LR: 0.000100
Epoch [27] Batch [50/51] Loss: 0.4079 LR: 0.000100

Epoch [28/350] - Time: 5.09s
Train Loss: 0.2643 | Val Loss: 2.5109
Train Acc: 0.8852 | Val Acc: 0.5447
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.988    Count 2: 0.810    Count 3: 0.815    Count 4: 0.324    Count 5: 0.235  
  Count 6: 0.388    Count 7: 0.604    Count 8: 0.182    Count 9: 0.100    Count 10: 0.614  
Epoch [28] Batch [0/51] Loss: 0.3550 LR: 0.000100
Epoch [28] Batch [10/51] Loss: 0.1992 LR: 0.000100
Epoch [28] Batch [20/51] Loss: 0.1218 LR: 0.000100
Epoch [28] Batch [30/51] Loss: 0.3486 LR: 0.000100
Epoch [28] Batch [40/51] Loss: 0.5709 LR: 0.000100
Epoch [28] Batch [50/51] Loss: 0.7999 LR: 0.000100

Epoch [29/350] - Time: 5.04s
Train Loss: 0.3019 | Val Loss: 1.8157
Train Acc: 0.8815 | Val Acc: 0.6462
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.960    Count 3: 0.870    Count 4: 0.705    Count 5: 0.833  
  Count 6: 0.449    Count 7: 0.448    Count 8: 0.091    Count 9: 0.200    Count 10: 0.511  
Epoch [29] Batch [0/51] Loss: 0.3136 LR: 0.000100
Epoch [29] Batch [10/51] Loss: 0.0913 LR: 0.000100
Epoch [29] Batch [20/51] Loss: 0.2839 LR: 0.000100
Epoch [29] Batch [30/51] Loss: 0.0944 LR: 0.000100
Epoch [29] Batch [40/51] Loss: 0.1418 LR: 0.000100
Epoch [29] Batch [50/51] Loss: 0.5589 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [30/350] - Time: 5.12s
Train Loss: 0.2469 | Val Loss: 2.0684
Train Acc: 0.9062 | Val Acc: 0.6620
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.975    Count 2: 0.992    Count 3: 0.852    Count 4: 0.819    Count 5: 0.627  
  Count 6: 0.480    Count 7: 0.552    Count 8: 0.172    Count 9: 0.233    Count 10: 0.545  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/best_single_image_model.pth
  → New best model! Validation accuracy: 0.6620
新的最佳模型! 验证准确率: 0.6620
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/single_image_checkpoint_epoch_29.pth
Epoch [30] Batch [0/51] Loss: 0.2150 LR: 0.000100
Epoch [30] Batch [10/51] Loss: 0.1423 LR: 0.000100
Epoch [30] Batch [20/51] Loss: 0.3544 LR: 0.000100
Epoch [30] Batch [30/51] Loss: 0.2707 LR: 0.000100
Epoch [30] Batch [40/51] Loss: 0.1647 LR: 0.000100
Epoch [30] Batch [50/51] Loss: 0.0768 LR: 0.000100

Epoch [31/350] - Time: 5.08s
Train Loss: 0.1875 | Val Loss: 2.1292
Train Acc: 0.9432 | Val Acc: 0.6592
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.960    Count 3: 0.880    Count 4: 0.743    Count 5: 0.608  
  Count 6: 0.561    Count 7: 0.635    Count 8: 0.162    Count 9: 0.189    Count 10: 0.477  
Epoch [31] Batch [0/51] Loss: 0.2576 LR: 0.000100
Epoch [31] Batch [10/51] Loss: 0.0492 LR: 0.000100
Epoch [31] Batch [20/51] Loss: 0.0850 LR: 0.000100
Epoch [31] Batch [30/51] Loss: 0.0319 LR: 0.000100
Epoch [31] Batch [40/51] Loss: 0.2623 LR: 0.000100
Epoch [31] Batch [50/51] Loss: 0.2546 LR: 0.000100

Epoch [32/350] - Time: 5.06s
Train Loss: 0.1871 | Val Loss: 2.0829
Train Acc: 0.9259 | Val Acc: 0.6378
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.921    Count 3: 0.815    Count 4: 0.800    Count 5: 0.647  
  Count 6: 0.714    Count 7: 0.365    Count 8: 0.121    Count 9: 0.089    Count 10: 0.511  
Epoch [32] Batch [0/51] Loss: 0.2789 LR: 0.000100
Epoch [32] Batch [10/51] Loss: 0.1337 LR: 0.000100
Epoch [32] Batch [20/51] Loss: 0.3415 LR: 0.000100
Epoch [32] Batch [30/51] Loss: 0.5188 LR: 0.000100
Epoch [32] Batch [40/51] Loss: 0.1659 LR: 0.000100
Epoch [32] Batch [50/51] Loss: 0.0916 LR: 0.000100

Epoch [33/350] - Time: 5.17s
Train Loss: 0.1911 | Val Loss: 2.4347
Train Acc: 0.9333 | Val Acc: 0.6453
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.975    Count 2: 0.937    Count 3: 0.907    Count 4: 0.810    Count 5: 0.539  
  Count 6: 0.490    Count 7: 0.531    Count 8: 0.152    Count 9: 0.278    Count 10: 0.455  
Epoch [33] Batch [0/51] Loss: 0.4415 LR: 0.000100
Epoch [33] Batch [10/51] Loss: 0.7274 LR: 0.000100
Epoch [33] Batch [20/51] Loss: 0.1935 LR: 0.000100
Epoch [33] Batch [30/51] Loss: 0.4266 LR: 0.000100
Epoch [33] Batch [40/51] Loss: 0.4042 LR: 0.000100
Epoch [33] Batch [50/51] Loss: 0.0805 LR: 0.000100

Epoch [34/350] - Time: 4.94s
Train Loss: 0.2575 | Val Loss: 2.3339
Train Acc: 0.8914 | Val Acc: 0.6201
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.944    Count 3: 0.870    Count 4: 0.657    Count 5: 0.667  
  Count 6: 0.582    Count 7: 0.219    Count 8: 0.121    Count 9: 0.300    Count 10: 0.432  
Epoch [34] Batch [0/51] Loss: 0.1596 LR: 0.000100
Epoch [34] Batch [10/51] Loss: 0.1396 LR: 0.000100
Epoch [34] Batch [20/51] Loss: 0.0499 LR: 0.000100
Epoch [34] Batch [30/51] Loss: 0.2185 LR: 0.000100
Epoch [34] Batch [40/51] Loss: 0.2082 LR: 0.000100
Epoch [34] Batch [50/51] Loss: 0.1439 LR: 0.000100

Epoch [35/350] - Time: 5.47s
Train Loss: 0.1962 | Val Loss: 2.5516
Train Acc: 0.9309 | Val Acc: 0.6052
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.944    Count 3: 0.852    Count 4: 0.714    Count 5: 0.637  
  Count 6: 0.418    Count 7: 0.219    Count 8: 0.121    Count 9: 0.178    Count 10: 0.534  
Epoch [35] Batch [0/51] Loss: 0.0836 LR: 0.000100
Epoch [35] Batch [10/51] Loss: 0.0529 LR: 0.000100
Epoch [35] Batch [20/51] Loss: 0.0674 LR: 0.000100
Epoch [35] Batch [30/51] Loss: 1.0924 LR: 0.000100
Epoch [35] Batch [40/51] Loss: 0.2584 LR: 0.000100
Epoch [35] Batch [50/51] Loss: 1.0854 LR: 0.000100

Epoch [36/350] - Time: 5.10s
Train Loss: 0.2345 | Val Loss: 2.9667
Train Acc: 0.9160 | Val Acc: 0.5912
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.969    Count 2: 0.865    Count 3: 0.741    Count 4: 0.629    Count 5: 0.412  
  Count 6: 0.510    Count 7: 0.667    Count 8: 0.131    Count 9: 0.122    Count 10: 0.489  
Epoch [36] Batch [0/51] Loss: 0.1277 LR: 0.000100
Epoch [36] Batch [10/51] Loss: 0.0266 LR: 0.000100
Epoch [36] Batch [20/51] Loss: 0.1474 LR: 0.000100
Epoch [36] Batch [30/51] Loss: 0.3949 LR: 0.000100
Epoch [36] Batch [40/51] Loss: 0.6472 LR: 0.000100
Epoch [36] Batch [50/51] Loss: 0.1462 LR: 0.000100

Epoch [37/350] - Time: 5.14s
Train Loss: 0.1949 | Val Loss: 2.8602
Train Acc: 0.9235 | Val Acc: 0.6378
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.944    Count 3: 0.870    Count 4: 0.705    Count 5: 0.471  
  Count 6: 0.612    Count 7: 0.385    Count 8: 0.141    Count 9: 0.333    Count 10: 0.545  
Epoch [37] Batch [0/51] Loss: 0.2041 LR: 0.000100
Epoch [37] Batch [10/51] Loss: 0.1271 LR: 0.000100
Epoch [37] Batch [20/51] Loss: 0.2837 LR: 0.000100
Epoch [37] Batch [30/51] Loss: 0.1124 LR: 0.000100
Epoch [37] Batch [40/51] Loss: 0.0957 LR: 0.000100
Epoch [37] Batch [50/51] Loss: 0.1991 LR: 0.000100

Epoch [38/350] - Time: 5.20s
Train Loss: 0.1937 | Val Loss: 3.4092
Train Acc: 0.9247 | Val Acc: 0.5978
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.981    Count 2: 0.937    Count 3: 0.796    Count 4: 0.533    Count 5: 0.373  
  Count 6: 0.531    Count 7: 0.542    Count 8: 0.141    Count 9: 0.144    Count 10: 0.614  
Epoch [38] Batch [0/51] Loss: 0.3185 LR: 0.000100
Epoch [38] Batch [10/51] Loss: 0.0062 LR: 0.000100
Epoch [38] Batch [20/51] Loss: 0.3247 LR: 0.000100
Epoch [38] Batch [30/51] Loss: 0.1265 LR: 0.000100
Epoch [38] Batch [40/51] Loss: 0.1862 LR: 0.000100
Epoch [38] Batch [50/51] Loss: 0.0638 LR: 0.000100

Epoch [39/350] - Time: 5.09s
Train Loss: 0.1321 | Val Loss: 3.1677
Train Acc: 0.9642 | Val Acc: 0.6015
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.897    Count 3: 0.861    Count 4: 0.600    Count 5: 0.618  
  Count 6: 0.449    Count 7: 0.312    Count 8: 0.091    Count 9: 0.244    Count 10: 0.545  
Epoch [39] Batch [0/51] Loss: 0.1526 LR: 0.000100
Epoch [39] Batch [10/51] Loss: 0.1039 LR: 0.000100
Epoch [39] Batch [20/51] Loss: 0.2985 LR: 0.000100
Epoch [39] Batch [30/51] Loss: 0.0073 LR: 0.000100
Epoch [39] Batch [40/51] Loss: 0.3151 LR: 0.000100
Epoch [39] Batch [50/51] Loss: 0.1269 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [40/350] - Time: 5.02s
Train Loss: 0.1434 | Val Loss: 2.8773
Train Acc: 0.9420 | Val Acc: 0.6257
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.988    Count 2: 0.952    Count 3: 0.898    Count 4: 0.705    Count 5: 0.706  
  Count 6: 0.439    Count 7: 0.260    Count 8: 0.131    Count 9: 0.300    Count 10: 0.466  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/single_image_checkpoint_epoch_39.pth
Epoch [40] Batch [0/51] Loss: 0.4665 LR: 0.000100
Epoch [40] Batch [10/51] Loss: 0.2077 LR: 0.000100
Epoch [40] Batch [20/51] Loss: 0.3050 LR: 0.000100
Epoch [40] Batch [30/51] Loss: 0.1241 LR: 0.000100
Epoch [40] Batch [40/51] Loss: 0.0278 LR: 0.000100
Epoch [40] Batch [50/51] Loss: 0.0355 LR: 0.000100

Epoch [41/350] - Time: 5.04s
Train Loss: 0.1139 | Val Loss: 3.0004
Train Acc: 0.9617 | Val Acc: 0.6499
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.968    Count 3: 0.907    Count 4: 0.762    Count 5: 0.804  
  Count 6: 0.531    Count 7: 0.302    Count 8: 0.071    Count 9: 0.278    Count 10: 0.477  
Epoch [41] Batch [0/51] Loss: 0.0980 LR: 0.000100
Epoch [41] Batch [10/51] Loss: 0.2426 LR: 0.000100
Epoch [41] Batch [20/51] Loss: 0.4305 LR: 0.000100
Epoch [41] Batch [30/51] Loss: 0.4948 LR: 0.000100
Epoch [41] Batch [40/51] Loss: 0.3788 LR: 0.000100
Epoch [41] Batch [50/51] Loss: 0.3299 LR: 0.000100

Epoch [42/350] - Time: 5.02s
Train Loss: 0.1957 | Val Loss: 2.9361
Train Acc: 0.9333 | Val Acc: 0.6238
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.981    Count 2: 0.976    Count 3: 0.889    Count 4: 0.829    Count 5: 0.647  
  Count 6: 0.388    Count 7: 0.260    Count 8: 0.121    Count 9: 0.256    Count 10: 0.466  
Epoch [42] Batch [0/51] Loss: 0.1199 LR: 0.000100
Epoch [42] Batch [10/51] Loss: 0.0287 LR: 0.000100
Epoch [42] Batch [20/51] Loss: 0.0263 LR: 0.000100
Epoch [42] Batch [30/51] Loss: 0.0643 LR: 0.000100
Epoch [42] Batch [40/51] Loss: 0.0041 LR: 0.000100
Epoch [42] Batch [50/51] Loss: 0.0208 LR: 0.000100

Epoch [43/350] - Time: 5.44s
Train Loss: 0.0809 | Val Loss: 3.1073
Train Acc: 0.9728 | Val Acc: 0.6564
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.976    Count 3: 0.880    Count 4: 0.848    Count 5: 0.716  
  Count 6: 0.480    Count 7: 0.333    Count 8: 0.172    Count 9: 0.222    Count 10: 0.545  
Epoch [43] Batch [0/51] Loss: 0.3592 LR: 0.000100
Epoch [43] Batch [10/51] Loss: 0.0191 LR: 0.000100
Epoch [43] Batch [20/51] Loss: 0.0253 LR: 0.000100
Epoch [43] Batch [30/51] Loss: 0.1324 LR: 0.000100
Epoch [43] Batch [40/51] Loss: 0.0209 LR: 0.000100
Epoch [43] Batch [50/51] Loss: 0.2128 LR: 0.000100

Epoch [44/350] - Time: 5.14s
Train Loss: 0.0980 | Val Loss: 3.2807
Train Acc: 0.9679 | Val Acc: 0.6322
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.988    Count 2: 0.937    Count 3: 0.898    Count 4: 0.800    Count 5: 0.627  
  Count 6: 0.541    Count 7: 0.302    Count 8: 0.091    Count 9: 0.256    Count 10: 0.477  
Epoch [44] Batch [0/51] Loss: 0.0481 LR: 0.000100
Epoch [44] Batch [10/51] Loss: 0.3539 LR: 0.000100
Epoch [44] Batch [20/51] Loss: 0.0225 LR: 0.000100
Epoch [44] Batch [30/51] Loss: 0.0092 LR: 0.000100
Epoch [44] Batch [40/51] Loss: 0.0730 LR: 0.000100
Epoch [44] Batch [50/51] Loss: 0.0061 LR: 0.000100

Epoch [45/350] - Time: 5.14s
Train Loss: 0.1049 | Val Loss: 3.0584
Train Acc: 0.9605 | Val Acc: 0.6555
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.988    Count 2: 0.968    Count 3: 0.907    Count 4: 0.819    Count 5: 0.657  
  Count 6: 0.653    Count 7: 0.302    Count 8: 0.101    Count 9: 0.267    Count 10: 0.500  
Epoch [45] Batch [0/51] Loss: 0.0060 LR: 0.000100
Epoch [45] Batch [10/51] Loss: 1.2741 LR: 0.000100
Epoch [45] Batch [20/51] Loss: 0.0936 LR: 0.000100
Epoch [45] Batch [30/51] Loss: 0.0086 LR: 0.000100
Epoch [45] Batch [40/51] Loss: 0.0303 LR: 0.000100
Epoch [45] Batch [50/51] Loss: 0.0275 LR: 0.000100

Epoch [46/350] - Time: 5.08s
Train Loss: 0.1604 | Val Loss: 3.4338
Train Acc: 0.9420 | Val Acc: 0.6192
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.968    Count 3: 0.796    Count 4: 0.695    Count 5: 0.696  
  Count 6: 0.459    Count 7: 0.240    Count 8: 0.091    Count 9: 0.311    Count 10: 0.534  
Epoch [46] Batch [0/51] Loss: 0.0227 LR: 0.000100
Epoch [46] Batch [10/51] Loss: 0.2969 LR: 0.000100
Epoch [46] Batch [20/51] Loss: 0.0771 LR: 0.000100
Epoch [46] Batch [30/51] Loss: 0.0282 LR: 0.000100
Epoch [46] Batch [40/51] Loss: 0.4660 LR: 0.000100
Epoch [46] Batch [50/51] Loss: 0.0048 LR: 0.000100

Epoch [47/350] - Time: 5.11s
Train Loss: 0.2426 | Val Loss: 2.7616
Train Acc: 0.9123 | Val Acc: 0.6480
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.937    Count 3: 0.907    Count 4: 0.790    Count 5: 0.520  
  Count 6: 0.643    Count 7: 0.406    Count 8: 0.051    Count 9: 0.333    Count 10: 0.523  
Epoch [47] Batch [0/51] Loss: 0.0560 LR: 0.000100
Epoch [47] Batch [10/51] Loss: 0.6145 LR: 0.000100
Epoch [47] Batch [20/51] Loss: 0.0019 LR: 0.000100
Epoch [47] Batch [30/51] Loss: 0.7600 LR: 0.000100
Epoch [47] Batch [40/51] Loss: 0.1158 LR: 0.000100
Epoch [47] Batch [50/51] Loss: 0.6930 LR: 0.000100

Epoch [48/350] - Time: 5.06s
Train Loss: 0.1143 | Val Loss: 3.8155
Train Acc: 0.9593 | Val Acc: 0.6425
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.937    Count 3: 0.889    Count 4: 0.752    Count 5: 0.490  
  Count 6: 0.510    Count 7: 0.438    Count 8: 0.091    Count 9: 0.367    Count 10: 0.591  
Epoch [48] Batch [0/51] Loss: 0.1521 LR: 0.000100
Epoch [48] Batch [10/51] Loss: 0.0369 LR: 0.000100
Epoch [48] Batch [20/51] Loss: 0.2251 LR: 0.000100
Epoch [48] Batch [30/51] Loss: 0.0037 LR: 0.000100
Epoch [48] Batch [40/51] Loss: 0.2260 LR: 0.000100
Epoch [48] Batch [50/51] Loss: 0.0048 LR: 0.000100

Epoch [49/350] - Time: 5.13s
Train Loss: 0.1105 | Val Loss: 3.7076
Train Acc: 0.9617 | Val Acc: 0.6108
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.960    Count 3: 0.833    Count 4: 0.800    Count 5: 0.627  
  Count 6: 0.439    Count 7: 0.208    Count 8: 0.061    Count 9: 0.311    Count 10: 0.432  
Epoch [49] Batch [0/51] Loss: 0.0122 LR: 0.000100
Epoch [49] Batch [10/51] Loss: 0.0166 LR: 0.000100
Epoch [49] Batch [20/51] Loss: 0.1230 LR: 0.000100
Epoch [49] Batch [30/51] Loss: 0.6688 LR: 0.000100
Epoch [49] Batch [40/51] Loss: 0.3032 LR: 0.000100
Epoch [49] Batch [50/51] Loss: 0.1106 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [50/350] - Time: 5.04s
Train Loss: 0.1503 | Val Loss: 3.4310
Train Acc: 0.9531 | Val Acc: 0.6490
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.952    Count 3: 0.889    Count 4: 0.810    Count 5: 0.706  
  Count 6: 0.469    Count 7: 0.448    Count 8: 0.101    Count 9: 0.233    Count 10: 0.477  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/single_image_checkpoint_epoch_49.pth
Epoch [50] Batch [0/51] Loss: 0.0210 LR: 0.000100
Epoch [50] Batch [10/51] Loss: 0.0526 LR: 0.000100
Epoch [50] Batch [20/51] Loss: 0.0232 LR: 0.000100
Epoch [50] Batch [30/51] Loss: 0.0459 LR: 0.000100
Epoch [50] Batch [40/51] Loss: 0.0158 LR: 0.000100
Epoch [50] Batch [50/51] Loss: 0.1328 LR: 0.000100

Epoch [51/350] - Time: 5.18s
Train Loss: 0.1186 | Val Loss: 3.7841
Train Acc: 0.9519 | Val Acc: 0.6108
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.988    Count 2: 0.881    Count 3: 0.870    Count 4: 0.638    Count 5: 0.451  
  Count 6: 0.612    Count 7: 0.344    Count 8: 0.162    Count 9: 0.233    Count 10: 0.545  
Epoch [51] Batch [0/51] Loss: 0.0386 LR: 0.000100
Epoch [51] Batch [10/51] Loss: 0.0136 LR: 0.000100
Epoch [51] Batch [20/51] Loss: 0.0492 LR: 0.000100
Epoch [51] Batch [30/51] Loss: 0.0358 LR: 0.000100
Epoch [51] Batch [40/51] Loss: 0.0571 LR: 0.000100
Epoch [51] Batch [50/51] Loss: 0.0481 LR: 0.000100

Epoch [52/350] - Time: 5.49s
Train Loss: 0.0875 | Val Loss: 3.5665
Train Acc: 0.9667 | Val Acc: 0.6397
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.913    Count 3: 0.889    Count 4: 0.838    Count 5: 0.824  
  Count 6: 0.459    Count 7: 0.240    Count 8: 0.071    Count 9: 0.211    Count 10: 0.545  
Epoch [52] Batch [0/51] Loss: 0.1254 LR: 0.000100
Epoch [52] Batch [10/51] Loss: 0.3924 LR: 0.000100
Epoch [52] Batch [20/51] Loss: 0.1049 LR: 0.000100
Epoch [52] Batch [30/51] Loss: 0.0286 LR: 0.000100
Epoch [52] Batch [40/51] Loss: 0.4751 LR: 0.000100
Epoch [52] Batch [50/51] Loss: 0.0027 LR: 0.000100

Epoch [53/350] - Time: 5.19s
Train Loss: 0.0775 | Val Loss: 3.9554
Train Acc: 0.9704 | Val Acc: 0.6229
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.841    Count 3: 0.870    Count 4: 0.800    Count 5: 0.549  
  Count 6: 0.571    Count 7: 0.396    Count 8: 0.141    Count 9: 0.056    Count 10: 0.625  
Epoch [53] Batch [0/51] Loss: 0.0161 LR: 0.000100
Epoch [53] Batch [10/51] Loss: 0.0013 LR: 0.000100
Epoch [53] Batch [20/51] Loss: 0.0188 LR: 0.000100
Epoch [53] Batch [30/51] Loss: 0.0526 LR: 0.000100
Epoch [53] Batch [40/51] Loss: 0.0194 LR: 0.000100
Epoch [53] Batch [50/51] Loss: 0.0302 LR: 0.000100

Epoch [54/350] - Time: 5.03s
Train Loss: 0.0474 | Val Loss: 3.9387
Train Acc: 0.9840 | Val Acc: 0.6397
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.952    Count 3: 0.907    Count 4: 0.810    Count 5: 0.725  
  Count 6: 0.541    Count 7: 0.344    Count 8: 0.131    Count 9: 0.067    Count 10: 0.500  
Epoch [54] Batch [0/51] Loss: 0.0340 LR: 0.000100
Epoch [54] Batch [10/51] Loss: 0.0030 LR: 0.000100
Epoch [54] Batch [20/51] Loss: 0.0162 LR: 0.000100
Epoch [54] Batch [30/51] Loss: 0.0033 LR: 0.000100
Epoch [54] Batch [40/51] Loss: 0.0104 LR: 0.000100
Epoch [54] Batch [50/51] Loss: 0.0068 LR: 0.000100

Epoch [55/350] - Time: 5.14s
Train Loss: 0.0396 | Val Loss: 3.9566
Train Acc: 0.9852 | Val Acc: 0.6508
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.937    Count 3: 0.889    Count 4: 0.762    Count 5: 0.676  
  Count 6: 0.520    Count 7: 0.448    Count 8: 0.111    Count 9: 0.189    Count 10: 0.602  
Epoch [55] Batch [0/51] Loss: 0.0168 LR: 0.000100
Epoch [55] Batch [10/51] Loss: 0.5227 LR: 0.000100
Epoch [55] Batch [20/51] Loss: 0.0012 LR: 0.000100
Epoch [55] Batch [30/51] Loss: 0.2596 LR: 0.000100
Epoch [55] Batch [40/51] Loss: 0.0095 LR: 0.000100
Epoch [55] Batch [50/51] Loss: 0.0032 LR: 0.000100

Epoch [56/350] - Time: 5.12s
Train Loss: 0.1225 | Val Loss: 4.1952
Train Acc: 0.9593 | Val Acc: 0.6350
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.988    Count 2: 0.944    Count 3: 0.917    Count 4: 0.838    Count 5: 0.804  
  Count 6: 0.408    Count 7: 0.375    Count 8: 0.091    Count 9: 0.033    Count 10: 0.523  
Epoch [56] Batch [0/51] Loss: 0.2655 LR: 0.000100
Epoch [56] Batch [10/51] Loss: 0.0046 LR: 0.000100
Epoch [56] Batch [20/51] Loss: 0.0075 LR: 0.000100
Epoch [56] Batch [30/51] Loss: 0.0019 LR: 0.000100
Epoch [56] Batch [40/51] Loss: 0.1569 LR: 0.000100
Epoch [56] Batch [50/51] Loss: 0.0018 LR: 0.000100

Epoch [57/350] - Time: 5.07s
Train Loss: 0.0846 | Val Loss: 4.3233
Train Acc: 0.9716 | Val Acc: 0.6350
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.960    Count 3: 0.898    Count 4: 0.857    Count 5: 0.647  
  Count 6: 0.520    Count 7: 0.240    Count 8: 0.101    Count 9: 0.267    Count 10: 0.443  
Epoch [57] Batch [0/51] Loss: 0.0032 LR: 0.000100
Epoch [57] Batch [10/51] Loss: 0.0013 LR: 0.000100
Epoch [57] Batch [20/51] Loss: 0.0449 LR: 0.000100
Epoch [57] Batch [30/51] Loss: 0.0050 LR: 0.000100
Epoch [57] Batch [40/51] Loss: 0.0011 LR: 0.000100
Epoch [57] Batch [50/51] Loss: 0.1949 LR: 0.000100

Epoch [58/350] - Time: 5.06s
Train Loss: 0.0633 | Val Loss: 4.4944
Train Acc: 0.9778 | Val Acc: 0.6155
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.960    Count 3: 0.880    Count 4: 0.771    Count 5: 0.765  
  Count 6: 0.439    Count 7: 0.208    Count 8: 0.131    Count 9: 0.122    Count 10: 0.420  
Epoch [58] Batch [0/51] Loss: 0.0034 LR: 0.000100
Epoch [58] Batch [10/51] Loss: 0.0092 LR: 0.000100
Epoch [58] Batch [20/51] Loss: 0.0610 LR: 0.000100
Epoch [58] Batch [30/51] Loss: 0.0501 LR: 0.000100
Epoch [58] Batch [40/51] Loss: 0.2117 LR: 0.000100
Epoch [58] Batch [50/51] Loss: 0.0100 LR: 0.000100

Epoch [59/350] - Time: 5.11s
Train Loss: 0.0741 | Val Loss: 4.3671
Train Acc: 0.9704 | Val Acc: 0.6508
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.988    Count 2: 0.992    Count 3: 0.889    Count 4: 0.857    Count 5: 0.657  
  Count 6: 0.653    Count 7: 0.229    Count 8: 0.162    Count 9: 0.178    Count 10: 0.489  
Epoch [59] Batch [0/51] Loss: 0.0006 LR: 0.000100
Epoch [59] Batch [10/51] Loss: 0.3646 LR: 0.000100
Epoch [59] Batch [20/51] Loss: 0.0436 LR: 0.000100
Epoch [59] Batch [30/51] Loss: 0.0033 LR: 0.000100
Epoch [59] Batch [40/51] Loss: 0.0014 LR: 0.000100
Epoch [59] Batch [50/51] Loss: 0.0046 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [60/350] - Time: 5.29s
Train Loss: 0.0631 | Val Loss: 4.3753
Train Acc: 0.9790 | Val Acc: 0.6425
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.984    Count 3: 0.843    Count 4: 0.838    Count 5: 0.686  
  Count 6: 0.571    Count 7: 0.260    Count 8: 0.131    Count 9: 0.233    Count 10: 0.466  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/single_image_checkpoint_epoch_59.pth
Epoch [60] Batch [0/51] Loss: 0.0186 LR: 0.000100
Epoch [60] Batch [10/51] Loss: 0.0007 LR: 0.000100
Epoch [60] Batch [20/51] Loss: 0.1800 LR: 0.000100
Epoch [60] Batch [30/51] Loss: 0.0019 LR: 0.000100
Epoch [60] Batch [40/51] Loss: 0.0116 LR: 0.000100
Epoch [60] Batch [50/51] Loss: 0.0022 LR: 0.000100

Epoch [61/350] - Time: 5.19s
Train Loss: 0.0390 | Val Loss: 4.4896
Train Acc: 0.9864 | Val Acc: 0.6294
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.969    Count 2: 0.921    Count 3: 0.907    Count 4: 0.781    Count 5: 0.667  
  Count 6: 0.561    Count 7: 0.271    Count 8: 0.101    Count 9: 0.256    Count 10: 0.466  
Epoch [61] Batch [0/51] Loss: 0.0072 LR: 0.000100
Epoch [61] Batch [10/51] Loss: 0.0933 LR: 0.000100
Epoch [61] Batch [20/51] Loss: 0.0009 LR: 0.000100
Epoch [61] Batch [30/51] Loss: 0.0077 LR: 0.000100
Epoch [61] Batch [40/51] Loss: 0.0213 LR: 0.000100
Epoch [61] Batch [50/51] Loss: 0.0006 LR: 0.000100

Epoch [62/350] - Time: 5.06s
Train Loss: 0.0290 | Val Loss: 4.5420
Train Acc: 0.9889 | Val Acc: 0.6248
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.988    Count 2: 0.921    Count 3: 0.898    Count 4: 0.762    Count 5: 0.578  
  Count 6: 0.500    Count 7: 0.365    Count 8: 0.131    Count 9: 0.133    Count 10: 0.568  
Epoch [62] Batch [0/51] Loss: 0.0040 LR: 0.000100
Epoch [62] Batch [10/51] Loss: 0.1938 LR: 0.000100
Epoch [62] Batch [20/51] Loss: 0.2398 LR: 0.000100
Epoch [62] Batch [30/51] Loss: 0.3725 LR: 0.000100
Epoch [62] Batch [40/51] Loss: 0.0650 LR: 0.000100
Epoch [62] Batch [50/51] Loss: 0.0680 LR: 0.000100

Epoch [63/350] - Time: 5.04s
Train Loss: 0.0892 | Val Loss: 4.7587
Train Acc: 0.9753 | Val Acc: 0.6331
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.988    Count 2: 0.937    Count 3: 0.889    Count 4: 0.810    Count 5: 0.725  
  Count 6: 0.490    Count 7: 0.229    Count 8: 0.162    Count 9: 0.122    Count 10: 0.568  
Epoch [63] Batch [0/51] Loss: 0.0120 LR: 0.000100
Epoch [63] Batch [10/51] Loss: 0.0334 LR: 0.000100
Epoch [63] Batch [20/51] Loss: 0.0012 LR: 0.000100
Epoch [63] Batch [30/51] Loss: 0.0013 LR: 0.000100
Epoch [63] Batch [40/51] Loss: 0.0066 LR: 0.000100
Epoch [63] Batch [50/51] Loss: 0.0053 LR: 0.000100

Epoch [64/350] - Time: 5.07s
Train Loss: 0.0372 | Val Loss: 5.5171
Train Acc: 0.9889 | Val Acc: 0.6201
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.988    Count 2: 0.929    Count 3: 0.870    Count 4: 0.762    Count 5: 0.549  
  Count 6: 0.561    Count 7: 0.396    Count 8: 0.141    Count 9: 0.056    Count 10: 0.534  
Epoch [64] Batch [0/51] Loss: 0.0015 LR: 0.000100
Epoch [64] Batch [10/51] Loss: 0.1973 LR: 0.000100
Epoch [64] Batch [20/51] Loss: 0.1854 LR: 0.000100
Epoch [64] Batch [30/51] Loss: 0.9589 LR: 0.000100
Epoch [64] Batch [40/51] Loss: 0.0065 LR: 0.000100
Epoch [64] Batch [50/51] Loss: 0.0018 LR: 0.000100

Epoch [65/350] - Time: 5.06s
Train Loss: 0.0831 | Val Loss: 4.7192
Train Acc: 0.9716 | Val Acc: 0.6490
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.952    Count 3: 0.889    Count 4: 0.762    Count 5: 0.559  
  Count 6: 0.633    Count 7: 0.458    Count 8: 0.152    Count 9: 0.122    Count 10: 0.580  
Epoch [65] Batch [0/51] Loss: 0.0067 LR: 0.000100
Epoch [65] Batch [10/51] Loss: 0.0132 LR: 0.000100
Epoch [65] Batch [20/51] Loss: 0.2881 LR: 0.000100
Epoch [65] Batch [30/51] Loss: 0.0195 LR: 0.000100
Epoch [65] Batch [40/51] Loss: 0.2681 LR: 0.000100
Epoch [65] Batch [50/51] Loss: 0.0701 LR: 0.000100

Epoch [66/350] - Time: 5.07s
Train Loss: 0.0572 | Val Loss: 4.7209
Train Acc: 0.9778 | Val Acc: 0.6443
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.981    Count 2: 0.937    Count 3: 0.898    Count 4: 0.848    Count 5: 0.676  
  Count 6: 0.592    Count 7: 0.198    Count 8: 0.091    Count 9: 0.233    Count 10: 0.602  
Epoch [66] Batch [0/51] Loss: 0.1152 LR: 0.000100
Epoch [66] Batch [10/51] Loss: 0.0007 LR: 0.000100
Epoch [66] Batch [20/51] Loss: 0.0009 LR: 0.000100
Epoch [66] Batch [30/51] Loss: 0.0019 LR: 0.000100
Epoch [66] Batch [40/51] Loss: 0.1025 LR: 0.000100
Epoch [66] Batch [50/51] Loss: 0.0465 LR: 0.000100

Epoch [67/350] - Time: 5.10s
Train Loss: 0.0266 | Val Loss: 5.1151
Train Acc: 0.9901 | Val Acc: 0.6387
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.988    Count 2: 0.929    Count 3: 0.907    Count 4: 0.838    Count 5: 0.735  
  Count 6: 0.520    Count 7: 0.292    Count 8: 0.152    Count 9: 0.111    Count 10: 0.500  
Epoch [67] Batch [0/51] Loss: 0.1950 LR: 0.000100
Epoch [67] Batch [10/51] Loss: 0.0009 LR: 0.000100
Epoch [67] Batch [20/51] Loss: 0.0042 LR: 0.000100
Epoch [67] Batch [30/51] Loss: 0.0271 LR: 0.000100
Epoch [67] Batch [40/51] Loss: 0.1957 LR: 0.000100
Epoch [67] Batch [50/51] Loss: 0.0021 LR: 0.000100

Epoch [68/350] - Time: 5.36s
Train Loss: 0.0233 | Val Loss: 4.9792
Train Acc: 0.9901 | Val Acc: 0.6564
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.952    Count 3: 0.898    Count 4: 0.838    Count 5: 0.706  
  Count 6: 0.551    Count 7: 0.427    Count 8: 0.162    Count 9: 0.111    Count 10: 0.523  
Epoch [68] Batch [0/51] Loss: 0.0005 LR: 0.000100
Epoch [68] Batch [10/51] Loss: 0.0006 LR: 0.000100
Epoch [68] Batch [20/51] Loss: 0.0491 LR: 0.000100
Epoch [68] Batch [30/51] Loss: 0.1058 LR: 0.000100
Epoch [68] Batch [40/51] Loss: 0.0064 LR: 0.000100
Epoch [68] Batch [50/51] Loss: 0.5224 LR: 0.000100

Epoch [69/350] - Time: 5.14s
Train Loss: 0.0519 | Val Loss: 5.0201
Train Acc: 0.9852 | Val Acc: 0.6378
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.944    Count 3: 0.870    Count 4: 0.838    Count 5: 0.735  
  Count 6: 0.551    Count 7: 0.260    Count 8: 0.141    Count 9: 0.122    Count 10: 0.500  
Epoch [69] Batch [0/51] Loss: 0.0010 LR: 0.000100
Epoch [69] Batch [10/51] Loss: 0.0021 LR: 0.000100
Epoch [69] Batch [20/51] Loss: 0.0020 LR: 0.000100
Epoch [69] Batch [30/51] Loss: 0.0003 LR: 0.000100
Epoch [69] Batch [40/51] Loss: 0.0002 LR: 0.000100
Epoch [69] Batch [50/51] Loss: 0.0005 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [70/350] - Time: 5.07s
Train Loss: 0.0164 | Val Loss: 5.1576
Train Acc: 0.9975 | Val Acc: 0.6248
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.988    Count 2: 0.952    Count 3: 0.898    Count 4: 0.829    Count 5: 0.667  
  Count 6: 0.449    Count 7: 0.302    Count 8: 0.152    Count 9: 0.078    Count 10: 0.500  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/single_image_checkpoint_epoch_69.pth
Epoch [70] Batch [0/51] Loss: 0.0021 LR: 0.000100
Epoch [70] Batch [10/51] Loss: 0.0009 LR: 0.000100
Epoch [70] Batch [20/51] Loss: 0.0113 LR: 0.000100
Epoch [70] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [70] Batch [40/51] Loss: 0.0752 LR: 0.000100
Epoch [70] Batch [50/51] Loss: 0.0002 LR: 0.000100

Epoch [71/350] - Time: 5.08s
Train Loss: 0.0386 | Val Loss: 4.7495
Train Acc: 0.9877 | Val Acc: 0.6490
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.976    Count 3: 0.907    Count 4: 0.810    Count 5: 0.686  
  Count 6: 0.571    Count 7: 0.271    Count 8: 0.101    Count 9: 0.244    Count 10: 0.523  
Epoch [71] Batch [0/51] Loss: 0.0135 LR: 0.000100
Epoch [71] Batch [10/51] Loss: 0.0015 LR: 0.000100
Epoch [71] Batch [20/51] Loss: 0.0006 LR: 0.000100
Epoch [71] Batch [30/51] Loss: 0.0011 LR: 0.000100
Epoch [71] Batch [40/51] Loss: 0.0690 LR: 0.000100
Epoch [71] Batch [50/51] Loss: 0.3558 LR: 0.000100

Epoch [72/350] - Time: 5.07s
Train Loss: 0.0380 | Val Loss: 5.2662
Train Acc: 0.9840 | Val Acc: 0.6201
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.952    Count 3: 0.843    Count 4: 0.686    Count 5: 0.706  
  Count 6: 0.490    Count 7: 0.260    Count 8: 0.111    Count 9: 0.256    Count 10: 0.477  
Epoch [72] Batch [0/51] Loss: 0.1314 LR: 0.000100
Epoch [72] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [72] Batch [20/51] Loss: 0.0007 LR: 0.000100
Epoch [72] Batch [30/51] Loss: 0.0001 LR: 0.000100
Epoch [72] Batch [40/51] Loss: 0.0040 LR: 0.000100
Epoch [72] Batch [50/51] Loss: 0.0001 LR: 0.000100

Epoch [73/350] - Time: 5.15s
Train Loss: 0.0208 | Val Loss: 5.4648
Train Acc: 0.9938 | Val Acc: 0.6425
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.960    Count 3: 0.898    Count 4: 0.819    Count 5: 0.618  
  Count 6: 0.520    Count 7: 0.396    Count 8: 0.152    Count 9: 0.067    Count 10: 0.591  
Epoch [73] Batch [0/51] Loss: 0.0052 LR: 0.000100
Epoch [73] Batch [10/51] Loss: 0.0071 LR: 0.000100
Epoch [73] Batch [20/51] Loss: 0.0001 LR: 0.000100
Epoch [73] Batch [30/51] Loss: 0.2776 LR: 0.000100
Epoch [73] Batch [40/51] Loss: 0.0082 LR: 0.000100
Epoch [73] Batch [50/51] Loss: 0.0004 LR: 0.000100

Epoch [74/350] - Time: 4.96s
Train Loss: 0.0392 | Val Loss: 5.3919
Train Acc: 0.9852 | Val Acc: 0.6313
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.960    Count 3: 0.889    Count 4: 0.829    Count 5: 0.608  
  Count 6: 0.531    Count 7: 0.260    Count 8: 0.101    Count 9: 0.167    Count 10: 0.557  
Epoch [74] Batch [0/51] Loss: 0.0028 LR: 0.000100
Epoch [74] Batch [10/51] Loss: 0.0165 LR: 0.000100
Epoch [74] Batch [20/51] Loss: 0.0010 LR: 0.000100
Epoch [74] Batch [30/51] Loss: 0.0019 LR: 0.000100
Epoch [74] Batch [40/51] Loss: 0.0004 LR: 0.000100
Epoch [74] Batch [50/51] Loss: 0.0108 LR: 0.000100

Epoch [75/350] - Time: 5.07s
Train Loss: 0.0390 | Val Loss: 4.9394
Train Acc: 0.9852 | Val Acc: 0.6443
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.952    Count 3: 0.898    Count 4: 0.829    Count 5: 0.696  
  Count 6: 0.592    Count 7: 0.271    Count 8: 0.071    Count 9: 0.167    Count 10: 0.568  
Epoch [75] Batch [0/51] Loss: 0.0005 LR: 0.000100
Epoch [75] Batch [10/51] Loss: 0.0024 LR: 0.000100
Epoch [75] Batch [20/51] Loss: 0.1864 LR: 0.000100
Epoch [75] Batch [30/51] Loss: 0.1739 LR: 0.000100
Epoch [75] Batch [40/51] Loss: 0.0002 LR: 0.000100
Epoch [75] Batch [50/51] Loss: 0.5853 LR: 0.000100

Epoch [76/350] - Time: 5.01s
Train Loss: 0.0551 | Val Loss: 5.3012
Train Acc: 0.9827 | Val Acc: 0.6257
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.988    Count 2: 0.960    Count 3: 0.824    Count 4: 0.743    Count 5: 0.627  
  Count 6: 0.500    Count 7: 0.365    Count 8: 0.091    Count 9: 0.222    Count 10: 0.534  
Epoch [76] Batch [0/51] Loss: 0.0002 LR: 0.000100
Epoch [76] Batch [10/51] Loss: 0.2178 LR: 0.000100
Epoch [76] Batch [20/51] Loss: 0.0166 LR: 0.000100
Epoch [76] Batch [30/51] Loss: 0.0019 LR: 0.000100
Epoch [76] Batch [40/51] Loss: 0.0173 LR: 0.000100
Epoch [76] Batch [50/51] Loss: 0.0004 LR: 0.000100

Epoch [77/350] - Time: 5.38s
Train Loss: 0.0762 | Val Loss: 5.0929
Train Acc: 0.9753 | Val Acc: 0.6471
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.992    Count 3: 0.889    Count 4: 0.838    Count 5: 0.784  
  Count 6: 0.469    Count 7: 0.271    Count 8: 0.081    Count 9: 0.156    Count 10: 0.568  
Epoch [77] Batch [0/51] Loss: 0.0060 LR: 0.000100
Epoch [77] Batch [10/51] Loss: 0.0002 LR: 0.000100
Epoch [77] Batch [20/51] Loss: 0.0136 LR: 0.000100
Epoch [77] Batch [30/51] Loss: 0.3290 LR: 0.000100
Epoch [77] Batch [40/51] Loss: 0.0022 LR: 0.000100
Epoch [77] Batch [50/51] Loss: 0.0001 LR: 0.000100

Epoch [78/350] - Time: 5.12s
Train Loss: 0.0271 | Val Loss: 5.2029
Train Acc: 0.9938 | Val Acc: 0.6434
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.984    Count 3: 0.898    Count 4: 0.838    Count 5: 0.706  
  Count 6: 0.571    Count 7: 0.333    Count 8: 0.172    Count 9: 0.044    Count 10: 0.455  
Epoch [78] Batch [0/51] Loss: 0.0006 LR: 0.000100
Epoch [78] Batch [10/51] Loss: 0.0001 LR: 0.000100
Epoch [78] Batch [20/51] Loss: 0.0020 LR: 0.000100
Epoch [78] Batch [30/51] Loss: 0.0029 LR: 0.000100
Epoch [78] Batch [40/51] Loss: 0.0172 LR: 0.000100
Epoch [78] Batch [50/51] Loss: 0.0005 LR: 0.000100

Epoch [79/350] - Time: 5.14s
Train Loss: 0.0169 | Val Loss: 5.7236
Train Acc: 0.9914 | Val Acc: 0.6341
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.944    Count 3: 0.907    Count 4: 0.752    Count 5: 0.686  
  Count 6: 0.459    Count 7: 0.333    Count 8: 0.131    Count 9: 0.044    Count 10: 0.670  
Epoch [79] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [79] Batch [10/51] Loss: 0.0010 LR: 0.000100
Epoch [79] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [79] Batch [30/51] Loss: 0.0011 LR: 0.000100
Epoch [79] Batch [40/51] Loss: 0.0039 LR: 0.000100
Epoch [79] Batch [50/51] Loss: 0.0037 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [80/350] - Time: 5.00s
Train Loss: 0.0333 | Val Loss: 5.3919
Train Acc: 0.9938 | Val Acc: 0.6508
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.968    Count 3: 0.889    Count 4: 0.810    Count 5: 0.745  
  Count 6: 0.531    Count 7: 0.302    Count 8: 0.141    Count 9: 0.178    Count 10: 0.534  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/single_image_checkpoint_epoch_79.pth
Epoch [80] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [80] Batch [10/51] Loss: 0.0576 LR: 0.000100
Epoch [80] Batch [20/51] Loss: 0.0010 LR: 0.000100
Epoch [80] Batch [30/51] Loss: 0.4399 LR: 0.000100
Epoch [80] Batch [40/51] Loss: 0.3243 LR: 0.000100
Epoch [80] Batch [50/51] Loss: 0.2582 LR: 0.000100

Epoch [81/350] - Time: 5.08s
Train Loss: 0.0699 | Val Loss: 5.9573
Train Acc: 0.9741 | Val Acc: 0.6276
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.952    Count 3: 0.898    Count 4: 0.819    Count 5: 0.745  
  Count 6: 0.418    Count 7: 0.312    Count 8: 0.152    Count 9: 0.144    Count 10: 0.398  
Epoch [81] Batch [0/51] Loss: 0.0266 LR: 0.000100
Epoch [81] Batch [10/51] Loss: 0.0144 LR: 0.000100
Epoch [81] Batch [20/51] Loss: 0.0104 LR: 0.000100
Epoch [81] Batch [30/51] Loss: 0.0387 LR: 0.000100
Epoch [81] Batch [40/51] Loss: 0.0017 LR: 0.000100
Epoch [81] Batch [50/51] Loss: 0.0001 LR: 0.000100

Epoch [82/350] - Time: 5.10s
Train Loss: 0.0514 | Val Loss: 5.1743
Train Acc: 0.9877 | Val Acc: 0.6415
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.921    Count 3: 0.870    Count 4: 0.829    Count 5: 0.745  
  Count 6: 0.582    Count 7: 0.312    Count 8: 0.091    Count 9: 0.200    Count 10: 0.466  
Epoch [82] Batch [0/51] Loss: 0.0049 LR: 0.000100
Epoch [82] Batch [10/51] Loss: 0.0061 LR: 0.000100
Epoch [82] Batch [20/51] Loss: 0.0001 LR: 0.000100
Epoch [82] Batch [30/51] Loss: 0.0001 LR: 0.000100
Epoch [82] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [82] Batch [50/51] Loss: 0.0001 LR: 0.000100

Epoch [83/350] - Time: 5.10s
Train Loss: 0.0142 | Val Loss: 5.9391
Train Acc: 0.9951 | Val Acc: 0.6350
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.921    Count 3: 0.898    Count 4: 0.867    Count 5: 0.578  
  Count 6: 0.469    Count 7: 0.333    Count 8: 0.172    Count 9: 0.222    Count 10: 0.489  
Epoch [83] Batch [0/51] Loss: 0.0003 LR: 0.000100
Epoch [83] Batch [10/51] Loss: 0.0002 LR: 0.000100
Epoch [83] Batch [20/51] Loss: 0.0191 LR: 0.000100
Epoch [83] Batch [30/51] Loss: 0.0296 LR: 0.000100
Epoch [83] Batch [40/51] Loss: 0.3013 LR: 0.000100
Epoch [83] Batch [50/51] Loss: 0.0024 LR: 0.000100

Epoch [84/350] - Time: 5.12s
Train Loss: 0.0469 | Val Loss: 5.5825
Train Acc: 0.9815 | Val Acc: 0.6434
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.968    Count 3: 0.907    Count 4: 0.829    Count 5: 0.598  
  Count 6: 0.449    Count 7: 0.365    Count 8: 0.131    Count 9: 0.178    Count 10: 0.614  
Epoch [84] Batch [0/51] Loss: 0.0001 LR: 0.000100
Epoch [84] Batch [10/51] Loss: 0.0058 LR: 0.000100
Epoch [84] Batch [20/51] Loss: 0.0001 LR: 0.000100
Epoch [84] Batch [30/51] Loss: 0.0007 LR: 0.000100
Epoch [84] Batch [40/51] Loss: 0.0014 LR: 0.000100
Epoch [84] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [85/350] - Time: 5.39s
Train Loss: 0.0304 | Val Loss: 5.3854
Train Acc: 0.9901 | Val Acc: 0.6434
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.952    Count 3: 0.880    Count 4: 0.790    Count 5: 0.647  
  Count 6: 0.612    Count 7: 0.219    Count 8: 0.141    Count 9: 0.289    Count 10: 0.500  
Epoch [85] Batch [0/51] Loss: 0.0049 LR: 0.000100
Epoch [85] Batch [10/51] Loss: 0.0022 LR: 0.000100
Epoch [85] Batch [20/51] Loss: 0.0002 LR: 0.000100
Epoch [85] Batch [30/51] Loss: 0.0002 LR: 0.000100
Epoch [85] Batch [40/51] Loss: 0.0017 LR: 0.000100
Epoch [85] Batch [50/51] Loss: 0.0008 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [86/350] - Time: 5.05s
Train Loss: 0.0140 | Val Loss: 5.1566
Train Acc: 0.9951 | Val Acc: 0.6648
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.984    Count 3: 0.889    Count 4: 0.800    Count 5: 0.706  
  Count 6: 0.592    Count 7: 0.250    Count 8: 0.141    Count 9: 0.311    Count 10: 0.602  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/best_single_image_model.pth
  → New best model! Validation accuracy: 0.6648
新的最佳模型! 验证准确率: 0.6648
Epoch [86] Batch [0/51] Loss: 0.1125 LR: 0.000100
Epoch [86] Batch [10/51] Loss: 0.0050 LR: 0.000100
Epoch [86] Batch [20/51] Loss: 0.0001 LR: 0.000100
Epoch [86] Batch [30/51] Loss: 0.0276 LR: 0.000100
Epoch [86] Batch [40/51] Loss: 0.0082 LR: 0.000100
Epoch [86] Batch [50/51] Loss: 0.0387 LR: 0.000100

Epoch [87/350] - Time: 5.10s
Train Loss: 0.0724 | Val Loss: 6.1112
Train Acc: 0.9815 | Val Acc: 0.6331
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.905    Count 3: 0.917    Count 4: 0.829    Count 5: 0.618  
  Count 6: 0.531    Count 7: 0.271    Count 8: 0.091    Count 9: 0.167    Count 10: 0.614  
Epoch [87] Batch [0/51] Loss: 0.0001 LR: 0.000100
Epoch [87] Batch [10/51] Loss: 0.0022 LR: 0.000100
Epoch [87] Batch [20/51] Loss: 0.0010 LR: 0.000100
Epoch [87] Batch [30/51] Loss: 0.0006 LR: 0.000100
Epoch [87] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [87] Batch [50/51] Loss: 0.0001 LR: 0.000100

Epoch [88/350] - Time: 5.13s
Train Loss: 0.0119 | Val Loss: 6.3528
Train Acc: 0.9938 | Val Acc: 0.6294
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.975    Count 2: 0.889    Count 3: 0.852    Count 4: 0.810    Count 5: 0.559  
  Count 6: 0.561    Count 7: 0.417    Count 8: 0.061    Count 9: 0.189    Count 10: 0.614  
Epoch [88] Batch [0/51] Loss: 0.0565 LR: 0.000100
Epoch [88] Batch [10/51] Loss: 0.0022 LR: 0.000100
Epoch [88] Batch [20/51] Loss: 0.0005 LR: 0.000100
Epoch [88] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [88] Batch [40/51] Loss: 0.0001 LR: 0.000100
Epoch [88] Batch [50/51] Loss: 0.0001 LR: 0.000100

Epoch [89/350] - Time: 5.16s
Train Loss: 0.0068 | Val Loss: 6.2281
Train Acc: 0.9963 | Val Acc: 0.6248
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.929    Count 3: 0.889    Count 4: 0.752    Count 5: 0.627  
  Count 6: 0.490    Count 7: 0.302    Count 8: 0.081    Count 9: 0.289    Count 10: 0.477  
Epoch [89] Batch [0/51] Loss: 0.0023 LR: 0.000100
Epoch [89] Batch [10/51] Loss: 0.0144 LR: 0.000100
Epoch [89] Batch [20/51] Loss: 0.0001 LR: 0.000100
Epoch [89] Batch [30/51] Loss: 0.0001 LR: 0.000100
Epoch [89] Batch [40/51] Loss: 0.0001 LR: 0.000100
Epoch [89] Batch [50/51] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [90/350] - Time: 5.01s
Train Loss: 0.0101 | Val Loss: 6.6905
Train Acc: 0.9975 | Val Acc: 0.6276
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.913    Count 3: 0.889    Count 4: 0.810    Count 5: 0.578  
  Count 6: 0.480    Count 7: 0.333    Count 8: 0.162    Count 9: 0.144    Count 10: 0.557  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/single_image_checkpoint_epoch_89.pth
Epoch [90] Batch [0/51] Loss: 0.0001 LR: 0.000100
Epoch [90] Batch [10/51] Loss: 0.0349 LR: 0.000100
Epoch [90] Batch [20/51] Loss: 0.0003 LR: 0.000100
Epoch [90] Batch [30/51] Loss: 0.0081 LR: 0.000100
Epoch [90] Batch [40/51] Loss: 0.0003 LR: 0.000100
Epoch [90] Batch [50/51] Loss: 0.0003 LR: 0.000100

Epoch [91/350] - Time: 5.11s
Train Loss: 0.0249 | Val Loss: 6.1739
Train Acc: 0.9926 | Val Acc: 0.6322
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.944    Count 3: 0.898    Count 4: 0.810    Count 5: 0.696  
  Count 6: 0.510    Count 7: 0.323    Count 8: 0.051    Count 9: 0.144    Count 10: 0.534  
Epoch [91] Batch [0/51] Loss: 0.0002 LR: 0.000100
Epoch [91] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [91] Batch [20/51] Loss: 0.0003 LR: 0.000100
Epoch [91] Batch [30/51] Loss: 0.0003 LR: 0.000100
Epoch [91] Batch [40/51] Loss: 0.0001 LR: 0.000100
Epoch [91] Batch [50/51] Loss: 0.0027 LR: 0.000100

Epoch [92/350] - Time: 5.05s
Train Loss: 0.0019 | Val Loss: 6.5485
Train Acc: 1.0000 | Val Acc: 0.6322
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.937    Count 3: 0.870    Count 4: 0.810    Count 5: 0.539  
  Count 6: 0.561    Count 7: 0.333    Count 8: 0.172    Count 9: 0.133    Count 10: 0.568  
Epoch [92] Batch [0/51] Loss: 0.0703 LR: 0.000100
Epoch [92] Batch [10/51] Loss: 0.0001 LR: 0.000100
Epoch [92] Batch [20/51] Loss: 0.0024 LR: 0.000100
Epoch [92] Batch [30/51] Loss: 0.0001 LR: 0.000100
Epoch [92] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [92] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [93/350] - Time: 5.09s
Train Loss: 0.0331 | Val Loss: 6.4555
Train Acc: 0.9926 | Val Acc: 0.6276
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.952    Count 3: 0.889    Count 4: 0.762    Count 5: 0.598  
  Count 6: 0.582    Count 7: 0.219    Count 8: 0.111    Count 9: 0.222    Count 10: 0.534  
Epoch [93] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [93] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [93] Batch [20/51] Loss: 0.0004 LR: 0.000100
Epoch [93] Batch [30/51] Loss: 0.0002 LR: 0.000100
Epoch [93] Batch [40/51] Loss: 0.0049 LR: 0.000100
Epoch [93] Batch [50/51] Loss: 0.0149 LR: 0.000100

Epoch [94/350] - Time: 5.52s
Train Loss: 0.0310 | Val Loss: 6.5025
Train Acc: 0.9926 | Val Acc: 0.6238
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.988    Count 2: 0.937    Count 3: 0.889    Count 4: 0.810    Count 5: 0.559  
  Count 6: 0.490    Count 7: 0.281    Count 8: 0.162    Count 9: 0.244    Count 10: 0.466  
Epoch [94] Batch [0/51] Loss: 0.0452 LR: 0.000100
Epoch [94] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [94] Batch [20/51] Loss: 0.0001 LR: 0.000100
Epoch [94] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [94] Batch [40/51] Loss: 0.0070 LR: 0.000100
Epoch [94] Batch [50/51] Loss: 0.0001 LR: 0.000100

Epoch [95/350] - Time: 5.06s
Train Loss: 0.0214 | Val Loss: 5.9536
Train Acc: 0.9901 | Val Acc: 0.6350
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.937    Count 3: 0.889    Count 4: 0.771    Count 5: 0.784  
  Count 6: 0.541    Count 7: 0.198    Count 8: 0.081    Count 9: 0.322    Count 10: 0.409  
Epoch [95] Batch [0/51] Loss: 0.0239 LR: 0.000100
Epoch [95] Batch [10/51] Loss: 0.0327 LR: 0.000100
Epoch [95] Batch [20/51] Loss: 0.0006 LR: 0.000100
Epoch [95] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [95] Batch [40/51] Loss: 0.0001 LR: 0.000100
Epoch [95] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [96/350] - Time: 5.01s
Train Loss: 0.0563 | Val Loss: 6.2655
Train Acc: 0.9852 | Val Acc: 0.6350
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.913    Count 3: 0.880    Count 4: 0.790    Count 5: 0.627  
  Count 6: 0.510    Count 7: 0.292    Count 8: 0.172    Count 9: 0.222    Count 10: 0.557  
Epoch [96] Batch [0/51] Loss: 0.0001 LR: 0.000100
Epoch [96] Batch [10/51] Loss: 0.0638 LR: 0.000100
Epoch [96] Batch [20/51] Loss: 0.0013 LR: 0.000100
Epoch [96] Batch [30/51] Loss: 0.0012 LR: 0.000100
Epoch [96] Batch [40/51] Loss: 0.0006 LR: 0.000100
Epoch [96] Batch [50/51] Loss: 0.0005 LR: 0.000100

Epoch [97/350] - Time: 5.10s
Train Loss: 0.0376 | Val Loss: 6.4958
Train Acc: 0.9926 | Val Acc: 0.6304
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.988    Count 2: 0.952    Count 3: 0.880    Count 4: 0.838    Count 5: 0.637  
  Count 6: 0.510    Count 7: 0.302    Count 8: 0.172    Count 9: 0.122    Count 10: 0.477  
Epoch [97] Batch [0/51] Loss: 0.1243 LR: 0.000100
Epoch [97] Batch [10/51] Loss: 0.0002 LR: 0.000100
Epoch [97] Batch [20/51] Loss: 0.0001 LR: 0.000100
Epoch [97] Batch [30/51] Loss: 0.0362 LR: 0.000100
Epoch [97] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [97] Batch [50/51] Loss: 0.0099 LR: 0.000100

Epoch [98/350] - Time: 5.08s
Train Loss: 0.0217 | Val Loss: 7.0464
Train Acc: 0.9926 | Val Acc: 0.6080
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.988    Count 2: 0.889    Count 3: 0.870    Count 4: 0.590    Count 5: 0.529  
  Count 6: 0.582    Count 7: 0.281    Count 8: 0.051    Count 9: 0.311    Count 10: 0.614  
Epoch [98] Batch [0/51] Loss: 0.0003 LR: 0.000100
Epoch [98] Batch [10/51] Loss: 0.2521 LR: 0.000100
Epoch [98] Batch [20/51] Loss: 0.0002 LR: 0.000100
Epoch [98] Batch [30/51] Loss: 0.0044 LR: 0.000100
Epoch [98] Batch [40/51] Loss: 0.0151 LR: 0.000100
Epoch [98] Batch [50/51] Loss: 0.0001 LR: 0.000100

Epoch [99/350] - Time: 5.09s
Train Loss: 0.0379 | Val Loss: 5.8013
Train Acc: 0.9852 | Val Acc: 0.6555
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.976    Count 3: 0.889    Count 4: 0.829    Count 5: 0.716  
  Count 6: 0.520    Count 7: 0.333    Count 8: 0.172    Count 9: 0.156    Count 10: 0.568  
Epoch [99] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [99] Batch [10/51] Loss: 0.0001 LR: 0.000100
Epoch [99] Batch [20/51] Loss: 0.0001 LR: 0.000100
Epoch [99] Batch [30/51] Loss: 0.0023 LR: 0.000100
Epoch [99] Batch [40/51] Loss: 0.0023 LR: 0.000100
Epoch [99] Batch [50/51] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [100/350] - Time: 5.05s
Train Loss: 0.0598 | Val Loss: 5.8503
Train Acc: 0.9901 | Val Acc: 0.6443
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.992    Count 3: 0.889    Count 4: 0.781    Count 5: 0.588  
  Count 6: 0.582    Count 7: 0.312    Count 8: 0.081    Count 9: 0.211    Count 10: 0.614  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/single_image_checkpoint_epoch_99.pth
Epoch [100] Batch [0/51] Loss: 0.0040 LR: 0.000100
Epoch [100] Batch [10/51] Loss: 0.0002 LR: 0.000100
Epoch [100] Batch [20/51] Loss: 0.0023 LR: 0.000100
Epoch [100] Batch [30/51] Loss: 0.0004 LR: 0.000100
Epoch [100] Batch [40/51] Loss: 0.0019 LR: 0.000100
Epoch [100] Batch [50/51] Loss: 0.0001 LR: 0.000100

Epoch [101/350] - Time: 5.12s
Train Loss: 0.0531 | Val Loss: 6.2483
Train Acc: 0.9864 | Val Acc: 0.6220
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.992    Count 3: 0.889    Count 4: 0.800    Count 5: 0.647  
  Count 6: 0.408    Count 7: 0.240    Count 8: 0.121    Count 9: 0.156    Count 10: 0.534  
Epoch [101] Batch [0/51] Loss: 0.0002 LR: 0.000100
Epoch [101] Batch [10/51] Loss: 0.6281 LR: 0.000100
Epoch [101] Batch [20/51] Loss: 0.0003 LR: 0.000100
Epoch [101] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [101] Batch [40/51] Loss: 0.0003 LR: 0.000100
Epoch [101] Batch [50/51] Loss: 0.0009 LR: 0.000100

Epoch [102/350] - Time: 5.41s
Train Loss: 0.0352 | Val Loss: 6.0235
Train Acc: 0.9889 | Val Acc: 0.6415
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.960    Count 3: 0.907    Count 4: 0.857    Count 5: 0.667  
  Count 6: 0.500    Count 7: 0.292    Count 8: 0.152    Count 9: 0.044    Count 10: 0.625  
Epoch [102] Batch [0/51] Loss: 0.0001 LR: 0.000100
Epoch [102] Batch [10/51] Loss: 0.0001 LR: 0.000100
Epoch [102] Batch [20/51] Loss: 0.0049 LR: 0.000100
Epoch [102] Batch [30/51] Loss: 0.0003 LR: 0.000100
Epoch [102] Batch [40/51] Loss: 0.0002 LR: 0.000100
Epoch [102] Batch [50/51] Loss: 0.0121 LR: 0.000100

Epoch [103/350] - Time: 5.11s
Train Loss: 0.0086 | Val Loss: 6.5405
Train Acc: 0.9963 | Val Acc: 0.6210
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.988    Count 2: 0.865    Count 3: 0.907    Count 4: 0.790    Count 5: 0.647  
  Count 6: 0.571    Count 7: 0.219    Count 8: 0.101    Count 9: 0.156    Count 10: 0.568  
Epoch [103] Batch [0/51] Loss: 0.1551 LR: 0.000100
Epoch [103] Batch [10/51] Loss: 0.0002 LR: 0.000100
Epoch [103] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [103] Batch [30/51] Loss: 0.0004 LR: 0.000100
Epoch [103] Batch [40/51] Loss: 0.0001 LR: 0.000100
Epoch [103] Batch [50/51] Loss: 0.0176 LR: 0.000100

Epoch [104/350] - Time: 5.07s
Train Loss: 0.0084 | Val Loss: 7.2102
Train Acc: 0.9963 | Val Acc: 0.6034
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.889    Count 3: 0.898    Count 4: 0.752    Count 5: 0.559  
  Count 6: 0.480    Count 7: 0.271    Count 8: 0.091    Count 9: 0.133    Count 10: 0.534  
Epoch [104] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [104] Batch [10/51] Loss: 0.0001 LR: 0.000100
Epoch [104] Batch [20/51] Loss: 0.0008 LR: 0.000100
Epoch [104] Batch [30/51] Loss: 0.2231 LR: 0.000100
Epoch [104] Batch [40/51] Loss: 0.0002 LR: 0.000100
Epoch [104] Batch [50/51] Loss: 0.0002 LR: 0.000100

Epoch [105/350] - Time: 5.10s
Train Loss: 0.0273 | Val Loss: 6.2590
Train Acc: 0.9951 | Val Acc: 0.6229
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.905    Count 3: 0.889    Count 4: 0.790    Count 5: 0.667  
  Count 6: 0.520    Count 7: 0.281    Count 8: 0.091    Count 9: 0.178    Count 10: 0.489  
Epoch [105] Batch [0/51] Loss: 0.0248 LR: 0.000100
Epoch [105] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [105] Batch [20/51] Loss: 0.0001 LR: 0.000100
Epoch [105] Batch [30/51] Loss: 0.0006 LR: 0.000100
Epoch [105] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [105] Batch [50/51] Loss: 0.4633 LR: 0.000100

Epoch [106/350] - Time: 5.19s
Train Loss: 0.0143 | Val Loss: 6.5491
Train Acc: 0.9951 | Val Acc: 0.6331
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.921    Count 3: 0.870    Count 4: 0.829    Count 5: 0.608  
  Count 6: 0.643    Count 7: 0.333    Count 8: 0.061    Count 9: 0.111    Count 10: 0.557  
Epoch [106] Batch [0/51] Loss: 0.0065 LR: 0.000100
Epoch [106] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [106] Batch [20/51] Loss: 0.8534 LR: 0.000100
Epoch [106] Batch [30/51] Loss: 0.0001 LR: 0.000100
Epoch [106] Batch [40/51] Loss: 0.0874 LR: 0.000100
Epoch [106] Batch [50/51] Loss: 0.1185 LR: 0.000100

Epoch [107/350] - Time: 5.10s
Train Loss: 0.0375 | Val Loss: 7.2199
Train Acc: 0.9864 | Val Acc: 0.6192
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.921    Count 3: 0.898    Count 4: 0.781    Count 5: 0.725  
  Count 6: 0.520    Count 7: 0.198    Count 8: 0.182    Count 9: 0.089    Count 10: 0.432  
Epoch [107] Batch [0/51] Loss: 0.3942 LR: 0.000100
Epoch [107] Batch [10/51] Loss: 0.0011 LR: 0.000100
Epoch [107] Batch [20/51] Loss: 0.0004 LR: 0.000100
Epoch [107] Batch [30/51] Loss: 0.0029 LR: 0.000100
Epoch [107] Batch [40/51] Loss: 0.1418 LR: 0.000100
Epoch [107] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [108/350] - Time: 5.12s
Train Loss: 0.0501 | Val Loss: 6.8900
Train Acc: 0.9827 | Val Acc: 0.6173
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.988    Count 2: 0.913    Count 3: 0.843    Count 4: 0.819    Count 5: 0.667  
  Count 6: 0.449    Count 7: 0.333    Count 8: 0.172    Count 9: 0.044    Count 10: 0.523  
Epoch [108] Batch [0/51] Loss: 0.0005 LR: 0.000100
Epoch [108] Batch [10/51] Loss: 0.0004 LR: 0.000100
Epoch [108] Batch [20/51] Loss: 0.0010 LR: 0.000100
Epoch [108] Batch [30/51] Loss: 0.0001 LR: 0.000100
Epoch [108] Batch [40/51] Loss: 0.1271 LR: 0.000100
Epoch [108] Batch [50/51] Loss: 0.0022 LR: 0.000100

Epoch [109/350] - Time: 5.13s
Train Loss: 0.0336 | Val Loss: 6.7655
Train Acc: 0.9901 | Val Acc: 0.6220
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.913    Count 3: 0.731    Count 4: 0.733    Count 5: 0.667  
  Count 6: 0.633    Count 7: 0.260    Count 8: 0.152    Count 9: 0.244    Count 10: 0.500  
Epoch [109] Batch [0/51] Loss: 0.0018 LR: 0.000100
Epoch [109] Batch [10/51] Loss: 0.1124 LR: 0.000100
Epoch [109] Batch [20/51] Loss: 0.0022 LR: 0.000100
Epoch [109] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [109] Batch [40/51] Loss: 0.0008 LR: 0.000100
Epoch [109] Batch [50/51] Loss: 0.0001 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [110/350] - Time: 5.40s
Train Loss: 0.0838 | Val Loss: 6.9456
Train Acc: 0.9864 | Val Acc: 0.6387
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.968    Count 3: 0.889    Count 4: 0.810    Count 5: 0.725  
  Count 6: 0.500    Count 7: 0.448    Count 8: 0.111    Count 9: 0.033    Count 10: 0.466  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/single_image_checkpoint_epoch_109.pth
Epoch [110] Batch [0/51] Loss: 0.0435 LR: 0.000100
Epoch [110] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [110] Batch [20/51] Loss: 0.0011 LR: 0.000100
Epoch [110] Batch [30/51] Loss: 0.0001 LR: 0.000100
Epoch [110] Batch [40/51] Loss: 0.0028 LR: 0.000100
Epoch [110] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [111/350] - Time: 5.02s
Train Loss: 0.0163 | Val Loss: 6.8560
Train Acc: 0.9951 | Val Acc: 0.6117
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.952    Count 3: 0.917    Count 4: 0.733    Count 5: 0.716  
  Count 6: 0.500    Count 7: 0.167    Count 8: 0.061    Count 9: 0.233    Count 10: 0.398  
Epoch [111] Batch [0/51] Loss: 0.0002 LR: 0.000100
Epoch [111] Batch [10/51] Loss: 0.0001 LR: 0.000100
Epoch [111] Batch [20/51] Loss: 0.0001 LR: 0.000100
Epoch [111] Batch [30/51] Loss: 0.0061 LR: 0.000100
Epoch [111] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [111] Batch [50/51] Loss: 0.0002 LR: 0.000100

Epoch [112/350] - Time: 5.09s
Train Loss: 0.0238 | Val Loss: 6.3841
Train Acc: 0.9926 | Val Acc: 0.6601
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.976    Count 3: 0.917    Count 4: 0.838    Count 5: 0.755  
  Count 6: 0.592    Count 7: 0.208    Count 8: 0.121    Count 9: 0.300    Count 10: 0.500  
Epoch [112] Batch [0/51] Loss: 0.0010 LR: 0.000100
Epoch [112] Batch [10/51] Loss: 0.0004 LR: 0.000100
Epoch [112] Batch [20/51] Loss: 0.0001 LR: 0.000100
Epoch [112] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [112] Batch [40/51] Loss: 0.0001 LR: 0.000100
Epoch [112] Batch [50/51] Loss: 0.0155 LR: 0.000100

Epoch [113/350] - Time: 5.05s
Train Loss: 0.0444 | Val Loss: 6.7965
Train Acc: 0.9827 | Val Acc: 0.6322
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.913    Count 3: 0.852    Count 4: 0.743    Count 5: 0.618  
  Count 6: 0.551    Count 7: 0.448    Count 8: 0.121    Count 9: 0.156    Count 10: 0.534  
Epoch [113] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [113] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [113] Batch [20/51] Loss: 0.0106 LR: 0.000100
Epoch [113] Batch [30/51] Loss: 0.0100 LR: 0.000100
Epoch [113] Batch [40/51] Loss: 0.0001 LR: 0.000100
Epoch [113] Batch [50/51] Loss: 0.0006 LR: 0.000100

Epoch [114/350] - Time: 5.10s
Train Loss: 0.0087 | Val Loss: 6.3807
Train Acc: 0.9988 | Val Acc: 0.6536
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.968    Count 3: 0.907    Count 4: 0.829    Count 5: 0.657  
  Count 6: 0.531    Count 7: 0.292    Count 8: 0.131    Count 9: 0.200    Count 10: 0.636  
Epoch [114] Batch [0/51] Loss: 0.0043 LR: 0.000100
Epoch [114] Batch [10/51] Loss: 0.0033 LR: 0.000100
Epoch [114] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [114] Batch [30/51] Loss: 0.0009 LR: 0.000100
Epoch [114] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [114] Batch [50/51] Loss: 0.0835 LR: 0.000100

Epoch [115/350] - Time: 5.18s
Train Loss: 0.0504 | Val Loss: 6.9300
Train Acc: 0.9901 | Val Acc: 0.6406
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.929    Count 3: 0.907    Count 4: 0.819    Count 5: 0.647  
  Count 6: 0.469    Count 7: 0.406    Count 8: 0.131    Count 9: 0.144    Count 10: 0.545  
Epoch [115] Batch [0/51] Loss: 0.0005 LR: 0.000100
Epoch [115] Batch [10/51] Loss: 0.0023 LR: 0.000100
Epoch [115] Batch [20/51] Loss: 0.0001 LR: 0.000100
Epoch [115] Batch [30/51] Loss: 0.0003 LR: 0.000100
Epoch [115] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [115] Batch [50/51] Loss: 0.0155 LR: 0.000100

Epoch [116/350] - Time: 5.12s
Train Loss: 0.0178 | Val Loss: 6.7918
Train Acc: 0.9914 | Val Acc: 0.6508
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.960    Count 3: 0.898    Count 4: 0.857    Count 5: 0.618  
  Count 6: 0.561    Count 7: 0.344    Count 8: 0.061    Count 9: 0.211    Count 10: 0.602  
Epoch [116] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [116] Batch [10/51] Loss: 0.0003 LR: 0.000100
Epoch [116] Batch [20/51] Loss: 0.0003 LR: 0.000100
Epoch [116] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [116] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [116] Batch [50/51] Loss: 0.0826 LR: 0.000100

Epoch [117/350] - Time: 5.10s
Train Loss: 0.0340 | Val Loss: 6.9255
Train Acc: 0.9926 | Val Acc: 0.6341
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.944    Count 3: 0.907    Count 4: 0.800    Count 5: 0.627  
  Count 6: 0.551    Count 7: 0.281    Count 8: 0.091    Count 9: 0.200    Count 10: 0.534  
Epoch [117] Batch [0/51] Loss: 0.0534 LR: 0.000100
Epoch [117] Batch [10/51] Loss: 0.0004 LR: 0.000100
Epoch [117] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [117] Batch [30/51] Loss: 0.0001 LR: 0.000100
Epoch [117] Batch [40/51] Loss: 0.0001 LR: 0.000100
Epoch [117] Batch [50/51] Loss: 0.0005 LR: 0.000100

Epoch [118/350] - Time: 5.07s
Train Loss: 0.0374 | Val Loss: 7.8694
Train Acc: 0.9901 | Val Acc: 0.6201
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.913    Count 3: 0.852    Count 4: 0.781    Count 5: 0.667  
  Count 6: 0.612    Count 7: 0.250    Count 8: 0.111    Count 9: 0.067    Count 10: 0.523  
Epoch [118] Batch [0/51] Loss: 0.0001 LR: 0.000100
Epoch [118] Batch [10/51] Loss: 0.0557 LR: 0.000100
Epoch [118] Batch [20/51] Loss: 0.0008 LR: 0.000100
Epoch [118] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [118] Batch [40/51] Loss: 0.0001 LR: 0.000100
Epoch [118] Batch [50/51] Loss: 0.0280 LR: 0.000100

Epoch [119/350] - Time: 5.43s
Train Loss: 0.0137 | Val Loss: 7.5922
Train Acc: 0.9951 | Val Acc: 0.6238
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.944    Count 3: 0.907    Count 4: 0.790    Count 5: 0.676  
  Count 6: 0.449    Count 7: 0.292    Count 8: 0.152    Count 9: 0.044    Count 10: 0.557  
Epoch [119] Batch [0/51] Loss: 0.0029 LR: 0.000100
Epoch [119] Batch [10/51] Loss: 0.0006 LR: 0.000100
Epoch [119] Batch [20/51] Loss: 0.0339 LR: 0.000100
Epoch [119] Batch [30/51] Loss: 0.0020 LR: 0.000100
Epoch [119] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [119] Batch [50/51] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [120/350] - Time: 5.24s
Train Loss: 0.0720 | Val Loss: 6.6136
Train Acc: 0.9914 | Val Acc: 0.6453
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.944    Count 3: 0.898    Count 4: 0.752    Count 5: 0.824  
  Count 6: 0.500    Count 7: 0.281    Count 8: 0.172    Count 9: 0.089    Count 10: 0.580  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/single_image_checkpoint_epoch_119.pth
Epoch [120] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [120] Batch [10/51] Loss: 0.0844 LR: 0.000100
Epoch [120] Batch [20/51] Loss: 0.0001 LR: 0.000100
Epoch [120] Batch [30/51] Loss: 0.0001 LR: 0.000100
Epoch [120] Batch [40/51] Loss: 0.0003 LR: 0.000100
Epoch [120] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [121/350] - Time: 5.09s
Train Loss: 0.0829 | Val Loss: 7.7454
Train Acc: 0.9840 | Val Acc: 0.6350
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.937    Count 3: 0.917    Count 4: 0.819    Count 5: 0.755  
  Count 6: 0.439    Count 7: 0.344    Count 8: 0.172    Count 9: 0.011    Count 10: 0.534  
Epoch [121] Batch [0/51] Loss: 0.0009 LR: 0.000100
Epoch [121] Batch [10/51] Loss: 0.0004 LR: 0.000100
Epoch [121] Batch [20/51] Loss: 0.0001 LR: 0.000100
Epoch [121] Batch [30/51] Loss: 0.0042 LR: 0.000100
Epoch [121] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [121] Batch [50/51] Loss: 0.0001 LR: 0.000100

Epoch [122/350] - Time: 5.08s
Train Loss: 0.0199 | Val Loss: 6.8806
Train Acc: 0.9951 | Val Acc: 0.6471
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.960    Count 3: 0.870    Count 4: 0.810    Count 5: 0.755  
  Count 6: 0.582    Count 7: 0.312    Count 8: 0.061    Count 9: 0.200    Count 10: 0.523  
Epoch [122] Batch [0/51] Loss: 0.0187 LR: 0.000100
Epoch [122] Batch [10/51] Loss: 0.0583 LR: 0.000100
Epoch [122] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [122] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [122] Batch [40/51] Loss: 0.0006 LR: 0.000100
Epoch [122] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [123/350] - Time: 5.12s
Train Loss: 0.0240 | Val Loss: 7.4426
Train Acc: 0.9938 | Val Acc: 0.6276
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.988    Count 2: 0.944    Count 3: 0.898    Count 4: 0.781    Count 5: 0.588  
  Count 6: 0.551    Count 7: 0.250    Count 8: 0.091    Count 9: 0.244    Count 10: 0.534  
Epoch [123] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [123] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [123] Batch [20/51] Loss: 0.0003 LR: 0.000100
Epoch [123] Batch [30/51] Loss: 0.0016 LR: 0.000100
Epoch [123] Batch [40/51] Loss: 0.0002 LR: 0.000100
Epoch [123] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [124/350] - Time: 5.10s
Train Loss: 0.0239 | Val Loss: 7.3493
Train Acc: 0.9914 | Val Acc: 0.5708
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.944    Count 3: 0.889    Count 4: 0.524    Count 5: 0.667  
  Count 6: 0.378    Count 7: 0.167    Count 8: 0.081    Count 9: 0.200    Count 10: 0.398  
Epoch [124] Batch [0/51] Loss: 0.0035 LR: 0.000100
Epoch [124] Batch [10/51] Loss: 0.2325 LR: 0.000100
Epoch [124] Batch [20/51] Loss: 0.0011 LR: 0.000100
Epoch [124] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [124] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [124] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [125/350] - Time: 5.09s
Train Loss: 0.0465 | Val Loss: 6.9358
Train Acc: 0.9852 | Val Acc: 0.6369
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.952    Count 3: 0.917    Count 4: 0.819    Count 5: 0.745  
  Count 6: 0.480    Count 7: 0.302    Count 8: 0.162    Count 9: 0.144    Count 10: 0.420  
Epoch [125] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [125] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [125] Batch [20/51] Loss: 0.0022 LR: 0.000100
Epoch [125] Batch [30/51] Loss: 0.0001 LR: 0.000100
Epoch [125] Batch [40/51] Loss: 0.0008 LR: 0.000100
Epoch [125] Batch [50/51] Loss: 0.0002 LR: 0.000100

Epoch [126/350] - Time: 5.09s
Train Loss: 0.0135 | Val Loss: 7.1429
Train Acc: 0.9963 | Val Acc: 0.6406
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.976    Count 3: 0.907    Count 4: 0.857    Count 5: 0.716  
  Count 6: 0.408    Count 7: 0.302    Count 8: 0.162    Count 9: 0.056    Count 10: 0.602  
Epoch [126] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [126] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [126] Batch [20/51] Loss: 0.0009 LR: 0.000100
Epoch [126] Batch [30/51] Loss: 0.0001 LR: 0.000100
Epoch [126] Batch [40/51] Loss: 0.0001 LR: 0.000100
Epoch [126] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [127/350] - Time: 5.44s
Train Loss: 0.0413 | Val Loss: 7.5546
Train Acc: 0.9938 | Val Acc: 0.6276
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.988    Count 2: 0.937    Count 3: 0.917    Count 4: 0.829    Count 5: 0.706  
  Count 6: 0.490    Count 7: 0.354    Count 8: 0.091    Count 9: 0.111    Count 10: 0.420  
Epoch [127] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [127] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [127] Batch [20/51] Loss: 0.0001 LR: 0.000100
Epoch [127] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [127] Batch [40/51] Loss: 0.0003 LR: 0.000100
Epoch [127] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [128/350] - Time: 5.08s
Train Loss: 0.0185 | Val Loss: 7.0920
Train Acc: 0.9951 | Val Acc: 0.6266
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.988    Count 2: 0.913    Count 3: 0.917    Count 4: 0.781    Count 5: 0.627  
  Count 6: 0.490    Count 7: 0.271    Count 8: 0.121    Count 9: 0.167    Count 10: 0.591  
Epoch [128] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [128] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [128] Batch [20/51] Loss: 0.0062 LR: 0.000100
Epoch [128] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [128] Batch [40/51] Loss: 0.0373 LR: 0.000100
Epoch [128] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [129/350] - Time: 5.04s
Train Loss: 0.0117 | Val Loss: 6.9201
Train Acc: 0.9951 | Val Acc: 0.6397
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.944    Count 3: 0.907    Count 4: 0.848    Count 5: 0.686  
  Count 6: 0.531    Count 7: 0.281    Count 8: 0.091    Count 9: 0.133    Count 10: 0.557  
Epoch [129] Batch [0/51] Loss: 0.0375 LR: 0.000100
Epoch [129] Batch [10/51] Loss: 0.0166 LR: 0.000100
Epoch [129] Batch [20/51] Loss: 0.1419 LR: 0.000100
Epoch [129] Batch [30/51] Loss: 0.0014 LR: 0.000100
Epoch [129] Batch [40/51] Loss: 0.0001 LR: 0.000100
Epoch [129] Batch [50/51] Loss: 0.0008 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [130/350] - Time: 5.13s
Train Loss: 0.0887 | Val Loss: 7.3033
Train Acc: 0.9753 | Val Acc: 0.6294
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.937    Count 3: 0.880    Count 4: 0.819    Count 5: 0.735  
  Count 6: 0.531    Count 7: 0.240    Count 8: 0.040    Count 9: 0.211    Count 10: 0.489  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/single_image_checkpoint_epoch_129.pth
Epoch [130] Batch [0/51] Loss: 0.0014 LR: 0.000100
Epoch [130] Batch [10/51] Loss: 0.0004 LR: 0.000100
Epoch [130] Batch [20/51] Loss: 0.1050 LR: 0.000100
Epoch [130] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [130] Batch [40/51] Loss: 0.0001 LR: 0.000100
Epoch [130] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [131/350] - Time: 5.11s
Train Loss: 0.0213 | Val Loss: 7.6497
Train Acc: 0.9938 | Val Acc: 0.6127
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.921    Count 3: 0.778    Count 4: 0.743    Count 5: 0.569  
  Count 6: 0.490    Count 7: 0.344    Count 8: 0.172    Count 9: 0.167    Count 10: 0.545  
Epoch [131] Batch [0/51] Loss: 0.0035 LR: 0.000100
Epoch [131] Batch [10/51] Loss: 0.0002 LR: 0.000100
Epoch [131] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [131] Batch [30/51] Loss: 0.0193 LR: 0.000100
Epoch [131] Batch [40/51] Loss: 0.0030 LR: 0.000100
Epoch [131] Batch [50/51] Loss: 0.0641 LR: 0.000100

Epoch [132/350] - Time: 5.19s
Train Loss: 0.0539 | Val Loss: 7.3951
Train Acc: 0.9901 | Val Acc: 0.6462
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.976    Count 3: 0.880    Count 4: 0.848    Count 5: 0.686  
  Count 6: 0.459    Count 7: 0.365    Count 8: 0.131    Count 9: 0.100    Count 10: 0.614  
Epoch [132] Batch [0/51] Loss: 0.0538 LR: 0.000100
Epoch [132] Batch [10/51] Loss: 0.0003 LR: 0.000100
Epoch [132] Batch [20/51] Loss: 0.0020 LR: 0.000100
Epoch [132] Batch [30/51] Loss: 0.3250 LR: 0.000100
Epoch [132] Batch [40/51] Loss: 0.1640 LR: 0.000100
Epoch [132] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [133/350] - Time: 5.09s
Train Loss: 0.0668 | Val Loss: 7.1678
Train Acc: 0.9815 | Val Acc: 0.6192
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.913    Count 3: 0.870    Count 4: 0.705    Count 5: 0.627  
  Count 6: 0.541    Count 7: 0.281    Count 8: 0.172    Count 9: 0.178    Count 10: 0.500  
Epoch [133] Batch [0/51] Loss: 0.0004 LR: 0.000100
Epoch [133] Batch [10/51] Loss: 0.0003 LR: 0.000100
Epoch [133] Batch [20/51] Loss: 0.0001 LR: 0.000100
Epoch [133] Batch [30/51] Loss: 0.0001 LR: 0.000100
Epoch [133] Batch [40/51] Loss: 0.0002 LR: 0.000100
Epoch [133] Batch [50/51] Loss: 0.0044 LR: 0.000100

Epoch [134/350] - Time: 5.14s
Train Loss: 0.0278 | Val Loss: 6.6232
Train Acc: 0.9901 | Val Acc: 0.6238
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.937    Count 3: 0.907    Count 4: 0.829    Count 5: 0.716  
  Count 6: 0.449    Count 7: 0.281    Count 8: 0.131    Count 9: 0.100    Count 10: 0.443  
Epoch [134] Batch [0/51] Loss: 0.0001 LR: 0.000100
Epoch [134] Batch [10/51] Loss: 0.0001 LR: 0.000100
Epoch [134] Batch [20/51] Loss: 0.0002 LR: 0.000100
Epoch [134] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [134] Batch [40/51] Loss: 0.0050 LR: 0.000100
Epoch [134] Batch [50/51] Loss: 0.0421 LR: 0.000100

Epoch [135/350] - Time: 5.08s
Train Loss: 0.0266 | Val Loss: 6.9306
Train Acc: 0.9938 | Val Acc: 0.6294
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.984    Count 3: 0.907    Count 4: 0.819    Count 5: 0.775  
  Count 6: 0.367    Count 7: 0.281    Count 8: 0.131    Count 9: 0.144    Count 10: 0.443  
Epoch [135] Batch [0/51] Loss: 0.0545 LR: 0.000100
Epoch [135] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [135] Batch [20/51] Loss: 0.0060 LR: 0.000100
Epoch [135] Batch [30/51] Loss: 0.1522 LR: 0.000100
Epoch [135] Batch [40/51] Loss: 0.0011 LR: 0.000100
Epoch [135] Batch [50/51] Loss: 0.0003 LR: 0.000100

Epoch [136/350] - Time: 5.33s
Train Loss: 0.2083 | Val Loss: 7.1006
Train Acc: 0.9667 | Val Acc: 0.6350
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.929    Count 3: 0.898    Count 4: 0.857    Count 5: 0.657  
  Count 6: 0.490    Count 7: 0.260    Count 8: 0.172    Count 9: 0.178    Count 10: 0.500  
Epoch [136] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [136] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [136] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [136] Batch [30/51] Loss: 0.0001 LR: 0.000100
Epoch [136] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [136] Batch [50/51] Loss: 0.0028 LR: 0.000100

Epoch [137/350] - Time: 5.11s
Train Loss: 0.0081 | Val Loss: 7.6252
Train Acc: 0.9951 | Val Acc: 0.6229
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.889    Count 3: 0.889    Count 4: 0.752    Count 5: 0.588  
  Count 6: 0.612    Count 7: 0.188    Count 8: 0.051    Count 9: 0.289    Count 10: 0.580  
Epoch [137] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [137] Batch [10/51] Loss: 0.0010 LR: 0.000100
Epoch [137] Batch [20/51] Loss: 0.8580 LR: 0.000100
Epoch [137] Batch [30/51] Loss: 0.0031 LR: 0.000100
Epoch [137] Batch [40/51] Loss: 0.0002 LR: 0.000100
Epoch [137] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [138/350] - Time: 5.26s
Train Loss: 0.0439 | Val Loss: 6.9719
Train Acc: 0.9926 | Val Acc: 0.6117
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.944    Count 3: 0.815    Count 4: 0.667    Count 5: 0.549  
  Count 6: 0.633    Count 7: 0.198    Count 8: 0.101    Count 9: 0.244    Count 10: 0.568  
Epoch [138] Batch [0/51] Loss: 0.0031 LR: 0.000100
Epoch [138] Batch [10/51] Loss: 0.0001 LR: 0.000100
Epoch [138] Batch [20/51] Loss: 0.0069 LR: 0.000100
Epoch [138] Batch [30/51] Loss: 0.0024 LR: 0.000100
Epoch [138] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [138] Batch [50/51] Loss: 0.0273 LR: 0.000100

Epoch [139/350] - Time: 5.14s
Train Loss: 0.0260 | Val Loss: 8.9747
Train Acc: 0.9938 | Val Acc: 0.6192
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.988    Count 2: 0.929    Count 3: 0.852    Count 4: 0.810    Count 5: 0.529  
  Count 6: 0.541    Count 7: 0.448    Count 8: 0.111    Count 9: 0.044    Count 10: 0.523  
Epoch [139] Batch [0/51] Loss: 0.0216 LR: 0.000100
Epoch [139] Batch [10/51] Loss: 0.0005 LR: 0.000100
Epoch [139] Batch [20/51] Loss: 0.4440 LR: 0.000100
Epoch [139] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [139] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [139] Batch [50/51] Loss: 0.0052 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [140/350] - Time: 5.08s
Train Loss: 0.0551 | Val Loss: 8.3807
Train Acc: 0.9840 | Val Acc: 0.6276
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.937    Count 3: 0.917    Count 4: 0.876    Count 5: 0.618  
  Count 6: 0.449    Count 7: 0.281    Count 8: 0.182    Count 9: 0.022    Count 10: 0.568  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/single_image_checkpoint_epoch_139.pth
Epoch [140] Batch [0/51] Loss: 0.2207 LR: 0.000100
Epoch [140] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [140] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [140] Batch [30/51] Loss: 0.2695 LR: 0.000100
Epoch [140] Batch [40/51] Loss: 0.0002 LR: 0.000100
Epoch [140] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [141/350] - Time: 4.97s
Train Loss: 0.0230 | Val Loss: 7.0578
Train Acc: 0.9926 | Val Acc: 0.6387
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.952    Count 3: 0.898    Count 4: 0.848    Count 5: 0.627  
  Count 6: 0.510    Count 7: 0.292    Count 8: 0.131    Count 9: 0.156    Count 10: 0.568  
Epoch [141] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [141] Batch [10/51] Loss: 0.0001 LR: 0.000100
Epoch [141] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [141] Batch [30/51] Loss: 0.0003 LR: 0.000100
Epoch [141] Batch [40/51] Loss: 0.0001 LR: 0.000100
Epoch [141] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [142/350] - Time: 5.13s
Train Loss: 0.0265 | Val Loss: 6.9915
Train Acc: 0.9914 | Val Acc: 0.6397
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.929    Count 3: 0.898    Count 4: 0.800    Count 5: 0.686  
  Count 6: 0.592    Count 7: 0.240    Count 8: 0.121    Count 9: 0.256    Count 10: 0.477  
Epoch [142] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [142] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [142] Batch [20/51] Loss: 0.0001 LR: 0.000100
Epoch [142] Batch [30/51] Loss: 0.0001 LR: 0.000100
Epoch [142] Batch [40/51] Loss: 0.0006 LR: 0.000100
Epoch [142] Batch [50/51] Loss: 0.0001 LR: 0.000100

Epoch [143/350] - Time: 5.11s
Train Loss: 0.0167 | Val Loss: 7.2584
Train Acc: 0.9926 | Val Acc: 0.6499
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.968    Count 3: 0.898    Count 4: 0.819    Count 5: 0.765  
  Count 6: 0.500    Count 7: 0.198    Count 8: 0.040    Count 9: 0.311    Count 10: 0.602  
Epoch [143] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [143] Batch [10/51] Loss: 0.0009 LR: 0.000100
Epoch [143] Batch [20/51] Loss: 0.0054 LR: 0.000100
Epoch [143] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [143] Batch [40/51] Loss: 0.0005 LR: 0.000100
Epoch [143] Batch [50/51] Loss: 0.0001 LR: 0.000100

Epoch [144/350] - Time: 5.55s
Train Loss: 0.0377 | Val Loss: 7.3224
Train Acc: 0.9926 | Val Acc: 0.6480
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.929    Count 3: 0.889    Count 4: 0.819    Count 5: 0.588  
  Count 6: 0.612    Count 7: 0.427    Count 8: 0.111    Count 9: 0.122    Count 10: 0.602  
Epoch [144] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [144] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [144] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [144] Batch [30/51] Loss: 0.0022 LR: 0.000100
Epoch [144] Batch [40/51] Loss: 0.0001 LR: 0.000100
Epoch [144] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [145/350] - Time: 5.11s
Train Loss: 0.0137 | Val Loss: 7.5160
Train Acc: 0.9963 | Val Acc: 0.6359
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.960    Count 3: 0.898    Count 4: 0.867    Count 5: 0.804  
  Count 6: 0.480    Count 7: 0.177    Count 8: 0.000    Count 9: 0.289    Count 10: 0.466  
Epoch [145] Batch [0/51] Loss: 0.0002 LR: 0.000100
Epoch [145] Batch [10/51] Loss: 0.0002 LR: 0.000100
Epoch [145] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [145] Batch [30/51] Loss: 0.0040 LR: 0.000100
Epoch [145] Batch [40/51] Loss: 0.0019 LR: 0.000100
Epoch [145] Batch [50/51] Loss: 0.0001 LR: 0.000100

Epoch [146/350] - Time: 5.13s
Train Loss: 0.0502 | Val Loss: 6.9359
Train Acc: 0.9877 | Val Acc: 0.6341
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.921    Count 3: 0.861    Count 4: 0.762    Count 5: 0.667  
  Count 6: 0.592    Count 7: 0.219    Count 8: 0.051    Count 9: 0.322    Count 10: 0.557  
Epoch [146] Batch [0/51] Loss: 0.0640 LR: 0.000100
Epoch [146] Batch [10/51] Loss: 0.0001 LR: 0.000100
Epoch [146] Batch [20/51] Loss: 0.0055 LR: 0.000100
Epoch [146] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [146] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [146] Batch [50/51] Loss: 0.0002 LR: 0.000100

Epoch [147/350] - Time: 5.09s
Train Loss: 0.0612 | Val Loss: 7.2702
Train Acc: 0.9864 | Val Acc: 0.6257
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.968    Count 3: 0.889    Count 4: 0.848    Count 5: 0.686  
  Count 6: 0.316    Count 7: 0.344    Count 8: 0.162    Count 9: 0.156    Count 10: 0.455  
Epoch [147] Batch [0/51] Loss: 0.0004 LR: 0.000100
Epoch [147] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [147] Batch [20/51] Loss: 0.0001 LR: 0.000100
Epoch [147] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [147] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [147] Batch [50/51] Loss: 0.0001 LR: 0.000100

Epoch [148/350] - Time: 5.13s
Train Loss: 0.0253 | Val Loss: 7.0786
Train Acc: 0.9926 | Val Acc: 0.6415
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.921    Count 3: 0.898    Count 4: 0.800    Count 5: 0.657  
  Count 6: 0.449    Count 7: 0.323    Count 8: 0.152    Count 9: 0.189    Count 10: 0.648  
Epoch [148] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [148] Batch [10/51] Loss: 0.0003 LR: 0.000100
Epoch [148] Batch [20/51] Loss: 0.0002 LR: 0.000100
Epoch [148] Batch [30/51] Loss: 0.0014 LR: 0.000100
Epoch [148] Batch [40/51] Loss: 0.0751 LR: 0.000100
Epoch [148] Batch [50/51] Loss: 0.0009 LR: 0.000100

Epoch [149/350] - Time: 5.26s
Train Loss: 0.0347 | Val Loss: 7.2576
Train Acc: 0.9889 | Val Acc: 0.6359
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.944    Count 3: 0.898    Count 4: 0.857    Count 5: 0.775  
  Count 6: 0.449    Count 7: 0.229    Count 8: 0.051    Count 9: 0.333    Count 10: 0.409  
Epoch [149] Batch [0/51] Loss: 0.0001 LR: 0.000100
Epoch [149] Batch [10/51] Loss: 0.0005 LR: 0.000100
Epoch [149] Batch [20/51] Loss: 0.0001 LR: 0.000100
Epoch [149] Batch [30/51] Loss: 0.0034 LR: 0.000100
Epoch [149] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [149] Batch [50/51] Loss: 0.0010 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [150/350] - Time: 5.35s
Train Loss: 0.0108 | Val Loss: 7.3679
Train Acc: 0.9963 | Val Acc: 0.6229
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.975    Count 2: 0.929    Count 3: 0.917    Count 4: 0.790    Count 5: 0.637  
  Count 6: 0.480    Count 7: 0.312    Count 8: 0.061    Count 9: 0.189    Count 10: 0.534  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/single_image_checkpoint_epoch_149.pth
Epoch [150] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [150] Batch [10/51] Loss: 0.0001 LR: 0.000100
Epoch [150] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [150] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [150] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [150] Batch [50/51] Loss: 0.0424 LR: 0.000100

Epoch [151/350] - Time: 5.06s
Train Loss: 0.0086 | Val Loss: 7.8632
Train Acc: 0.9975 | Val Acc: 0.6285
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.929    Count 3: 0.833    Count 4: 0.733    Count 5: 0.559  
  Count 6: 0.561    Count 7: 0.344    Count 8: 0.162    Count 9: 0.211    Count 10: 0.568  
Epoch [151] Batch [0/51] Loss: 0.0001 LR: 0.000100
Epoch [151] Batch [10/51] Loss: 0.0001 LR: 0.000100
Epoch [151] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [151] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [151] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [151] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [152/350] - Time: 5.47s
Train Loss: 0.0230 | Val Loss: 8.0396
Train Acc: 0.9926 | Val Acc: 0.6285
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.981    Count 2: 0.905    Count 3: 0.898    Count 4: 0.876    Count 5: 0.647  
  Count 6: 0.429    Count 7: 0.312    Count 8: 0.091    Count 9: 0.178    Count 10: 0.568  
Epoch [152] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [152] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [152] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [152] Batch [30/51] Loss: 0.2922 LR: 0.000100
Epoch [152] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [152] Batch [50/51] Loss: 0.2783 LR: 0.000100

Epoch [153/350] - Time: 5.17s
Train Loss: 0.0334 | Val Loss: 8.2170
Train Acc: 0.9914 | Val Acc: 0.6229
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.913    Count 3: 0.880    Count 4: 0.838    Count 5: 0.549  
  Count 6: 0.561    Count 7: 0.292    Count 8: 0.141    Count 9: 0.167    Count 10: 0.477  
Epoch [153] Batch [0/51] Loss: 0.0001 LR: 0.000100
Epoch [153] Batch [10/51] Loss: 0.0003 LR: 0.000100
Epoch [153] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [153] Batch [30/51] Loss: 0.0001 LR: 0.000100
Epoch [153] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [153] Batch [50/51] Loss: 0.0007 LR: 0.000100

Epoch [154/350] - Time: 5.27s
Train Loss: 0.0197 | Val Loss: 7.1779
Train Acc: 0.9951 | Val Acc: 0.6471
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.976    Count 3: 0.889    Count 4: 0.829    Count 5: 0.696  
  Count 6: 0.500    Count 7: 0.250    Count 8: 0.141    Count 9: 0.244    Count 10: 0.534  
Epoch [154] Batch [0/51] Loss: 0.0038 LR: 0.000100
Epoch [154] Batch [10/51] Loss: 0.0005 LR: 0.000100
Epoch [154] Batch [20/51] Loss: 0.0001 LR: 0.000100
Epoch [154] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [154] Batch [40/51] Loss: 0.0012 LR: 0.000100
Epoch [154] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [155/350] - Time: 5.21s
Train Loss: 0.0126 | Val Loss: 7.6830
Train Acc: 0.9975 | Val Acc: 0.6508
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.988    Count 2: 0.976    Count 3: 0.907    Count 4: 0.810    Count 5: 0.794  
  Count 6: 0.541    Count 7: 0.312    Count 8: 0.152    Count 9: 0.167    Count 10: 0.443  
Epoch [155] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [155] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [155] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [155] Batch [30/51] Loss: 0.0003 LR: 0.000100
Epoch [155] Batch [40/51] Loss: 0.0001 LR: 0.000100
Epoch [155] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [156/350] - Time: 5.09s
Train Loss: 0.0086 | Val Loss: 7.1436
Train Acc: 0.9938 | Val Acc: 0.6331
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.921    Count 3: 0.917    Count 4: 0.800    Count 5: 0.667  
  Count 6: 0.449    Count 7: 0.281    Count 8: 0.111    Count 9: 0.189    Count 10: 0.602  
Epoch [156] Batch [0/51] Loss: 0.0001 LR: 0.000100
Epoch [156] Batch [10/51] Loss: 0.0013 LR: 0.000100
Epoch [156] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [156] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [156] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [156] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [157/350] - Time: 5.15s
Train Loss: 0.0211 | Val Loss: 7.6725
Train Acc: 0.9951 | Val Acc: 0.6397
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.975    Count 2: 0.905    Count 3: 0.917    Count 4: 0.800    Count 5: 0.716  
  Count 6: 0.602    Count 7: 0.312    Count 8: 0.071    Count 9: 0.178    Count 10: 0.534  
Epoch [157] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [157] Batch [10/51] Loss: 0.0039 LR: 0.000100
Epoch [157] Batch [20/51] Loss: 0.0001 LR: 0.000100
Epoch [157] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [157] Batch [40/51] Loss: 0.0004 LR: 0.000100
Epoch [157] Batch [50/51] Loss: 0.0078 LR: 0.000100

Epoch [158/350] - Time: 5.20s
Train Loss: 0.0145 | Val Loss: 8.4648
Train Acc: 0.9951 | Val Acc: 0.6182
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.881    Count 3: 0.917    Count 4: 0.819    Count 5: 0.588  
  Count 6: 0.408    Count 7: 0.260    Count 8: 0.081    Count 9: 0.122    Count 10: 0.705  
Epoch [158] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [158] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [158] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [158] Batch [30/51] Loss: 0.0001 LR: 0.000100
Epoch [158] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [158] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [159/350] - Time: 4.99s
Train Loss: 0.0499 | Val Loss: 8.6530
Train Acc: 0.9901 | Val Acc: 0.6136
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.975    Count 2: 0.873    Count 3: 0.852    Count 4: 0.733    Count 5: 0.549  
  Count 6: 0.520    Count 7: 0.260    Count 8: 0.152    Count 9: 0.178    Count 10: 0.670  
Epoch [159] Batch [0/51] Loss: 0.3084 LR: 0.000100
Epoch [159] Batch [10/51] Loss: 0.6922 LR: 0.000100
Epoch [159] Batch [20/51] Loss: 0.0001 LR: 0.000100
Epoch [159] Batch [30/51] Loss: 0.0965 LR: 0.000100
Epoch [159] Batch [40/51] Loss: 0.0909 LR: 0.000100
Epoch [159] Batch [50/51] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [160/350] - Time: 5.08s
Train Loss: 0.0743 | Val Loss: 7.4616
Train Acc: 0.9753 | Val Acc: 0.6257
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.937    Count 3: 0.898    Count 4: 0.790    Count 5: 0.775  
  Count 6: 0.622    Count 7: 0.156    Count 8: 0.010    Count 9: 0.211    Count 10: 0.420  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/single_image_checkpoint_epoch_159.pth
Epoch [160] Batch [0/51] Loss: 0.0006 LR: 0.000100
Epoch [160] Batch [10/51] Loss: 0.2712 LR: 0.000100
Epoch [160] Batch [20/51] Loss: 0.0006 LR: 0.000100
Epoch [160] Batch [30/51] Loss: 0.3717 LR: 0.000100
Epoch [160] Batch [40/51] Loss: 0.0086 LR: 0.000100
Epoch [160] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [161/350] - Time: 5.54s
Train Loss: 0.0659 | Val Loss: 8.5955
Train Acc: 0.9864 | Val Acc: 0.6034
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.921    Count 3: 0.898    Count 4: 0.819    Count 5: 0.676  
  Count 6: 0.459    Count 7: 0.188    Count 8: 0.051    Count 9: 0.200    Count 10: 0.364  
Epoch [161] Batch [0/51] Loss: 0.1857 LR: 0.000100
Epoch [161] Batch [10/51] Loss: 0.0017 LR: 0.000100
Epoch [161] Batch [20/51] Loss: 0.0002 LR: 0.000100
Epoch [161] Batch [30/51] Loss: 0.0066 LR: 0.000100
Epoch [161] Batch [40/51] Loss: 0.1413 LR: 0.000100
Epoch [161] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [162/350] - Time: 5.14s
Train Loss: 0.1130 | Val Loss: 6.9520
Train Acc: 0.9802 | Val Acc: 0.6536
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.921    Count 3: 0.898    Count 4: 0.829    Count 5: 0.686  
  Count 6: 0.510    Count 7: 0.500    Count 8: 0.040    Count 9: 0.156    Count 10: 0.625  
Epoch [162] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [162] Batch [10/51] Loss: 0.0001 LR: 0.000100
Epoch [162] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [162] Batch [30/51] Loss: 0.0003 LR: 0.000100
Epoch [162] Batch [40/51] Loss: 0.0001 LR: 0.000100
Epoch [162] Batch [50/51] Loss: 0.0002 LR: 0.000100

Epoch [163/350] - Time: 5.09s
Train Loss: 0.0379 | Val Loss: 8.3900
Train Acc: 0.9889 | Val Acc: 0.6192
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.881    Count 3: 0.861    Count 4: 0.771    Count 5: 0.618  
  Count 6: 0.459    Count 7: 0.229    Count 8: 0.172    Count 9: 0.200    Count 10: 0.614  
Epoch [163] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [163] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [163] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [163] Batch [30/51] Loss: 0.2078 LR: 0.000100
Epoch [163] Batch [40/51] Loss: 0.0032 LR: 0.000100
Epoch [163] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [164/350] - Time: 5.15s
Train Loss: 0.0492 | Val Loss: 7.9275
Train Acc: 0.9914 | Val Acc: 0.6238
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.929    Count 3: 0.907    Count 4: 0.857    Count 5: 0.627  
  Count 6: 0.398    Count 7: 0.271    Count 8: 0.030    Count 9: 0.200    Count 10: 0.614  
Epoch [164] Batch [0/51] Loss: 0.0116 LR: 0.000100
Epoch [164] Batch [10/51] Loss: 0.0094 LR: 0.000100
Epoch [164] Batch [20/51] Loss: 0.0053 LR: 0.000100
Epoch [164] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [164] Batch [40/51] Loss: 0.1549 LR: 0.000100
Epoch [164] Batch [50/51] Loss: 0.0080 LR: 0.000100

Epoch [165/350] - Time: 5.10s
Train Loss: 0.0195 | Val Loss: 7.1543
Train Acc: 0.9951 | Val Acc: 0.6359
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.913    Count 3: 0.917    Count 4: 0.857    Count 5: 0.745  
  Count 6: 0.388    Count 7: 0.250    Count 8: 0.131    Count 9: 0.189    Count 10: 0.568  
Epoch [165] Batch [0/51] Loss: 0.0002 LR: 0.000100
Epoch [165] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [165] Batch [20/51] Loss: 0.0001 LR: 0.000100
Epoch [165] Batch [30/51] Loss: 0.0005 LR: 0.000100
Epoch [165] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [165] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [166/350] - Time: 5.16s
Train Loss: 0.0263 | Val Loss: 7.2228
Train Acc: 0.9938 | Val Acc: 0.6257
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.929    Count 3: 0.907    Count 4: 0.800    Count 5: 0.559  
  Count 6: 0.418    Count 7: 0.333    Count 8: 0.152    Count 9: 0.178    Count 10: 0.580  
Epoch [166] Batch [0/51] Loss: 0.0178 LR: 0.000100
Epoch [166] Batch [10/51] Loss: 0.0040 LR: 0.000100
Epoch [166] Batch [20/51] Loss: 0.0005 LR: 0.000100
Epoch [166] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [166] Batch [40/51] Loss: 0.0054 LR: 0.000100
Epoch [166] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [167/350] - Time: 5.14s
Train Loss: 0.0016 | Val Loss: 7.0668
Train Acc: 1.0000 | Val Acc: 0.6350
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.929    Count 3: 0.907    Count 4: 0.838    Count 5: 0.667  
  Count 6: 0.500    Count 7: 0.260    Count 8: 0.061    Count 9: 0.256    Count 10: 0.534  
Epoch [167] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [167] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [167] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [167] Batch [30/51] Loss: 0.0001 LR: 0.000100
Epoch [167] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [167] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [168/350] - Time: 5.09s
Train Loss: 0.0196 | Val Loss: 7.0565
Train Acc: 0.9963 | Val Acc: 0.6276
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.921    Count 3: 0.917    Count 4: 0.790    Count 5: 0.657  
  Count 6: 0.520    Count 7: 0.250    Count 8: 0.121    Count 9: 0.278    Count 10: 0.409  
Epoch [168] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [168] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [168] Batch [20/51] Loss: 0.0011 LR: 0.000100
Epoch [168] Batch [30/51] Loss: 0.3851 LR: 0.000100
Epoch [168] Batch [40/51] Loss: 0.0135 LR: 0.000100
Epoch [168] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [169/350] - Time: 5.31s
Train Loss: 0.0557 | Val Loss: 6.5212
Train Acc: 0.9877 | Val Acc: 0.6471
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.929    Count 3: 0.907    Count 4: 0.867    Count 5: 0.804  
  Count 6: 0.510    Count 7: 0.260    Count 8: 0.091    Count 9: 0.167    Count 10: 0.534  
Epoch [169] Batch [0/51] Loss: 0.0004 LR: 0.000100
Epoch [169] Batch [10/51] Loss: 0.2262 LR: 0.000100
Epoch [169] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [169] Batch [30/51] Loss: 0.0001 LR: 0.000100
Epoch [169] Batch [40/51] Loss: 0.0922 LR: 0.000100
Epoch [169] Batch [50/51] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [170/350] - Time: 5.20s
Train Loss: 0.0536 | Val Loss: 6.5062
Train Acc: 0.9852 | Val Acc: 0.6518
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.944    Count 3: 0.907    Count 4: 0.838    Count 5: 0.716  
  Count 6: 0.469    Count 7: 0.479    Count 8: 0.071    Count 9: 0.089    Count 10: 0.614  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/single_image_checkpoint_epoch_169.pth
Epoch [170] Batch [0/51] Loss: 0.0001 LR: 0.000100
Epoch [170] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [170] Batch [20/51] Loss: 0.0033 LR: 0.000100
Epoch [170] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [170] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [170] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [171/350] - Time: 5.07s
Train Loss: 0.0197 | Val Loss: 7.4961
Train Acc: 0.9975 | Val Acc: 0.6313
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.937    Count 3: 0.907    Count 4: 0.819    Count 5: 0.755  
  Count 6: 0.429    Count 7: 0.240    Count 8: 0.030    Count 9: 0.311    Count 10: 0.466  
Epoch [171] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [171] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [171] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [171] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [171] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [171] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [172/350] - Time: 5.20s
Train Loss: 0.0077 | Val Loss: 7.1964
Train Acc: 0.9963 | Val Acc: 0.6425
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.937    Count 3: 0.889    Count 4: 0.829    Count 5: 0.588  
  Count 6: 0.622    Count 7: 0.229    Count 8: 0.040    Count 9: 0.300    Count 10: 0.614  
Epoch [172] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [172] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [172] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [172] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [172] Batch [40/51] Loss: 0.0009 LR: 0.000100
Epoch [172] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [173/350] - Time: 5.05s
Train Loss: 0.0148 | Val Loss: 7.3996
Train Acc: 0.9975 | Val Acc: 0.6359
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.952    Count 3: 0.889    Count 4: 0.800    Count 5: 0.676  
  Count 6: 0.510    Count 7: 0.312    Count 8: 0.162    Count 9: 0.133    Count 10: 0.511  
Epoch [173] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [173] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [173] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [173] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [173] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [173] Batch [50/51] Loss: 0.0085 LR: 0.000100

Epoch [174/350] - Time: 5.13s
Train Loss: 0.0014 | Val Loss: 7.3490
Train Acc: 1.0000 | Val Acc: 0.6322
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.921    Count 3: 0.898    Count 4: 0.790    Count 5: 0.686  
  Count 6: 0.582    Count 7: 0.198    Count 8: 0.051    Count 9: 0.244    Count 10: 0.557  
Epoch [174] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [174] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [174] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [174] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [174] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [174] Batch [50/51] Loss: 0.0010 LR: 0.000100

Epoch [175/350] - Time: 5.03s
Train Loss: 0.0291 | Val Loss: 7.9622
Train Acc: 0.9926 | Val Acc: 0.6220
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.952    Count 3: 0.889    Count 4: 0.819    Count 5: 0.647  
  Count 6: 0.439    Count 7: 0.240    Count 8: 0.182    Count 9: 0.067    Count 10: 0.557  
Epoch [175] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [175] Batch [10/51] Loss: 0.0512 LR: 0.000100
Epoch [175] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [175] Batch [30/51] Loss: 0.0002 LR: 0.000100
Epoch [175] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [175] Batch [50/51] Loss: 0.1620 LR: 0.000100

Epoch [176/350] - Time: 5.11s
Train Loss: 0.0324 | Val Loss: 7.8063
Train Acc: 0.9877 | Val Acc: 0.6238
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.937    Count 3: 0.907    Count 4: 0.790    Count 5: 0.647  
  Count 6: 0.490    Count 7: 0.250    Count 8: 0.182    Count 9: 0.056    Count 10: 0.545  
Epoch [176] Batch [0/51] Loss: 0.0001 LR: 0.000100
Epoch [176] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [176] Batch [20/51] Loss: 0.0005 LR: 0.000100
Epoch [176] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [176] Batch [40/51] Loss: 0.0001 LR: 0.000100
Epoch [176] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [177/350] - Time: 5.11s
Train Loss: 0.0129 | Val Loss: 7.2485
Train Acc: 0.9951 | Val Acc: 0.6201
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.988    Count 2: 0.865    Count 3: 0.898    Count 4: 0.724    Count 5: 0.598  
  Count 6: 0.541    Count 7: 0.396    Count 8: 0.051    Count 9: 0.144    Count 10: 0.614  
Epoch [177] Batch [0/51] Loss: 0.0003 LR: 0.000100
Epoch [177] Batch [10/51] Loss: 0.1049 LR: 0.000100
Epoch [177] Batch [20/51] Loss: 0.0114 LR: 0.000100
Epoch [177] Batch [30/51] Loss: 0.0720 LR: 0.000100
Epoch [177] Batch [40/51] Loss: 0.0005 LR: 0.000100
Epoch [177] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [178/350] - Time: 5.29s
Train Loss: 0.0723 | Val Loss: 7.4075
Train Acc: 0.9840 | Val Acc: 0.6406
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.944    Count 3: 0.917    Count 4: 0.857    Count 5: 0.598  
  Count 6: 0.439    Count 7: 0.427    Count 8: 0.182    Count 9: 0.111    Count 10: 0.523  
Epoch [178] Batch [0/51] Loss: 0.0010 LR: 0.000100
Epoch [178] Batch [10/51] Loss: 0.0110 LR: 0.000100
Epoch [178] Batch [20/51] Loss: 0.0001 LR: 0.000100
Epoch [178] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [178] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [178] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [179/350] - Time: 4.97s
Train Loss: 0.0128 | Val Loss: 7.5598
Train Acc: 0.9951 | Val Acc: 0.6331
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.921    Count 3: 0.907    Count 4: 0.857    Count 5: 0.706  
  Count 6: 0.459    Count 7: 0.240    Count 8: 0.172    Count 9: 0.156    Count 10: 0.500  
Epoch [179] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [179] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [179] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [179] Batch [30/51] Loss: 0.0007 LR: 0.000100
Epoch [179] Batch [40/51] Loss: 0.0161 LR: 0.000100
Epoch [179] Batch [50/51] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [180/350] - Time: 5.00s
Train Loss: 0.0116 | Val Loss: 6.8068
Train Acc: 0.9975 | Val Acc: 0.6480
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.897    Count 3: 0.917    Count 4: 0.857    Count 5: 0.716  
  Count 6: 0.582    Count 7: 0.281    Count 8: 0.162    Count 9: 0.178    Count 10: 0.500  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/single_image_checkpoint_epoch_179.pth
Epoch [180] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [180] Batch [10/51] Loss: 0.0001 LR: 0.000100
Epoch [180] Batch [20/51] Loss: 0.0001 LR: 0.000100
Epoch [180] Batch [30/51] Loss: 0.0024 LR: 0.000100
Epoch [180] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [180] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [181/350] - Time: 5.06s
Train Loss: 0.0263 | Val Loss: 8.5317
Train Acc: 0.9914 | Val Acc: 0.6341
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.905    Count 3: 0.898    Count 4: 0.790    Count 5: 0.578  
  Count 6: 0.490    Count 7: 0.333    Count 8: 0.162    Count 9: 0.056    Count 10: 0.750  
Epoch [181] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [181] Batch [10/51] Loss: 0.0829 LR: 0.000100
Epoch [181] Batch [20/51] Loss: 0.2646 LR: 0.000100
Epoch [181] Batch [30/51] Loss: 0.0004 LR: 0.000100
Epoch [181] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [181] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [182/350] - Time: 5.15s
Train Loss: 0.0247 | Val Loss: 7.7597
Train Acc: 0.9926 | Val Acc: 0.6248
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.897    Count 3: 0.880    Count 4: 0.838    Count 5: 0.676  
  Count 6: 0.490    Count 7: 0.271    Count 8: 0.081    Count 9: 0.211    Count 10: 0.489  
Epoch [182] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [182] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [182] Batch [20/51] Loss: 0.1923 LR: 0.000100
Epoch [182] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [182] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [182] Batch [50/51] Loss: 0.0001 LR: 0.000100

Epoch [183/350] - Time: 5.03s
Train Loss: 0.0347 | Val Loss: 7.3372
Train Acc: 0.9926 | Val Acc: 0.6248
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.913    Count 3: 0.917    Count 4: 0.867    Count 5: 0.745  
  Count 6: 0.520    Count 7: 0.229    Count 8: 0.000    Count 9: 0.200    Count 10: 0.432  
Epoch [183] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [183] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [183] Batch [20/51] Loss: 0.0003 LR: 0.000100
Epoch [183] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [183] Batch [40/51] Loss: 0.0036 LR: 0.000100
Epoch [183] Batch [50/51] Loss: 0.0220 LR: 0.000100

Epoch [184/350] - Time: 5.11s
Train Loss: 0.0134 | Val Loss: 8.1080
Train Acc: 0.9951 | Val Acc: 0.6052
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.897    Count 3: 0.880    Count 4: 0.610    Count 5: 0.402  
  Count 6: 0.571    Count 7: 0.333    Count 8: 0.172    Count 9: 0.200    Count 10: 0.602  
Epoch [184] Batch [0/51] Loss: 0.0012 LR: 0.000100
Epoch [184] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [184] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [184] Batch [30/51] Loss: 0.1545 LR: 0.000100
Epoch [184] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [184] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [185/350] - Time: 5.06s
Train Loss: 0.0270 | Val Loss: 6.9499
Train Acc: 0.9914 | Val Acc: 0.6425
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.929    Count 3: 0.898    Count 4: 0.848    Count 5: 0.696  
  Count 6: 0.531    Count 7: 0.271    Count 8: 0.121    Count 9: 0.256    Count 10: 0.477  
Epoch [185] Batch [0/51] Loss: 0.0002 LR: 0.000100
Epoch [185] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [185] Batch [20/51] Loss: 0.0554 LR: 0.000100
Epoch [185] Batch [30/51] Loss: 0.0001 LR: 0.000100
Epoch [185] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [185] Batch [50/51] Loss: 0.0002 LR: 0.000100

Epoch [186/350] - Time: 5.36s
Train Loss: 0.0135 | Val Loss: 8.0092
Train Acc: 0.9926 | Val Acc: 0.6173
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.921    Count 3: 0.898    Count 4: 0.838    Count 5: 0.775  
  Count 6: 0.469    Count 7: 0.333    Count 8: 0.020    Count 9: 0.189    Count 10: 0.284  
Epoch [186] Batch [0/51] Loss: 0.0048 LR: 0.000100
Epoch [186] Batch [10/51] Loss: 0.2163 LR: 0.000100
Epoch [186] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [186] Batch [30/51] Loss: 0.0004 LR: 0.000100
Epoch [186] Batch [40/51] Loss: 0.0001 LR: 0.000100
Epoch [186] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [187/350] - Time: 5.13s
Train Loss: 0.0564 | Val Loss: 7.6117
Train Acc: 0.9914 | Val Acc: 0.6443
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.937    Count 3: 0.907    Count 4: 0.838    Count 5: 0.706  
  Count 6: 0.520    Count 7: 0.312    Count 8: 0.101    Count 9: 0.156    Count 10: 0.568  
Epoch [187] Batch [0/51] Loss: 0.0001 LR: 0.000100
Epoch [187] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [187] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [187] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [187] Batch [40/51] Loss: 0.0058 LR: 0.000100
Epoch [187] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [188/350] - Time: 5.12s
Train Loss: 0.0051 | Val Loss: 7.9822
Train Acc: 0.9988 | Val Acc: 0.6443
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.937    Count 3: 0.907    Count 4: 0.838    Count 5: 0.755  
  Count 6: 0.510    Count 7: 0.281    Count 8: 0.172    Count 9: 0.122    Count 10: 0.511  
Epoch [188] Batch [0/51] Loss: 0.0101 LR: 0.000100
Epoch [188] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [188] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [188] Batch [30/51] Loss: 0.0001 LR: 0.000100
Epoch [188] Batch [40/51] Loss: 0.0012 LR: 0.000100
Epoch [188] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [189/350] - Time: 4.99s
Train Loss: 0.0286 | Val Loss: 8.5102
Train Acc: 0.9951 | Val Acc: 0.6257
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.921    Count 3: 0.889    Count 4: 0.771    Count 5: 0.608  
  Count 6: 0.459    Count 7: 0.396    Count 8: 0.172    Count 9: 0.022    Count 10: 0.614  
Epoch [189] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [189] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [189] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [189] Batch [30/51] Loss: 0.0001 LR: 0.000100
Epoch [189] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [189] Batch [50/51] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [190/350] - Time: 5.14s
Train Loss: 0.0140 | Val Loss: 7.6394
Train Acc: 0.9951 | Val Acc: 0.6238
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.921    Count 3: 0.898    Count 4: 0.829    Count 5: 0.676  
  Count 6: 0.520    Count 7: 0.271    Count 8: 0.141    Count 9: 0.100    Count 10: 0.455  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/single_image_checkpoint_epoch_189.pth
Epoch [190] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [190] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [190] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [190] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [190] Batch [40/51] Loss: 0.0002 LR: 0.000100
Epoch [190] Batch [50/51] Loss: 0.0092 LR: 0.000100

Epoch [191/350] - Time: 5.00s
Train Loss: 0.0397 | Val Loss: 8.3087
Train Acc: 0.9963 | Val Acc: 0.6229
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.905    Count 3: 0.880    Count 4: 0.762    Count 5: 0.657  
  Count 6: 0.551    Count 7: 0.198    Count 8: 0.040    Count 9: 0.244    Count 10: 0.602  
Epoch [191] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [191] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [191] Batch [20/51] Loss: 0.3361 LR: 0.000100
Epoch [191] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [191] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [191] Batch [50/51] Loss: 0.0001 LR: 0.000100

Epoch [192/350] - Time: 5.11s
Train Loss: 0.0417 | Val Loss: 7.5469
Train Acc: 0.9877 | Val Acc: 0.6266
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.889    Count 3: 0.870    Count 4: 0.752    Count 5: 0.627  
  Count 6: 0.510    Count 7: 0.375    Count 8: 0.121    Count 9: 0.089    Count 10: 0.648  
Epoch [192] Batch [0/51] Loss: 0.0001 LR: 0.000100
Epoch [192] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [192] Batch [20/51] Loss: 0.0001 LR: 0.000100
Epoch [192] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [192] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [192] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [193/350] - Time: 5.07s
Train Loss: 0.0220 | Val Loss: 8.4081
Train Acc: 0.9951 | Val Acc: 0.6164
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.988    Count 2: 0.921    Count 3: 0.889    Count 4: 0.771    Count 5: 0.627  
  Count 6: 0.510    Count 7: 0.281    Count 8: 0.152    Count 9: 0.156    Count 10: 0.443  
Epoch [193] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [193] Batch [10/51] Loss: 0.0908 LR: 0.000100
Epoch [193] Batch [20/51] Loss: 0.0055 LR: 0.000100
Epoch [193] Batch [30/51] Loss: 0.0001 LR: 0.000100
Epoch [193] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [193] Batch [50/51] Loss: 0.2707 LR: 0.000100

Epoch [194/350] - Time: 5.32s
Train Loss: 0.0345 | Val Loss: 7.1819
Train Acc: 0.9864 | Val Acc: 0.6313
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.976    Count 3: 0.917    Count 4: 0.829    Count 5: 0.725  
  Count 6: 0.531    Count 7: 0.219    Count 8: 0.030    Count 9: 0.233    Count 10: 0.420  
Epoch [194] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [194] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [194] Batch [20/51] Loss: 0.0227 LR: 0.000100
Epoch [194] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [194] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [194] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [195/350] - Time: 4.99s
Train Loss: 0.0151 | Val Loss: 7.9504
Train Acc: 0.9963 | Val Acc: 0.6415
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.960    Count 3: 0.898    Count 4: 0.876    Count 5: 0.686  
  Count 6: 0.408    Count 7: 0.260    Count 8: 0.081    Count 9: 0.222    Count 10: 0.625  
Epoch [195] Batch [0/51] Loss: 0.0002 LR: 0.000100
Epoch [195] Batch [10/51] Loss: 0.0001 LR: 0.000100
Epoch [195] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [195] Batch [30/51] Loss: 0.0001 LR: 0.000100
Epoch [195] Batch [40/51] Loss: 0.0001 LR: 0.000100
Epoch [195] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [196/350] - Time: 5.15s
Train Loss: 0.0761 | Val Loss: 8.1514
Train Acc: 0.9926 | Val Acc: 0.6276
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.937    Count 3: 0.898    Count 4: 0.819    Count 5: 0.667  
  Count 6: 0.480    Count 7: 0.219    Count 8: 0.162    Count 9: 0.100    Count 10: 0.580  
Epoch [196] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [196] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [196] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [196] Batch [30/51] Loss: 0.0054 LR: 0.000100
Epoch [196] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [196] Batch [50/51] Loss: 0.0001 LR: 0.000100

Epoch [197/350] - Time: 5.01s
Train Loss: 0.0048 | Val Loss: 8.2265
Train Acc: 0.9988 | Val Acc: 0.6266
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.960    Count 3: 0.880    Count 4: 0.867    Count 5: 0.588  
  Count 6: 0.480    Count 7: 0.260    Count 8: 0.172    Count 9: 0.100    Count 10: 0.523  
Epoch [197] Batch [0/51] Loss: 0.0017 LR: 0.000100
Epoch [197] Batch [10/51] Loss: 0.0002 LR: 0.000100
Epoch [197] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [197] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [197] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [197] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [198/350] - Time: 5.07s
Train Loss: 0.0401 | Val Loss: 8.3739
Train Acc: 0.9951 | Val Acc: 0.6266
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.905    Count 3: 0.917    Count 4: 0.838    Count 5: 0.696  
  Count 6: 0.490    Count 7: 0.260    Count 8: 0.141    Count 9: 0.144    Count 10: 0.455  
Epoch [198] Batch [0/51] Loss: 0.0004 LR: 0.000100
Epoch [198] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [198] Batch [20/51] Loss: 0.0026 LR: 0.000100
Epoch [198] Batch [30/51] Loss: 0.0009 LR: 0.000100
Epoch [198] Batch [40/51] Loss: 0.0010 LR: 0.000100
Epoch [198] Batch [50/51] Loss: 0.0011 LR: 0.000100

Epoch [199/350] - Time: 4.95s
Train Loss: 0.0657 | Val Loss: 8.4549
Train Acc: 0.9840 | Val Acc: 0.5801
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.849    Count 3: 0.741    Count 4: 0.524    Count 5: 0.559  
  Count 6: 0.561    Count 7: 0.208    Count 8: 0.071    Count 9: 0.211    Count 10: 0.705  
Epoch [199] Batch [0/51] Loss: 0.0040 LR: 0.000100
Epoch [199] Batch [10/51] Loss: 0.1402 LR: 0.000100
Epoch [199] Batch [20/51] Loss: 0.0008 LR: 0.000100
Epoch [199] Batch [30/51] Loss: 0.0002 LR: 0.000100
Epoch [199] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [199] Batch [50/51] Loss: 0.0231 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [200/350] - Time: 5.07s
Train Loss: 0.0271 | Val Loss: 7.6939
Train Acc: 0.9901 | Val Acc: 0.6257
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.976    Count 3: 0.898    Count 4: 0.838    Count 5: 0.814  
  Count 6: 0.408    Count 7: 0.104    Count 8: 0.010    Count 9: 0.389    Count 10: 0.386  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/single_image_checkpoint_epoch_199.pth
Epoch [200] Batch [0/51] Loss: 0.0051 LR: 0.000100
Epoch [200] Batch [10/51] Loss: 0.2978 LR: 0.000100
Epoch [200] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [200] Batch [30/51] Loss: 0.0178 LR: 0.000100
Epoch [200] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [200] Batch [50/51] Loss: 0.9720 LR: 0.000100

Epoch [201/350] - Time: 5.07s
Train Loss: 0.0642 | Val Loss: 7.1928
Train Acc: 0.9852 | Val Acc: 0.6006
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.833    Count 3: 0.806    Count 4: 0.695    Count 5: 0.824  
  Count 6: 0.500    Count 7: 0.156    Count 8: 0.051    Count 9: 0.278    Count 10: 0.466  
Epoch [201] Batch [0/51] Loss: 0.4517 LR: 0.000100
Epoch [201] Batch [10/51] Loss: 0.4956 LR: 0.000100
Epoch [201] Batch [20/51] Loss: 0.0007 LR: 0.000100
Epoch [201] Batch [30/51] Loss: 0.0024 LR: 0.000100
Epoch [201] Batch [40/51] Loss: 0.0097 LR: 0.000100
Epoch [201] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [202/350] - Time: 5.07s
Train Loss: 0.0812 | Val Loss: 7.0674
Train Acc: 0.9815 | Val Acc: 0.6220
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.937    Count 3: 0.880    Count 4: 0.848    Count 5: 0.676  
  Count 6: 0.490    Count 7: 0.177    Count 8: 0.020    Count 9: 0.344    Count 10: 0.432  
Epoch [202] Batch [0/51] Loss: 0.0163 LR: 0.000100
Epoch [202] Batch [10/51] Loss: 0.0002 LR: 0.000100
Epoch [202] Batch [20/51] Loss: 0.0001 LR: 0.000100
Epoch [202] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [202] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [202] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [203/350] - Time: 5.26s
Train Loss: 0.0180 | Val Loss: 7.1024
Train Acc: 0.9951 | Val Acc: 0.6341
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.913    Count 3: 0.880    Count 4: 0.829    Count 5: 0.618  
  Count 6: 0.490    Count 7: 0.240    Count 8: 0.152    Count 9: 0.233    Count 10: 0.602  
Epoch [203] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [203] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [203] Batch [20/51] Loss: 0.0681 LR: 0.000100
Epoch [203] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [203] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [203] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [204/350] - Time: 5.10s
Train Loss: 0.0016 | Val Loss: 6.9903
Train Acc: 0.9988 | Val Acc: 0.6285
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.913    Count 3: 0.880    Count 4: 0.848    Count 5: 0.618  
  Count 6: 0.469    Count 7: 0.229    Count 8: 0.162    Count 9: 0.200    Count 10: 0.568  
Epoch [204] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [204] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [204] Batch [20/51] Loss: 0.0144 LR: 0.000100
Epoch [204] Batch [30/51] Loss: 0.0147 LR: 0.000100
Epoch [204] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [204] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [205/350] - Time: 5.02s
Train Loss: 0.0012 | Val Loss: 7.1339
Train Acc: 1.0000 | Val Acc: 0.6313
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.937    Count 3: 0.907    Count 4: 0.819    Count 5: 0.745  
  Count 6: 0.500    Count 7: 0.188    Count 8: 0.131    Count 9: 0.178    Count 10: 0.489  
Epoch [205] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [205] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [205] Batch [20/51] Loss: 0.0008 LR: 0.000100
Epoch [205] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [205] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [205] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [206/350] - Time: 5.07s
Train Loss: 0.0127 | Val Loss: 7.0289
Train Acc: 0.9938 | Val Acc: 0.6453
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.913    Count 3: 0.917    Count 4: 0.876    Count 5: 0.686  
  Count 6: 0.480    Count 7: 0.219    Count 8: 0.152    Count 9: 0.244    Count 10: 0.580  
Epoch [206] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [206] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [206] Batch [20/51] Loss: 0.0003 LR: 0.000100
Epoch [206] Batch [30/51] Loss: 0.0322 LR: 0.000100
Epoch [206] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [206] Batch [50/51] Loss: 0.0067 LR: 0.000100

Epoch [207/350] - Time: 5.07s
Train Loss: 0.0060 | Val Loss: 7.4033
Train Acc: 0.9988 | Val Acc: 0.6350
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.921    Count 3: 0.880    Count 4: 0.829    Count 5: 0.598  
  Count 6: 0.500    Count 7: 0.292    Count 8: 0.101    Count 9: 0.289    Count 10: 0.557  
Epoch [207] Batch [0/51] Loss: 0.0041 LR: 0.000100
Epoch [207] Batch [10/51] Loss: 0.3165 LR: 0.000100
Epoch [207] Batch [20/51] Loss: 0.0066 LR: 0.000100
Epoch [207] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [207] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [207] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [208/350] - Time: 5.02s
Train Loss: 0.0162 | Val Loss: 8.1892
Train Acc: 0.9938 | Val Acc: 0.6294
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.937    Count 3: 0.870    Count 4: 0.800    Count 5: 0.588  
  Count 6: 0.510    Count 7: 0.271    Count 8: 0.141    Count 9: 0.156    Count 10: 0.614  
Epoch [208] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [208] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [208] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [208] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [208] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [208] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [209/350] - Time: 5.09s
Train Loss: 0.0303 | Val Loss: 7.4985
Train Acc: 0.9951 | Val Acc: 0.6499
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.937    Count 3: 0.907    Count 4: 0.819    Count 5: 0.627  
  Count 6: 0.592    Count 7: 0.188    Count 8: 0.000    Count 9: 0.489    Count 10: 0.580  
Epoch [209] Batch [0/51] Loss: 0.0008 LR: 0.000100
Epoch [209] Batch [10/51] Loss: 0.0002 LR: 0.000100
Epoch [209] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [209] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [209] Batch [40/51] Loss: 0.0002 LR: 0.000100
Epoch [209] Batch [50/51] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [210/350] - Time: 5.15s
Train Loss: 0.0106 | Val Loss: 8.1081
Train Acc: 0.9975 | Val Acc: 0.6313
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.889    Count 3: 0.880    Count 4: 0.829    Count 5: 0.618  
  Count 6: 0.439    Count 7: 0.344    Count 8: 0.172    Count 9: 0.156    Count 10: 0.602  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/single_image_checkpoint_epoch_209.pth
Epoch [210] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [210] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [210] Batch [20/51] Loss: 0.0147 LR: 0.000100
Epoch [210] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [210] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [210] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [211/350] - Time: 5.35s
Train Loss: 0.0097 | Val Loss: 7.6507
Train Acc: 0.9975 | Val Acc: 0.6471
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.937    Count 3: 0.898    Count 4: 0.848    Count 5: 0.637  
  Count 6: 0.643    Count 7: 0.260    Count 8: 0.121    Count 9: 0.256    Count 10: 0.477  
Epoch [211] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [211] Batch [10/51] Loss: 0.0066 LR: 0.000100
Epoch [211] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [211] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [211] Batch [40/51] Loss: 0.0002 LR: 0.000100
Epoch [211] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [212/350] - Time: 5.11s
Train Loss: 0.0167 | Val Loss: 8.0265
Train Acc: 0.9914 | Val Acc: 0.6229
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.921    Count 3: 0.880    Count 4: 0.781    Count 5: 0.618  
  Count 6: 0.622    Count 7: 0.188    Count 8: 0.000    Count 9: 0.322    Count 10: 0.500  
Epoch [212] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [212] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [212] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [212] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [212] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [212] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [213/350] - Time: 5.12s
Train Loss: 0.0013 | Val Loss: 8.3832
Train Acc: 1.0000 | Val Acc: 0.6369
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.929    Count 3: 0.898    Count 4: 0.848    Count 5: 0.608  
  Count 6: 0.531    Count 7: 0.250    Count 8: 0.121    Count 9: 0.244    Count 10: 0.545  
Epoch [213] Batch [0/51] Loss: 0.3366 LR: 0.000100
Epoch [213] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [213] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [213] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [213] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [213] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [214/350] - Time: 5.08s
Train Loss: 0.0078 | Val Loss: 8.1663
Train Acc: 0.9988 | Val Acc: 0.6387
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.952    Count 3: 0.880    Count 4: 0.848    Count 5: 0.618  
  Count 6: 0.582    Count 7: 0.302    Count 8: 0.111    Count 9: 0.178    Count 10: 0.511  
Epoch [214] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [214] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [214] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [214] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [214] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [214] Batch [50/51] Loss: 0.0012 LR: 0.000100

Epoch [215/350] - Time: 5.07s
Train Loss: 0.0289 | Val Loss: 7.5570
Train Acc: 0.9951 | Val Acc: 0.6462
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.984    Count 3: 0.907    Count 4: 0.867    Count 5: 0.696  
  Count 6: 0.531    Count 7: 0.271    Count 8: 0.131    Count 9: 0.178    Count 10: 0.477  
Epoch [215] Batch [0/51] Loss: 0.0004 LR: 0.000100
Epoch [215] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [215] Batch [20/51] Loss: 0.0010 LR: 0.000100
Epoch [215] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [215] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [215] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [216/350] - Time: 5.12s
Train Loss: 0.0118 | Val Loss: 7.7293
Train Acc: 0.9963 | Val Acc: 0.6378
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.960    Count 3: 0.907    Count 4: 0.886    Count 5: 0.725  
  Count 6: 0.520    Count 7: 0.260    Count 8: 0.051    Count 9: 0.200    Count 10: 0.443  
Epoch [216] Batch [0/51] Loss: 0.3777 LR: 0.000100
Epoch [216] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [216] Batch [20/51] Loss: 0.0040 LR: 0.000100
Epoch [216] Batch [30/51] Loss: 0.0003 LR: 0.000100
Epoch [216] Batch [40/51] Loss: 0.1968 LR: 0.000100
Epoch [216] Batch [50/51] Loss: 0.0015 LR: 0.000100

Epoch [217/350] - Time: 5.02s
Train Loss: 0.0820 | Val Loss: 7.5954
Train Acc: 0.9864 | Val Acc: 0.6229
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.937    Count 3: 0.898    Count 4: 0.810    Count 5: 0.647  
  Count 6: 0.449    Count 7: 0.240    Count 8: 0.162    Count 9: 0.189    Count 10: 0.477  
Epoch [217] Batch [0/51] Loss: 0.0003 LR: 0.000100
Epoch [217] Batch [10/51] Loss: 0.2593 LR: 0.000100
Epoch [217] Batch [20/51] Loss: 0.0425 LR: 0.000100
Epoch [217] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [217] Batch [40/51] Loss: 0.0030 LR: 0.000100
Epoch [217] Batch [50/51] Loss: 0.5618 LR: 0.000100

Epoch [218/350] - Time: 5.08s
Train Loss: 0.0423 | Val Loss: 7.6448
Train Acc: 0.9864 | Val Acc: 0.6387
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.944    Count 3: 0.880    Count 4: 0.867    Count 5: 0.735  
  Count 6: 0.490    Count 7: 0.292    Count 8: 0.172    Count 9: 0.111    Count 10: 0.466  
Epoch [218] Batch [0/51] Loss: 0.0189 LR: 0.000100
Epoch [218] Batch [10/51] Loss: 0.0001 LR: 0.000100
Epoch [218] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [218] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [218] Batch [40/51] Loss: 0.0660 LR: 0.000100
Epoch [218] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [219/350] - Time: 5.06s
Train Loss: 0.0059 | Val Loss: 7.7937
Train Acc: 0.9975 | Val Acc: 0.6294
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.929    Count 3: 0.889    Count 4: 0.810    Count 5: 0.657  
  Count 6: 0.612    Count 7: 0.240    Count 8: 0.121    Count 9: 0.189    Count 10: 0.432  
Epoch [219] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [219] Batch [10/51] Loss: 0.0011 LR: 0.000100
Epoch [219] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [219] Batch [30/51] Loss: 0.0038 LR: 0.000100
Epoch [219] Batch [40/51] Loss: 0.0112 LR: 0.000100
Epoch [219] Batch [50/51] Loss: 0.0058 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [220/350] - Time: 5.32s
Train Loss: 0.0032 | Val Loss: 8.5965
Train Acc: 1.0000 | Val Acc: 0.6276
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.905    Count 3: 0.861    Count 4: 0.790    Count 5: 0.588  
  Count 6: 0.500    Count 7: 0.344    Count 8: 0.131    Count 9: 0.211    Count 10: 0.557  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/single_image_checkpoint_epoch_219.pth
Epoch [220] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [220] Batch [10/51] Loss: 0.0001 LR: 0.000100
Epoch [220] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [220] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [220] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [220] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [221/350] - Time: 5.14s
Train Loss: 0.0187 | Val Loss: 8.5339
Train Acc: 0.9951 | Val Acc: 0.6304
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.913    Count 3: 0.889    Count 4: 0.867    Count 5: 0.637  
  Count 6: 0.429    Count 7: 0.281    Count 8: 0.141    Count 9: 0.133    Count 10: 0.614  
Epoch [221] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [221] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [221] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [221] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [221] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [221] Batch [50/51] Loss: 0.0001 LR: 0.000100

Epoch [222/350] - Time: 5.08s
Train Loss: 0.0165 | Val Loss: 8.0117
Train Acc: 0.9963 | Val Acc: 0.6294
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.921    Count 3: 0.889    Count 4: 0.790    Count 5: 0.598  
  Count 6: 0.541    Count 7: 0.302    Count 8: 0.172    Count 9: 0.089    Count 10: 0.591  
Epoch [222] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [222] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [222] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [222] Batch [30/51] Loss: 0.4087 LR: 0.000100
Epoch [222] Batch [40/51] Loss: 0.0001 LR: 0.000100
Epoch [222] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [223/350] - Time: 5.06s
Train Loss: 0.0369 | Val Loss: 7.8254
Train Acc: 0.9926 | Val Acc: 0.6378
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.913    Count 3: 0.898    Count 4: 0.829    Count 5: 0.686  
  Count 6: 0.449    Count 7: 0.323    Count 8: 0.131    Count 9: 0.167    Count 10: 0.591  
Epoch [223] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [223] Batch [10/51] Loss: 0.0015 LR: 0.000100
Epoch [223] Batch [20/51] Loss: 0.0001 LR: 0.000100
Epoch [223] Batch [30/51] Loss: 0.0035 LR: 0.000100
Epoch [223] Batch [40/51] Loss: 0.0002 LR: 0.000100
Epoch [223] Batch [50/51] Loss: 0.0001 LR: 0.000100

Epoch [224/350] - Time: 5.16s
Train Loss: 0.0192 | Val Loss: 7.5588
Train Acc: 0.9951 | Val Acc: 0.6574
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.944    Count 3: 0.889    Count 4: 0.857    Count 5: 0.765  
  Count 6: 0.643    Count 7: 0.177    Count 8: 0.020    Count 9: 0.400    Count 10: 0.500  
Epoch [224] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [224] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [224] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [224] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [224] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [224] Batch [50/51] Loss: 0.0001 LR: 0.000100

Epoch [225/350] - Time: 5.16s
Train Loss: 0.0002 | Val Loss: 7.7334
Train Acc: 1.0000 | Val Acc: 0.6425
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.913    Count 3: 0.889    Count 4: 0.838    Count 5: 0.608  
  Count 6: 0.592    Count 7: 0.271    Count 8: 0.091    Count 9: 0.256    Count 10: 0.591  
Epoch [225] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [225] Batch [10/51] Loss: 0.0002 LR: 0.000100
Epoch [225] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [225] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [225] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [225] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [226/350] - Time: 5.09s
Train Loss: 0.0001 | Val Loss: 7.8245
Train Acc: 1.0000 | Val Acc: 0.6499
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.937    Count 3: 0.889    Count 4: 0.876    Count 5: 0.637  
  Count 6: 0.602    Count 7: 0.281    Count 8: 0.091    Count 9: 0.211    Count 10: 0.591  
Epoch [226] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [226] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [226] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [226] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [226] Batch [40/51] Loss: 0.0001 LR: 0.000100
Epoch [226] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [227/350] - Time: 5.11s
Train Loss: 0.0000 | Val Loss: 7.6600
Train Acc: 1.0000 | Val Acc: 0.6508
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.944    Count 3: 0.898    Count 4: 0.857    Count 5: 0.657  
  Count 6: 0.622    Count 7: 0.260    Count 8: 0.091    Count 9: 0.200    Count 10: 0.591  
Epoch [227] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [227] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [227] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [227] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [227] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [227] Batch [50/51] Loss: 0.0002 LR: 0.000100

Epoch [228/350] - Time: 5.35s
Train Loss: 0.0000 | Val Loss: 7.5648
Train Acc: 1.0000 | Val Acc: 0.6471
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.944    Count 3: 0.907    Count 4: 0.848    Count 5: 0.667  
  Count 6: 0.571    Count 7: 0.260    Count 8: 0.091    Count 9: 0.200    Count 10: 0.591  
Epoch [228] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [228] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [228] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [228] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [228] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [228] Batch [50/51] Loss: 0.0001 LR: 0.000100

Epoch [229/350] - Time: 5.05s
Train Loss: 0.0000 | Val Loss: 7.6274
Train Acc: 1.0000 | Val Acc: 0.6499
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.937    Count 3: 0.917    Count 4: 0.848    Count 5: 0.657  
  Count 6: 0.612    Count 7: 0.250    Count 8: 0.091    Count 9: 0.211    Count 10: 0.591  
Epoch [229] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [229] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [229] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [229] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [229] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [229] Batch [50/51] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [230/350] - Time: 5.01s
Train Loss: 0.0017 | Val Loss: 7.8309
Train Acc: 0.9988 | Val Acc: 0.6490
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.937    Count 3: 0.907    Count 4: 0.867    Count 5: 0.627  
  Count 6: 0.571    Count 7: 0.281    Count 8: 0.101    Count 9: 0.222    Count 10: 0.591  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/single_image_checkpoint_epoch_229.pth
Epoch [230] Batch [0/51] Loss: 0.0002 LR: 0.000100
Epoch [230] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [230] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [230] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [230] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [230] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [231/350] - Time: 5.04s
Train Loss: 0.0017 | Val Loss: 7.5750
Train Acc: 0.9988 | Val Acc: 0.6434
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.988    Count 2: 0.905    Count 3: 0.870    Count 4: 0.857    Count 5: 0.637  
  Count 6: 0.673    Count 7: 0.250    Count 8: 0.121    Count 9: 0.178    Count 10: 0.568  
Epoch [231] Batch [0/51] Loss: 0.0018 LR: 0.000100
Epoch [231] Batch [10/51] Loss: 0.0009 LR: 0.000100
Epoch [231] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [231] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [231] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [231] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [232/350] - Time: 5.14s
Train Loss: 0.0012 | Val Loss: 7.4631
Train Acc: 1.0000 | Val Acc: 0.6490
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.944    Count 3: 0.898    Count 4: 0.819    Count 5: 0.735  
  Count 6: 0.663    Count 7: 0.219    Count 8: 0.040    Count 9: 0.256    Count 10: 0.523  
Epoch [232] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [232] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [232] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [232] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [232] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [232] Batch [50/51] Loss: 0.0017 LR: 0.000100

Epoch [233/350] - Time: 5.11s
Train Loss: 0.0002 | Val Loss: 7.3637
Train Acc: 1.0000 | Val Acc: 0.6462
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.921    Count 3: 0.880    Count 4: 0.800    Count 5: 0.657  
  Count 6: 0.622    Count 7: 0.229    Count 8: 0.121    Count 9: 0.267    Count 10: 0.591  
Epoch [233] Batch [0/51] Loss: 0.0038 LR: 0.000100
Epoch [233] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [233] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [233] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [233] Batch [40/51] Loss: 0.0002 LR: 0.000100
Epoch [233] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [234/350] - Time: 5.15s
Train Loss: 0.0100 | Val Loss: 8.8932
Train Acc: 0.9963 | Val Acc: 0.5866
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.975    Count 2: 0.873    Count 3: 0.787    Count 4: 0.571    Count 5: 0.520  
  Count 6: 0.602    Count 7: 0.208    Count 8: 0.051    Count 9: 0.300    Count 10: 0.602  
Epoch [234] Batch [0/51] Loss: 0.0718 LR: 0.000100
Epoch [234] Batch [10/51] Loss: 0.0004 LR: 0.000100
Epoch [234] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [234] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [234] Batch [40/51] Loss: 0.0001 LR: 0.000100
Epoch [234] Batch [50/51] Loss: 0.7978 LR: 0.000100

Epoch [235/350] - Time: 5.05s
Train Loss: 0.0364 | Val Loss: 8.9874
Train Acc: 0.9926 | Val Acc: 0.6220
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.988    Count 2: 0.889    Count 3: 0.889    Count 4: 0.800    Count 5: 0.588  
  Count 6: 0.459    Count 7: 0.240    Count 8: 0.162    Count 9: 0.178    Count 10: 0.636  
Epoch [235] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [235] Batch [10/51] Loss: 0.6591 LR: 0.000100
Epoch [235] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [235] Batch [30/51] Loss: 0.0011 LR: 0.000100
Epoch [235] Batch [40/51] Loss: 0.1658 LR: 0.000100
Epoch [235] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [236/350] - Time: 5.46s
Train Loss: 0.0658 | Val Loss: 7.7431
Train Acc: 0.9815 | Val Acc: 0.6331
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.944    Count 3: 0.889    Count 4: 0.829    Count 5: 0.657  
  Count 6: 0.571    Count 7: 0.208    Count 8: 0.081    Count 9: 0.156    Count 10: 0.580  
Epoch [236] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [236] Batch [10/51] Loss: 0.8422 LR: 0.000100
Epoch [236] Batch [20/51] Loss: 0.0005 LR: 0.000100
Epoch [236] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [236] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [236] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [237/350] - Time: 5.09s
Train Loss: 0.0822 | Val Loss: 7.4563
Train Acc: 0.9877 | Val Acc: 0.6238
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.944    Count 3: 0.898    Count 4: 0.771    Count 5: 0.598  
  Count 6: 0.592    Count 7: 0.250    Count 8: 0.040    Count 9: 0.189    Count 10: 0.545  
Epoch [237] Batch [0/51] Loss: 0.0015 LR: 0.000100
Epoch [237] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [237] Batch [20/51] Loss: 0.0033 LR: 0.000100
Epoch [237] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [237] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [237] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [238/350] - Time: 5.04s
Train Loss: 0.0361 | Val Loss: 8.7202
Train Acc: 0.9926 | Val Acc: 0.6359
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.952    Count 3: 0.907    Count 4: 0.838    Count 5: 0.696  
  Count 6: 0.520    Count 7: 0.333    Count 8: 0.141    Count 9: 0.122    Count 10: 0.420  
Epoch [238] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [238] Batch [10/51] Loss: 0.0143 LR: 0.000100
Epoch [238] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [238] Batch [30/51] Loss: 0.0001 LR: 0.000100
Epoch [238] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [238] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [239/350] - Time: 5.04s
Train Loss: 0.0204 | Val Loss: 9.7893
Train Acc: 0.9975 | Val Acc: 0.6034
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.981    Count 2: 0.913    Count 3: 0.861    Count 4: 0.867    Count 5: 0.598  
  Count 6: 0.408    Count 7: 0.354    Count 8: 0.121    Count 9: 0.033    Count 10: 0.455  
Epoch [239] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [239] Batch [10/51] Loss: 0.1419 LR: 0.000100
Epoch [239] Batch [20/51] Loss: 0.0001 LR: 0.000100
Epoch [239] Batch [30/51] Loss: 0.1236 LR: 0.000100
Epoch [239] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [239] Batch [50/51] Loss: 0.0142 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [240/350] - Time: 5.08s
Train Loss: 0.0439 | Val Loss: 7.7283
Train Acc: 0.9889 | Val Acc: 0.6378
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.937    Count 3: 0.880    Count 4: 0.829    Count 5: 0.725  
  Count 6: 0.408    Count 7: 0.323    Count 8: 0.162    Count 9: 0.189    Count 10: 0.523  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/single_image_checkpoint_epoch_239.pth
Epoch [240] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [240] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [240] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [240] Batch [30/51] Loss: 0.0021 LR: 0.000100
Epoch [240] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [240] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [241/350] - Time: 5.08s
Train Loss: 0.0172 | Val Loss: 7.3234
Train Acc: 0.9963 | Val Acc: 0.6499
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.937    Count 3: 0.907    Count 4: 0.819    Count 5: 0.676  
  Count 6: 0.510    Count 7: 0.250    Count 8: 0.111    Count 9: 0.278    Count 10: 0.636  
Epoch [241] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [241] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [241] Batch [20/51] Loss: 0.0015 LR: 0.000100
Epoch [241] Batch [30/51] Loss: 0.0001 LR: 0.000100
Epoch [241] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [241] Batch [50/51] Loss: 0.0068 LR: 0.000100

Epoch [242/350] - Time: 5.13s
Train Loss: 0.0158 | Val Loss: 8.1059
Train Acc: 0.9951 | Val Acc: 0.6434
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.929    Count 3: 0.898    Count 4: 0.838    Count 5: 0.696  
  Count 6: 0.469    Count 7: 0.375    Count 8: 0.172    Count 9: 0.189    Count 10: 0.466  
Epoch [242] Batch [0/51] Loss: 0.0002 LR: 0.000100
Epoch [242] Batch [10/51] Loss: 0.0589 LR: 0.000100
Epoch [242] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [242] Batch [30/51] Loss: 0.0001 LR: 0.000100
Epoch [242] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [242] Batch [50/51] Loss: 0.0001 LR: 0.000100

Epoch [243/350] - Time: 4.96s
Train Loss: 0.0268 | Val Loss: 9.5039
Train Acc: 0.9901 | Val Acc: 0.5857
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.944    Count 3: 0.880    Count 4: 0.762    Count 5: 0.598  
  Count 6: 0.337    Count 7: 0.219    Count 8: 0.131    Count 9: 0.044    Count 10: 0.466  
Epoch [243] Batch [0/51] Loss: 0.0001 LR: 0.000100
Epoch [243] Batch [10/51] Loss: 0.0029 LR: 0.000100
Epoch [243] Batch [20/51] Loss: 0.0002 LR: 0.000100
Epoch [243] Batch [30/51] Loss: 0.1102 LR: 0.000100
Epoch [243] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [243] Batch [50/51] Loss: 0.0147 LR: 0.000100

Epoch [244/350] - Time: 5.11s
Train Loss: 0.0723 | Val Loss: 7.7923
Train Acc: 0.9852 | Val Acc: 0.6415
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.975    Count 2: 0.944    Count 3: 0.898    Count 4: 0.848    Count 5: 0.775  
  Count 6: 0.633    Count 7: 0.146    Count 8: 0.020    Count 9: 0.267    Count 10: 0.511  
Epoch [244] Batch [0/51] Loss: 0.0023 LR: 0.000100
Epoch [244] Batch [10/51] Loss: 0.0006 LR: 0.000100
Epoch [244] Batch [20/51] Loss: 0.1938 LR: 0.000100
Epoch [244] Batch [30/51] Loss: 0.0002 LR: 0.000100
Epoch [244] Batch [40/51] Loss: 0.3473 LR: 0.000100
Epoch [244] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [245/350] - Time: 5.58s
Train Loss: 0.0478 | Val Loss: 8.1792
Train Acc: 0.9889 | Val Acc: 0.6220
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.905    Count 3: 0.907    Count 4: 0.819    Count 5: 0.676  
  Count 6: 0.490    Count 7: 0.208    Count 8: 0.061    Count 9: 0.200    Count 10: 0.545  
Epoch [245] Batch [0/51] Loss: 0.0001 LR: 0.000100
Epoch [245] Batch [10/51] Loss: 0.9443 LR: 0.000100
Epoch [245] Batch [20/51] Loss: 0.0015 LR: 0.000100
Epoch [245] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [245] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [245] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [246/350] - Time: 5.15s
Train Loss: 0.0480 | Val Loss: 9.0476
Train Acc: 0.9901 | Val Acc: 0.5829
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.952    Count 3: 0.889    Count 4: 0.781    Count 5: 0.588  
  Count 6: 0.245    Count 7: 0.240    Count 8: 0.030    Count 9: 0.144    Count 10: 0.500  
Epoch [246] Batch [0/51] Loss: 0.4489 LR: 0.000100
Epoch [246] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [246] Batch [20/51] Loss: 0.2660 LR: 0.000100
Epoch [246] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [246] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [246] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [247/350] - Time: 5.16s
Train Loss: 0.0236 | Val Loss: 8.0219
Train Acc: 0.9914 | Val Acc: 0.6173
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.881    Count 3: 0.907    Count 4: 0.771    Count 5: 0.647  
  Count 6: 0.582    Count 7: 0.188    Count 8: 0.051    Count 9: 0.267    Count 10: 0.477  
Epoch [247] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [247] Batch [10/51] Loss: 0.0017 LR: 0.000100
Epoch [247] Batch [20/51] Loss: 0.0001 LR: 0.000100
Epoch [247] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [247] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [247] Batch [50/51] Loss: 0.0002 LR: 0.000100

Epoch [248/350] - Time: 5.05s
Train Loss: 0.0084 | Val Loss: 9.2762
Train Acc: 0.9975 | Val Acc: 0.6108
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.981    Count 2: 0.944    Count 3: 0.898    Count 4: 0.857    Count 5: 0.676  
  Count 6: 0.255    Count 7: 0.292    Count 8: 0.111    Count 9: 0.033    Count 10: 0.625  
Epoch [248] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [248] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [248] Batch [20/51] Loss: 0.0840 LR: 0.000100
Epoch [248] Batch [30/51] Loss: 0.0008 LR: 0.000100
Epoch [248] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [248] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [249/350] - Time: 5.13s
Train Loss: 0.0363 | Val Loss: 8.0836
Train Acc: 0.9901 | Val Acc: 0.6304
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.921    Count 3: 0.898    Count 4: 0.819    Count 5: 0.745  
  Count 6: 0.520    Count 7: 0.219    Count 8: 0.010    Count 9: 0.300    Count 10: 0.466  
Epoch [249] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [249] Batch [10/51] Loss: 0.1528 LR: 0.000100
Epoch [249] Batch [20/51] Loss: 0.0024 LR: 0.000100
Epoch [249] Batch [30/51] Loss: 0.0015 LR: 0.000100
Epoch [249] Batch [40/51] Loss: 0.0001 LR: 0.000100
Epoch [249] Batch [50/51] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [250/350] - Time: 5.08s
Train Loss: 0.0104 | Val Loss: 8.2765
Train Acc: 0.9975 | Val Acc: 0.6322
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.913    Count 3: 0.898    Count 4: 0.829    Count 5: 0.667  
  Count 6: 0.459    Count 7: 0.375    Count 8: 0.131    Count 9: 0.100    Count 10: 0.545  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/single_image_checkpoint_epoch_249.pth
Epoch [250] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [250] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [250] Batch [20/51] Loss: 0.0001 LR: 0.000100
Epoch [250] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [250] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [250] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [251/350] - Time: 5.10s
Train Loss: 0.0054 | Val Loss: 8.7711
Train Acc: 0.9988 | Val Acc: 0.6238
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.937    Count 3: 0.907    Count 4: 0.781    Count 5: 0.598  
  Count 6: 0.490    Count 7: 0.292    Count 8: 0.091    Count 9: 0.144    Count 10: 0.591  
Epoch [251] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [251] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [251] Batch [20/51] Loss: 0.0001 LR: 0.000100
Epoch [251] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [251] Batch [40/51] Loss: 0.0079 LR: 0.000100
Epoch [251] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [252/350] - Time: 5.09s
Train Loss: 0.0013 | Val Loss: 8.5976
Train Acc: 1.0000 | Val Acc: 0.6359
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.937    Count 3: 0.898    Count 4: 0.829    Count 5: 0.529  
  Count 6: 0.520    Count 7: 0.333    Count 8: 0.141    Count 9: 0.133    Count 10: 0.636  
Epoch [252] Batch [0/51] Loss: 0.0234 LR: 0.000100
Epoch [252] Batch [10/51] Loss: 0.0002 LR: 0.000100
Epoch [252] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [252] Batch [30/51] Loss: 0.0019 LR: 0.000100
Epoch [252] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [252] Batch [50/51] Loss: 0.0025 LR: 0.000100

Epoch [253/350] - Time: 5.57s
Train Loss: 0.0556 | Val Loss: 8.1698
Train Acc: 0.9901 | Val Acc: 0.6341
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.944    Count 3: 0.898    Count 4: 0.838    Count 5: 0.725  
  Count 6: 0.439    Count 7: 0.302    Count 8: 0.131    Count 9: 0.089    Count 10: 0.557  
Epoch [253] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [253] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [253] Batch [20/51] Loss: 0.0356 LR: 0.000100
Epoch [253] Batch [30/51] Loss: 0.0006 LR: 0.000100
Epoch [253] Batch [40/51] Loss: 0.0020 LR: 0.000100
Epoch [253] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [254/350] - Time: 5.12s
Train Loss: 0.0634 | Val Loss: 9.3810
Train Acc: 0.9877 | Val Acc: 0.6294
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.929    Count 3: 0.907    Count 4: 0.829    Count 5: 0.706  
  Count 6: 0.633    Count 7: 0.208    Count 8: 0.071    Count 9: 0.200    Count 10: 0.375  
Epoch [254] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [254] Batch [10/51] Loss: 0.0002 LR: 0.000100
Epoch [254] Batch [20/51] Loss: 0.0002 LR: 0.000100
Epoch [254] Batch [30/51] Loss: 0.0002 LR: 0.000100
Epoch [254] Batch [40/51] Loss: 0.0001 LR: 0.000100
Epoch [254] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [255/350] - Time: 5.11s
Train Loss: 0.0230 | Val Loss: 9.0482
Train Acc: 0.9951 | Val Acc: 0.5996
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.944    Count 3: 0.898    Count 4: 0.781    Count 5: 0.588  
  Count 6: 0.378    Count 7: 0.198    Count 8: 0.172    Count 9: 0.133    Count 10: 0.455  
Epoch [255] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [255] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [255] Batch [20/51] Loss: 0.0001 LR: 0.000100
Epoch [255] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [255] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [255] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [256/350] - Time: 5.12s
Train Loss: 0.0087 | Val Loss: 8.7376
Train Acc: 0.9975 | Val Acc: 0.6192
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.905    Count 3: 0.870    Count 4: 0.705    Count 5: 0.598  
  Count 6: 0.592    Count 7: 0.240    Count 8: 0.101    Count 9: 0.211    Count 10: 0.580  
Epoch [256] Batch [0/51] Loss: 0.0041 LR: 0.000100
Epoch [256] Batch [10/51] Loss: 0.0037 LR: 0.000100
Epoch [256] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [256] Batch [30/51] Loss: 0.0001 LR: 0.000100
Epoch [256] Batch [40/51] Loss: 0.1745 LR: 0.000100
Epoch [256] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [257/350] - Time: 5.01s
Train Loss: 0.0098 | Val Loss: 8.9492
Train Acc: 0.9963 | Val Acc: 0.5996
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.976    Count 3: 0.907    Count 4: 0.781    Count 5: 0.569  
  Count 6: 0.337    Count 7: 0.219    Count 8: 0.111    Count 9: 0.144    Count 10: 0.489  
Epoch [257] Batch [0/51] Loss: 0.0088 LR: 0.000100
Epoch [257] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [257] Batch [20/51] Loss: 0.0001 LR: 0.000100
Epoch [257] Batch [30/51] Loss: 0.1770 LR: 0.000100
Epoch [257] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [257] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [258/350] - Time: 5.03s
Train Loss: 0.0330 | Val Loss: 8.8333
Train Acc: 0.9914 | Val Acc: 0.5996
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.976    Count 3: 0.907    Count 4: 0.819    Count 5: 0.510  
  Count 6: 0.367    Count 7: 0.250    Count 8: 0.061    Count 9: 0.111    Count 10: 0.534  
Epoch [258] Batch [0/51] Loss: 0.1994 LR: 0.000100
Epoch [258] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [258] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [258] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [258] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [258] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [259/350] - Time: 5.04s
Train Loss: 0.0115 | Val Loss: 9.1162
Train Acc: 0.9963 | Val Acc: 0.6173
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.937    Count 3: 0.898    Count 4: 0.857    Count 5: 0.676  
  Count 6: 0.469    Count 7: 0.229    Count 8: 0.101    Count 9: 0.144    Count 10: 0.409  
Epoch [259] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [259] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [259] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [259] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [259] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [259] Batch [50/51] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [260/350] - Time: 5.13s
Train Loss: 0.0205 | Val Loss: 9.0853
Train Acc: 0.9938 | Val Acc: 0.6201
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.897    Count 3: 0.907    Count 4: 0.848    Count 5: 0.657  
  Count 6: 0.531    Count 7: 0.229    Count 8: 0.101    Count 9: 0.167    Count 10: 0.432  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/single_image_checkpoint_epoch_259.pth
Epoch [260] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [260] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [260] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [260] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [260] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [260] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [261/350] - Time: 5.19s
Train Loss: 0.0144 | Val Loss: 8.7271
Train Acc: 0.9963 | Val Acc: 0.6257
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.929    Count 3: 0.898    Count 4: 0.800    Count 5: 0.618  
  Count 6: 0.459    Count 7: 0.198    Count 8: 0.182    Count 9: 0.222    Count 10: 0.534  
Epoch [261] Batch [0/51] Loss: 0.0014 LR: 0.000100
Epoch [261] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [261] Batch [20/51] Loss: 0.0001 LR: 0.000100
Epoch [261] Batch [30/51] Loss: 0.0019 LR: 0.000100
Epoch [261] Batch [40/51] Loss: 0.0001 LR: 0.000100
Epoch [261] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [262/350] - Time: 6.32s
Train Loss: 0.0598 | Val Loss: 8.3192
Train Acc: 0.9889 | Val Acc: 0.6387
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.921    Count 3: 0.889    Count 4: 0.867    Count 5: 0.637  
  Count 6: 0.480    Count 7: 0.240    Count 8: 0.152    Count 9: 0.244    Count 10: 0.568  
Epoch [262] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [262] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [262] Batch [20/51] Loss: 0.0051 LR: 0.000100
Epoch [262] Batch [30/51] Loss: 1.0221 LR: 0.000100
Epoch [262] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [262] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [263/350] - Time: 5.07s
Train Loss: 0.0558 | Val Loss: 8.4359
Train Acc: 0.9926 | Val Acc: 0.6248
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.937    Count 3: 0.898    Count 4: 0.829    Count 5: 0.745  
  Count 6: 0.429    Count 7: 0.188    Count 8: 0.101    Count 9: 0.200    Count 10: 0.489  
Epoch [263] Batch [0/51] Loss: 0.0009 LR: 0.000100
Epoch [263] Batch [10/51] Loss: 0.0001 LR: 0.000100
Epoch [263] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [263] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [263] Batch [40/51] Loss: 0.0010 LR: 0.000100
Epoch [263] Batch [50/51] Loss: 0.0020 LR: 0.000100

Epoch [264/350] - Time: 4.97s
Train Loss: 0.0074 | Val Loss: 9.4688
Train Acc: 0.9975 | Val Acc: 0.5996
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.873    Count 3: 0.796    Count 4: 0.771    Count 5: 0.569  
  Count 6: 0.592    Count 7: 0.250    Count 8: 0.141    Count 9: 0.089    Count 10: 0.500  
Epoch [264] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [264] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [264] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [264] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [264] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [264] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [265/350] - Time: 5.09s
Train Loss: 0.0256 | Val Loss: 8.8444
Train Acc: 0.9926 | Val Acc: 0.6359
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.968    Count 3: 0.898    Count 4: 0.848    Count 5: 0.618  
  Count 6: 0.551    Count 7: 0.240    Count 8: 0.131    Count 9: 0.111    Count 10: 0.568  
Epoch [265] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [265] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [265] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [265] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [265] Batch [40/51] Loss: 0.0012 LR: 0.000100
Epoch [265] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [266/350] - Time: 5.09s
Train Loss: 0.0232 | Val Loss: 8.4843
Train Acc: 0.9963 | Val Acc: 0.6369
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.937    Count 3: 0.898    Count 4: 0.781    Count 5: 0.569  
  Count 6: 0.684    Count 7: 0.229    Count 8: 0.141    Count 9: 0.167    Count 10: 0.568  
Epoch [266] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [266] Batch [10/51] Loss: 0.2243 LR: 0.000100
Epoch [266] Batch [20/51] Loss: 0.0008 LR: 0.000100
Epoch [266] Batch [30/51] Loss: 0.0006 LR: 0.000100
Epoch [266] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [266] Batch [50/51] Loss: 0.0003 LR: 0.000100

Epoch [267/350] - Time: 5.13s
Train Loss: 0.0048 | Val Loss: 9.1023
Train Acc: 0.9988 | Val Acc: 0.6331
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.897    Count 3: 0.907    Count 4: 0.829    Count 5: 0.775  
  Count 6: 0.459    Count 7: 0.240    Count 8: 0.121    Count 9: 0.167    Count 10: 0.534  
Epoch [267] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [267] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [267] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [267] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [267] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [267] Batch [50/51] Loss: 0.0004 LR: 0.000100

Epoch [268/350] - Time: 5.14s
Train Loss: 0.0163 | Val Loss: 9.0925
Train Acc: 0.9975 | Val Acc: 0.6173
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.929    Count 3: 0.907    Count 4: 0.762    Count 5: 0.775  
  Count 6: 0.449    Count 7: 0.229    Count 8: 0.010    Count 9: 0.222    Count 10: 0.455  
Epoch [268] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [268] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [268] Batch [20/51] Loss: 0.0017 LR: 0.000100
Epoch [268] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [268] Batch [40/51] Loss: 0.0263 LR: 0.000100
Epoch [268] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [269/350] - Time: 5.15s
Train Loss: 0.0144 | Val Loss: 9.3947
Train Acc: 0.9938 | Val Acc: 0.6117
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.921    Count 3: 0.898    Count 4: 0.743    Count 5: 0.647  
  Count 6: 0.480    Count 7: 0.208    Count 8: 0.172    Count 9: 0.122    Count 10: 0.489  
Epoch [269] Batch [0/51] Loss: 0.0001 LR: 0.000100
Epoch [269] Batch [10/51] Loss: 0.0013 LR: 0.000100
Epoch [269] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [269] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [269] Batch [40/51] Loss: 0.0215 LR: 0.000100
Epoch [269] Batch [50/51] Loss: 3.0343 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [270/350] - Time: 5.41s
Train Loss: 0.0705 | Val Loss: 9.0124
Train Acc: 0.9914 | Val Acc: 0.6080
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.905    Count 3: 0.898    Count 4: 0.876    Count 5: 0.510  
  Count 6: 0.245    Count 7: 0.365    Count 8: 0.172    Count 9: 0.156    Count 10: 0.523  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/single_image_checkpoint_epoch_269.pth
Epoch [270] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [270] Batch [10/51] Loss: 0.4238 LR: 0.000100
Epoch [270] Batch [20/51] Loss: 0.0002 LR: 0.000100
Epoch [270] Batch [30/51] Loss: 0.1974 LR: 0.000100
Epoch [270] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [270] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [271/350] - Time: 5.08s
Train Loss: 0.0447 | Val Loss: 8.1236
Train Acc: 0.9864 | Val Acc: 0.6192
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.921    Count 3: 0.889    Count 4: 0.714    Count 5: 0.637  
  Count 6: 0.500    Count 7: 0.198    Count 8: 0.162    Count 9: 0.300    Count 10: 0.466  
Epoch [271] Batch [0/51] Loss: 0.2081 LR: 0.000100
Epoch [271] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [271] Batch [20/51] Loss: 0.0011 LR: 0.000100
Epoch [271] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [271] Batch [40/51] Loss: 0.0003 LR: 0.000100
Epoch [271] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [272/350] - Time: 5.07s
Train Loss: 0.0226 | Val Loss: 8.1547
Train Acc: 0.9938 | Val Acc: 0.6397
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.937    Count 3: 0.917    Count 4: 0.857    Count 5: 0.618  
  Count 6: 0.439    Count 7: 0.240    Count 8: 0.081    Count 9: 0.333    Count 10: 0.591  
Epoch [272] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [272] Batch [10/51] Loss: 0.0016 LR: 0.000100
Epoch [272] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [272] Batch [30/51] Loss: 0.0001 LR: 0.000100
Epoch [272] Batch [40/51] Loss: 0.0008 LR: 0.000100
Epoch [272] Batch [50/51] Loss: 0.0653 LR: 0.000100

Epoch [273/350] - Time: 5.11s
Train Loss: 0.0877 | Val Loss: 8.3229
Train Acc: 0.9901 | Val Acc: 0.6359
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.988    Count 2: 0.921    Count 3: 0.833    Count 4: 0.771    Count 5: 0.686  
  Count 6: 0.673    Count 7: 0.156    Count 8: 0.111    Count 9: 0.233    Count 10: 0.602  
Epoch [273] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [273] Batch [10/51] Loss: 0.0001 LR: 0.000100
Epoch [273] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [273] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [273] Batch [40/51] Loss: 0.4930 LR: 0.000100
Epoch [273] Batch [50/51] Loss: 0.0055 LR: 0.000100

Epoch [274/350] - Time: 5.16s
Train Loss: 0.0803 | Val Loss: 8.4898
Train Acc: 0.9852 | Val Acc: 0.6276
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.929    Count 3: 0.889    Count 4: 0.857    Count 5: 0.598  
  Count 6: 0.449    Count 7: 0.156    Count 8: 0.101    Count 9: 0.322    Count 10: 0.580  
Epoch [274] Batch [0/51] Loss: 0.0410 LR: 0.000100
Epoch [274] Batch [10/51] Loss: 1.0129 LR: 0.000100
Epoch [274] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [274] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [274] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [274] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [275/350] - Time: 5.02s
Train Loss: 0.1218 | Val Loss: 8.1814
Train Acc: 0.9852 | Val Acc: 0.6434
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.921    Count 3: 0.907    Count 4: 0.867    Count 5: 0.667  
  Count 6: 0.418    Count 7: 0.281    Count 8: 0.182    Count 9: 0.167    Count 10: 0.636  
Epoch [275] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [275] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [275] Batch [20/51] Loss: 0.0012 LR: 0.000100
Epoch [275] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [275] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [275] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [276/350] - Time: 5.05s
Train Loss: 0.0023 | Val Loss: 7.3151
Train Acc: 1.0000 | Val Acc: 0.6359
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.921    Count 3: 0.889    Count 4: 0.752    Count 5: 0.745  
  Count 6: 0.531    Count 7: 0.198    Count 8: 0.010    Count 9: 0.389    Count 10: 0.545  
Epoch [276] Batch [0/51] Loss: 0.0274 LR: 0.000100
Epoch [276] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [276] Batch [20/51] Loss: 0.0053 LR: 0.000100
Epoch [276] Batch [30/51] Loss: 0.0002 LR: 0.000100
Epoch [276] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [276] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [277/350] - Time: 5.08s
Train Loss: 0.0552 | Val Loss: 7.9762
Train Acc: 0.9951 | Val Acc: 0.6322
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.913    Count 3: 0.889    Count 4: 0.829    Count 5: 0.696  
  Count 6: 0.571    Count 7: 0.198    Count 8: 0.111    Count 9: 0.267    Count 10: 0.443  
Epoch [277] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [277] Batch [10/51] Loss: 0.2255 LR: 0.000100
Epoch [277] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [277] Batch [30/51] Loss: 0.0170 LR: 0.000100
Epoch [277] Batch [40/51] Loss: 0.0002 LR: 0.000100
Epoch [277] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [278/350] - Time: 5.44s
Train Loss: 0.0180 | Val Loss: 7.9492
Train Acc: 0.9938 | Val Acc: 0.6387
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.929    Count 3: 0.889    Count 4: 0.829    Count 5: 0.598  
  Count 6: 0.520    Count 7: 0.312    Count 8: 0.121    Count 9: 0.222    Count 10: 0.580  
Epoch [278] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [278] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [278] Batch [20/51] Loss: 0.0002 LR: 0.000100
Epoch [278] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [278] Batch [40/51] Loss: 0.0006 LR: 0.000100
Epoch [278] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [279/350] - Time: 4.97s
Train Loss: 0.0069 | Val Loss: 7.9468
Train Acc: 0.9963 | Val Acc: 0.6415
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.960    Count 3: 0.898    Count 4: 0.829    Count 5: 0.686  
  Count 6: 0.561    Count 7: 0.198    Count 8: 0.020    Count 9: 0.356    Count 10: 0.500  
Epoch [279] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [279] Batch [10/51] Loss: 0.0001 LR: 0.000100
Epoch [279] Batch [20/51] Loss: 0.0014 LR: 0.000100
Epoch [279] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [279] Batch [40/51] Loss: 0.0001 LR: 0.000100
Epoch [279] Batch [50/51] Loss: 0.3351 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [280/350] - Time: 5.19s
Train Loss: 0.0352 | Val Loss: 7.7355
Train Acc: 0.9914 | Val Acc: 0.6304
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.944    Count 3: 0.917    Count 4: 0.790    Count 5: 0.647  
  Count 6: 0.418    Count 7: 0.208    Count 8: 0.121    Count 9: 0.222    Count 10: 0.636  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/single_image_checkpoint_epoch_279.pth
Epoch [280] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [280] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [280] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [280] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [280] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [280] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [281/350] - Time: 5.10s
Train Loss: 0.0074 | Val Loss: 7.5878
Train Acc: 0.9988 | Val Acc: 0.6350
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.913    Count 3: 0.880    Count 4: 0.781    Count 5: 0.618  
  Count 6: 0.612    Count 7: 0.240    Count 8: 0.131    Count 9: 0.189    Count 10: 0.591  
Epoch [281] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [281] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [281] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [281] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [281] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [281] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [282/350] - Time: 5.13s
Train Loss: 0.0162 | Val Loss: 7.2987
Train Acc: 0.9988 | Val Acc: 0.6499
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.937    Count 3: 0.907    Count 4: 0.810    Count 5: 0.725  
  Count 6: 0.643    Count 7: 0.219    Count 8: 0.111    Count 9: 0.233    Count 10: 0.511  
Epoch [282] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [282] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [282] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [282] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [282] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [282] Batch [50/51] Loss: 0.0161 LR: 0.000100

Epoch [283/350] - Time: 5.11s
Train Loss: 0.0002 | Val Loss: 7.4722
Train Acc: 1.0000 | Val Acc: 0.6434
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.921    Count 3: 0.880    Count 4: 0.819    Count 5: 0.588  
  Count 6: 0.602    Count 7: 0.198    Count 8: 0.081    Count 9: 0.311    Count 10: 0.659  
Epoch [283] Batch [0/51] Loss: 0.0001 LR: 0.000100
Epoch [283] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [283] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [283] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [283] Batch [40/51] Loss: 0.0002 LR: 0.000100
Epoch [283] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [284/350] - Time: 5.01s
Train Loss: 0.0020 | Val Loss: 7.6098
Train Acc: 0.9988 | Val Acc: 0.6453
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.944    Count 3: 0.907    Count 4: 0.857    Count 5: 0.755  
  Count 6: 0.541    Count 7: 0.229    Count 8: 0.091    Count 9: 0.200    Count 10: 0.511  
Epoch [284] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [284] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [284] Batch [20/51] Loss: 0.0019 LR: 0.000100
Epoch [284] Batch [30/51] Loss: 0.3401 LR: 0.000100
Epoch [284] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [284] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [285/350] - Time: 5.06s
Train Loss: 0.0204 | Val Loss: 7.9547
Train Acc: 0.9975 | Val Acc: 0.6294
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.913    Count 3: 0.889    Count 4: 0.876    Count 5: 0.735  
  Count 6: 0.490    Count 7: 0.125    Count 8: 0.020    Count 9: 0.333    Count 10: 0.500  
Epoch [285] Batch [0/51] Loss: 0.0012 LR: 0.000100
Epoch [285] Batch [10/51] Loss: 0.0002 LR: 0.000100
Epoch [285] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [285] Batch [30/51] Loss: 0.0497 LR: 0.000100
Epoch [285] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [285] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [286/350] - Time: 5.01s
Train Loss: 0.0526 | Val Loss: 7.6598
Train Acc: 0.9938 | Val Acc: 0.6425
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.913    Count 3: 0.880    Count 4: 0.886    Count 5: 0.725  
  Count 6: 0.551    Count 7: 0.208    Count 8: 0.111    Count 9: 0.267    Count 10: 0.489  
Epoch [286] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [286] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [286] Batch [20/51] Loss: 0.0002 LR: 0.000100
Epoch [286] Batch [30/51] Loss: 0.0001 LR: 0.000100
Epoch [286] Batch [40/51] Loss: 0.0002 LR: 0.000100
Epoch [286] Batch [50/51] Loss: 0.0033 LR: 0.000100

Epoch [287/350] - Time: 5.38s
Train Loss: 0.0013 | Val Loss: 7.6307
Train Acc: 0.9988 | Val Acc: 0.6518
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.937    Count 3: 0.907    Count 4: 0.876    Count 5: 0.667  
  Count 6: 0.541    Count 7: 0.240    Count 8: 0.081    Count 9: 0.333    Count 10: 0.557  
Epoch [287] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [287] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [287] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [287] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [287] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [287] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [288/350] - Time: 5.09s
Train Loss: 0.0050 | Val Loss: 8.1781
Train Acc: 0.9988 | Val Acc: 0.6285
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.921    Count 3: 0.889    Count 4: 0.705    Count 5: 0.549  
  Count 6: 0.592    Count 7: 0.240    Count 8: 0.081    Count 9: 0.367    Count 10: 0.568  
Epoch [288] Batch [0/51] Loss: 0.0001 LR: 0.000100
Epoch [288] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [288] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [288] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [288] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [288] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [289/350] - Time: 5.19s
Train Loss: 0.0150 | Val Loss: 8.1246
Train Acc: 0.9975 | Val Acc: 0.6378
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.937    Count 3: 0.907    Count 4: 0.838    Count 5: 0.608  
  Count 6: 0.561    Count 7: 0.219    Count 8: 0.121    Count 9: 0.278    Count 10: 0.511  
Epoch [289] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [289] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [289] Batch [20/51] Loss: 0.0001 LR: 0.000100
Epoch [289] Batch [30/51] Loss: 0.1226 LR: 0.000100
Epoch [289] Batch [40/51] Loss: 0.0026 LR: 0.000100
Epoch [289] Batch [50/51] Loss: 0.3232 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [290/350] - Time: 5.11s
Train Loss: 0.0551 | Val Loss: 8.6261
Train Acc: 0.9889 | Val Acc: 0.6257
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.952    Count 3: 0.917    Count 4: 0.838    Count 5: 0.647  
  Count 6: 0.459    Count 7: 0.281    Count 8: 0.051    Count 9: 0.222    Count 10: 0.455  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/single_image_checkpoint_epoch_289.pth
Epoch [290] Batch [0/51] Loss: 0.0002 LR: 0.000100
Epoch [290] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [290] Batch [20/51] Loss: 0.0763 LR: 0.000100
Epoch [290] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [290] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [290] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [291/350] - Time: 5.09s
Train Loss: 0.0129 | Val Loss: 7.8579
Train Acc: 0.9951 | Val Acc: 0.6350
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.937    Count 3: 0.898    Count 4: 0.781    Count 5: 0.716  
  Count 6: 0.571    Count 7: 0.198    Count 8: 0.071    Count 9: 0.300    Count 10: 0.466  
Epoch [291] Batch [0/51] Loss: 0.0001 LR: 0.000100
Epoch [291] Batch [10/51] Loss: 0.0001 LR: 0.000100
Epoch [291] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [291] Batch [30/51] Loss: 0.0008 LR: 0.000100
Epoch [291] Batch [40/51] Loss: 0.0001 LR: 0.000100
Epoch [291] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [292/350] - Time: 5.15s
Train Loss: 0.0019 | Val Loss: 8.1400
Train Acc: 0.9988 | Val Acc: 0.6276
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.929    Count 3: 0.889    Count 4: 0.724    Count 5: 0.696  
  Count 6: 0.602    Count 7: 0.260    Count 8: 0.111    Count 9: 0.200    Count 10: 0.443  
Epoch [292] Batch [0/51] Loss: 0.0002 LR: 0.000100
Epoch [292] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [292] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [292] Batch [30/51] Loss: 0.0001 LR: 0.000100
Epoch [292] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [292] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [293/350] - Time: 5.14s
Train Loss: 0.0133 | Val Loss: 8.5001
Train Acc: 0.9975 | Val Acc: 0.6201
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.929    Count 3: 0.898    Count 4: 0.838    Count 5: 0.637  
  Count 6: 0.480    Count 7: 0.250    Count 8: 0.111    Count 9: 0.167    Count 10: 0.455  
Epoch [293] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [293] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [293] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [293] Batch [30/51] Loss: 0.0026 LR: 0.000100
Epoch [293] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [293] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [294/350] - Time: 5.02s
Train Loss: 0.0144 | Val Loss: 8.8339
Train Acc: 0.9938 | Val Acc: 0.6341
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.913    Count 3: 0.898    Count 4: 0.819    Count 5: 0.667  
  Count 6: 0.469    Count 7: 0.219    Count 8: 0.131    Count 9: 0.244    Count 10: 0.580  
Epoch [294] Batch [0/51] Loss: 0.0002 LR: 0.000100
Epoch [294] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [294] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [294] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [294] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [294] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [295/350] - Time: 5.34s
Train Loss: 0.0231 | Val Loss: 9.7019
Train Acc: 0.9938 | Val Acc: 0.5987
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.988    Count 2: 0.897    Count 3: 0.769    Count 4: 0.695    Count 5: 0.490  
  Count 6: 0.582    Count 7: 0.271    Count 8: 0.172    Count 9: 0.133    Count 10: 0.591  
Epoch [295] Batch [0/51] Loss: 0.0528 LR: 0.000100
Epoch [295] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [295] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [295] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [295] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [295] Batch [50/51] Loss: 0.0001 LR: 0.000100

Epoch [296/350] - Time: 5.16s
Train Loss: 0.0151 | Val Loss: 7.7846
Train Acc: 0.9975 | Val Acc: 0.6387
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.937    Count 3: 0.889    Count 4: 0.838    Count 5: 0.686  
  Count 6: 0.622    Count 7: 0.219    Count 8: 0.071    Count 9: 0.233    Count 10: 0.477  
Epoch [296] Batch [0/51] Loss: 0.0179 LR: 0.000100
Epoch [296] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [296] Batch [20/51] Loss: 0.0006 LR: 0.000100
Epoch [296] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [296] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [296] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [297/350] - Time: 5.16s
Train Loss: 0.0208 | Val Loss: 7.4780
Train Acc: 0.9975 | Val Acc: 0.6387
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.952    Count 3: 0.907    Count 4: 0.857    Count 5: 0.676  
  Count 6: 0.510    Count 7: 0.240    Count 8: 0.111    Count 9: 0.211    Count 10: 0.500  
Epoch [297] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [297] Batch [10/51] Loss: 0.0904 LR: 0.000100
Epoch [297] Batch [20/51] Loss: 0.8067 LR: 0.000100
Epoch [297] Batch [30/51] Loss: 0.0005 LR: 0.000100
Epoch [297] Batch [40/51] Loss: 0.0001 LR: 0.000100
Epoch [297] Batch [50/51] Loss: 0.0002 LR: 0.000100

Epoch [298/350] - Time: 5.08s
Train Loss: 0.0394 | Val Loss: 7.4173
Train Acc: 0.9889 | Val Acc: 0.6276
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.960    Count 3: 0.898    Count 4: 0.810    Count 5: 0.706  
  Count 6: 0.551    Count 7: 0.115    Count 8: 0.111    Count 9: 0.267    Count 10: 0.420  
Epoch [298] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [298] Batch [10/51] Loss: 0.0004 LR: 0.000100
Epoch [298] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [298] Batch [30/51] Loss: 0.0001 LR: 0.000100
Epoch [298] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [298] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [299/350] - Time: 5.08s
Train Loss: 0.0055 | Val Loss: 7.3565
Train Acc: 0.9975 | Val Acc: 0.6359
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.929    Count 3: 0.880    Count 4: 0.819    Count 5: 0.696  
  Count 6: 0.551    Count 7: 0.219    Count 8: 0.040    Count 9: 0.233    Count 10: 0.591  
Epoch [299] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [299] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [299] Batch [20/51] Loss: 0.0008 LR: 0.000100
Epoch [299] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [299] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [299] Batch [50/51] Loss: 0.0001 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [300/350] - Time: 5.03s
Train Loss: 0.0000 | Val Loss: 7.1239
Train Acc: 1.0000 | Val Acc: 0.6499
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.944    Count 3: 0.889    Count 4: 0.810    Count 5: 0.735  
  Count 6: 0.643    Count 7: 0.188    Count 8: 0.040    Count 9: 0.311    Count 10: 0.545  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/single_image_checkpoint_epoch_299.pth
Epoch [300] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [300] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [300] Batch [20/51] Loss: 0.0079 LR: 0.000100
Epoch [300] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [300] Batch [40/51] Loss: 0.2048 LR: 0.000100
Epoch [300] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [301/350] - Time: 5.15s
Train Loss: 0.0254 | Val Loss: 7.5501
Train Acc: 0.9938 | Val Acc: 0.6490
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.905    Count 3: 0.907    Count 4: 0.867    Count 5: 0.696  
  Count 6: 0.622    Count 7: 0.188    Count 8: 0.051    Count 9: 0.400    Count 10: 0.466  
Epoch [301] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [301] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [301] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [301] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [301] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [301] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [302/350] - Time: 5.09s
Train Loss: 0.0044 | Val Loss: 7.8042
Train Acc: 0.9988 | Val Acc: 0.6583
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.929    Count 3: 0.880    Count 4: 0.876    Count 5: 0.716  
  Count 6: 0.551    Count 7: 0.219    Count 8: 0.081    Count 9: 0.300    Count 10: 0.670  
Epoch [302] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [302] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [302] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [302] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [302] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [302] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [303/350] - Time: 5.54s
Train Loss: 0.0001 | Val Loss: 8.0322
Train Acc: 1.0000 | Val Acc: 0.6536
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.937    Count 3: 0.889    Count 4: 0.876    Count 5: 0.676  
  Count 6: 0.561    Count 7: 0.208    Count 8: 0.091    Count 9: 0.289    Count 10: 0.636  
Epoch [303] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [303] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [303] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [303] Batch [30/51] Loss: 0.0003 LR: 0.000100
Epoch [303] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [303] Batch [50/51] Loss: 0.0010 LR: 0.000100

Epoch [304/350] - Time: 5.15s
Train Loss: 0.0206 | Val Loss: 7.5476
Train Acc: 0.9963 | Val Acc: 0.6387
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.889    Count 3: 0.889    Count 4: 0.848    Count 5: 0.608  
  Count 6: 0.612    Count 7: 0.240    Count 8: 0.141    Count 9: 0.244    Count 10: 0.534  
Epoch [304] Batch [0/51] Loss: 0.2983 LR: 0.000100
Epoch [304] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [304] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [304] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [304] Batch [40/51] Loss: 0.0009 LR: 0.000100
Epoch [304] Batch [50/51] Loss: 0.0003 LR: 0.000100

Epoch [305/350] - Time: 5.10s
Train Loss: 0.0290 | Val Loss: 8.1025
Train Acc: 0.9901 | Val Acc: 0.6201
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.952    Count 3: 0.889    Count 4: 0.857    Count 5: 0.588  
  Count 6: 0.449    Count 7: 0.240    Count 8: 0.111    Count 9: 0.133    Count 10: 0.545  
Epoch [305] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [305] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [305] Batch [20/51] Loss: 0.9352 LR: 0.000100
Epoch [305] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [305] Batch [40/51] Loss: 0.0259 LR: 0.000100
Epoch [305] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [306/350] - Time: 5.07s
Train Loss: 0.0382 | Val Loss: 8.6844
Train Acc: 0.9938 | Val Acc: 0.6285
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.913    Count 3: 0.870    Count 4: 0.876    Count 5: 0.637  
  Count 6: 0.694    Count 7: 0.208    Count 8: 0.121    Count 9: 0.133    Count 10: 0.409  
Epoch [306] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [306] Batch [10/51] Loss: 0.1282 LR: 0.000100
Epoch [306] Batch [20/51] Loss: 0.0148 LR: 0.000100
Epoch [306] Batch [30/51] Loss: 0.0046 LR: 0.000100
Epoch [306] Batch [40/51] Loss: 0.0173 LR: 0.000100
Epoch [306] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [307/350] - Time: 5.12s
Train Loss: 0.0970 | Val Loss: 8.3168
Train Acc: 0.9840 | Val Acc: 0.5950
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.960    Count 3: 0.917    Count 4: 0.819    Count 5: 0.569  
  Count 6: 0.388    Count 7: 0.188    Count 8: 0.091    Count 9: 0.111    Count 10: 0.443  
Epoch [307] Batch [0/51] Loss: 0.0206 LR: 0.000100
Epoch [307] Batch [10/51] Loss: 0.0067 LR: 0.000100
Epoch [307] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [307] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [307] Batch [40/51] Loss: 0.0001 LR: 0.000100
Epoch [307] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [308/350] - Time: 5.12s
Train Loss: 0.0166 | Val Loss: 8.6384
Train Acc: 0.9963 | Val Acc: 0.6192
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.937    Count 3: 0.898    Count 4: 0.857    Count 5: 0.608  
  Count 6: 0.531    Count 7: 0.177    Count 8: 0.172    Count 9: 0.111    Count 10: 0.466  
Epoch [308] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [308] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [308] Batch [20/51] Loss: 0.0001 LR: 0.000100
Epoch [308] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [308] Batch [40/51] Loss: 0.0003 LR: 0.000100
Epoch [308] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [309/350] - Time: 5.09s
Train Loss: 0.0093 | Val Loss: 8.1140
Train Acc: 0.9963 | Val Acc: 0.6257
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.944    Count 3: 0.898    Count 4: 0.857    Count 5: 0.637  
  Count 6: 0.439    Count 7: 0.208    Count 8: 0.071    Count 9: 0.222    Count 10: 0.568  
Epoch [309] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [309] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [309] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [309] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [309] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [309] Batch [50/51] Loss: 0.0003 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [310/350] - Time: 5.08s
Train Loss: 0.0032 | Val Loss: 7.5950
Train Acc: 0.9975 | Val Acc: 0.6220
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.944    Count 3: 0.907    Count 4: 0.829    Count 5: 0.637  
  Count 6: 0.510    Count 7: 0.219    Count 8: 0.071    Count 9: 0.178    Count 10: 0.489  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/single_image_checkpoint_epoch_309.pth
Epoch [310] Batch [0/51] Loss: 0.0003 LR: 0.000100
Epoch [310] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [310] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [310] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [310] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [310] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [311/350] - Time: 5.11s
Train Loss: 0.0130 | Val Loss: 7.8896
Train Acc: 0.9951 | Val Acc: 0.6397
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.968    Count 3: 0.898    Count 4: 0.848    Count 5: 0.647  
  Count 6: 0.571    Count 7: 0.177    Count 8: 0.000    Count 9: 0.344    Count 10: 0.534  
Epoch [311] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [311] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [311] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [311] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [311] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [311] Batch [50/51] Loss: 0.0004 LR: 0.000100

Epoch [312/350] - Time: 5.34s
Train Loss: 0.0584 | Val Loss: 8.4074
Train Acc: 0.9864 | Val Acc: 0.6182
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.921    Count 3: 0.861    Count 4: 0.800    Count 5: 0.627  
  Count 6: 0.439    Count 7: 0.312    Count 8: 0.172    Count 9: 0.122    Count 10: 0.511  
Epoch [312] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [312] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [312] Batch [20/51] Loss: 0.0311 LR: 0.000100
Epoch [312] Batch [30/51] Loss: 0.2380 LR: 0.000100
Epoch [312] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [312] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [313/350] - Time: 5.15s
Train Loss: 0.0060 | Val Loss: 8.2342
Train Acc: 0.9988 | Val Acc: 0.6266
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.960    Count 3: 0.889    Count 4: 0.848    Count 5: 0.686  
  Count 6: 0.439    Count 7: 0.271    Count 8: 0.162    Count 9: 0.111    Count 10: 0.466  
Epoch [313] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [313] Batch [10/51] Loss: 0.0073 LR: 0.000100
Epoch [313] Batch [20/51] Loss: 0.0001 LR: 0.000100
Epoch [313] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [313] Batch [40/51] Loss: 0.0006 LR: 0.000100
Epoch [313] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [314/350] - Time: 5.10s
Train Loss: 0.0013 | Val Loss: 8.2689
Train Acc: 1.0000 | Val Acc: 0.6331
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.937    Count 3: 0.880    Count 4: 0.810    Count 5: 0.627  
  Count 6: 0.582    Count 7: 0.229    Count 8: 0.182    Count 9: 0.200    Count 10: 0.477  
Epoch [314] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [314] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [314] Batch [20/51] Loss: 0.0007 LR: 0.000100
Epoch [314] Batch [30/51] Loss: 0.0003 LR: 0.000100
Epoch [314] Batch [40/51] Loss: 0.0005 LR: 0.000100
Epoch [314] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [315/350] - Time: 5.04s
Train Loss: 0.0014 | Val Loss: 8.7762
Train Acc: 0.9988 | Val Acc: 0.6359
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.981    Count 2: 0.929    Count 3: 0.852    Count 4: 0.867    Count 5: 0.618  
  Count 6: 0.510    Count 7: 0.229    Count 8: 0.051    Count 9: 0.300    Count 10: 0.648  
Epoch [315] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [315] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [315] Batch [20/51] Loss: 0.0007 LR: 0.000100
Epoch [315] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [315] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [315] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [316/350] - Time: 5.07s
Train Loss: 0.0132 | Val Loss: 8.6324
Train Acc: 0.9988 | Val Acc: 0.6145
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.937    Count 3: 0.926    Count 4: 0.829    Count 5: 0.598  
  Count 6: 0.510    Count 7: 0.188    Count 8: 0.071    Count 9: 0.200    Count 10: 0.455  
Epoch [316] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [316] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [316] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [316] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [316] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [316] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [317/350] - Time: 5.16s
Train Loss: 0.0046 | Val Loss: 8.1003
Train Acc: 0.9975 | Val Acc: 0.6276
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.960    Count 3: 0.917    Count 4: 0.848    Count 5: 0.559  
  Count 6: 0.622    Count 7: 0.167    Count 8: 0.081    Count 9: 0.244    Count 10: 0.455  
Epoch [317] Batch [0/51] Loss: 0.0002 LR: 0.000100
Epoch [317] Batch [10/51] Loss: 0.3936 LR: 0.000100
Epoch [317] Batch [20/51] Loss: 0.0016 LR: 0.000100
Epoch [317] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [317] Batch [40/51] Loss: 0.0044 LR: 0.000100
Epoch [317] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [318/350] - Time: 5.00s
Train Loss: 0.0270 | Val Loss: 7.7172
Train Acc: 0.9951 | Val Acc: 0.6173
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.937    Count 3: 0.898    Count 4: 0.810    Count 5: 0.657  
  Count 6: 0.459    Count 7: 0.198    Count 8: 0.000    Count 9: 0.189    Count 10: 0.602  
Epoch [318] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [318] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [318] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [318] Batch [30/51] Loss: 0.0001 LR: 0.000100
Epoch [318] Batch [40/51] Loss: 0.0001 LR: 0.000100
Epoch [318] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [319/350] - Time: 5.00s
Train Loss: 0.0119 | Val Loss: 7.2399
Train Acc: 0.9988 | Val Acc: 0.6425
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.944    Count 3: 0.907    Count 4: 0.733    Count 5: 0.676  
  Count 6: 0.602    Count 7: 0.208    Count 8: 0.051    Count 9: 0.300    Count 10: 0.625  
Epoch [319] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [319] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [319] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [319] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [319] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [319] Batch [50/51] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [320/350] - Time: 5.34s
Train Loss: 0.0176 | Val Loss: 7.9522
Train Acc: 0.9963 | Val Acc: 0.6043
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.929    Count 3: 0.926    Count 4: 0.790    Count 5: 0.588  
  Count 6: 0.296    Count 7: 0.271    Count 8: 0.141    Count 9: 0.100    Count 10: 0.568  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/single_image_checkpoint_epoch_319.pth
Epoch [320] Batch [0/51] Loss: 0.0001 LR: 0.000100
Epoch [320] Batch [10/51] Loss: 0.0001 LR: 0.000100
Epoch [320] Batch [20/51] Loss: 0.0009 LR: 0.000100
Epoch [320] Batch [30/51] Loss: 0.0072 LR: 0.000100
Epoch [320] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [320] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [321/350] - Time: 5.11s
Train Loss: 0.0095 | Val Loss: 8.0036
Train Acc: 0.9975 | Val Acc: 0.6238
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.905    Count 3: 0.880    Count 4: 0.848    Count 5: 0.627  
  Count 6: 0.469    Count 7: 0.281    Count 8: 0.020    Count 9: 0.167    Count 10: 0.648  
Epoch [321] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [321] Batch [10/51] Loss: 0.0001 LR: 0.000100
Epoch [321] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [321] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [321] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [321] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [322/350] - Time: 5.09s
Train Loss: 0.0021 | Val Loss: 7.9981
Train Acc: 0.9988 | Val Acc: 0.6108
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.975    Count 2: 0.881    Count 3: 0.750    Count 4: 0.657    Count 5: 0.686  
  Count 6: 0.694    Count 7: 0.229    Count 8: 0.061    Count 9: 0.278    Count 10: 0.523  
Epoch [322] Batch [0/51] Loss: 0.0016 LR: 0.000100
Epoch [322] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [322] Batch [20/51] Loss: 0.9282 LR: 0.000100
Epoch [322] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [322] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [322] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [323/350] - Time: 5.19s
Train Loss: 0.0277 | Val Loss: 9.4136
Train Acc: 0.9975 | Val Acc: 0.6257
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.921    Count 3: 0.898    Count 4: 0.848    Count 5: 0.618  
  Count 6: 0.357    Count 7: 0.479    Count 8: 0.111    Count 9: 0.000    Count 10: 0.614  
Epoch [323] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [323] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [323] Batch [20/51] Loss: 0.0716 LR: 0.000100
Epoch [323] Batch [30/51] Loss: 0.0205 LR: 0.000100
Epoch [323] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [323] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [324/350] - Time: 5.05s
Train Loss: 0.0259 | Val Loss: 9.6992
Train Acc: 0.9926 | Val Acc: 0.6071
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.952    Count 3: 0.917    Count 4: 0.838    Count 5: 0.735  
  Count 6: 0.337    Count 7: 0.208    Count 8: 0.020    Count 9: 0.267    Count 10: 0.341  
Epoch [324] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [324] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [324] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [324] Batch [30/51] Loss: 0.0008 LR: 0.000100
Epoch [324] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [324] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [325/350] - Time: 5.38s
Train Loss: 0.0207 | Val Loss: 8.6918
Train Acc: 0.9963 | Val Acc: 0.6304
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.929    Count 3: 0.907    Count 4: 0.838    Count 5: 0.627  
  Count 6: 0.490    Count 7: 0.208    Count 8: 0.172    Count 9: 0.178    Count 10: 0.545  
Epoch [325] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [325] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [325] Batch [20/51] Loss: 0.0001 LR: 0.000100
Epoch [325] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [325] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [325] Batch [50/51] Loss: 0.0002 LR: 0.000100

Epoch [326/350] - Time: 5.10s
Train Loss: 0.0005 | Val Loss: 8.9513
Train Acc: 1.0000 | Val Acc: 0.6248
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.988    Count 2: 0.897    Count 3: 0.861    Count 4: 0.657    Count 5: 0.608  
  Count 6: 0.684    Count 7: 0.208    Count 8: 0.172    Count 9: 0.222    Count 10: 0.568  
Epoch [326] Batch [0/51] Loss: 0.0005 LR: 0.000100
Epoch [326] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [326] Batch [20/51] Loss: 0.0035 LR: 0.000100
Epoch [326] Batch [30/51] Loss: 0.4376 LR: 0.000100
Epoch [326] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [326] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [327/350] - Time: 5.07s
Train Loss: 0.0113 | Val Loss: 9.4334
Train Acc: 0.9975 | Val Acc: 0.6173
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.981    Count 2: 0.897    Count 3: 0.870    Count 4: 0.819    Count 5: 0.598  
  Count 6: 0.459    Count 7: 0.198    Count 8: 0.101    Count 9: 0.211    Count 10: 0.648  
Epoch [327] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [327] Batch [10/51] Loss: 0.0034 LR: 0.000100
Epoch [327] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [327] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [327] Batch [40/51] Loss: 0.4052 LR: 0.000100
Epoch [327] Batch [50/51] Loss: 0.4347 LR: 0.000100

Epoch [328/350] - Time: 5.26s
Train Loss: 0.0149 | Val Loss: 9.4685
Train Acc: 0.9951 | Val Acc: 0.6043
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.988    Count 2: 0.889    Count 3: 0.722    Count 4: 0.752    Count 5: 0.627  
  Count 6: 0.694    Count 7: 0.188    Count 8: 0.051    Count 9: 0.289    Count 10: 0.443  
Epoch [328] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [328] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [328] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [328] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [328] Batch [40/51] Loss: 0.0474 LR: 0.000100
Epoch [328] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [329/350] - Time: 5.65s
Train Loss: 0.0397 | Val Loss: 10.2628
Train Acc: 0.9926 | Val Acc: 0.5708
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.929    Count 3: 0.898    Count 4: 0.667    Count 5: 0.490  
  Count 6: 0.327    Count 7: 0.229    Count 8: 0.111    Count 9: 0.178    Count 10: 0.409  
Epoch [329] Batch [0/51] Loss: 0.2423 LR: 0.000100
Epoch [329] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [329] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [329] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [329] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [329] Batch [50/51] Loss: 0.0001 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [330/350] - Time: 5.01s
Train Loss: 0.0708 | Val Loss: 9.6640
Train Acc: 0.9827 | Val Acc: 0.5475
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.907    Count 2: 0.746    Count 3: 0.574    Count 4: 0.752    Count 5: 0.637  
  Count 6: 0.541    Count 7: 0.250    Count 8: 0.010    Count 9: 0.267    Count 10: 0.443  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/single_image_checkpoint_epoch_329.pth
Epoch [330] Batch [0/51] Loss: 0.0001 LR: 0.000100
Epoch [330] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [330] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [330] Batch [30/51] Loss: 0.0001 LR: 0.000100
Epoch [330] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [330] Batch [50/51] Loss: 0.0452 LR: 0.000100

Epoch [331/350] - Time: 5.12s
Train Loss: 0.0239 | Val Loss: 8.7416
Train Acc: 0.9938 | Val Acc: 0.6182
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.988    Count 2: 0.952    Count 3: 0.926    Count 4: 0.838    Count 5: 0.657  
  Count 6: 0.439    Count 7: 0.240    Count 8: 0.152    Count 9: 0.111    Count 10: 0.432  
Epoch [331] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [331] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [331] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [331] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [331] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [331] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [332/350] - Time: 5.12s
Train Loss: 0.0098 | Val Loss: 9.2012
Train Acc: 0.9975 | Val Acc: 0.6136
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.975    Count 2: 0.905    Count 3: 0.870    Count 4: 0.838    Count 5: 0.627  
  Count 6: 0.449    Count 7: 0.198    Count 8: 0.182    Count 9: 0.133    Count 10: 0.545  
Epoch [332] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [332] Batch [10/51] Loss: 0.0010 LR: 0.000100
Epoch [332] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [332] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [332] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [332] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [333/350] - Time: 5.10s
Train Loss: 0.0047 | Val Loss: 9.2522
Train Acc: 0.9988 | Val Acc: 0.6155
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.873    Count 3: 0.907    Count 4: 0.733    Count 5: 0.627  
  Count 6: 0.490    Count 7: 0.177    Count 8: 0.030    Count 9: 0.356    Count 10: 0.580  
Epoch [333] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [333] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [333] Batch [20/51] Loss: 0.0002 LR: 0.000100
Epoch [333] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [333] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [333] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [334/350] - Time: 5.01s
Train Loss: 0.0166 | Val Loss: 8.8836
Train Acc: 0.9975 | Val Acc: 0.6248
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.952    Count 3: 0.907    Count 4: 0.886    Count 5: 0.588  
  Count 6: 0.347    Count 7: 0.198    Count 8: 0.141    Count 9: 0.244    Count 10: 0.557  
Epoch [334] Batch [0/51] Loss: 0.0001 LR: 0.000100
Epoch [334] Batch [10/51] Loss: 0.0009 LR: 0.000100
Epoch [334] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [334] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [334] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [334] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [335/350] - Time: 5.03s
Train Loss: 0.0071 | Val Loss: 8.8388
Train Acc: 0.9963 | Val Acc: 0.6127
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.929    Count 3: 0.907    Count 4: 0.762    Count 5: 0.637  
  Count 6: 0.469    Count 7: 0.146    Count 8: 0.081    Count 9: 0.267    Count 10: 0.500  
Epoch [335] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [335] Batch [10/51] Loss: 0.0118 LR: 0.000100
Epoch [335] Batch [20/51] Loss: 0.1474 LR: 0.000100
Epoch [335] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [335] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [335] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [336/350] - Time: 5.00s
Train Loss: 0.0721 | Val Loss: 8.7523
Train Acc: 0.9840 | Val Acc: 0.6164
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.929    Count 3: 0.907    Count 4: 0.819    Count 5: 0.696  
  Count 6: 0.337    Count 7: 0.250    Count 8: 0.101    Count 9: 0.200    Count 10: 0.489  
Epoch [336] Batch [0/51] Loss: 0.0005 LR: 0.000100
Epoch [336] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [336] Batch [20/51] Loss: 0.0015 LR: 0.000100
Epoch [336] Batch [30/51] Loss: 0.0001 LR: 0.000100
Epoch [336] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [336] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [337/350] - Time: 5.43s
Train Loss: 0.0284 | Val Loss: 9.5558
Train Acc: 0.9951 | Val Acc: 0.6061
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.988    Count 2: 0.841    Count 3: 0.861    Count 4: 0.676    Count 5: 0.667  
  Count 6: 0.633    Count 7: 0.229    Count 8: 0.020    Count 9: 0.267    Count 10: 0.489  
Epoch [337] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [337] Batch [10/51] Loss: 0.0004 LR: 0.000100
Epoch [337] Batch [20/51] Loss: 0.0001 LR: 0.000100
Epoch [337] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [337] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [337] Batch [50/51] Loss: 0.0004 LR: 0.000100

Epoch [338/350] - Time: 5.09s
Train Loss: 0.0017 | Val Loss: 8.1664
Train Acc: 0.9988 | Val Acc: 0.6508
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.988    Count 2: 0.968    Count 3: 0.935    Count 4: 0.867    Count 5: 0.627  
  Count 6: 0.602    Count 7: 0.250    Count 8: 0.071    Count 9: 0.233    Count 10: 0.568  
Epoch [338] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [338] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [338] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [338] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [338] Batch [40/51] Loss: 0.8296 LR: 0.000100
Epoch [338] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [339/350] - Time: 5.09s
Train Loss: 0.0430 | Val Loss: 7.6485
Train Acc: 0.9951 | Val Acc: 0.6536
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.921    Count 3: 0.926    Count 4: 0.876    Count 5: 0.647  
  Count 6: 0.520    Count 7: 0.219    Count 8: 0.152    Count 9: 0.278    Count 10: 0.614  
Epoch [339] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [339] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [339] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [339] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [339] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [339] Batch [50/51] Loss: 0.0002 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [340/350] - Time: 5.11s
Train Loss: 0.0296 | Val Loss: 7.5339
Train Acc: 0.9963 | Val Acc: 0.6453
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.988    Count 2: 0.960    Count 3: 0.917    Count 4: 0.857    Count 5: 0.618  
  Count 6: 0.531    Count 7: 0.260    Count 8: 0.051    Count 9: 0.311    Count 10: 0.568  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/single_image_checkpoint_epoch_339.pth
Epoch [340] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [340] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [340] Batch [20/51] Loss: 0.2171 LR: 0.000100
Epoch [340] Batch [30/51] Loss: 0.0003 LR: 0.000100
Epoch [340] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [340] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [341/350] - Time: 5.03s
Train Loss: 0.0231 | Val Loss: 7.7818
Train Acc: 0.9951 | Val Acc: 0.6564
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.988    Count 2: 0.937    Count 3: 0.898    Count 4: 0.848    Count 5: 0.647  
  Count 6: 0.622    Count 7: 0.260    Count 8: 0.101    Count 9: 0.333    Count 10: 0.557  
Epoch [341] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [341] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [341] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [341] Batch [30/51] Loss: 0.0002 LR: 0.000100
Epoch [341] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [341] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [342/350] - Time: 4.99s
Train Loss: 0.0032 | Val Loss: 8.0113
Train Acc: 0.9988 | Val Acc: 0.6369
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.929    Count 3: 0.907    Count 4: 0.810    Count 5: 0.647  
  Count 6: 0.592    Count 7: 0.188    Count 8: 0.030    Count 9: 0.367    Count 10: 0.511  
Epoch [342] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [342] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [342] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [342] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [342] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [342] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [343/350] - Time: 5.03s
Train Loss: 0.0000 | Val Loss: 8.1029
Train Acc: 1.0000 | Val Acc: 0.6359
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.937    Count 3: 0.907    Count 4: 0.819    Count 5: 0.637  
  Count 6: 0.571    Count 7: 0.198    Count 8: 0.040    Count 9: 0.367    Count 10: 0.489  
Epoch [343] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [343] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [343] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [343] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [343] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [343] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [344/350] - Time: 5.09s
Train Loss: 0.0002 | Val Loss: 9.2775
Train Acc: 1.0000 | Val Acc: 0.6238
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.937    Count 3: 0.917    Count 4: 0.848    Count 5: 0.618  
  Count 6: 0.429    Count 7: 0.198    Count 8: 0.051    Count 9: 0.267    Count 10: 0.557  
Epoch [344] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [344] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [344] Batch [20/51] Loss: 0.0004 LR: 0.000100
Epoch [344] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [344] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [344] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [345/350] - Time: 5.49s
Train Loss: 0.0093 | Val Loss: 9.1985
Train Acc: 0.9975 | Val Acc: 0.6248
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.981    Count 2: 0.905    Count 3: 0.870    Count 4: 0.838    Count 5: 0.618  
  Count 6: 0.561    Count 7: 0.271    Count 8: 0.081    Count 9: 0.200    Count 10: 0.523  
Epoch [345] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [345] Batch [10/51] Loss: 0.0001 LR: 0.000100
Epoch [345] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [345] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [345] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [345] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [346/350] - Time: 5.09s
Train Loss: 0.0035 | Val Loss: 9.1363
Train Acc: 0.9988 | Val Acc: 0.6285
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.921    Count 3: 0.889    Count 4: 0.886    Count 5: 0.608  
  Count 6: 0.449    Count 7: 0.240    Count 8: 0.121    Count 9: 0.178    Count 10: 0.591  
Epoch [346] Batch [0/51] Loss: 0.0109 LR: 0.000100
Epoch [346] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [346] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [346] Batch [30/51] Loss: 0.0002 LR: 0.000100
Epoch [346] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [346] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [347/350] - Time: 5.00s
Train Loss: 0.0003 | Val Loss: 8.5912
Train Acc: 1.0000 | Val Acc: 0.6369
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.929    Count 3: 0.889    Count 4: 0.848    Count 5: 0.608  
  Count 6: 0.561    Count 7: 0.250    Count 8: 0.101    Count 9: 0.200    Count 10: 0.580  
Epoch [347] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [347] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [347] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [347] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [347] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [347] Batch [50/51] Loss: 0.0000 LR: 0.000100

Epoch [348/350] - Time: 5.09s
Train Loss: 0.0070 | Val Loss: 8.0054
Train Acc: 0.9988 | Val Acc: 0.6499
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.968    Count 3: 0.907    Count 4: 0.876    Count 5: 0.637  
  Count 6: 0.643    Count 7: 0.198    Count 8: 0.091    Count 9: 0.256    Count 10: 0.511  
Epoch [348] Batch [0/51] Loss: 0.0000 LR: 0.000100
Epoch [348] Batch [10/51] Loss: 0.0000 LR: 0.000100
Epoch [348] Batch [20/51] Loss: 0.0000 LR: 0.000100
Epoch [348] Batch [30/51] Loss: 0.0604 LR: 0.000100
Epoch [348] Batch [40/51] Loss: 0.0000 LR: 0.000100
Epoch [348] Batch [50/51] Loss: 0.0008 LR: 0.000100

Epoch [349/350] - Time: 5.17s
Train Loss: 0.0014 | Val Loss: 9.6214
Train Acc: 0.9988 | Val Acc: 0.6266
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.952    Count 3: 0.935    Count 4: 0.867    Count 5: 0.686  
  Count 6: 0.510    Count 7: 0.115    Count 8: 0.000    Count 9: 0.400    Count 10: 0.364  
Epoch [349] Batch [0/51] Loss: 0.0008 LR: 0.000100
Epoch [349] Batch [10/51] Loss: 0.0010 LR: 0.000100
Epoch [349] Batch [20/51] Loss: 0.1221 LR: 0.000100
Epoch [349] Batch [30/51] Loss: 0.0000 LR: 0.000100
Epoch [349] Batch [40/51] Loss: 0.7710 LR: 0.000100
Epoch [349] Batch [50/51] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [350/350] - Time: 5.04s
Train Loss: 0.0521 | Val Loss: 9.7013
Train Acc: 0.9877 | Val Acc: 0.6127
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.921    Count 3: 0.880    Count 4: 0.781    Count 5: 0.569  
  Count 6: 0.612    Count 7: 0.177    Count 8: 0.000    Count 9: 0.378    Count 10: 0.386  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/single_image_checkpoint_epoch_349.pth
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/final_single_image_model.pth

训练完成!
最佳验证准确率: 0.6648
最佳验证损失: 5.1566
最终混淆矩阵保存到: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/final_confusion_matrix.png
详细报告保存到: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/20data/check_points/classification_report.txt

训练成功完成！
程序结束。
=== 完成 ===
结束时间: Sun 20 Jul 00:45:38 BST 2025
