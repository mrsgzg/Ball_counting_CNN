=== 作业信息 ===
节点: node802
GPU: 1 (ID: 0)
CPU核心: 4
内存: 20480 MB
开始时间: Sun 20 Jul 01:34:24 BST 2025
=== 激活环境 ===
/var/spool/slurmd/job3583393/slurm_script: line 25: --version: command not found
Python版本: 
当前环境: cgtest
=== 开始训练 ===
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:279: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(test_path, map_location=self.device)
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')
============================================================
单图像分类模型训练配置
============================================================
基础配置:
  device: cuda
  batch_size: 16
  learning_rate: 0.0001
  total_epochs: 350
  image_mode: rgb

数据配置:
  data_root: /mnt/iusers01/fatpou01/compsci01/k09562zs/scratch/Ball_counting_CNN/ball_data_collection
  train_csv: scratch/Ball_counting_CNN/Tools_script/ball_counting_dataset_train.csv
  val_csv: scratch/Ball_counting_CNN/Tools_script/ball_counting_dataset_val.csv

模型配置:
  use_attention: True
  cnn_layers: 3
  cnn_channels: [64, 128, 256]
  feature_dim: 256
  attention_heads: 1
  dropout: 0.1

训练配置:
  scheduler_type: none
  label_smoothing: 0.0
  grad_clip_norm: 1.0

保存配置:
  save_dir: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points
  log_dir: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/logs
  save_every: 10
============================================================
所有路径验证通过
配置保存到: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/single_image_config.json

正在初始化单图像分类模型训练器...
图像模式: RGB
使用注意力机制: True
标签范围: 1-10 (对应10个类别)
SingleImageClassifier初始化:
  CNN层数: 3
  CNN通道: [64, 128, 256]
  输入通道: 3
  输出类别: 10 (对应标签1-10)
  特征维度: 256
  隐藏维度: 256
  使用注意力: True
  注意力头数: 1
创建带注意力机制的单图像分类模型 (标签1-10)
✓ Model initialization validation passed
=== 创建单图像数据加载器 - 图像模式: RGB ===
标签: 直接使用ball_count
单图像数据集构建完成:
  原始序列数: 946
  提取的单图像样本数: 4161
  图像模式: rgb
  标签: 直接使用ball_count
单图像数据集构建完成:
  原始序列数: 242
  提取的单图像样本数: 1074
  图像模式: rgb
  标签: 直接使用ball_count

训练集类别分布:
  球数 1: 648 样本
  球数 2: 492 样本
  球数 3: 428 样本
  球数 4: 405 样本
  球数 5: 384 样本
  球数 6: 364 样本
  球数 7: 368 样本
  球数 8: 360 样本
  球数 9: 360 样本
  球数 10: 352 样本

验证集类别分布:
  球数 1: 162 样本
  球数 2: 126 样本
  球数 3: 108 样本
  球数 4: 105 样本
  球数 5: 102 样本
  球数 6: 98 样本
  球数 7: 96 样本
  球数 8: 99 样本
  球数 9: 90 样本
  球数 10: 88 样本
SingleImageTrainer initialized:
  Model parameters: 734,858
  Training samples: 4,161
  Validation samples: 1,074
  Image mode: RGB
  Use attention: True
  Label mapping: 1-10 -> 0-9 (for loss calculation)

开始训练单图像分类模型...
目标: 作为具身计数模型的对比基线
使用所有帧图像，标签为对应序列的ball_count

开始训练单图像分类模型
总计 350 个epoch
设备: cuda
图像模式: rgb
Testing model save/load functionality...
✓ Model save/load test passed
Saving initial model state...
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/initial_single_image_model.pth

Performing initial validation...
Initial validation - Loss: 2.3060, Accuracy: 0.1006
Epoch [0] Batch [0/261] Loss: 2.2992 LR: 0.000100
Epoch [0] Batch [10/261] Loss: 2.2644 LR: 0.000100
Epoch [0] Batch [20/261] Loss: 2.2130 LR: 0.000100
Epoch [0] Batch [30/261] Loss: 2.1968 LR: 0.000100
Epoch [0] Batch [40/261] Loss: 2.2342 LR: 0.000100
Epoch [0] Batch [50/261] Loss: 2.1315 LR: 0.000100
Epoch [0] Batch [60/261] Loss: 1.9609 LR: 0.000100
Epoch [0] Batch [70/261] Loss: 1.7914 LR: 0.000100
Epoch [0] Batch [80/261] Loss: 1.7114 LR: 0.000100
Epoch [0] Batch [90/261] Loss: 1.6724 LR: 0.000100
Epoch [0] Batch [100/261] Loss: 1.6058 LR: 0.000100
Epoch [0] Batch [110/261] Loss: 1.8671 LR: 0.000100
Epoch [0] Batch [120/261] Loss: 1.8149 LR: 0.000100
Epoch [0] Batch [130/261] Loss: 1.3863 LR: 0.000100
Epoch [0] Batch [140/261] Loss: 1.5186 LR: 0.000100
Epoch [0] Batch [150/261] Loss: 1.6343 LR: 0.000100
Epoch [0] Batch [160/261] Loss: 1.6516 LR: 0.000100
Epoch [0] Batch [170/261] Loss: 1.4295 LR: 0.000100
Epoch [0] Batch [180/261] Loss: 1.6275 LR: 0.000100
Epoch [0] Batch [190/261] Loss: 1.2320 LR: 0.000100
Epoch [0] Batch [200/261] Loss: 1.7625 LR: 0.000100
Epoch [0] Batch [210/261] Loss: 1.1500 LR: 0.000100
Epoch [0] Batch [220/261] Loss: 1.1465 LR: 0.000100
Epoch [0] Batch [230/261] Loss: 1.6340 LR: 0.000100
Epoch [0] Batch [240/261] Loss: 1.1208 LR: 0.000100
Epoch [0] Batch [250/261] Loss: 1.1979 LR: 0.000100
Epoch [0] Batch [260/261] Loss: 2.9250 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [1/350] - Time: 30.46s
Train Loss: 1.7291 | Val Loss: 1.3075
Train Acc: 0.3141 | Val Acc: 0.3650
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.119    Count 3: 0.000    Count 4: 0.533    Count 5: 0.245  
  Count 6: 0.143    Count 7: 0.260    Count 8: 0.121    Count 9: 0.000    Count 10: 0.955  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/best_single_image_model.pth
  → New best model! Validation accuracy: 0.3650
新的最佳模型! 验证准确率: 0.3650
Epoch [1] Batch [0/261] Loss: 1.3067 LR: 0.000100
Epoch [1] Batch [10/261] Loss: 1.3049 LR: 0.000100
Epoch [1] Batch [20/261] Loss: 1.6175 LR: 0.000100
Epoch [1] Batch [30/261] Loss: 1.4496 LR: 0.000100
Epoch [1] Batch [40/261] Loss: 1.3263 LR: 0.000100
Epoch [1] Batch [50/261] Loss: 1.3532 LR: 0.000100
Epoch [1] Batch [60/261] Loss: 1.1467 LR: 0.000100
Epoch [1] Batch [70/261] Loss: 1.2072 LR: 0.000100
Epoch [1] Batch [80/261] Loss: 0.8454 LR: 0.000100
Epoch [1] Batch [90/261] Loss: 1.5076 LR: 0.000100
Epoch [1] Batch [100/261] Loss: 1.1014 LR: 0.000100
Epoch [1] Batch [110/261] Loss: 0.8971 LR: 0.000100
Epoch [1] Batch [120/261] Loss: 0.9071 LR: 0.000100
Epoch [1] Batch [130/261] Loss: 1.2472 LR: 0.000100
Epoch [1] Batch [140/261] Loss: 1.6529 LR: 0.000100
Epoch [1] Batch [150/261] Loss: 1.5575 LR: 0.000100
Epoch [1] Batch [160/261] Loss: 1.1197 LR: 0.000100
Epoch [1] Batch [170/261] Loss: 0.9960 LR: 0.000100
Epoch [1] Batch [180/261] Loss: 0.7126 LR: 0.000100
Epoch [1] Batch [190/261] Loss: 1.0650 LR: 0.000100
Epoch [1] Batch [200/261] Loss: 1.4659 LR: 0.000100
Epoch [1] Batch [210/261] Loss: 1.0909 LR: 0.000100
Epoch [1] Batch [220/261] Loss: 0.7355 LR: 0.000100
Epoch [1] Batch [230/261] Loss: 0.8277 LR: 0.000100
Epoch [1] Batch [240/261] Loss: 0.8459 LR: 0.000100
Epoch [1] Batch [250/261] Loss: 0.9746 LR: 0.000100
Epoch [1] Batch [260/261] Loss: 2.8692 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [2/350] - Time: 14.33s
Train Loss: 1.1899 | Val Loss: 1.0024
Train Acc: 0.4888 | Val Acc: 0.5261
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.857    Count 3: 0.287    Count 4: 0.790    Count 5: 0.039  
  Count 6: 0.786    Count 7: 0.219    Count 8: 0.263    Count 9: 0.211    Count 10: 0.398  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/best_single_image_model.pth
  → New best model! Validation accuracy: 0.5261
新的最佳模型! 验证准确率: 0.5261
Epoch [2] Batch [0/261] Loss: 1.5933 LR: 0.000100
Epoch [2] Batch [10/261] Loss: 0.7446 LR: 0.000100
Epoch [2] Batch [20/261] Loss: 1.1953 LR: 0.000100
Epoch [2] Batch [30/261] Loss: 0.8338 LR: 0.000100
Epoch [2] Batch [40/261] Loss: 1.0094 LR: 0.000100
Epoch [2] Batch [50/261] Loss: 1.3534 LR: 0.000100
Epoch [2] Batch [60/261] Loss: 1.1989 LR: 0.000100
Epoch [2] Batch [70/261] Loss: 0.7669 LR: 0.000100
Epoch [2] Batch [80/261] Loss: 0.7188 LR: 0.000100
Epoch [2] Batch [90/261] Loss: 0.8603 LR: 0.000100
Epoch [2] Batch [100/261] Loss: 0.6045 LR: 0.000100
Epoch [2] Batch [110/261] Loss: 0.7768 LR: 0.000100
Epoch [2] Batch [120/261] Loss: 1.4106 LR: 0.000100
Epoch [2] Batch [130/261] Loss: 0.6406 LR: 0.000100
Epoch [2] Batch [140/261] Loss: 0.7354 LR: 0.000100
Epoch [2] Batch [150/261] Loss: 0.6402 LR: 0.000100
Epoch [2] Batch [160/261] Loss: 0.5216 LR: 0.000100
Epoch [2] Batch [170/261] Loss: 0.8009 LR: 0.000100
Epoch [2] Batch [180/261] Loss: 1.3361 LR: 0.000100
Epoch [2] Batch [190/261] Loss: 0.6738 LR: 0.000100
Epoch [2] Batch [200/261] Loss: 0.7618 LR: 0.000100
Epoch [2] Batch [210/261] Loss: 0.5607 LR: 0.000100
Epoch [2] Batch [220/261] Loss: 0.6503 LR: 0.000100
Epoch [2] Batch [230/261] Loss: 0.8374 LR: 0.000100
Epoch [2] Batch [240/261] Loss: 0.7345 LR: 0.000100
Epoch [2] Batch [250/261] Loss: 1.2279 LR: 0.000100
Epoch [2] Batch [260/261] Loss: 5.4183 LR: 0.000100

Epoch [3/350] - Time: 13.84s
Train Loss: 0.9200 | Val Loss: 1.2780
Train Acc: 0.6018 | Val Acc: 0.3920
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.881    Count 3: 0.565    Count 4: 0.476    Count 5: 0.010  
  Count 6: 0.061    Count 7: 0.083    Count 8: 0.141    Count 9: 0.000    Count 10: 0.102  
Epoch [3] Batch [0/261] Loss: 0.9064 LR: 0.000100
Epoch [3] Batch [10/261] Loss: 0.7151 LR: 0.000100
Epoch [3] Batch [20/261] Loss: 0.6379 LR: 0.000100
Epoch [3] Batch [30/261] Loss: 0.5693 LR: 0.000100
Epoch [3] Batch [40/261] Loss: 0.9866 LR: 0.000100
Epoch [3] Batch [50/261] Loss: 0.8546 LR: 0.000100
Epoch [3] Batch [60/261] Loss: 0.5100 LR: 0.000100
Epoch [3] Batch [70/261] Loss: 0.6401 LR: 0.000100
Epoch [3] Batch [80/261] Loss: 1.3499 LR: 0.000100
Epoch [3] Batch [90/261] Loss: 0.6426 LR: 0.000100
Epoch [3] Batch [100/261] Loss: 0.7360 LR: 0.000100
Epoch [3] Batch [110/261] Loss: 0.5373 LR: 0.000100
Epoch [3] Batch [120/261] Loss: 1.2824 LR: 0.000100
Epoch [3] Batch [130/261] Loss: 0.8689 LR: 0.000100
Epoch [3] Batch [140/261] Loss: 0.8861 LR: 0.000100
Epoch [3] Batch [150/261] Loss: 0.5158 LR: 0.000100
Epoch [3] Batch [160/261] Loss: 0.8905 LR: 0.000100
Epoch [3] Batch [170/261] Loss: 1.1004 LR: 0.000100
Epoch [3] Batch [180/261] Loss: 0.6961 LR: 0.000100
Epoch [3] Batch [190/261] Loss: 0.8427 LR: 0.000100
Epoch [3] Batch [200/261] Loss: 0.6277 LR: 0.000100
Epoch [3] Batch [210/261] Loss: 0.7128 LR: 0.000100
Epoch [3] Batch [220/261] Loss: 0.3623 LR: 0.000100
Epoch [3] Batch [230/261] Loss: 0.6739 LR: 0.000100
Epoch [3] Batch [240/261] Loss: 0.9815 LR: 0.000100
Epoch [3] Batch [250/261] Loss: 0.8351 LR: 0.000100
Epoch [3] Batch [260/261] Loss: 2.9216 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [4/350] - Time: 14.01s
Train Loss: 0.7715 | Val Loss: 0.5540
Train Acc: 0.6575 | Val Acc: 0.7803
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.981    Count 2: 1.000    Count 3: 0.981    Count 4: 0.962    Count 5: 0.922  
  Count 6: 0.918    Count 7: 0.625    Count 8: 0.505    Count 9: 0.044    Count 10: 0.545  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/best_single_image_model.pth
  → New best model! Validation accuracy: 0.7803
新的最佳模型! 验证准确率: 0.7803
Epoch [4] Batch [0/261] Loss: 0.5412 LR: 0.000100
Epoch [4] Batch [10/261] Loss: 0.6287 LR: 0.000100
Epoch [4] Batch [20/261] Loss: 0.4976 LR: 0.000100
Epoch [4] Batch [30/261] Loss: 0.9725 LR: 0.000100
Epoch [4] Batch [40/261] Loss: 0.4586 LR: 0.000100
Epoch [4] Batch [50/261] Loss: 0.6889 LR: 0.000100
Epoch [4] Batch [60/261] Loss: 0.5011 LR: 0.000100
Epoch [4] Batch [70/261] Loss: 0.3136 LR: 0.000100
Epoch [4] Batch [80/261] Loss: 0.3795 LR: 0.000100
Epoch [4] Batch [90/261] Loss: 0.4012 LR: 0.000100
Epoch [4] Batch [100/261] Loss: 0.4637 LR: 0.000100
Epoch [4] Batch [110/261] Loss: 0.6054 LR: 0.000100
Epoch [4] Batch [120/261] Loss: 0.7300 LR: 0.000100
Epoch [4] Batch [130/261] Loss: 0.5422 LR: 0.000100
Epoch [4] Batch [140/261] Loss: 0.5704 LR: 0.000100
Epoch [4] Batch [150/261] Loss: 0.8132 LR: 0.000100
Epoch [4] Batch [160/261] Loss: 1.6448 LR: 0.000100
Epoch [4] Batch [170/261] Loss: 0.7978 LR: 0.000100
Epoch [4] Batch [180/261] Loss: 0.5358 LR: 0.000100
Epoch [4] Batch [190/261] Loss: 0.5080 LR: 0.000100
Epoch [4] Batch [200/261] Loss: 0.4345 LR: 0.000100
Epoch [4] Batch [210/261] Loss: 0.9722 LR: 0.000100
Epoch [4] Batch [220/261] Loss: 0.4920 LR: 0.000100
Epoch [4] Batch [230/261] Loss: 0.3013 LR: 0.000100
Epoch [4] Batch [240/261] Loss: 0.5634 LR: 0.000100
Epoch [4] Batch [250/261] Loss: 0.6130 LR: 0.000100
Epoch [4] Batch [260/261] Loss: 0.1032 LR: 0.000100

Epoch [5/350] - Time: 13.90s
Train Loss: 0.6261 | Val Loss: 0.6085
Train Acc: 0.7239 | Val Acc: 0.7067
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.926    Count 4: 0.933    Count 5: 0.431  
  Count 6: 0.643    Count 7: 0.615    Count 8: 0.202    Count 9: 0.489    Count 10: 0.489  
Epoch [5] Batch [0/261] Loss: 0.6309 LR: 0.000100
Epoch [5] Batch [10/261] Loss: 0.7746 LR: 0.000100
Epoch [5] Batch [20/261] Loss: 0.4150 LR: 0.000100
Epoch [5] Batch [30/261] Loss: 0.4744 LR: 0.000100
Epoch [5] Batch [40/261] Loss: 1.6069 LR: 0.000100
Epoch [5] Batch [50/261] Loss: 0.9673 LR: 0.000100
Epoch [5] Batch [60/261] Loss: 0.3428 LR: 0.000100
Epoch [5] Batch [70/261] Loss: 0.2851 LR: 0.000100
Epoch [5] Batch [80/261] Loss: 0.7857 LR: 0.000100
Epoch [5] Batch [90/261] Loss: 0.5872 LR: 0.000100
Epoch [5] Batch [100/261] Loss: 0.4379 LR: 0.000100
Epoch [5] Batch [110/261] Loss: 0.1768 LR: 0.000100
Epoch [5] Batch [120/261] Loss: 0.5094 LR: 0.000100
Epoch [5] Batch [130/261] Loss: 0.5748 LR: 0.000100
Epoch [5] Batch [140/261] Loss: 0.4102 LR: 0.000100
Epoch [5] Batch [150/261] Loss: 0.9849 LR: 0.000100
Epoch [5] Batch [160/261] Loss: 0.6262 LR: 0.000100
Epoch [5] Batch [170/261] Loss: 0.5901 LR: 0.000100
Epoch [5] Batch [180/261] Loss: 0.3582 LR: 0.000100
Epoch [5] Batch [190/261] Loss: 0.8326 LR: 0.000100
Epoch [5] Batch [200/261] Loss: 0.5246 LR: 0.000100
Epoch [5] Batch [210/261] Loss: 0.7275 LR: 0.000100
Epoch [5] Batch [220/261] Loss: 0.8736 LR: 0.000100
Epoch [5] Batch [230/261] Loss: 1.1485 LR: 0.000100
Epoch [5] Batch [240/261] Loss: 0.5445 LR: 0.000100
Epoch [5] Batch [250/261] Loss: 0.3061 LR: 0.000100
Epoch [5] Batch [260/261] Loss: 0.2685 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [6/350] - Time: 13.70s
Train Loss: 0.5594 | Val Loss: 0.4040
Train Acc: 0.7563 | Val Acc: 0.8305
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.963    Count 4: 0.933    Count 5: 0.951  
  Count 6: 0.857    Count 7: 0.802    Count 8: 0.556    Count 9: 0.689    Count 10: 0.307  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/best_single_image_model.pth
  → New best model! Validation accuracy: 0.8305
新的最佳模型! 验证准确率: 0.8305
Epoch [6] Batch [0/261] Loss: 0.4331 LR: 0.000100
Epoch [6] Batch [10/261] Loss: 0.3076 LR: 0.000100
Epoch [6] Batch [20/261] Loss: 0.4844 LR: 0.000100
Epoch [6] Batch [30/261] Loss: 0.3592 LR: 0.000100
Epoch [6] Batch [40/261] Loss: 0.2217 LR: 0.000100
Epoch [6] Batch [50/261] Loss: 0.6683 LR: 0.000100
Epoch [6] Batch [60/261] Loss: 0.4203 LR: 0.000100
Epoch [6] Batch [70/261] Loss: 0.2989 LR: 0.000100
Epoch [6] Batch [80/261] Loss: 1.2311 LR: 0.000100
Epoch [6] Batch [90/261] Loss: 0.4029 LR: 0.000100
Epoch [6] Batch [100/261] Loss: 0.5892 LR: 0.000100
Epoch [6] Batch [110/261] Loss: 0.5308 LR: 0.000100
Epoch [6] Batch [120/261] Loss: 0.2385 LR: 0.000100
Epoch [6] Batch [130/261] Loss: 0.8366 LR: 0.000100
Epoch [6] Batch [140/261] Loss: 0.2961 LR: 0.000100
Epoch [6] Batch [150/261] Loss: 0.9095 LR: 0.000100
Epoch [6] Batch [160/261] Loss: 0.1764 LR: 0.000100
Epoch [6] Batch [170/261] Loss: 0.7347 LR: 0.000100
Epoch [6] Batch [180/261] Loss: 0.1854 LR: 0.000100
Epoch [6] Batch [190/261] Loss: 0.1703 LR: 0.000100
Epoch [6] Batch [200/261] Loss: 0.5540 LR: 0.000100
Epoch [6] Batch [210/261] Loss: 0.4260 LR: 0.000100
Epoch [6] Batch [220/261] Loss: 0.3736 LR: 0.000100
Epoch [6] Batch [230/261] Loss: 0.6884 LR: 0.000100
Epoch [6] Batch [240/261] Loss: 0.3541 LR: 0.000100
Epoch [6] Batch [250/261] Loss: 0.5538 LR: 0.000100
Epoch [6] Batch [260/261] Loss: 1.5086 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [7/350] - Time: 13.18s
Train Loss: 0.4666 | Val Loss: 0.3483
Train Acc: 0.8000 | Val Acc: 0.8659
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.981    Count 4: 0.933    Count 5: 0.922  
  Count 6: 0.908    Count 7: 0.604    Count 8: 0.929    Count 9: 0.600    Count 10: 0.580  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/best_single_image_model.pth
  → New best model! Validation accuracy: 0.8659
新的最佳模型! 验证准确率: 0.8659
Epoch [7] Batch [0/261] Loss: 0.3369 LR: 0.000100
Epoch [7] Batch [10/261] Loss: 0.2209 LR: 0.000100
Epoch [7] Batch [20/261] Loss: 0.2990 LR: 0.000100
Epoch [7] Batch [30/261] Loss: 0.2721 LR: 0.000100
Epoch [7] Batch [40/261] Loss: 0.2890 LR: 0.000100
Epoch [7] Batch [50/261] Loss: 0.5419 LR: 0.000100
Epoch [7] Batch [60/261] Loss: 0.2941 LR: 0.000100
Epoch [7] Batch [70/261] Loss: 0.3715 LR: 0.000100
Epoch [7] Batch [80/261] Loss: 0.3767 LR: 0.000100
Epoch [7] Batch [90/261] Loss: 0.5787 LR: 0.000100
Epoch [7] Batch [100/261] Loss: 0.2533 LR: 0.000100
Epoch [7] Batch [110/261] Loss: 0.3759 LR: 0.000100
Epoch [7] Batch [120/261] Loss: 0.3116 LR: 0.000100
Epoch [7] Batch [130/261] Loss: 0.2964 LR: 0.000100
Epoch [7] Batch [140/261] Loss: 0.6804 LR: 0.000100
Epoch [7] Batch [150/261] Loss: 0.2456 LR: 0.000100
Epoch [7] Batch [160/261] Loss: 0.2352 LR: 0.000100
Epoch [7] Batch [170/261] Loss: 0.4298 LR: 0.000100
Epoch [7] Batch [180/261] Loss: 0.6806 LR: 0.000100
Epoch [7] Batch [190/261] Loss: 0.2220 LR: 0.000100
Epoch [7] Batch [200/261] Loss: 0.3701 LR: 0.000100
Epoch [7] Batch [210/261] Loss: 0.7670 LR: 0.000100
Epoch [7] Batch [220/261] Loss: 0.3357 LR: 0.000100
Epoch [7] Batch [230/261] Loss: 0.2794 LR: 0.000100
Epoch [7] Batch [240/261] Loss: 0.1826 LR: 0.000100
Epoch [7] Batch [250/261] Loss: 0.3790 LR: 0.000100
Epoch [7] Batch [260/261] Loss: 0.0469 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [8/350] - Time: 13.47s
Train Loss: 0.3873 | Val Loss: 0.3049
Train Acc: 0.8395 | Val Acc: 0.8724
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.981    Count 4: 0.962    Count 5: 0.990  
  Count 6: 0.694    Count 7: 0.844    Count 8: 0.343    Count 9: 0.789    Count 10: 0.989  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/best_single_image_model.pth
  → New best model! Validation accuracy: 0.8724
新的最佳模型! 验证准确率: 0.8724
Epoch [8] Batch [0/261] Loss: 0.3581 LR: 0.000100
Epoch [8] Batch [10/261] Loss: 0.4097 LR: 0.000100
Epoch [8] Batch [20/261] Loss: 0.4169 LR: 0.000100
Epoch [8] Batch [30/261] Loss: 0.2170 LR: 0.000100
Epoch [8] Batch [40/261] Loss: 0.3210 LR: 0.000100
Epoch [8] Batch [50/261] Loss: 0.1516 LR: 0.000100
Epoch [8] Batch [60/261] Loss: 0.2368 LR: 0.000100
Epoch [8] Batch [70/261] Loss: 0.3890 LR: 0.000100
Epoch [8] Batch [80/261] Loss: 0.3414 LR: 0.000100
Epoch [8] Batch [90/261] Loss: 0.2476 LR: 0.000100
Epoch [8] Batch [100/261] Loss: 0.3226 LR: 0.000100
Epoch [8] Batch [110/261] Loss: 0.2713 LR: 0.000100
Epoch [8] Batch [120/261] Loss: 0.2066 LR: 0.000100
Epoch [8] Batch [130/261] Loss: 0.2786 LR: 0.000100
Epoch [8] Batch [140/261] Loss: 0.2456 LR: 0.000100
Epoch [8] Batch [150/261] Loss: 0.0977 LR: 0.000100
Epoch [8] Batch [160/261] Loss: 0.2546 LR: 0.000100
Epoch [8] Batch [170/261] Loss: 0.1335 LR: 0.000100
Epoch [8] Batch [180/261] Loss: 0.1603 LR: 0.000100
Epoch [8] Batch [190/261] Loss: 0.2513 LR: 0.000100
Epoch [8] Batch [200/261] Loss: 0.2431 LR: 0.000100
Epoch [8] Batch [210/261] Loss: 0.2888 LR: 0.000100
Epoch [8] Batch [220/261] Loss: 0.2524 LR: 0.000100
Epoch [8] Batch [230/261] Loss: 0.1515 LR: 0.000100
Epoch [8] Batch [240/261] Loss: 0.3013 LR: 0.000100
Epoch [8] Batch [250/261] Loss: 0.1616 LR: 0.000100
Epoch [8] Batch [260/261] Loss: 0.8791 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [9/350] - Time: 13.64s
Train Loss: 0.3154 | Val Loss: 0.2707
Train Acc: 0.8697 | Val Acc: 0.8929
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.981    Count 4: 0.971    Count 5: 0.980  
  Count 6: 0.908    Count 7: 0.771    Count 8: 0.929    Count 9: 0.756    Count 10: 0.455  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/best_single_image_model.pth
  → New best model! Validation accuracy: 0.8929
新的最佳模型! 验证准确率: 0.8929
Epoch [9] Batch [0/261] Loss: 0.1930 LR: 0.000100
Epoch [9] Batch [10/261] Loss: 0.3025 LR: 0.000100
Epoch [9] Batch [20/261] Loss: 0.3345 LR: 0.000100
Epoch [9] Batch [30/261] Loss: 0.3892 LR: 0.000100
Epoch [9] Batch [40/261] Loss: 0.1799 LR: 0.000100
Epoch [9] Batch [50/261] Loss: 0.1371 LR: 0.000100
Epoch [9] Batch [60/261] Loss: 0.4446 LR: 0.000100
Epoch [9] Batch [70/261] Loss: 0.4579 LR: 0.000100
Epoch [9] Batch [80/261] Loss: 0.3471 LR: 0.000100
Epoch [9] Batch [90/261] Loss: 0.1013 LR: 0.000100
Epoch [9] Batch [100/261] Loss: 0.1495 LR: 0.000100
Epoch [9] Batch [110/261] Loss: 0.2860 LR: 0.000100
Epoch [9] Batch [120/261] Loss: 0.7175 LR: 0.000100
Epoch [9] Batch [130/261] Loss: 0.2570 LR: 0.000100
Epoch [9] Batch [140/261] Loss: 0.1671 LR: 0.000100
Epoch [9] Batch [150/261] Loss: 0.7105 LR: 0.000100
Epoch [9] Batch [160/261] Loss: 0.7115 LR: 0.000100
Epoch [9] Batch [170/261] Loss: 0.1122 LR: 0.000100
Epoch [9] Batch [180/261] Loss: 0.6153 LR: 0.000100
Epoch [9] Batch [190/261] Loss: 0.1477 LR: 0.000100
Epoch [9] Batch [200/261] Loss: 0.4285 LR: 0.000100
Epoch [9] Batch [210/261] Loss: 0.1163 LR: 0.000100
Epoch [9] Batch [220/261] Loss: 0.3169 LR: 0.000100
Epoch [9] Batch [230/261] Loss: 0.5194 LR: 0.000100
Epoch [9] Batch [240/261] Loss: 0.2738 LR: 0.000100
Epoch [9] Batch [250/261] Loss: 0.0826 LR: 0.000100
Epoch [9] Batch [260/261] Loss: 1.7197 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [10/350] - Time: 13.56s
Train Loss: 0.3108 | Val Loss: 0.2169
Train Acc: 0.8750 | Val Acc: 0.9274
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.972    Count 4: 0.952    Count 5: 1.000  
  Count 6: 0.898    Count 7: 0.833    Count 8: 0.889    Count 9: 0.733    Count 10: 0.898  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/best_single_image_model.pth
  → New best model! Validation accuracy: 0.9274
新的最佳模型! 验证准确率: 0.9274
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/single_image_checkpoint_epoch_9.pth
Epoch [10] Batch [0/261] Loss: 0.2863 LR: 0.000100
Epoch [10] Batch [10/261] Loss: 0.0800 LR: 0.000100
Epoch [10] Batch [20/261] Loss: 0.4715 LR: 0.000100
Epoch [10] Batch [30/261] Loss: 0.3188 LR: 0.000100
Epoch [10] Batch [40/261] Loss: 0.1768 LR: 0.000100
Epoch [10] Batch [50/261] Loss: 0.1496 LR: 0.000100
Epoch [10] Batch [60/261] Loss: 0.3842 LR: 0.000100
Epoch [10] Batch [70/261] Loss: 0.5524 LR: 0.000100
Epoch [10] Batch [80/261] Loss: 0.1337 LR: 0.000100
Epoch [10] Batch [90/261] Loss: 0.1558 LR: 0.000100
Epoch [10] Batch [100/261] Loss: 0.1124 LR: 0.000100
Epoch [10] Batch [110/261] Loss: 0.1968 LR: 0.000100
Epoch [10] Batch [120/261] Loss: 1.1219 LR: 0.000100
Epoch [10] Batch [130/261] Loss: 0.1284 LR: 0.000100
Epoch [10] Batch [140/261] Loss: 0.0757 LR: 0.000100
Epoch [10] Batch [150/261] Loss: 0.3691 LR: 0.000100
Epoch [10] Batch [160/261] Loss: 0.3695 LR: 0.000100
Epoch [10] Batch [170/261] Loss: 0.2548 LR: 0.000100
Epoch [10] Batch [180/261] Loss: 0.2012 LR: 0.000100
Epoch [10] Batch [190/261] Loss: 0.2086 LR: 0.000100
Epoch [10] Batch [200/261] Loss: 0.2712 LR: 0.000100
Epoch [10] Batch [210/261] Loss: 0.2920 LR: 0.000100
Epoch [10] Batch [220/261] Loss: 0.3139 LR: 0.000100
Epoch [10] Batch [230/261] Loss: 0.0483 LR: 0.000100
Epoch [10] Batch [240/261] Loss: 0.5353 LR: 0.000100
Epoch [10] Batch [250/261] Loss: 0.0884 LR: 0.000100
Epoch [10] Batch [260/261] Loss: 1.8970 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [11/350] - Time: 13.79s
Train Loss: 0.2747 | Val Loss: 0.1626
Train Acc: 0.8919 | Val Acc: 0.9348
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.981    Count 4: 0.952    Count 5: 0.931  
  Count 6: 0.929    Count 7: 0.875    Count 8: 0.838    Count 9: 0.778    Count 10: 0.989  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/best_single_image_model.pth
  → New best model! Validation accuracy: 0.9348
新的最佳模型! 验证准确率: 0.9348
Epoch [11] Batch [0/261] Loss: 0.1529 LR: 0.000100
Epoch [11] Batch [10/261] Loss: 0.2599 LR: 0.000100
Epoch [11] Batch [20/261] Loss: 0.3161 LR: 0.000100
Epoch [11] Batch [30/261] Loss: 0.5941 LR: 0.000100
Epoch [11] Batch [40/261] Loss: 0.2334 LR: 0.000100
Epoch [11] Batch [50/261] Loss: 0.2231 LR: 0.000100
Epoch [11] Batch [60/261] Loss: 0.3977 LR: 0.000100
Epoch [11] Batch [70/261] Loss: 0.4716 LR: 0.000100
Epoch [11] Batch [80/261] Loss: 0.1052 LR: 0.000100
Epoch [11] Batch [90/261] Loss: 0.2050 LR: 0.000100
Epoch [11] Batch [100/261] Loss: 0.4623 LR: 0.000100
Epoch [11] Batch [110/261] Loss: 0.3958 LR: 0.000100
Epoch [11] Batch [120/261] Loss: 0.2141 LR: 0.000100
Epoch [11] Batch [130/261] Loss: 0.1069 LR: 0.000100
Epoch [11] Batch [140/261] Loss: 0.4809 LR: 0.000100
Epoch [11] Batch [150/261] Loss: 0.2870 LR: 0.000100
Epoch [11] Batch [160/261] Loss: 0.1704 LR: 0.000100
Epoch [11] Batch [170/261] Loss: 0.2139 LR: 0.000100
Epoch [11] Batch [180/261] Loss: 0.1471 LR: 0.000100
Epoch [11] Batch [190/261] Loss: 0.1043 LR: 0.000100
Epoch [11] Batch [200/261] Loss: 0.1700 LR: 0.000100
Epoch [11] Batch [210/261] Loss: 0.1688 LR: 0.000100
Epoch [11] Batch [220/261] Loss: 0.2414 LR: 0.000100
Epoch [11] Batch [230/261] Loss: 0.0990 LR: 0.000100
Epoch [11] Batch [240/261] Loss: 0.0703 LR: 0.000100
Epoch [11] Batch [250/261] Loss: 0.1161 LR: 0.000100
Epoch [11] Batch [260/261] Loss: 0.0007 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [12/350] - Time: 13.60s
Train Loss: 0.2494 | Val Loss: 0.1711
Train Acc: 0.9027 | Val Acc: 0.9460
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 1.000    Count 3: 0.981    Count 4: 0.981    Count 5: 0.961  
  Count 6: 0.898    Count 7: 0.969    Count 8: 0.919    Count 9: 0.900    Count 10: 0.784  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/best_single_image_model.pth
  → New best model! Validation accuracy: 0.9460
新的最佳模型! 验证准确率: 0.9460
Epoch [12] Batch [0/261] Loss: 0.1609 LR: 0.000100
Epoch [12] Batch [10/261] Loss: 0.1641 LR: 0.000100
Epoch [12] Batch [20/261] Loss: 0.0322 LR: 0.000100
Epoch [12] Batch [30/261] Loss: 0.4167 LR: 0.000100
Epoch [12] Batch [40/261] Loss: 0.8566 LR: 0.000100
Epoch [12] Batch [50/261] Loss: 0.0742 LR: 0.000100
Epoch [12] Batch [60/261] Loss: 0.3235 LR: 0.000100
Epoch [12] Batch [70/261] Loss: 0.5345 LR: 0.000100
Epoch [12] Batch [80/261] Loss: 0.1188 LR: 0.000100
Epoch [12] Batch [90/261] Loss: 0.0929 LR: 0.000100
Epoch [12] Batch [100/261] Loss: 0.2286 LR: 0.000100
Epoch [12] Batch [110/261] Loss: 0.1045 LR: 0.000100
Epoch [12] Batch [120/261] Loss: 0.1716 LR: 0.000100
Epoch [12] Batch [130/261] Loss: 0.1002 LR: 0.000100
Epoch [12] Batch [140/261] Loss: 0.2535 LR: 0.000100
Epoch [12] Batch [150/261] Loss: 0.0675 LR: 0.000100
Epoch [12] Batch [160/261] Loss: 0.0346 LR: 0.000100
Epoch [12] Batch [170/261] Loss: 0.0269 LR: 0.000100
Epoch [12] Batch [180/261] Loss: 0.3070 LR: 0.000100
Epoch [12] Batch [190/261] Loss: 0.8037 LR: 0.000100
Epoch [12] Batch [200/261] Loss: 0.6223 LR: 0.000100
Epoch [12] Batch [210/261] Loss: 0.2895 LR: 0.000100
Epoch [12] Batch [220/261] Loss: 0.4454 LR: 0.000100
Epoch [12] Batch [230/261] Loss: 0.1092 LR: 0.000100
Epoch [12] Batch [240/261] Loss: 0.1244 LR: 0.000100
Epoch [12] Batch [250/261] Loss: 0.1255 LR: 0.000100
Epoch [12] Batch [260/261] Loss: 0.1501 LR: 0.000100

Epoch [13/350] - Time: 13.37s
Train Loss: 0.2705 | Val Loss: 1.4940
Train Acc: 0.8993 | Val Acc: 0.5764
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 1.000    Count 3: 0.935    Count 4: 0.676    Count 5: 0.441  
  Count 6: 0.102    Count 7: 0.094    Count 8: 0.010    Count 9: 0.078    Count 10: 1.000  
Epoch [13] Batch [0/261] Loss: 1.8646 LR: 0.000100
Epoch [13] Batch [10/261] Loss: 0.5750 LR: 0.000100
Epoch [13] Batch [20/261] Loss: 0.0920 LR: 0.000100
Epoch [13] Batch [30/261] Loss: 0.2180 LR: 0.000100
Epoch [13] Batch [40/261] Loss: 0.9738 LR: 0.000100
Epoch [13] Batch [50/261] Loss: 0.4886 LR: 0.000100
Epoch [13] Batch [60/261] Loss: 0.2260 LR: 0.000100
Epoch [13] Batch [70/261] Loss: 0.0383 LR: 0.000100
Epoch [13] Batch [80/261] Loss: 0.4154 LR: 0.000100
Epoch [13] Batch [90/261] Loss: 0.1713 LR: 0.000100
Epoch [13] Batch [100/261] Loss: 0.3378 LR: 0.000100
Epoch [13] Batch [110/261] Loss: 0.4017 LR: 0.000100
Epoch [13] Batch [120/261] Loss: 0.2058 LR: 0.000100
Epoch [13] Batch [130/261] Loss: 0.1688 LR: 0.000100
Epoch [13] Batch [140/261] Loss: 0.1545 LR: 0.000100
Epoch [13] Batch [150/261] Loss: 0.0486 LR: 0.000100
Epoch [13] Batch [160/261] Loss: 0.2047 LR: 0.000100
Epoch [13] Batch [170/261] Loss: 0.2417 LR: 0.000100
Epoch [13] Batch [180/261] Loss: 0.1057 LR: 0.000100
Epoch [13] Batch [190/261] Loss: 0.3294 LR: 0.000100
Epoch [13] Batch [200/261] Loss: 0.0185 LR: 0.000100
Epoch [13] Batch [210/261] Loss: 0.0843 LR: 0.000100
Epoch [13] Batch [220/261] Loss: 0.1723 LR: 0.000100
Epoch [13] Batch [230/261] Loss: 0.0589 LR: 0.000100
Epoch [13] Batch [240/261] Loss: 0.1667 LR: 0.000100
Epoch [13] Batch [250/261] Loss: 0.0553 LR: 0.000100
Epoch [13] Batch [260/261] Loss: 23.2767 LR: 0.000100

Epoch [14/350] - Time: 13.35s
Train Loss: 0.3192 | Val Loss: 0.2787
Train Acc: 0.8974 | Val Acc: 0.8715
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.990    Count 5: 0.902  
  Count 6: 0.837    Count 7: 0.646    Count 8: 0.475    Count 9: 0.733    Count 10: 0.989  
Epoch [14] Batch [0/261] Loss: 0.4128 LR: 0.000100
Epoch [14] Batch [10/261] Loss: 0.0977 LR: 0.000100
Epoch [14] Batch [20/261] Loss: 0.0225 LR: 0.000100
Epoch [14] Batch [30/261] Loss: 0.0711 LR: 0.000100
Epoch [14] Batch [40/261] Loss: 0.0533 LR: 0.000100
Epoch [14] Batch [50/261] Loss: 0.0628 LR: 0.000100
Epoch [14] Batch [60/261] Loss: 0.0671 LR: 0.000100
Epoch [14] Batch [70/261] Loss: 0.1874 LR: 0.000100
Epoch [14] Batch [80/261] Loss: 0.0202 LR: 0.000100
Epoch [14] Batch [90/261] Loss: 0.0523 LR: 0.000100
Epoch [14] Batch [100/261] Loss: 0.2484 LR: 0.000100
Epoch [14] Batch [110/261] Loss: 0.0302 LR: 0.000100
Epoch [14] Batch [120/261] Loss: 0.0847 LR: 0.000100
Epoch [14] Batch [130/261] Loss: 0.0902 LR: 0.000100
Epoch [14] Batch [140/261] Loss: 0.0463 LR: 0.000100
Epoch [14] Batch [150/261] Loss: 0.0212 LR: 0.000100
Epoch [14] Batch [160/261] Loss: 0.1765 LR: 0.000100
Epoch [14] Batch [170/261] Loss: 0.1874 LR: 0.000100
Epoch [14] Batch [180/261] Loss: 0.4280 LR: 0.000100
Epoch [14] Batch [190/261] Loss: 0.1530 LR: 0.000100
Epoch [14] Batch [200/261] Loss: 0.2419 LR: 0.000100
Epoch [14] Batch [210/261] Loss: 0.0615 LR: 0.000100
Epoch [14] Batch [220/261] Loss: 0.8152 LR: 0.000100
Epoch [14] Batch [230/261] Loss: 0.0955 LR: 0.000100
Epoch [14] Batch [240/261] Loss: 0.2871 LR: 0.000100
Epoch [14] Batch [250/261] Loss: 0.1718 LR: 0.000100
Epoch [14] Batch [260/261] Loss: 0.0004 LR: 0.000100

Epoch [15/350] - Time: 13.11s
Train Loss: 0.1912 | Val Loss: 0.1558
Train Acc: 0.9298 | Val Acc: 0.9423
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.981    Count 5: 1.000  
  Count 6: 0.980    Count 7: 0.938    Count 8: 0.919    Count 9: 0.856    Count 10: 0.659  
Epoch [15] Batch [0/261] Loss: 0.2718 LR: 0.000100
Epoch [15] Batch [10/261] Loss: 0.0464 LR: 0.000100
Epoch [15] Batch [20/261] Loss: 0.0159 LR: 0.000100
Epoch [15] Batch [30/261] Loss: 0.8111 LR: 0.000100
Epoch [15] Batch [40/261] Loss: 0.0545 LR: 0.000100
Epoch [15] Batch [50/261] Loss: 0.0082 LR: 0.000100
Epoch [15] Batch [60/261] Loss: 0.0286 LR: 0.000100
Epoch [15] Batch [70/261] Loss: 0.0130 LR: 0.000100
Epoch [15] Batch [80/261] Loss: 0.0171 LR: 0.000100
Epoch [15] Batch [90/261] Loss: 0.0388 LR: 0.000100
Epoch [15] Batch [100/261] Loss: 0.0371 LR: 0.000100
Epoch [15] Batch [110/261] Loss: 0.0151 LR: 0.000100
Epoch [15] Batch [120/261] Loss: 0.0687 LR: 0.000100
Epoch [15] Batch [130/261] Loss: 1.1483 LR: 0.000100
Epoch [15] Batch [140/261] Loss: 0.2682 LR: 0.000100
Epoch [15] Batch [150/261] Loss: 0.0236 LR: 0.000100
Epoch [15] Batch [160/261] Loss: 0.0171 LR: 0.000100
Epoch [15] Batch [170/261] Loss: 0.0257 LR: 0.000100
Epoch [15] Batch [180/261] Loss: 0.1112 LR: 0.000100
Epoch [15] Batch [190/261] Loss: 0.0286 LR: 0.000100
Epoch [15] Batch [200/261] Loss: 0.0045 LR: 0.000100
Epoch [15] Batch [210/261] Loss: 0.0867 LR: 0.000100
Epoch [15] Batch [220/261] Loss: 0.4276 LR: 0.000100
Epoch [15] Batch [230/261] Loss: 0.0367 LR: 0.000100
Epoch [15] Batch [240/261] Loss: 0.0030 LR: 0.000100
Epoch [15] Batch [250/261] Loss: 0.0054 LR: 0.000100
Epoch [15] Batch [260/261] Loss: 0.2212 LR: 0.000100

Epoch [16/350] - Time: 13.15s
Train Loss: 0.1645 | Val Loss: 0.1858
Train Acc: 0.9385 | Val Acc: 0.9302
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.972    Count 4: 0.990    Count 5: 1.000  
  Count 6: 0.888    Count 7: 0.906    Count 8: 0.980    Count 9: 0.822    Count 10: 0.625  
Epoch [16] Batch [0/261] Loss: 0.0801 LR: 0.000100
Epoch [16] Batch [10/261] Loss: 0.1637 LR: 0.000100
Epoch [16] Batch [20/261] Loss: 0.0145 LR: 0.000100
Epoch [16] Batch [30/261] Loss: 0.2769 LR: 0.000100
Epoch [16] Batch [40/261] Loss: 0.0816 LR: 0.000100
Epoch [16] Batch [50/261] Loss: 0.0385 LR: 0.000100
Epoch [16] Batch [60/261] Loss: 0.0963 LR: 0.000100
Epoch [16] Batch [70/261] Loss: 0.0060 LR: 0.000100
Epoch [16] Batch [80/261] Loss: 0.0604 LR: 0.000100
Epoch [16] Batch [90/261] Loss: 0.7505 LR: 0.000100
Epoch [16] Batch [100/261] Loss: 0.0513 LR: 0.000100
Epoch [16] Batch [110/261] Loss: 0.9308 LR: 0.000100
Epoch [16] Batch [120/261] Loss: 0.0012 LR: 0.000100
Epoch [16] Batch [130/261] Loss: 0.5749 LR: 0.000100
Epoch [16] Batch [140/261] Loss: 0.0381 LR: 0.000100
Epoch [16] Batch [150/261] Loss: 0.1445 LR: 0.000100
Epoch [16] Batch [160/261] Loss: 0.1466 LR: 0.000100
Epoch [16] Batch [170/261] Loss: 0.1348 LR: 0.000100
Epoch [16] Batch [180/261] Loss: 0.1531 LR: 0.000100
Epoch [16] Batch [190/261] Loss: 0.0371 LR: 0.000100
Epoch [16] Batch [200/261] Loss: 0.1058 LR: 0.000100
Epoch [16] Batch [210/261] Loss: 0.9298 LR: 0.000100
Epoch [16] Batch [220/261] Loss: 0.0087 LR: 0.000100
Epoch [16] Batch [230/261] Loss: 0.0317 LR: 0.000100
Epoch [16] Batch [240/261] Loss: 0.0188 LR: 0.000100
Epoch [16] Batch [250/261] Loss: 0.0798 LR: 0.000100
Epoch [16] Batch [260/261] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [17/350] - Time: 13.23s
Train Loss: 0.2675 | Val Loss: 0.1891
Train Acc: 0.9130 | Val Acc: 0.9479
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.870    Count 4: 0.876    Count 5: 0.931  
  Count 6: 0.959    Count 7: 0.885    Count 8: 0.990    Count 9: 0.944    Count 10: 0.989  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/best_single_image_model.pth
  → New best model! Validation accuracy: 0.9479
新的最佳模型! 验证准确率: 0.9479
Epoch [17] Batch [0/261] Loss: 0.1228 LR: 0.000100
Epoch [17] Batch [10/261] Loss: 2.0586 LR: 0.000100
Epoch [17] Batch [20/261] Loss: 0.3180 LR: 0.000100
Epoch [17] Batch [30/261] Loss: 0.0028 LR: 0.000100
Epoch [17] Batch [40/261] Loss: 0.2366 LR: 0.000100
Epoch [17] Batch [50/261] Loss: 0.0462 LR: 0.000100
Epoch [17] Batch [60/261] Loss: 0.3489 LR: 0.000100
Epoch [17] Batch [70/261] Loss: 0.0144 LR: 0.000100
Epoch [17] Batch [80/261] Loss: 0.1040 LR: 0.000100
Epoch [17] Batch [90/261] Loss: 0.0247 LR: 0.000100
Epoch [17] Batch [100/261] Loss: 0.0146 LR: 0.000100
Epoch [17] Batch [110/261] Loss: 0.1357 LR: 0.000100
Epoch [17] Batch [120/261] Loss: 0.0263 LR: 0.000100
Epoch [17] Batch [130/261] Loss: 0.9403 LR: 0.000100
Epoch [17] Batch [140/261] Loss: 0.2968 LR: 0.000100
Epoch [17] Batch [150/261] Loss: 0.0373 LR: 0.000100
Epoch [17] Batch [160/261] Loss: 0.4898 LR: 0.000100
Epoch [17] Batch [170/261] Loss: 0.0076 LR: 0.000100
Epoch [17] Batch [180/261] Loss: 0.0173 LR: 0.000100
Epoch [17] Batch [190/261] Loss: 0.0080 LR: 0.000100
Epoch [17] Batch [200/261] Loss: 0.4464 LR: 0.000100
Epoch [17] Batch [210/261] Loss: 0.0671 LR: 0.000100
Epoch [17] Batch [220/261] Loss: 0.0445 LR: 0.000100
Epoch [17] Batch [230/261] Loss: 0.4140 LR: 0.000100
Epoch [17] Batch [240/261] Loss: 0.0449 LR: 0.000100
Epoch [17] Batch [250/261] Loss: 0.0334 LR: 0.000100
Epoch [17] Batch [260/261] Loss: 2.5206 LR: 0.000100

Epoch [18/350] - Time: 13.23s
Train Loss: 0.1866 | Val Loss: 0.1889
Train Acc: 0.9385 | Val Acc: 0.9246
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.990    Count 5: 0.961  
  Count 6: 0.959    Count 7: 0.938    Count 8: 0.616    Count 9: 0.700    Count 10: 1.000  
Epoch [18] Batch [0/261] Loss: 0.2999 LR: 0.000100
Epoch [18] Batch [10/261] Loss: 0.3835 LR: 0.000100
Epoch [18] Batch [20/261] Loss: 0.0109 LR: 0.000100
Epoch [18] Batch [30/261] Loss: 0.0094 LR: 0.000100
Epoch [18] Batch [40/261] Loss: 0.1029 LR: 0.000100
Epoch [18] Batch [50/261] Loss: 0.0337 LR: 0.000100
Epoch [18] Batch [60/261] Loss: 0.0036 LR: 0.000100
Epoch [18] Batch [70/261] Loss: 0.0049 LR: 0.000100
Epoch [18] Batch [80/261] Loss: 0.2043 LR: 0.000100
Epoch [18] Batch [90/261] Loss: 0.3675 LR: 0.000100
Epoch [18] Batch [100/261] Loss: 0.0239 LR: 0.000100
Epoch [18] Batch [110/261] Loss: 0.0128 LR: 0.000100
Epoch [18] Batch [120/261] Loss: 0.1514 LR: 0.000100
Epoch [18] Batch [130/261] Loss: 0.0455 LR: 0.000100
Epoch [18] Batch [140/261] Loss: 0.5232 LR: 0.000100
Epoch [18] Batch [150/261] Loss: 0.3192 LR: 0.000100
Epoch [18] Batch [160/261] Loss: 0.0950 LR: 0.000100
Epoch [18] Batch [170/261] Loss: 0.0059 LR: 0.000100
Epoch [18] Batch [180/261] Loss: 0.0610 LR: 0.000100
Epoch [18] Batch [190/261] Loss: 0.0852 LR: 0.000100
Epoch [18] Batch [200/261] Loss: 0.2118 LR: 0.000100
Epoch [18] Batch [210/261] Loss: 0.3767 LR: 0.000100
Epoch [18] Batch [220/261] Loss: 0.3193 LR: 0.000100
Epoch [18] Batch [230/261] Loss: 0.0021 LR: 0.000100
Epoch [18] Batch [240/261] Loss: 0.0254 LR: 0.000100
Epoch [18] Batch [250/261] Loss: 0.0164 LR: 0.000100
Epoch [18] Batch [260/261] Loss: 2.6947 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [19/350] - Time: 13.60s
Train Loss: 0.1828 | Val Loss: 0.1304
Train Acc: 0.9363 | Val Acc: 0.9581
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.972    Count 4: 0.981    Count 5: 1.000  
  Count 6: 0.969    Count 7: 0.896    Count 8: 1.000    Count 9: 0.911    Count 10: 0.784  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/best_single_image_model.pth
  → New best model! Validation accuracy: 0.9581
新的最佳模型! 验证准确率: 0.9581
Epoch [19] Batch [0/261] Loss: 0.0074 LR: 0.000100
Epoch [19] Batch [10/261] Loss: 0.3989 LR: 0.000100
Epoch [19] Batch [20/261] Loss: 0.0304 LR: 0.000100
Epoch [19] Batch [30/261] Loss: 0.0152 LR: 0.000100
Epoch [19] Batch [40/261] Loss: 0.0651 LR: 0.000100
Epoch [19] Batch [50/261] Loss: 0.0436 LR: 0.000100
Epoch [19] Batch [60/261] Loss: 0.0144 LR: 0.000100
Epoch [19] Batch [70/261] Loss: 0.0220 LR: 0.000100
Epoch [19] Batch [80/261] Loss: 0.0822 LR: 0.000100
Epoch [19] Batch [90/261] Loss: 0.0090 LR: 0.000100
Epoch [19] Batch [100/261] Loss: 0.0073 LR: 0.000100
Epoch [19] Batch [110/261] Loss: 0.0336 LR: 0.000100
Epoch [19] Batch [120/261] Loss: 0.0117 LR: 0.000100
Epoch [19] Batch [130/261] Loss: 0.0415 LR: 0.000100
Epoch [19] Batch [140/261] Loss: 0.1042 LR: 0.000100
Epoch [19] Batch [150/261] Loss: 0.0296 LR: 0.000100
Epoch [19] Batch [160/261] Loss: 0.5187 LR: 0.000100
Epoch [19] Batch [170/261] Loss: 0.0119 LR: 0.000100
Epoch [19] Batch [180/261] Loss: 0.0040 LR: 0.000100
Epoch [19] Batch [190/261] Loss: 0.3049 LR: 0.000100
Epoch [19] Batch [200/261] Loss: 0.0749 LR: 0.000100
Epoch [19] Batch [210/261] Loss: 0.5839 LR: 0.000100
Epoch [19] Batch [220/261] Loss: 0.6362 LR: 0.000100
Epoch [19] Batch [230/261] Loss: 0.0264 LR: 0.000100
Epoch [19] Batch [240/261] Loss: 0.0120 LR: 0.000100
Epoch [19] Batch [250/261] Loss: 0.0321 LR: 0.000100
Epoch [19] Batch [260/261] Loss: 33.2654 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [20/350] - Time: 13.22s
Train Loss: 0.1450 | Val Loss: 0.2141
Train Acc: 0.9551 | Val Acc: 0.9339
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.992    Count 3: 0.972    Count 4: 0.981    Count 5: 0.941  
  Count 6: 0.959    Count 7: 0.906    Count 8: 0.909    Count 9: 0.911    Count 10: 0.670  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/single_image_checkpoint_epoch_19.pth
Epoch [20] Batch [0/261] Loss: 0.4025 LR: 0.000100
Epoch [20] Batch [10/261] Loss: 0.0011 LR: 0.000100
Epoch [20] Batch [20/261] Loss: 0.1014 LR: 0.000100
Epoch [20] Batch [30/261] Loss: 0.2273 LR: 0.000100
Epoch [20] Batch [40/261] Loss: 0.0537 LR: 0.000100
Epoch [20] Batch [50/261] Loss: 0.0326 LR: 0.000100
Epoch [20] Batch [60/261] Loss: 0.4065 LR: 0.000100
Epoch [20] Batch [70/261] Loss: 0.0012 LR: 0.000100
Epoch [20] Batch [80/261] Loss: 0.0092 LR: 0.000100
Epoch [20] Batch [90/261] Loss: 0.3376 LR: 0.000100
Epoch [20] Batch [100/261] Loss: 0.2772 LR: 0.000100
Epoch [20] Batch [110/261] Loss: 0.0074 LR: 0.000100
Epoch [20] Batch [120/261] Loss: 0.0980 LR: 0.000100
Epoch [20] Batch [130/261] Loss: 0.5586 LR: 0.000100
Epoch [20] Batch [140/261] Loss: 0.0916 LR: 0.000100
Epoch [20] Batch [150/261] Loss: 0.0204 LR: 0.000100
Epoch [20] Batch [160/261] Loss: 0.2394 LR: 0.000100
Epoch [20] Batch [170/261] Loss: 0.8832 LR: 0.000100
Epoch [20] Batch [180/261] Loss: 0.7803 LR: 0.000100
Epoch [20] Batch [190/261] Loss: 0.0007 LR: 0.000100
Epoch [20] Batch [200/261] Loss: 0.0386 LR: 0.000100
Epoch [20] Batch [210/261] Loss: 0.0146 LR: 0.000100
Epoch [20] Batch [220/261] Loss: 0.1504 LR: 0.000100
Epoch [20] Batch [230/261] Loss: 0.0251 LR: 0.000100
Epoch [20] Batch [240/261] Loss: 0.0158 LR: 0.000100
Epoch [20] Batch [250/261] Loss: 0.3949 LR: 0.000100
Epoch [20] Batch [260/261] Loss: 7.9561 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [21/350] - Time: 13.16s
Train Loss: 0.1374 | Val Loss: 0.1464
Train Acc: 0.9587 | Val Acc: 0.9590
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.972    Count 4: 0.971    Count 5: 1.000  
  Count 6: 0.939    Count 7: 0.906    Count 8: 0.939    Count 9: 0.967    Count 10: 0.841  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/best_single_image_model.pth
  → New best model! Validation accuracy: 0.9590
新的最佳模型! 验证准确率: 0.9590
Epoch [21] Batch [0/261] Loss: 0.0833 LR: 0.000100
Epoch [21] Batch [10/261] Loss: 0.0559 LR: 0.000100
Epoch [21] Batch [20/261] Loss: 0.0787 LR: 0.000100
Epoch [21] Batch [30/261] Loss: 0.0940 LR: 0.000100
Epoch [21] Batch [40/261] Loss: 0.0537 LR: 0.000100
Epoch [21] Batch [50/261] Loss: 0.0045 LR: 0.000100
Epoch [21] Batch [60/261] Loss: 0.1654 LR: 0.000100
Epoch [21] Batch [70/261] Loss: 0.1225 LR: 0.000100
Epoch [21] Batch [80/261] Loss: 1.3096 LR: 0.000100
Epoch [21] Batch [90/261] Loss: 0.0089 LR: 0.000100
Epoch [21] Batch [100/261] Loss: 0.0002 LR: 0.000100
Epoch [21] Batch [110/261] Loss: 0.3275 LR: 0.000100
Epoch [21] Batch [120/261] Loss: 0.0858 LR: 0.000100
Epoch [21] Batch [130/261] Loss: 0.0045 LR: 0.000100
Epoch [21] Batch [140/261] Loss: 0.0041 LR: 0.000100
Epoch [21] Batch [150/261] Loss: 0.0468 LR: 0.000100
Epoch [21] Batch [160/261] Loss: 0.1230 LR: 0.000100
Epoch [21] Batch [170/261] Loss: 0.0050 LR: 0.000100
Epoch [21] Batch [180/261] Loss: 0.0071 LR: 0.000100
Epoch [21] Batch [190/261] Loss: 0.0071 LR: 0.000100
Epoch [21] Batch [200/261] Loss: 0.0820 LR: 0.000100
Epoch [21] Batch [210/261] Loss: 0.0329 LR: 0.000100
Epoch [21] Batch [220/261] Loss: 0.6543 LR: 0.000100
Epoch [21] Batch [230/261] Loss: 0.0791 LR: 0.000100
Epoch [21] Batch [240/261] Loss: 0.0036 LR: 0.000100
Epoch [21] Batch [250/261] Loss: 0.0010 LR: 0.000100
Epoch [21] Batch [260/261] Loss: 4.0674 LR: 0.000100

Epoch [22/350] - Time: 13.50s
Train Loss: 0.1020 | Val Loss: 0.2671
Train Acc: 0.9664 | Val Acc: 0.9088
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.943    Count 5: 0.784  
  Count 6: 0.898    Count 7: 0.854    Count 8: 0.737    Count 9: 0.778    Count 10: 1.000  
Epoch [22] Batch [0/261] Loss: 1.2951 LR: 0.000100
Epoch [22] Batch [10/261] Loss: 0.0098 LR: 0.000100
Epoch [22] Batch [20/261] Loss: 0.0011 LR: 0.000100
Epoch [22] Batch [30/261] Loss: 0.0330 LR: 0.000100
Epoch [22] Batch [40/261] Loss: 0.4728 LR: 0.000100
Epoch [22] Batch [50/261] Loss: 0.0037 LR: 0.000100
Epoch [22] Batch [60/261] Loss: 0.1195 LR: 0.000100
Epoch [22] Batch [70/261] Loss: 0.0077 LR: 0.000100
Epoch [22] Batch [80/261] Loss: 0.2155 LR: 0.000100
Epoch [22] Batch [90/261] Loss: 0.0027 LR: 0.000100
Epoch [22] Batch [100/261] Loss: 0.0050 LR: 0.000100
Epoch [22] Batch [110/261] Loss: 0.6592 LR: 0.000100
Epoch [22] Batch [120/261] Loss: 0.0280 LR: 0.000100
Epoch [22] Batch [130/261] Loss: 0.0350 LR: 0.000100
Epoch [22] Batch [140/261] Loss: 1.0647 LR: 0.000100
Epoch [22] Batch [150/261] Loss: 0.3427 LR: 0.000100
Epoch [22] Batch [160/261] Loss: 0.0186 LR: 0.000100
Epoch [22] Batch [170/261] Loss: 0.3515 LR: 0.000100
Epoch [22] Batch [180/261] Loss: 0.0174 LR: 0.000100
Epoch [22] Batch [190/261] Loss: 0.5619 LR: 0.000100
Epoch [22] Batch [200/261] Loss: 0.0032 LR: 0.000100
Epoch [22] Batch [210/261] Loss: 0.4906 LR: 0.000100
Epoch [22] Batch [220/261] Loss: 0.0002 LR: 0.000100
Epoch [22] Batch [230/261] Loss: 0.0002 LR: 0.000100
Epoch [22] Batch [240/261] Loss: 0.0150 LR: 0.000100
Epoch [22] Batch [250/261] Loss: 0.3433 LR: 0.000100
Epoch [22] Batch [260/261] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [23/350] - Time: 13.51s
Train Loss: 0.1237 | Val Loss: 0.0523
Train Acc: 0.9666 | Val Acc: 0.9823
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 0.990  
  Count 6: 0.990    Count 7: 0.990    Count 8: 0.929    Count 9: 0.911    Count 10: 1.000  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/best_single_image_model.pth
  → New best model! Validation accuracy: 0.9823
新的最佳模型! 验证准确率: 0.9823
Epoch [23] Batch [0/261] Loss: 0.0035 LR: 0.000100
Epoch [23] Batch [10/261] Loss: 0.0145 LR: 0.000100
Epoch [23] Batch [20/261] Loss: 0.0012 LR: 0.000100
Epoch [23] Batch [30/261] Loss: 0.0087 LR: 0.000100
Epoch [23] Batch [40/261] Loss: 0.0009 LR: 0.000100
Epoch [23] Batch [50/261] Loss: 0.3997 LR: 0.000100
Epoch [23] Batch [60/261] Loss: 0.0081 LR: 0.000100
Epoch [23] Batch [70/261] Loss: 0.0044 LR: 0.000100
Epoch [23] Batch [80/261] Loss: 0.0422 LR: 0.000100
Epoch [23] Batch [90/261] Loss: 0.0132 LR: 0.000100
Epoch [23] Batch [100/261] Loss: 0.0303 LR: 0.000100
Epoch [23] Batch [110/261] Loss: 0.0045 LR: 0.000100
Epoch [23] Batch [120/261] Loss: 0.0013 LR: 0.000100
Epoch [23] Batch [130/261] Loss: 0.0019 LR: 0.000100
Epoch [23] Batch [140/261] Loss: 0.3654 LR: 0.000100
Epoch [23] Batch [150/261] Loss: 0.0042 LR: 0.000100
Epoch [23] Batch [160/261] Loss: 0.0062 LR: 0.000100
Epoch [23] Batch [170/261] Loss: 0.0007 LR: 0.000100
Epoch [23] Batch [180/261] Loss: 0.0026 LR: 0.000100
Epoch [23] Batch [190/261] Loss: 0.0680 LR: 0.000100
Epoch [23] Batch [200/261] Loss: 0.7425 LR: 0.000100
Epoch [23] Batch [210/261] Loss: 0.0002 LR: 0.000100
Epoch [23] Batch [220/261] Loss: 0.1467 LR: 0.000100
Epoch [23] Batch [230/261] Loss: 0.0027 LR: 0.000100
Epoch [23] Batch [240/261] Loss: 0.1669 LR: 0.000100
Epoch [23] Batch [250/261] Loss: 0.1651 LR: 0.000100
Epoch [23] Batch [260/261] Loss: 0.0006 LR: 0.000100

Epoch [24/350] - Time: 13.17s
Train Loss: 0.0797 | Val Loss: 0.0575
Train Acc: 0.9740 | Val Acc: 0.9823
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.990    Count 5: 1.000  
  Count 6: 0.990    Count 7: 0.969    Count 8: 1.000    Count 9: 0.911    Count 10: 0.943  
Epoch [24] Batch [0/261] Loss: 0.0011 LR: 0.000100
Epoch [24] Batch [10/261] Loss: 0.0448 LR: 0.000100
Epoch [24] Batch [20/261] Loss: 0.1123 LR: 0.000100
Epoch [24] Batch [30/261] Loss: 0.0083 LR: 0.000100
Epoch [24] Batch [40/261] Loss: 0.0008 LR: 0.000100
Epoch [24] Batch [50/261] Loss: 0.6073 LR: 0.000100
Epoch [24] Batch [60/261] Loss: 0.0002 LR: 0.000100
Epoch [24] Batch [70/261] Loss: 0.1485 LR: 0.000100
Epoch [24] Batch [80/261] Loss: 0.0161 LR: 0.000100
Epoch [24] Batch [90/261] Loss: 0.0036 LR: 0.000100
Epoch [24] Batch [100/261] Loss: 0.0013 LR: 0.000100
Epoch [24] Batch [110/261] Loss: 0.0254 LR: 0.000100
Epoch [24] Batch [120/261] Loss: 0.0321 LR: 0.000100
Epoch [24] Batch [130/261] Loss: 0.0880 LR: 0.000100
Epoch [24] Batch [140/261] Loss: 0.1159 LR: 0.000100
Epoch [24] Batch [150/261] Loss: 0.1005 LR: 0.000100
Epoch [24] Batch [160/261] Loss: 0.0395 LR: 0.000100
Epoch [24] Batch [170/261] Loss: 0.1343 LR: 0.000100
Epoch [24] Batch [180/261] Loss: 0.0002 LR: 0.000100
Epoch [24] Batch [190/261] Loss: 0.0006 LR: 0.000100
Epoch [24] Batch [200/261] Loss: 0.0005 LR: 0.000100
Epoch [24] Batch [210/261] Loss: 0.0008 LR: 0.000100
Epoch [24] Batch [220/261] Loss: 0.0129 LR: 0.000100
Epoch [24] Batch [230/261] Loss: 0.0009 LR: 0.000100
Epoch [24] Batch [240/261] Loss: 0.0467 LR: 0.000100
Epoch [24] Batch [250/261] Loss: 0.1413 LR: 0.000100
Epoch [24] Batch [260/261] Loss: 0.0003 LR: 0.000100

Epoch [25/350] - Time: 13.25s
Train Loss: 0.1159 | Val Loss: 1.0902
Train Acc: 0.9654 | Val Acc: 0.7542
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.886    Count 5: 0.892  
  Count 6: 0.500    Count 7: 0.438    Count 8: 0.596    Count 9: 0.467    Count 10: 0.443  
Epoch [25] Batch [0/261] Loss: 0.1434 LR: 0.000100
Epoch [25] Batch [10/261] Loss: 0.0305 LR: 0.000100
Epoch [25] Batch [20/261] Loss: 0.4687 LR: 0.000100
Epoch [25] Batch [30/261] Loss: 0.3870 LR: 0.000100
Epoch [25] Batch [40/261] Loss: 0.0009 LR: 0.000100
Epoch [25] Batch [50/261] Loss: 0.0039 LR: 0.000100
Epoch [25] Batch [60/261] Loss: 0.0105 LR: 0.000100
Epoch [25] Batch [70/261] Loss: 0.7226 LR: 0.000100
Epoch [25] Batch [80/261] Loss: 0.0460 LR: 0.000100
Epoch [25] Batch [90/261] Loss: 0.0006 LR: 0.000100
Epoch [25] Batch [100/261] Loss: 0.2492 LR: 0.000100
Epoch [25] Batch [110/261] Loss: 0.3539 LR: 0.000100
Epoch [25] Batch [120/261] Loss: 0.0043 LR: 0.000100
Epoch [25] Batch [130/261] Loss: 0.0054 LR: 0.000100
Epoch [25] Batch [140/261] Loss: 0.0005 LR: 0.000100
Epoch [25] Batch [150/261] Loss: 0.0142 LR: 0.000100
Epoch [25] Batch [160/261] Loss: 0.0008 LR: 0.000100
Epoch [25] Batch [170/261] Loss: 0.1259 LR: 0.000100
Epoch [25] Batch [180/261] Loss: 0.0014 LR: 0.000100
Epoch [25] Batch [190/261] Loss: 0.0773 LR: 0.000100
Epoch [25] Batch [200/261] Loss: 0.0004 LR: 0.000100
Epoch [25] Batch [210/261] Loss: 0.0007 LR: 0.000100
Epoch [25] Batch [220/261] Loss: 0.0056 LR: 0.000100
Epoch [25] Batch [230/261] Loss: 0.0155 LR: 0.000100
Epoch [25] Batch [240/261] Loss: 0.0009 LR: 0.000100
Epoch [25] Batch [250/261] Loss: 0.0008 LR: 0.000100
Epoch [25] Batch [260/261] Loss: 9.3299 LR: 0.000100

Epoch [26/350] - Time: 13.64s
Train Loss: 0.1122 | Val Loss: 0.0869
Train Acc: 0.9656 | Val Acc: 0.9758
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 0.990    Count 8: 1.000    Count 9: 0.989    Count 10: 0.739  
Epoch [26] Batch [0/261] Loss: 0.0082 LR: 0.000100
Epoch [26] Batch [10/261] Loss: 0.0013 LR: 0.000100
Epoch [26] Batch [20/261] Loss: 0.0799 LR: 0.000100
Epoch [26] Batch [30/261] Loss: 0.0004 LR: 0.000100
Epoch [26] Batch [40/261] Loss: 0.0006 LR: 0.000100
Epoch [26] Batch [50/261] Loss: 0.2818 LR: 0.000100
Epoch [26] Batch [60/261] Loss: 0.1670 LR: 0.000100
Epoch [26] Batch [70/261] Loss: 0.2506 LR: 0.000100
Epoch [26] Batch [80/261] Loss: 0.0010 LR: 0.000100
Epoch [26] Batch [90/261] Loss: 0.0084 LR: 0.000100
Epoch [26] Batch [100/261] Loss: 0.0129 LR: 0.000100
Epoch [26] Batch [110/261] Loss: 0.0008 LR: 0.000100
Epoch [26] Batch [120/261] Loss: 0.0006 LR: 0.000100
Epoch [26] Batch [130/261] Loss: 0.0001 LR: 0.000100
Epoch [26] Batch [140/261] Loss: 0.1395 LR: 0.000100
Epoch [26] Batch [150/261] Loss: 0.0009 LR: 0.000100
Epoch [26] Batch [160/261] Loss: 0.0071 LR: 0.000100
Epoch [26] Batch [170/261] Loss: 0.0005 LR: 0.000100
Epoch [26] Batch [180/261] Loss: 0.0007 LR: 0.000100
Epoch [26] Batch [190/261] Loss: 0.0967 LR: 0.000100
Epoch [26] Batch [200/261] Loss: 0.0594 LR: 0.000100
Epoch [26] Batch [210/261] Loss: 0.0017 LR: 0.000100
Epoch [26] Batch [220/261] Loss: 0.0064 LR: 0.000100
Epoch [26] Batch [230/261] Loss: 0.1559 LR: 0.000100
Epoch [26] Batch [240/261] Loss: 0.0531 LR: 0.000100
Epoch [26] Batch [250/261] Loss: 0.0002 LR: 0.000100
Epoch [26] Batch [260/261] Loss: 8.8165 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [27/350] - Time: 13.50s
Train Loss: 0.0831 | Val Loss: 0.0313
Train Acc: 0.9779 | Val Acc: 0.9888
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 0.990    Count 8: 0.990    Count 9: 0.978    Count 10: 0.932  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/best_single_image_model.pth
  → New best model! Validation accuracy: 0.9888
新的最佳模型! 验证准确率: 0.9888
Epoch [27] Batch [0/261] Loss: 0.0843 LR: 0.000100
Epoch [27] Batch [10/261] Loss: 0.0001 LR: 0.000100
Epoch [27] Batch [20/261] Loss: 0.0007 LR: 0.000100
Epoch [27] Batch [30/261] Loss: 0.2980 LR: 0.000100
Epoch [27] Batch [40/261] Loss: 0.0032 LR: 0.000100
Epoch [27] Batch [50/261] Loss: 0.0015 LR: 0.000100
Epoch [27] Batch [60/261] Loss: 0.0120 LR: 0.000100
Epoch [27] Batch [70/261] Loss: 0.0007 LR: 0.000100
Epoch [27] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [27] Batch [90/261] Loss: 0.0122 LR: 0.000100
Epoch [27] Batch [100/261] Loss: 0.0091 LR: 0.000100
Epoch [27] Batch [110/261] Loss: 0.0001 LR: 0.000100
Epoch [27] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [27] Batch [130/261] Loss: 0.0569 LR: 0.000100
Epoch [27] Batch [140/261] Loss: 0.0005 LR: 0.000100
Epoch [27] Batch [150/261] Loss: 0.0009 LR: 0.000100
Epoch [27] Batch [160/261] Loss: 0.0059 LR: 0.000100
Epoch [27] Batch [170/261] Loss: 0.0002 LR: 0.000100
Epoch [27] Batch [180/261] Loss: 0.2807 LR: 0.000100
Epoch [27] Batch [190/261] Loss: 0.0010 LR: 0.000100
Epoch [27] Batch [200/261] Loss: 0.0001 LR: 0.000100
Epoch [27] Batch [210/261] Loss: 0.0670 LR: 0.000100
Epoch [27] Batch [220/261] Loss: 0.0024 LR: 0.000100
Epoch [27] Batch [230/261] Loss: 0.1389 LR: 0.000100
Epoch [27] Batch [240/261] Loss: 0.0712 LR: 0.000100
Epoch [27] Batch [250/261] Loss: 0.0001 LR: 0.000100
Epoch [27] Batch [260/261] Loss: 0.0004 LR: 0.000100

Epoch [28/350] - Time: 13.33s
Train Loss: 0.0719 | Val Loss: 0.0962
Train Acc: 0.9796 | Val Acc: 0.9758
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 0.917    Count 8: 0.970    Count 9: 0.878    Count 10: 0.977  
Epoch [28] Batch [0/261] Loss: 0.0344 LR: 0.000100
Epoch [28] Batch [10/261] Loss: 0.0018 LR: 0.000100
Epoch [28] Batch [20/261] Loss: 0.1653 LR: 0.000100
Epoch [28] Batch [30/261] Loss: 0.0021 LR: 0.000100
Epoch [28] Batch [40/261] Loss: 0.0308 LR: 0.000100
Epoch [28] Batch [50/261] Loss: 1.5202 LR: 0.000100
Epoch [28] Batch [60/261] Loss: 0.0009 LR: 0.000100
Epoch [28] Batch [70/261] Loss: 0.0018 LR: 0.000100
Epoch [28] Batch [80/261] Loss: 0.0011 LR: 0.000100
Epoch [28] Batch [90/261] Loss: 0.0814 LR: 0.000100
Epoch [28] Batch [100/261] Loss: 0.0325 LR: 0.000100
Epoch [28] Batch [110/261] Loss: 0.0002 LR: 0.000100
Epoch [28] Batch [120/261] Loss: 0.0106 LR: 0.000100
Epoch [28] Batch [130/261] Loss: 0.0113 LR: 0.000100
Epoch [28] Batch [140/261] Loss: 0.0176 LR: 0.000100
Epoch [28] Batch [150/261] Loss: 0.0003 LR: 0.000100
Epoch [28] Batch [160/261] Loss: 0.0003 LR: 0.000100
Epoch [28] Batch [170/261] Loss: 0.0005 LR: 0.000100
Epoch [28] Batch [180/261] Loss: 0.0008 LR: 0.000100
Epoch [28] Batch [190/261] Loss: 0.0074 LR: 0.000100
Epoch [28] Batch [200/261] Loss: 0.0002 LR: 0.000100
Epoch [28] Batch [210/261] Loss: 0.0048 LR: 0.000100
Epoch [28] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [28] Batch [230/261] Loss: 0.0032 LR: 0.000100
Epoch [28] Batch [240/261] Loss: 0.0038 LR: 0.000100
Epoch [28] Batch [250/261] Loss: 0.0796 LR: 0.000100
Epoch [28] Batch [260/261] Loss: 24.5071 LR: 0.000100

Epoch [29/350] - Time: 13.66s
Train Loss: 0.0953 | Val Loss: 0.1054
Train Acc: 0.9752 | Val Acc: 0.9721
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 0.980  
  Count 6: 0.959    Count 7: 0.990    Count 8: 0.970    Count 9: 0.989    Count 10: 0.795  
Epoch [29] Batch [0/261] Loss: 0.0988 LR: 0.000100
Epoch [29] Batch [10/261] Loss: 0.0025 LR: 0.000100
Epoch [29] Batch [20/261] Loss: 0.0222 LR: 0.000100
Epoch [29] Batch [30/261] Loss: 0.0001 LR: 0.000100
Epoch [29] Batch [40/261] Loss: 0.0047 LR: 0.000100
Epoch [29] Batch [50/261] Loss: 0.0010 LR: 0.000100
Epoch [29] Batch [60/261] Loss: 0.0527 LR: 0.000100
Epoch [29] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [29] Batch [80/261] Loss: 0.0021 LR: 0.000100
Epoch [29] Batch [90/261] Loss: 0.0005 LR: 0.000100
Epoch [29] Batch [100/261] Loss: 0.0025 LR: 0.000100
Epoch [29] Batch [110/261] Loss: 0.2940 LR: 0.000100
Epoch [29] Batch [120/261] Loss: 0.0033 LR: 0.000100
Epoch [29] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [29] Batch [140/261] Loss: 0.0326 LR: 0.000100
Epoch [29] Batch [150/261] Loss: 0.0001 LR: 0.000100
Epoch [29] Batch [160/261] Loss: 0.0043 LR: 0.000100
Epoch [29] Batch [170/261] Loss: 0.0089 LR: 0.000100
Epoch [29] Batch [180/261] Loss: 0.0005 LR: 0.000100
Epoch [29] Batch [190/261] Loss: 0.0006 LR: 0.000100
Epoch [29] Batch [200/261] Loss: 0.0029 LR: 0.000100
Epoch [29] Batch [210/261] Loss: 0.0222 LR: 0.000100
Epoch [29] Batch [220/261] Loss: 0.0018 LR: 0.000100
Epoch [29] Batch [230/261] Loss: 0.2501 LR: 0.000100
Epoch [29] Batch [240/261] Loss: 0.0084 LR: 0.000100
Epoch [29] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [29] Batch [260/261] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [30/350] - Time: 13.11s
Train Loss: 0.0911 | Val Loss: 0.0888
Train Acc: 0.9748 | Val Acc: 0.9749
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.981    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.969    Count 7: 0.938    Count 8: 0.929    Count 9: 0.978    Count 10: 0.920  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/single_image_checkpoint_epoch_29.pth
Epoch [30] Batch [0/261] Loss: 0.0006 LR: 0.000100
Epoch [30] Batch [10/261] Loss: 0.0324 LR: 0.000100
Epoch [30] Batch [20/261] Loss: 0.0002 LR: 0.000100
Epoch [30] Batch [30/261] Loss: 1.1860 LR: 0.000100
Epoch [30] Batch [40/261] Loss: 0.0013 LR: 0.000100
Epoch [30] Batch [50/261] Loss: 0.0001 LR: 0.000100
Epoch [30] Batch [60/261] Loss: 0.0001 LR: 0.000100
Epoch [30] Batch [70/261] Loss: 0.0262 LR: 0.000100
Epoch [30] Batch [80/261] Loss: 0.0085 LR: 0.000100
Epoch [30] Batch [90/261] Loss: 0.0002 LR: 0.000100
Epoch [30] Batch [100/261] Loss: 0.1475 LR: 0.000100
Epoch [30] Batch [110/261] Loss: 0.1854 LR: 0.000100
Epoch [30] Batch [120/261] Loss: 0.0242 LR: 0.000100
Epoch [30] Batch [130/261] Loss: 0.0006 LR: 0.000100
Epoch [30] Batch [140/261] Loss: 0.0347 LR: 0.000100
Epoch [30] Batch [150/261] Loss: 0.0005 LR: 0.000100
Epoch [30] Batch [160/261] Loss: 0.0003 LR: 0.000100
Epoch [30] Batch [170/261] Loss: 0.0007 LR: 0.000100
Epoch [30] Batch [180/261] Loss: 0.0016 LR: 0.000100
Epoch [30] Batch [190/261] Loss: 0.0921 LR: 0.000100
Epoch [30] Batch [200/261] Loss: 0.0002 LR: 0.000100
Epoch [30] Batch [210/261] Loss: 0.0011 LR: 0.000100
Epoch [30] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [30] Batch [230/261] Loss: 0.1295 LR: 0.000100
Epoch [30] Batch [240/261] Loss: 0.0006 LR: 0.000100
Epoch [30] Batch [250/261] Loss: 0.0175 LR: 0.000100
Epoch [30] Batch [260/261] Loss: 0.0119 LR: 0.000100

Epoch [31/350] - Time: 13.43s
Train Loss: 0.0793 | Val Loss: 0.5517
Train Acc: 0.9776 | Val Acc: 0.8790
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 0.971  
  Count 6: 0.898    Count 7: 0.688    Count 8: 0.475    Count 9: 0.622    Count 10: 1.000  
Epoch [31] Batch [0/261] Loss: 0.2768 LR: 0.000100
Epoch [31] Batch [10/261] Loss: 0.0004 LR: 0.000100
Epoch [31] Batch [20/261] Loss: 0.0004 LR: 0.000100
Epoch [31] Batch [30/261] Loss: 0.0021 LR: 0.000100
Epoch [31] Batch [40/261] Loss: 0.0045 LR: 0.000100
Epoch [31] Batch [50/261] Loss: 0.3095 LR: 0.000100
Epoch [31] Batch [60/261] Loss: 0.0028 LR: 0.000100
Epoch [31] Batch [70/261] Loss: 0.1940 LR: 0.000100
Epoch [31] Batch [80/261] Loss: 0.0003 LR: 0.000100
Epoch [31] Batch [90/261] Loss: 0.0031 LR: 0.000100
Epoch [31] Batch [100/261] Loss: 0.0001 LR: 0.000100
Epoch [31] Batch [110/261] Loss: 0.0001 LR: 0.000100
Epoch [31] Batch [120/261] Loss: 0.0098 LR: 0.000100
Epoch [31] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [31] Batch [140/261] Loss: 0.0035 LR: 0.000100
Epoch [31] Batch [150/261] Loss: 0.0001 LR: 0.000100
Epoch [31] Batch [160/261] Loss: 0.0063 LR: 0.000100
Epoch [31] Batch [170/261] Loss: 0.0001 LR: 0.000100
Epoch [31] Batch [180/261] Loss: 0.0001 LR: 0.000100
Epoch [31] Batch [190/261] Loss: 0.0237 LR: 0.000100
Epoch [31] Batch [200/261] Loss: 0.0009 LR: 0.000100
Epoch [31] Batch [210/261] Loss: 0.0005 LR: 0.000100
Epoch [31] Batch [220/261] Loss: 0.0091 LR: 0.000100
Epoch [31] Batch [230/261] Loss: 0.0049 LR: 0.000100
Epoch [31] Batch [240/261] Loss: 0.0001 LR: 0.000100
Epoch [31] Batch [250/261] Loss: 0.0003 LR: 0.000100
Epoch [31] Batch [260/261] Loss: 0.0006 LR: 0.000100

Epoch [32/350] - Time: 13.25s
Train Loss: 0.0869 | Val Loss: 0.0577
Train Acc: 0.9793 | Val Acc: 0.9851
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 0.979    Count 8: 0.990    Count 9: 0.967    Count 10: 0.909  
Epoch [32] Batch [0/261] Loss: 0.0367 LR: 0.000100
Epoch [32] Batch [10/261] Loss: 0.0002 LR: 0.000100
Epoch [32] Batch [20/261] Loss: 0.0004 LR: 0.000100
Epoch [32] Batch [30/261] Loss: 0.0003 LR: 0.000100
Epoch [32] Batch [40/261] Loss: 0.0005 LR: 0.000100
Epoch [32] Batch [50/261] Loss: 0.3666 LR: 0.000100
Epoch [32] Batch [60/261] Loss: 0.0004 LR: 0.000100
Epoch [32] Batch [70/261] Loss: 0.2072 LR: 0.000100
Epoch [32] Batch [80/261] Loss: 0.0053 LR: 0.000100
Epoch [32] Batch [90/261] Loss: 0.0004 LR: 0.000100
Epoch [32] Batch [100/261] Loss: 0.0008 LR: 0.000100
Epoch [32] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [32] Batch [120/261] Loss: 0.0005 LR: 0.000100
Epoch [32] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [32] Batch [140/261] Loss: 0.0517 LR: 0.000100
Epoch [32] Batch [150/261] Loss: 0.1326 LR: 0.000100
Epoch [32] Batch [160/261] Loss: 0.0005 LR: 0.000100
Epoch [32] Batch [170/261] Loss: 0.0005 LR: 0.000100
Epoch [32] Batch [180/261] Loss: 0.0022 LR: 0.000100
Epoch [32] Batch [190/261] Loss: 0.0001 LR: 0.000100
Epoch [32] Batch [200/261] Loss: 0.0001 LR: 0.000100
Epoch [32] Batch [210/261] Loss: 0.0002 LR: 0.000100
Epoch [32] Batch [220/261] Loss: 0.0355 LR: 0.000100
Epoch [32] Batch [230/261] Loss: 0.0004 LR: 0.000100
Epoch [32] Batch [240/261] Loss: 0.0006 LR: 0.000100
Epoch [32] Batch [250/261] Loss: 0.1681 LR: 0.000100
Epoch [32] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [33/350] - Time: 13.23s
Train Loss: 0.0644 | Val Loss: 0.0512
Train Acc: 0.9844 | Val Acc: 0.9832
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 0.969    Count 8: 1.000    Count 9: 0.956    Count 10: 0.898  
Epoch [33] Batch [0/261] Loss: 0.0004 LR: 0.000100
Epoch [33] Batch [10/261] Loss: 0.5354 LR: 0.000100
Epoch [33] Batch [20/261] Loss: 0.0001 LR: 0.000100
Epoch [33] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [33] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [33] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [33] Batch [60/261] Loss: 0.0001 LR: 0.000100
Epoch [33] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [33] Batch [80/261] Loss: 0.0012 LR: 0.000100
Epoch [33] Batch [90/261] Loss: 0.0020 LR: 0.000100
Epoch [33] Batch [100/261] Loss: 0.0002 LR: 0.000100
Epoch [33] Batch [110/261] Loss: 0.0009 LR: 0.000100
Epoch [33] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [33] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [33] Batch [140/261] Loss: 0.0207 LR: 0.000100
Epoch [33] Batch [150/261] Loss: 0.0072 LR: 0.000100
Epoch [33] Batch [160/261] Loss: 0.0001 LR: 0.000100
Epoch [33] Batch [170/261] Loss: 0.0001 LR: 0.000100
Epoch [33] Batch [180/261] Loss: 0.8615 LR: 0.000100
Epoch [33] Batch [190/261] Loss: 0.0012 LR: 0.000100
Epoch [33] Batch [200/261] Loss: 0.0015 LR: 0.000100
Epoch [33] Batch [210/261] Loss: 0.0301 LR: 0.000100
Epoch [33] Batch [220/261] Loss: 0.0334 LR: 0.000100
Epoch [33] Batch [230/261] Loss: 0.0001 LR: 0.000100
Epoch [33] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [33] Batch [250/261] Loss: 0.7309 LR: 0.000100
Epoch [33] Batch [260/261] Loss: 0.0008 LR: 0.000100

Epoch [34/350] - Time: 13.53s
Train Loss: 0.0739 | Val Loss: 0.4423
Train Acc: 0.9817 | Val Acc: 0.9171
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.980  
  Count 6: 0.867    Count 7: 0.823    Count 8: 0.879    Count 9: 0.500    Count 10: 1.000  
Epoch [34] Batch [0/261] Loss: 0.0388 LR: 0.000100
Epoch [34] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [34] Batch [20/261] Loss: 0.0006 LR: 0.000100
Epoch [34] Batch [30/261] Loss: 0.0273 LR: 0.000100
Epoch [34] Batch [40/261] Loss: 0.0173 LR: 0.000100
Epoch [34] Batch [50/261] Loss: 0.0004 LR: 0.000100
Epoch [34] Batch [60/261] Loss: 0.0004 LR: 0.000100
Epoch [34] Batch [70/261] Loss: 0.0004 LR: 0.000100
Epoch [34] Batch [80/261] Loss: 0.0001 LR: 0.000100
Epoch [34] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [34] Batch [100/261] Loss: 0.0246 LR: 0.000100
Epoch [34] Batch [110/261] Loss: 0.0162 LR: 0.000100
Epoch [34] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [34] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [34] Batch [140/261] Loss: 0.0191 LR: 0.000100
Epoch [34] Batch [150/261] Loss: 0.0500 LR: 0.000100
Epoch [34] Batch [160/261] Loss: 0.0001 LR: 0.000100
Epoch [34] Batch [170/261] Loss: 0.0001 LR: 0.000100
Epoch [34] Batch [180/261] Loss: 0.5524 LR: 0.000100
Epoch [34] Batch [190/261] Loss: 0.0002 LR: 0.000100
Epoch [34] Batch [200/261] Loss: 0.0006 LR: 0.000100
Epoch [34] Batch [210/261] Loss: 0.0015 LR: 0.000100
Epoch [34] Batch [220/261] Loss: 0.0219 LR: 0.000100
Epoch [34] Batch [230/261] Loss: 0.0132 LR: 0.000100
Epoch [34] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [34] Batch [250/261] Loss: 0.1267 LR: 0.000100
Epoch [34] Batch [260/261] Loss: 2.0751 LR: 0.000100

Epoch [35/350] - Time: 13.77s
Train Loss: 0.1119 | Val Loss: 0.0605
Train Acc: 0.9736 | Val Acc: 0.9851
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.990    Count 5: 1.000  
  Count 6: 0.969    Count 7: 0.979    Count 8: 1.000    Count 9: 0.911    Count 10: 0.989  
Epoch [35] Batch [0/261] Loss: 0.0245 LR: 0.000100
Epoch [35] Batch [10/261] Loss: 0.0021 LR: 0.000100
Epoch [35] Batch [20/261] Loss: 0.3711 LR: 0.000100
Epoch [35] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [35] Batch [40/261] Loss: 0.0995 LR: 0.000100
Epoch [35] Batch [50/261] Loss: 0.0061 LR: 0.000100
Epoch [35] Batch [60/261] Loss: 0.0003 LR: 0.000100
Epoch [35] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [35] Batch [80/261] Loss: 0.0007 LR: 0.000100
Epoch [35] Batch [90/261] Loss: 0.0052 LR: 0.000100
Epoch [35] Batch [100/261] Loss: 0.0002 LR: 0.000100
Epoch [35] Batch [110/261] Loss: 0.0185 LR: 0.000100
Epoch [35] Batch [120/261] Loss: 0.0094 LR: 0.000100
Epoch [35] Batch [130/261] Loss: 0.0004 LR: 0.000100
Epoch [35] Batch [140/261] Loss: 0.0006 LR: 0.000100
Epoch [35] Batch [150/261] Loss: 0.0001 LR: 0.000100
Epoch [35] Batch [160/261] Loss: 0.0001 LR: 0.000100
Epoch [35] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [35] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [35] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [35] Batch [200/261] Loss: 0.0403 LR: 0.000100
Epoch [35] Batch [210/261] Loss: 0.0017 LR: 0.000100
Epoch [35] Batch [220/261] Loss: 0.3234 LR: 0.000100
Epoch [35] Batch [230/261] Loss: 0.0319 LR: 0.000100
Epoch [35] Batch [240/261] Loss: 0.0010 LR: 0.000100
Epoch [35] Batch [250/261] Loss: 0.0790 LR: 0.000100
Epoch [35] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [36/350] - Time: 13.21s
Train Loss: 0.0993 | Val Loss: 0.2746
Train Acc: 0.9762 | Val Acc: 0.9395
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.980    Count 7: 0.958    Count 8: 0.990    Count 9: 0.856    Count 10: 0.500  
Epoch [36] Batch [0/261] Loss: 0.0016 LR: 0.000100
Epoch [36] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [36] Batch [20/261] Loss: 0.0214 LR: 0.000100
Epoch [36] Batch [30/261] Loss: 0.0001 LR: 0.000100
Epoch [36] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [36] Batch [50/261] Loss: 0.0001 LR: 0.000100
Epoch [36] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [36] Batch [70/261] Loss: 0.0014 LR: 0.000100
Epoch [36] Batch [80/261] Loss: 0.0004 LR: 0.000100
Epoch [36] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [36] Batch [100/261] Loss: 0.0025 LR: 0.000100
Epoch [36] Batch [110/261] Loss: 0.0001 LR: 0.000100
Epoch [36] Batch [120/261] Loss: 0.0005 LR: 0.000100
Epoch [36] Batch [130/261] Loss: 0.0046 LR: 0.000100
Epoch [36] Batch [140/261] Loss: 0.0085 LR: 0.000100
Epoch [36] Batch [150/261] Loss: 0.1588 LR: 0.000100
Epoch [36] Batch [160/261] Loss: 0.8358 LR: 0.000100
Epoch [36] Batch [170/261] Loss: 0.2446 LR: 0.000100
Epoch [36] Batch [180/261] Loss: 0.0004 LR: 0.000100
Epoch [36] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [36] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [36] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [36] Batch [220/261] Loss: 0.0017 LR: 0.000100
Epoch [36] Batch [230/261] Loss: 0.0019 LR: 0.000100
Epoch [36] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [36] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [36] Batch [260/261] Loss: 0.0026 LR: 0.000100

Epoch [37/350] - Time: 13.62s
Train Loss: 0.0563 | Val Loss: 0.1050
Train Acc: 0.9861 | Val Acc: 0.9777
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 0.990    Count 8: 0.960    Count 9: 0.822    Count 10: 0.989  
Epoch [37] Batch [0/261] Loss: 0.0031 LR: 0.000100
Epoch [37] Batch [10/261] Loss: 0.3867 LR: 0.000100
Epoch [37] Batch [20/261] Loss: 0.7021 LR: 0.000100
Epoch [37] Batch [30/261] Loss: 0.0001 LR: 0.000100
Epoch [37] Batch [40/261] Loss: 0.0003 LR: 0.000100
Epoch [37] Batch [50/261] Loss: 0.0001 LR: 0.000100
Epoch [37] Batch [60/261] Loss: 0.0392 LR: 0.000100
Epoch [37] Batch [70/261] Loss: 0.0008 LR: 0.000100
Epoch [37] Batch [80/261] Loss: 0.0739 LR: 0.000100
Epoch [37] Batch [90/261] Loss: 0.0137 LR: 0.000100
Epoch [37] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [37] Batch [110/261] Loss: 0.0001 LR: 0.000100
Epoch [37] Batch [120/261] Loss: 0.0019 LR: 0.000100
Epoch [37] Batch [130/261] Loss: 0.0003 LR: 0.000100
Epoch [37] Batch [140/261] Loss: 0.0035 LR: 0.000100
Epoch [37] Batch [150/261] Loss: 0.0001 LR: 0.000100
Epoch [37] Batch [160/261] Loss: 0.0813 LR: 0.000100
Epoch [37] Batch [170/261] Loss: 0.0046 LR: 0.000100
Epoch [37] Batch [180/261] Loss: 0.0245 LR: 0.000100
Epoch [37] Batch [190/261] Loss: 0.0008 LR: 0.000100
Epoch [37] Batch [200/261] Loss: 1.8271 LR: 0.000100
Epoch [37] Batch [210/261] Loss: 0.0028 LR: 0.000100
Epoch [37] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [37] Batch [230/261] Loss: 0.0002 LR: 0.000100
Epoch [37] Batch [240/261] Loss: 0.0006 LR: 0.000100
Epoch [37] Batch [250/261] Loss: 0.0012 LR: 0.000100
Epoch [37] Batch [260/261] Loss: 0.0002 LR: 0.000100

Epoch [38/350] - Time: 13.41s
Train Loss: 0.1115 | Val Loss: 0.8919
Train Acc: 0.9755 | Val Acc: 0.8426
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.933    Count 5: 0.794  
  Count 6: 0.673    Count 7: 0.719    Count 8: 0.404    Count 9: 0.756    Count 10: 1.000  
Epoch [38] Batch [0/261] Loss: 0.1618 LR: 0.000100
Epoch [38] Batch [10/261] Loss: 0.0001 LR: 0.000100
Epoch [38] Batch [20/261] Loss: 0.1324 LR: 0.000100
Epoch [38] Batch [30/261] Loss: 0.0002 LR: 0.000100
Epoch [38] Batch [40/261] Loss: 0.0001 LR: 0.000100
Epoch [38] Batch [50/261] Loss: 0.0003 LR: 0.000100
Epoch [38] Batch [60/261] Loss: 0.0005 LR: 0.000100
Epoch [38] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [38] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [38] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [38] Batch [100/261] Loss: 0.0002 LR: 0.000100
Epoch [38] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [38] Batch [120/261] Loss: 0.0001 LR: 0.000100
Epoch [38] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [38] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [38] Batch [150/261] Loss: 0.0006 LR: 0.000100
Epoch [38] Batch [160/261] Loss: 0.0080 LR: 0.000100
Epoch [38] Batch [170/261] Loss: 0.0006 LR: 0.000100
Epoch [38] Batch [180/261] Loss: 0.3121 LR: 0.000100
Epoch [38] Batch [190/261] Loss: 0.0005 LR: 0.000100
Epoch [38] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [38] Batch [210/261] Loss: 0.6827 LR: 0.000100
Epoch [38] Batch [220/261] Loss: 0.0020 LR: 0.000100
Epoch [38] Batch [230/261] Loss: 0.0002 LR: 0.000100
Epoch [38] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [38] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [38] Batch [260/261] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [39/350] - Time: 13.25s
Train Loss: 0.0447 | Val Loss: 0.0488
Train Acc: 0.9894 | Val Acc: 0.9898
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.990    Count 5: 1.000  
  Count 6: 0.980    Count 7: 0.990    Count 8: 0.970    Count 9: 0.978    Count 10: 0.989  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/best_single_image_model.pth
  → New best model! Validation accuracy: 0.9898
新的最佳模型! 验证准确率: 0.9898
Epoch [39] Batch [0/261] Loss: 0.0012 LR: 0.000100
Epoch [39] Batch [10/261] Loss: 0.0121 LR: 0.000100
Epoch [39] Batch [20/261] Loss: 0.0550 LR: 0.000100
Epoch [39] Batch [30/261] Loss: 0.0392 LR: 0.000100
Epoch [39] Batch [40/261] Loss: 1.4494 LR: 0.000100
Epoch [39] Batch [50/261] Loss: 0.0002 LR: 0.000100
Epoch [39] Batch [60/261] Loss: 0.0004 LR: 0.000100
Epoch [39] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [39] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [39] Batch [90/261] Loss: 0.0003 LR: 0.000100
Epoch [39] Batch [100/261] Loss: 0.0928 LR: 0.000100
Epoch [39] Batch [110/261] Loss: 0.0001 LR: 0.000100
Epoch [39] Batch [120/261] Loss: 0.0031 LR: 0.000100
Epoch [39] Batch [130/261] Loss: 0.0002 LR: 0.000100
Epoch [39] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [39] Batch [150/261] Loss: 0.0014 LR: 0.000100
Epoch [39] Batch [160/261] Loss: 0.0001 LR: 0.000100
Epoch [39] Batch [170/261] Loss: 0.0585 LR: 0.000100
Epoch [39] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [39] Batch [190/261] Loss: 0.5459 LR: 0.000100
Epoch [39] Batch [200/261] Loss: 0.2103 LR: 0.000100
Epoch [39] Batch [210/261] Loss: 0.0011 LR: 0.000100
Epoch [39] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [39] Batch [230/261] Loss: 0.0019 LR: 0.000100
Epoch [39] Batch [240/261] Loss: 0.0098 LR: 0.000100
Epoch [39] Batch [250/261] Loss: 0.0001 LR: 0.000100
Epoch [39] Batch [260/261] Loss: 20.7038 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [40/350] - Time: 13.12s
Train Loss: 0.0941 | Val Loss: 0.0727
Train Acc: 0.9791 | Val Acc: 0.9842
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.990    Count 5: 0.990  
  Count 6: 0.990    Count 7: 1.000    Count 8: 0.939    Count 9: 0.922    Count 10: 1.000  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/single_image_checkpoint_epoch_39.pth
Epoch [40] Batch [0/261] Loss: 0.0001 LR: 0.000100
Epoch [40] Batch [10/261] Loss: 0.0023 LR: 0.000100
Epoch [40] Batch [20/261] Loss: 0.0001 LR: 0.000100
Epoch [40] Batch [30/261] Loss: 0.0514 LR: 0.000100
Epoch [40] Batch [40/261] Loss: 0.0005 LR: 0.000100
Epoch [40] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [40] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [40] Batch [70/261] Loss: 0.0001 LR: 0.000100
Epoch [40] Batch [80/261] Loss: 0.0006 LR: 0.000100
Epoch [40] Batch [90/261] Loss: 0.3381 LR: 0.000100
Epoch [40] Batch [100/261] Loss: 0.1520 LR: 0.000100
Epoch [40] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [40] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [40] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [40] Batch [140/261] Loss: 0.0049 LR: 0.000100
Epoch [40] Batch [150/261] Loss: 0.0035 LR: 0.000100
Epoch [40] Batch [160/261] Loss: 1.2947 LR: 0.000100
Epoch [40] Batch [170/261] Loss: 0.0018 LR: 0.000100
Epoch [40] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [40] Batch [190/261] Loss: 0.2454 LR: 0.000100
Epoch [40] Batch [200/261] Loss: 0.0069 LR: 0.000100
Epoch [40] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [40] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [40] Batch [230/261] Loss: 0.4230 LR: 0.000100
Epoch [40] Batch [240/261] Loss: 0.1956 LR: 0.000100
Epoch [40] Batch [250/261] Loss: 0.2727 LR: 0.000100
Epoch [40] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [41/350] - Time: 13.24s
Train Loss: 0.0878 | Val Loss: 1.6090
Train Acc: 0.9772 | Val Acc: 0.8194
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.990  
  Count 6: 0.908    Count 7: 0.708    Count 8: 0.273    Count 9: 0.067    Count 10: 1.000  
Epoch [41] Batch [0/261] Loss: 0.0757 LR: 0.000100
Epoch [41] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [41] Batch [20/261] Loss: 0.0001 LR: 0.000100
Epoch [41] Batch [30/261] Loss: 0.0122 LR: 0.000100
Epoch [41] Batch [40/261] Loss: 0.1677 LR: 0.000100
Epoch [41] Batch [50/261] Loss: 0.0004 LR: 0.000100
Epoch [41] Batch [60/261] Loss: 0.2768 LR: 0.000100
Epoch [41] Batch [70/261] Loss: 0.0002 LR: 0.000100
Epoch [41] Batch [80/261] Loss: 0.0001 LR: 0.000100
Epoch [41] Batch [90/261] Loss: 0.0084 LR: 0.000100
Epoch [41] Batch [100/261] Loss: 0.3653 LR: 0.000100
Epoch [41] Batch [110/261] Loss: 0.0002 LR: 0.000100
Epoch [41] Batch [120/261] Loss: 0.0005 LR: 0.000100
Epoch [41] Batch [130/261] Loss: 0.0149 LR: 0.000100
Epoch [41] Batch [140/261] Loss: 0.0179 LR: 0.000100
Epoch [41] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [41] Batch [160/261] Loss: 0.2498 LR: 0.000100
Epoch [41] Batch [170/261] Loss: 0.0004 LR: 0.000100
Epoch [41] Batch [180/261] Loss: 0.0003 LR: 0.000100
Epoch [41] Batch [190/261] Loss: 0.0164 LR: 0.000100
Epoch [41] Batch [200/261] Loss: 0.0025 LR: 0.000100
Epoch [41] Batch [210/261] Loss: 0.0001 LR: 0.000100
Epoch [41] Batch [220/261] Loss: 0.0344 LR: 0.000100
Epoch [41] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [41] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [41] Batch [250/261] Loss: 0.0847 LR: 0.000100
Epoch [41] Batch [260/261] Loss: 0.0041 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [42/350] - Time: 13.54s
Train Loss: 0.0796 | Val Loss: 0.0377
Train Acc: 0.9844 | Val Acc: 0.9935
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.980    Count 7: 0.979    Count 8: 0.990    Count 9: 0.978    Count 10: 1.000  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/best_single_image_model.pth
  → New best model! Validation accuracy: 0.9935
新的最佳模型! 验证准确率: 0.9935
Epoch [42] Batch [0/261] Loss: 0.0001 LR: 0.000100
Epoch [42] Batch [10/261] Loss: 0.0002 LR: 0.000100
Epoch [42] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [42] Batch [30/261] Loss: 0.0008 LR: 0.000100
Epoch [42] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [42] Batch [50/261] Loss: 0.0003 LR: 0.000100
Epoch [42] Batch [60/261] Loss: 0.0681 LR: 0.000100
Epoch [42] Batch [70/261] Loss: 0.1482 LR: 0.000100
Epoch [42] Batch [80/261] Loss: 0.0329 LR: 0.000100
Epoch [42] Batch [90/261] Loss: 0.0154 LR: 0.000100
Epoch [42] Batch [100/261] Loss: 0.0001 LR: 0.000100
Epoch [42] Batch [110/261] Loss: 0.0230 LR: 0.000100
Epoch [42] Batch [120/261] Loss: 0.0004 LR: 0.000100
Epoch [42] Batch [130/261] Loss: 0.0959 LR: 0.000100
Epoch [42] Batch [140/261] Loss: 0.8195 LR: 0.000100
Epoch [42] Batch [150/261] Loss: 0.0005 LR: 0.000100
Epoch [42] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [42] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [42] Batch [180/261] Loss: 0.0001 LR: 0.000100
Epoch [42] Batch [190/261] Loss: 0.0015 LR: 0.000100
Epoch [42] Batch [200/261] Loss: 0.0009 LR: 0.000100
Epoch [42] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [42] Batch [220/261] Loss: 0.0003 LR: 0.000100
Epoch [42] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [42] Batch [240/261] Loss: 0.0008 LR: 0.000100
Epoch [42] Batch [250/261] Loss: 0.0001 LR: 0.000100
Epoch [42] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [43/350] - Time: 13.42s
Train Loss: 0.0634 | Val Loss: 0.2904
Train Acc: 0.9841 | Val Acc: 0.9441
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.990    Count 5: 0.971  
  Count 6: 0.969    Count 7: 0.979    Count 8: 0.596    Count 9: 0.900    Count 10: 0.989  
Epoch [43] Batch [0/261] Loss: 0.0003 LR: 0.000100
Epoch [43] Batch [10/261] Loss: 0.1483 LR: 0.000100
Epoch [43] Batch [20/261] Loss: 0.0276 LR: 0.000100
Epoch [43] Batch [30/261] Loss: 0.0008 LR: 0.000100
Epoch [43] Batch [40/261] Loss: 0.9247 LR: 0.000100
Epoch [43] Batch [50/261] Loss: 0.0014 LR: 0.000100
Epoch [43] Batch [60/261] Loss: 0.0166 LR: 0.000100
Epoch [43] Batch [70/261] Loss: 0.0015 LR: 0.000100
Epoch [43] Batch [80/261] Loss: 0.0054 LR: 0.000100
Epoch [43] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [43] Batch [100/261] Loss: 0.4274 LR: 0.000100
Epoch [43] Batch [110/261] Loss: 0.2033 LR: 0.000100
Epoch [43] Batch [120/261] Loss: 0.0013 LR: 0.000100
Epoch [43] Batch [130/261] Loss: 0.0042 LR: 0.000100
Epoch [43] Batch [140/261] Loss: 0.0695 LR: 0.000100
Epoch [43] Batch [150/261] Loss: 0.0002 LR: 0.000100
Epoch [43] Batch [160/261] Loss: 0.0001 LR: 0.000100
Epoch [43] Batch [170/261] Loss: 0.0008 LR: 0.000100
Epoch [43] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [43] Batch [190/261] Loss: 0.2746 LR: 0.000100
Epoch [43] Batch [200/261] Loss: 0.1108 LR: 0.000100
Epoch [43] Batch [210/261] Loss: 0.2644 LR: 0.000100
Epoch [43] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [43] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [43] Batch [240/261] Loss: 0.0002 LR: 0.000100
Epoch [43] Batch [250/261] Loss: 0.0001 LR: 0.000100
Epoch [43] Batch [260/261] Loss: 30.7110 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [44/350] - Time: 12.62s
Train Loss: 0.0673 | Val Loss: 0.0242
Train Acc: 0.9834 | Val Acc: 0.9944
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.990  
  Count 6: 1.000    Count 7: 0.990    Count 8: 1.000    Count 9: 0.978    Count 10: 0.977  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/best_single_image_model.pth
  → New best model! Validation accuracy: 0.9944
新的最佳模型! 验证准确率: 0.9944
Epoch [44] Batch [0/261] Loss: 0.0001 LR: 0.000100
Epoch [44] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [44] Batch [20/261] Loss: 0.0008 LR: 0.000100
Epoch [44] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [44] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [44] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [44] Batch [60/261] Loss: 0.0001 LR: 0.000100
Epoch [44] Batch [70/261] Loss: 0.0003 LR: 0.000100
Epoch [44] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [44] Batch [90/261] Loss: 0.0061 LR: 0.000100
Epoch [44] Batch [100/261] Loss: 0.0722 LR: 0.000100
Epoch [44] Batch [110/261] Loss: 0.0159 LR: 0.000100
Epoch [44] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [44] Batch [130/261] Loss: 0.4833 LR: 0.000100
Epoch [44] Batch [140/261] Loss: 0.0029 LR: 0.000100
Epoch [44] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [44] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [44] Batch [170/261] Loss: 0.0001 LR: 0.000100
Epoch [44] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [44] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [44] Batch [200/261] Loss: 0.0359 LR: 0.000100
Epoch [44] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [44] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [44] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [44] Batch [240/261] Loss: 0.0008 LR: 0.000100
Epoch [44] Batch [250/261] Loss: 0.0001 LR: 0.000100
Epoch [44] Batch [260/261] Loss: 0.0005 LR: 0.000100

Epoch [45/350] - Time: 12.97s
Train Loss: 0.0493 | Val Loss: 0.0469
Train Acc: 0.9885 | Val Acc: 0.9860
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.990    Count 8: 0.970    Count 9: 0.978    Count 10: 0.898  
Epoch [45] Batch [0/261] Loss: 0.0002 LR: 0.000100
Epoch [45] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [45] Batch [20/261] Loss: 0.3877 LR: 0.000100
Epoch [45] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [45] Batch [40/261] Loss: 0.0052 LR: 0.000100
Epoch [45] Batch [50/261] Loss: 0.0001 LR: 0.000100
Epoch [45] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [45] Batch [70/261] Loss: 0.0445 LR: 0.000100
Epoch [45] Batch [80/261] Loss: 0.0001 LR: 0.000100
Epoch [45] Batch [90/261] Loss: 0.0434 LR: 0.000100
Epoch [45] Batch [100/261] Loss: 0.0002 LR: 0.000100
Epoch [45] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [45] Batch [120/261] Loss: 0.0243 LR: 0.000100
Epoch [45] Batch [130/261] Loss: 0.0001 LR: 0.000100
Epoch [45] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [45] Batch [150/261] Loss: 0.0001 LR: 0.000100
Epoch [45] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [45] Batch [170/261] Loss: 0.0539 LR: 0.000100
Epoch [45] Batch [180/261] Loss: 0.0047 LR: 0.000100
Epoch [45] Batch [190/261] Loss: 0.0834 LR: 0.000100
Epoch [45] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [45] Batch [210/261] Loss: 0.0003 LR: 0.000100
Epoch [45] Batch [220/261] Loss: 0.0003 LR: 0.000100
Epoch [45] Batch [230/261] Loss: 0.0004 LR: 0.000100
Epoch [45] Batch [240/261] Loss: 0.0001 LR: 0.000100
Epoch [45] Batch [250/261] Loss: 0.1581 LR: 0.000100
Epoch [45] Batch [260/261] Loss: 9.7374 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [46/350] - Time: 12.79s
Train Loss: 0.0590 | Val Loss: 0.0120
Train Acc: 0.9870 | Val Acc: 0.9963
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 0.990    Count 9: 0.967    Count 10: 1.000  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/best_single_image_model.pth
  → New best model! Validation accuracy: 0.9963
新的最佳模型! 验证准确率: 0.9963
Epoch [46] Batch [0/261] Loss: 0.0001 LR: 0.000100
Epoch [46] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [46] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [46] Batch [30/261] Loss: 0.0062 LR: 0.000100
Epoch [46] Batch [40/261] Loss: 0.0016 LR: 0.000100
Epoch [46] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [46] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [46] Batch [70/261] Loss: 0.0022 LR: 0.000100
Epoch [46] Batch [80/261] Loss: 0.0001 LR: 0.000100
Epoch [46] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [46] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [46] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [46] Batch [120/261] Loss: 0.0026 LR: 0.000100
Epoch [46] Batch [130/261] Loss: 0.0034 LR: 0.000100
Epoch [46] Batch [140/261] Loss: 0.0078 LR: 0.000100
Epoch [46] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [46] Batch [160/261] Loss: 0.0006 LR: 0.000100
Epoch [46] Batch [170/261] Loss: 0.0095 LR: 0.000100
Epoch [46] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [46] Batch [190/261] Loss: 1.0016 LR: 0.000100
Epoch [46] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [46] Batch [210/261] Loss: 0.0299 LR: 0.000100
Epoch [46] Batch [220/261] Loss: 0.0043 LR: 0.000100
Epoch [46] Batch [230/261] Loss: 0.0019 LR: 0.000100
Epoch [46] Batch [240/261] Loss: 0.3723 LR: 0.000100
Epoch [46] Batch [250/261] Loss: 0.0010 LR: 0.000100
Epoch [46] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [47/350] - Time: 12.73s
Train Loss: 0.0983 | Val Loss: 0.4019
Train Acc: 0.9801 | Val Acc: 0.9274
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.912  
  Count 6: 0.908    Count 7: 0.854    Count 8: 0.808    Count 9: 0.711    Count 10: 0.989  
Epoch [47] Batch [0/261] Loss: 0.5481 LR: 0.000100
Epoch [47] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [47] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [47] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [47] Batch [40/261] Loss: 0.0002 LR: 0.000100
Epoch [47] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [47] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [47] Batch [70/261] Loss: 0.0003 LR: 0.000100
Epoch [47] Batch [80/261] Loss: 0.2140 LR: 0.000100
Epoch [47] Batch [90/261] Loss: 0.0350 LR: 0.000100
Epoch [47] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [47] Batch [110/261] Loss: 0.0008 LR: 0.000100
Epoch [47] Batch [120/261] Loss: 0.0005 LR: 0.000100
Epoch [47] Batch [130/261] Loss: 0.4062 LR: 0.000100
Epoch [47] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [47] Batch [150/261] Loss: 0.0050 LR: 0.000100
Epoch [47] Batch [160/261] Loss: 0.0004 LR: 0.000100
Epoch [47] Batch [170/261] Loss: 0.8275 LR: 0.000100
Epoch [47] Batch [180/261] Loss: 0.6122 LR: 0.000100
Epoch [47] Batch [190/261] Loss: 0.0005 LR: 0.000100
Epoch [47] Batch [200/261] Loss: 0.3024 LR: 0.000100
Epoch [47] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [47] Batch [220/261] Loss: 0.0002 LR: 0.000100
Epoch [47] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [47] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [47] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [47] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [48/350] - Time: 13.00s
Train Loss: 0.0868 | Val Loss: 0.0243
Train Acc: 0.9791 | Val Acc: 0.9944
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 1.000    Count 9: 0.956    Count 10: 0.989  
Epoch [48] Batch [0/261] Loss: 0.0019 LR: 0.000100
Epoch [48] Batch [10/261] Loss: 0.0001 LR: 0.000100
Epoch [48] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [48] Batch [30/261] Loss: 0.0023 LR: 0.000100
Epoch [48] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [48] Batch [50/261] Loss: 0.0001 LR: 0.000100
Epoch [48] Batch [60/261] Loss: 0.0008 LR: 0.000100
Epoch [48] Batch [70/261] Loss: 0.3781 LR: 0.000100
Epoch [48] Batch [80/261] Loss: 0.1901 LR: 0.000100
Epoch [48] Batch [90/261] Loss: 0.0236 LR: 0.000100
Epoch [48] Batch [100/261] Loss: 0.2046 LR: 0.000100
Epoch [48] Batch [110/261] Loss: 0.0030 LR: 0.000100
Epoch [48] Batch [120/261] Loss: 0.0002 LR: 0.000100
Epoch [48] Batch [130/261] Loss: 0.0001 LR: 0.000100
Epoch [48] Batch [140/261] Loss: 0.0003 LR: 0.000100
Epoch [48] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [48] Batch [160/261] Loss: 0.0005 LR: 0.000100
Epoch [48] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [48] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [48] Batch [190/261] Loss: 0.0002 LR: 0.000100
Epoch [48] Batch [200/261] Loss: 0.0003 LR: 0.000100
Epoch [48] Batch [210/261] Loss: 0.0004 LR: 0.000100
Epoch [48] Batch [220/261] Loss: 0.0001 LR: 0.000100
Epoch [48] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [48] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [48] Batch [250/261] Loss: 0.0164 LR: 0.000100
Epoch [48] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [49/350] - Time: 12.78s
Train Loss: 0.0506 | Val Loss: 0.0425
Train Acc: 0.9892 | Val Acc: 0.9926
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 0.979    Count 8: 1.000    Count 9: 0.989    Count 10: 0.966  
Epoch [49] Batch [0/261] Loss: 0.0006 LR: 0.000100
Epoch [49] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [49] Batch [20/261] Loss: 0.0011 LR: 0.000100
Epoch [49] Batch [30/261] Loss: 0.0562 LR: 0.000100
Epoch [49] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [49] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [49] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [49] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [49] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [49] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [49] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [49] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [49] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [49] Batch [130/261] Loss: 0.0001 LR: 0.000100
Epoch [49] Batch [140/261] Loss: 0.0004 LR: 0.000100
Epoch [49] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [49] Batch [160/261] Loss: 0.0001 LR: 0.000100
Epoch [49] Batch [170/261] Loss: 0.0001 LR: 0.000100
Epoch [49] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [49] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [49] Batch [200/261] Loss: 0.0021 LR: 0.000100
Epoch [49] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [49] Batch [220/261] Loss: 0.0513 LR: 0.000100
Epoch [49] Batch [230/261] Loss: 0.0002 LR: 0.000100
Epoch [49] Batch [240/261] Loss: 0.0008 LR: 0.000100
Epoch [49] Batch [250/261] Loss: 0.0003 LR: 0.000100
Epoch [49] Batch [260/261] Loss: 13.1052 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [50/350] - Time: 12.96s
Train Loss: 0.0169 | Val Loss: 0.0318
Train Acc: 0.9962 | Val Acc: 0.9935
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 1.000    Count 8: 1.000    Count 9: 0.978    Count 10: 0.955  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/single_image_checkpoint_epoch_49.pth
Epoch [50] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [50] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [50] Batch [20/261] Loss: 0.0478 LR: 0.000100
Epoch [50] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [50] Batch [40/261] Loss: 0.0001 LR: 0.000100
Epoch [50] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [50] Batch [60/261] Loss: 0.4489 LR: 0.000100
Epoch [50] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [50] Batch [80/261] Loss: 0.0001 LR: 0.000100
Epoch [50] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [50] Batch [100/261] Loss: 0.0003 LR: 0.000100
Epoch [50] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [50] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [50] Batch [130/261] Loss: 0.1467 LR: 0.000100
Epoch [50] Batch [140/261] Loss: 0.0953 LR: 0.000100
Epoch [50] Batch [150/261] Loss: 0.1558 LR: 0.000100
Epoch [50] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [50] Batch [170/261] Loss: 0.0001 LR: 0.000100
Epoch [50] Batch [180/261] Loss: 0.0017 LR: 0.000100
Epoch [50] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [50] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [50] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [50] Batch [220/261] Loss: 0.0001 LR: 0.000100
Epoch [50] Batch [230/261] Loss: 0.0007 LR: 0.000100
Epoch [50] Batch [240/261] Loss: 0.0002 LR: 0.000100
Epoch [50] Batch [250/261] Loss: 0.0009 LR: 0.000100
Epoch [50] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [51/350] - Time: 13.16s
Train Loss: 0.0389 | Val Loss: 0.2565
Train Acc: 0.9904 | Val Acc: 0.9534
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 0.990  
  Count 6: 0.980    Count 7: 0.875    Count 8: 0.919    Count 9: 0.711    Count 10: 1.000  
Epoch [51] Batch [0/261] Loss: 0.0001 LR: 0.000100
Epoch [51] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [51] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [51] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [51] Batch [40/261] Loss: 0.1829 LR: 0.000100
Epoch [51] Batch [50/261] Loss: 0.9580 LR: 0.000100
Epoch [51] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [51] Batch [70/261] Loss: 0.0001 LR: 0.000100
Epoch [51] Batch [80/261] Loss: 0.6452 LR: 0.000100
Epoch [51] Batch [90/261] Loss: 0.0001 LR: 0.000100
Epoch [51] Batch [100/261] Loss: 0.3156 LR: 0.000100
Epoch [51] Batch [110/261] Loss: 0.0232 LR: 0.000100
Epoch [51] Batch [120/261] Loss: 0.0001 LR: 0.000100
Epoch [51] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [51] Batch [140/261] Loss: 0.0001 LR: 0.000100
Epoch [51] Batch [150/261] Loss: 0.0087 LR: 0.000100
Epoch [51] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [51] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [51] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [51] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [51] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [51] Batch [210/261] Loss: 0.0003 LR: 0.000100
Epoch [51] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [51] Batch [230/261] Loss: 0.0400 LR: 0.000100
Epoch [51] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [51] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [51] Batch [260/261] Loss: 4.1559 LR: 0.000100

Epoch [52/350] - Time: 12.67s
Train Loss: 0.0688 | Val Loss: 0.3647
Train Acc: 0.9837 | Val Acc: 0.9358
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.980    Count 7: 0.906    Count 8: 0.869    Count 9: 0.911    Count 10: 0.591  
Epoch [52] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [52] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [52] Batch [20/261] Loss: 0.0008 LR: 0.000100
Epoch [52] Batch [30/261] Loss: 0.0003 LR: 0.000100
Epoch [52] Batch [40/261] Loss: 0.2327 LR: 0.000100
Epoch [52] Batch [50/261] Loss: 0.0004 LR: 0.000100
Epoch [52] Batch [60/261] Loss: 0.0002 LR: 0.000100
Epoch [52] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [52] Batch [80/261] Loss: 0.0003 LR: 0.000100
Epoch [52] Batch [90/261] Loss: 0.0003 LR: 0.000100
Epoch [52] Batch [100/261] Loss: 0.0015 LR: 0.000100
Epoch [52] Batch [110/261] Loss: 0.0006 LR: 0.000100
Epoch [52] Batch [120/261] Loss: 0.0001 LR: 0.000100
Epoch [52] Batch [130/261] Loss: 0.0016 LR: 0.000100
Epoch [52] Batch [140/261] Loss: 0.0011 LR: 0.000100
Epoch [52] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [52] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [52] Batch [170/261] Loss: 0.2771 LR: 0.000100
Epoch [52] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [52] Batch [190/261] Loss: 0.0561 LR: 0.000100
Epoch [52] Batch [200/261] Loss: 0.0001 LR: 0.000100
Epoch [52] Batch [210/261] Loss: 0.0004 LR: 0.000100
Epoch [52] Batch [220/261] Loss: 0.0004 LR: 0.000100
Epoch [52] Batch [230/261] Loss: 0.0074 LR: 0.000100
Epoch [52] Batch [240/261] Loss: 0.0006 LR: 0.000100
Epoch [52] Batch [250/261] Loss: 0.0545 LR: 0.000100
Epoch [52] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [53/350] - Time: 12.81s
Train Loss: 0.0999 | Val Loss: 0.0675
Train Acc: 0.9815 | Val Acc: 0.9898
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 1.000    Count 8: 1.000    Count 9: 0.922    Count 10: 0.977  
Epoch [53] Batch [0/261] Loss: 0.1260 LR: 0.000100
Epoch [53] Batch [10/261] Loss: 0.0246 LR: 0.000100
Epoch [53] Batch [20/261] Loss: 0.2194 LR: 0.000100
Epoch [53] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [53] Batch [40/261] Loss: 0.0002 LR: 0.000100
Epoch [53] Batch [50/261] Loss: 0.0010 LR: 0.000100
Epoch [53] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [53] Batch [70/261] Loss: 0.0033 LR: 0.000100
Epoch [53] Batch [80/261] Loss: 0.0012 LR: 0.000100
Epoch [53] Batch [90/261] Loss: 0.0193 LR: 0.000100
Epoch [53] Batch [100/261] Loss: 0.0001 LR: 0.000100
Epoch [53] Batch [110/261] Loss: 0.0104 LR: 0.000100
Epoch [53] Batch [120/261] Loss: 0.0045 LR: 0.000100
Epoch [53] Batch [130/261] Loss: 0.0003 LR: 0.000100
Epoch [53] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [53] Batch [150/261] Loss: 0.0475 LR: 0.000100
Epoch [53] Batch [160/261] Loss: 0.0301 LR: 0.000100
Epoch [53] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [53] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [53] Batch [190/261] Loss: 0.0001 LR: 0.000100
Epoch [53] Batch [200/261] Loss: 0.0050 LR: 0.000100
Epoch [53] Batch [210/261] Loss: 0.0005 LR: 0.000100
Epoch [53] Batch [220/261] Loss: 0.4561 LR: 0.000100
Epoch [53] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [53] Batch [240/261] Loss: 1.2876 LR: 0.000100
Epoch [53] Batch [250/261] Loss: 0.0003 LR: 0.000100
Epoch [53] Batch [260/261] Loss: 8.0910 LR: 0.000100

Epoch [54/350] - Time: 12.96s
Train Loss: 0.0646 | Val Loss: 0.0240
Train Acc: 0.9861 | Val Acc: 0.9953
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 1.000    Count 8: 0.990    Count 9: 0.989    Count 10: 0.989  
Epoch [54] Batch [0/261] Loss: 0.0002 LR: 0.000100
Epoch [54] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [54] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [54] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [54] Batch [40/261] Loss: 0.0014 LR: 0.000100
Epoch [54] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [54] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [54] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [54] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [54] Batch [90/261] Loss: 0.0002 LR: 0.000100
Epoch [54] Batch [100/261] Loss: 0.0001 LR: 0.000100
Epoch [54] Batch [110/261] Loss: 0.0004 LR: 0.000100
Epoch [54] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [54] Batch [130/261] Loss: 0.0004 LR: 0.000100
Epoch [54] Batch [140/261] Loss: 0.0053 LR: 0.000100
Epoch [54] Batch [150/261] Loss: 0.0002 LR: 0.000100
Epoch [54] Batch [160/261] Loss: 0.0656 LR: 0.000100
Epoch [54] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [54] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [54] Batch [190/261] Loss: 0.0001 LR: 0.000100
Epoch [54] Batch [200/261] Loss: 0.0824 LR: 0.000100
Epoch [54] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [54] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [54] Batch [230/261] Loss: 0.0001 LR: 0.000100
Epoch [54] Batch [240/261] Loss: 0.0021 LR: 0.000100
Epoch [54] Batch [250/261] Loss: 0.0001 LR: 0.000100
Epoch [54] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [55/350] - Time: 13.06s
Train Loss: 0.0309 | Val Loss: 0.1473
Train Acc: 0.9933 | Val Acc: 0.9655
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.990    Count 5: 0.971  
  Count 6: 0.969    Count 7: 0.865    Count 8: 0.919    Count 9: 0.911    Count 10: 1.000  
Epoch [55] Batch [0/261] Loss: 0.0757 LR: 0.000100
Epoch [55] Batch [10/261] Loss: 0.0009 LR: 0.000100
Epoch [55] Batch [20/261] Loss: 0.0001 LR: 0.000100
Epoch [55] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [55] Batch [40/261] Loss: 0.0002 LR: 0.000100
Epoch [55] Batch [50/261] Loss: 0.0003 LR: 0.000100
Epoch [55] Batch [60/261] Loss: 0.0004 LR: 0.000100
Epoch [55] Batch [70/261] Loss: 0.0001 LR: 0.000100
Epoch [55] Batch [80/261] Loss: 0.5379 LR: 0.000100
Epoch [55] Batch [90/261] Loss: 0.0010 LR: 0.000100
Epoch [55] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [55] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [55] Batch [120/261] Loss: 0.0040 LR: 0.000100
Epoch [55] Batch [130/261] Loss: 0.6637 LR: 0.000100
Epoch [55] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [55] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [55] Batch [160/261] Loss: 0.0001 LR: 0.000100
Epoch [55] Batch [170/261] Loss: 0.0011 LR: 0.000100
Epoch [55] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [55] Batch [190/261] Loss: 0.0138 LR: 0.000100
Epoch [55] Batch [200/261] Loss: 0.0004 LR: 0.000100
Epoch [55] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [55] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [55] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [55] Batch [240/261] Loss: 0.0001 LR: 0.000100
Epoch [55] Batch [250/261] Loss: 0.1668 LR: 0.000100
Epoch [55] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [56/350] - Time: 12.79s
Train Loss: 0.0448 | Val Loss: 0.0740
Train Acc: 0.9909 | Val Acc: 0.9860
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.981    Count 5: 0.971  
  Count 6: 0.990    Count 7: 0.979    Count 8: 1.000    Count 9: 0.978    Count 10: 0.955  
Epoch [56] Batch [0/261] Loss: 0.0027 LR: 0.000100
Epoch [56] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [56] Batch [20/261] Loss: 0.0015 LR: 0.000100
Epoch [56] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [56] Batch [40/261] Loss: 0.4985 LR: 0.000100
Epoch [56] Batch [50/261] Loss: 0.0001 LR: 0.000100
Epoch [56] Batch [60/261] Loss: 0.0001 LR: 0.000100
Epoch [56] Batch [70/261] Loss: 0.0001 LR: 0.000100
Epoch [56] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [56] Batch [90/261] Loss: 0.0002 LR: 0.000100
Epoch [56] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [56] Batch [110/261] Loss: 0.0027 LR: 0.000100
Epoch [56] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [56] Batch [130/261] Loss: 0.0058 LR: 0.000100
Epoch [56] Batch [140/261] Loss: 0.0001 LR: 0.000100
Epoch [56] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [56] Batch [160/261] Loss: 0.0006 LR: 0.000100
Epoch [56] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [56] Batch [180/261] Loss: 0.0017 LR: 0.000100
Epoch [56] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [56] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [56] Batch [210/261] Loss: 0.0002 LR: 0.000100
Epoch [56] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [56] Batch [230/261] Loss: 0.0001 LR: 0.000100
Epoch [56] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [56] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [56] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [57/350] - Time: 12.81s
Train Loss: 0.0390 | Val Loss: 0.1362
Train Acc: 0.9933 | Val Acc: 0.9646
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.980    Count 7: 1.000    Count 8: 0.949    Count 9: 0.967    Count 10: 0.682  
Epoch [57] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [57] Batch [10/261] Loss: 0.0010 LR: 0.000100
Epoch [57] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [57] Batch [30/261] Loss: 0.0003 LR: 0.000100
Epoch [57] Batch [40/261] Loss: 0.0009 LR: 0.000100
Epoch [57] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [57] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [57] Batch [70/261] Loss: 0.0001 LR: 0.000100
Epoch [57] Batch [80/261] Loss: 0.0003 LR: 0.000100
Epoch [57] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [57] Batch [100/261] Loss: 0.0032 LR: 0.000100
Epoch [57] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [57] Batch [120/261] Loss: 0.0007 LR: 0.000100
Epoch [57] Batch [130/261] Loss: 0.0213 LR: 0.000100
Epoch [57] Batch [140/261] Loss: 0.0001 LR: 0.000100
Epoch [57] Batch [150/261] Loss: 0.2092 LR: 0.000100
Epoch [57] Batch [160/261] Loss: 0.0106 LR: 0.000100
Epoch [57] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [57] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [57] Batch [190/261] Loss: 1.4278 LR: 0.000100
Epoch [57] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [57] Batch [210/261] Loss: 0.5703 LR: 0.000100
Epoch [57] Batch [220/261] Loss: 0.0007 LR: 0.000100
Epoch [57] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [57] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [57] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [57] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [58/350] - Time: 12.77s
Train Loss: 0.0600 | Val Loss: 0.0574
Train Acc: 0.9870 | Val Acc: 0.9879
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 1.000    Count 8: 1.000    Count 9: 0.956    Count 10: 0.920  
Epoch [58] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [58] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [58] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [58] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [58] Batch [40/261] Loss: 0.6376 LR: 0.000100
Epoch [58] Batch [50/261] Loss: 0.0012 LR: 0.000100
Epoch [58] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [58] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [58] Batch [80/261] Loss: 0.0001 LR: 0.000100
Epoch [58] Batch [90/261] Loss: 0.0001 LR: 0.000100
Epoch [58] Batch [100/261] Loss: 0.8500 LR: 0.000100
Epoch [58] Batch [110/261] Loss: 0.3403 LR: 0.000100
Epoch [58] Batch [120/261] Loss: 0.0001 LR: 0.000100
Epoch [58] Batch [130/261] Loss: 0.0002 LR: 0.000100
Epoch [58] Batch [140/261] Loss: 0.0763 LR: 0.000100
Epoch [58] Batch [150/261] Loss: 0.0001 LR: 0.000100
Epoch [58] Batch [160/261] Loss: 0.0004 LR: 0.000100
Epoch [58] Batch [170/261] Loss: 0.0020 LR: 0.000100
Epoch [58] Batch [180/261] Loss: 0.0100 LR: 0.000100
Epoch [58] Batch [190/261] Loss: 0.1986 LR: 0.000100
Epoch [58] Batch [200/261] Loss: 0.0001 LR: 0.000100
Epoch [58] Batch [210/261] Loss: 0.0085 LR: 0.000100
Epoch [58] Batch [220/261] Loss: 0.0002 LR: 0.000100
Epoch [58] Batch [230/261] Loss: 0.1046 LR: 0.000100
Epoch [58] Batch [240/261] Loss: 0.0014 LR: 0.000100
Epoch [58] Batch [250/261] Loss: 0.0003 LR: 0.000100
Epoch [58] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [59/350] - Time: 13.08s
Train Loss: 0.0715 | Val Loss: 0.2322
Train Acc: 0.9834 | Val Acc: 0.9432
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.917    Count 8: 0.970    Count 9: 0.900    Count 10: 0.545  
Epoch [59] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [59] Batch [10/261] Loss: 0.0775 LR: 0.000100
Epoch [59] Batch [20/261] Loss: 0.0240 LR: 0.000100
Epoch [59] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [59] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [59] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [59] Batch [60/261] Loss: 0.0001 LR: 0.000100
Epoch [59] Batch [70/261] Loss: 0.0038 LR: 0.000100
Epoch [59] Batch [80/261] Loss: 0.0092 LR: 0.000100
Epoch [59] Batch [90/261] Loss: 0.5412 LR: 0.000100
Epoch [59] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [59] Batch [110/261] Loss: 0.0003 LR: 0.000100
Epoch [59] Batch [120/261] Loss: 0.2185 LR: 0.000100
Epoch [59] Batch [130/261] Loss: 0.0006 LR: 0.000100
Epoch [59] Batch [140/261] Loss: 0.3550 LR: 0.000100
Epoch [59] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [59] Batch [160/261] Loss: 0.4243 LR: 0.000100
Epoch [59] Batch [170/261] Loss: 0.0337 LR: 0.000100
Epoch [59] Batch [180/261] Loss: 0.0196 LR: 0.000100
Epoch [59] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [59] Batch [200/261] Loss: 0.0015 LR: 0.000100
Epoch [59] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [59] Batch [220/261] Loss: 0.0017 LR: 0.000100
Epoch [59] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [59] Batch [240/261] Loss: 0.0007 LR: 0.000100
Epoch [59] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [59] Batch [260/261] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [60/350] - Time: 12.90s
Train Loss: 0.0875 | Val Loss: 0.0530
Train Acc: 0.9822 | Val Acc: 0.9907
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.980    Count 7: 1.000    Count 8: 1.000    Count 9: 0.911    Count 10: 1.000  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/single_image_checkpoint_epoch_59.pth
Epoch [60] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [60] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [60] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [60] Batch [30/261] Loss: 0.0029 LR: 0.000100
Epoch [60] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [60] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [60] Batch [60/261] Loss: 0.0010 LR: 0.000100
Epoch [60] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [60] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [60] Batch [90/261] Loss: 0.0021 LR: 0.000100
Epoch [60] Batch [100/261] Loss: 0.0003 LR: 0.000100
Epoch [60] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [60] Batch [120/261] Loss: 0.0337 LR: 0.000100
Epoch [60] Batch [130/261] Loss: 0.0028 LR: 0.000100
Epoch [60] Batch [140/261] Loss: 0.0085 LR: 0.000100
Epoch [60] Batch [150/261] Loss: 0.0002 LR: 0.000100
Epoch [60] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [60] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [60] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [60] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [60] Batch [200/261] Loss: 0.0331 LR: 0.000100
Epoch [60] Batch [210/261] Loss: 0.0767 LR: 0.000100
Epoch [60] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [60] Batch [230/261] Loss: 0.0002 LR: 0.000100
Epoch [60] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [60] Batch [250/261] Loss: 0.0012 LR: 0.000100
Epoch [60] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [61/350] - Time: 12.64s
Train Loss: 0.0597 | Val Loss: 0.1034
Train Acc: 0.9897 | Val Acc: 0.9851
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.990    Count 8: 0.980    Count 9: 0.889    Count 10: 0.989  
Epoch [61] Batch [0/261] Loss: 0.0038 LR: 0.000100
Epoch [61] Batch [10/261] Loss: 0.0013 LR: 0.000100
Epoch [61] Batch [20/261] Loss: 0.0001 LR: 0.000100
Epoch [61] Batch [30/261] Loss: 0.0004 LR: 0.000100
Epoch [61] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [61] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [61] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [61] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [61] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [61] Batch [90/261] Loss: 0.0227 LR: 0.000100
Epoch [61] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [61] Batch [110/261] Loss: 0.8459 LR: 0.000100
Epoch [61] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [61] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [61] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [61] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [61] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [61] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [61] Batch [180/261] Loss: 0.1233 LR: 0.000100
Epoch [61] Batch [190/261] Loss: 0.0641 LR: 0.000100
Epoch [61] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [61] Batch [210/261] Loss: 0.0436 LR: 0.000100
Epoch [61] Batch [220/261] Loss: 0.0001 LR: 0.000100
Epoch [61] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [61] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [61] Batch [250/261] Loss: 0.0007 LR: 0.000100
Epoch [61] Batch [260/261] Loss: 0.0004 LR: 0.000100

Epoch [62/350] - Time: 12.78s
Train Loss: 0.0526 | Val Loss: 0.0991
Train Acc: 0.9897 | Val Acc: 0.9795
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 0.969    Count 8: 0.990    Count 9: 0.989    Count 10: 0.830  
Epoch [62] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [62] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [62] Batch [20/261] Loss: 0.4118 LR: 0.000100
Epoch [62] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [62] Batch [40/261] Loss: 0.0188 LR: 0.000100
Epoch [62] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [62] Batch [60/261] Loss: 0.0978 LR: 0.000100
Epoch [62] Batch [70/261] Loss: 0.2315 LR: 0.000100
Epoch [62] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [62] Batch [90/261] Loss: 0.8579 LR: 0.000100
Epoch [62] Batch [100/261] Loss: 0.0001 LR: 0.000100
Epoch [62] Batch [110/261] Loss: 0.0002 LR: 0.000100
Epoch [62] Batch [120/261] Loss: 0.0172 LR: 0.000100
Epoch [62] Batch [130/261] Loss: 0.0003 LR: 0.000100
Epoch [62] Batch [140/261] Loss: 0.0008 LR: 0.000100
Epoch [62] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [62] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [62] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [62] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [62] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [62] Batch [200/261] Loss: 0.0062 LR: 0.000100
Epoch [62] Batch [210/261] Loss: 0.0003 LR: 0.000100
Epoch [62] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [62] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [62] Batch [240/261] Loss: 0.0012 LR: 0.000100
Epoch [62] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [62] Batch [260/261] Loss: 30.5648 LR: 0.000100

Epoch [63/350] - Time: 12.78s
Train Loss: 0.0421 | Val Loss: 0.0594
Train Acc: 0.9925 | Val Acc: 0.9888
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.979    Count 8: 0.980    Count 9: 0.911    Count 10: 1.000  
Epoch [63] Batch [0/261] Loss: 0.0001 LR: 0.000100
Epoch [63] Batch [10/261] Loss: 0.0177 LR: 0.000100
Epoch [63] Batch [20/261] Loss: 0.0001 LR: 0.000100
Epoch [63] Batch [30/261] Loss: 0.0470 LR: 0.000100
Epoch [63] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [63] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [63] Batch [60/261] Loss: 0.0004 LR: 0.000100
Epoch [63] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [63] Batch [80/261] Loss: 0.0151 LR: 0.000100
Epoch [63] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [63] Batch [100/261] Loss: 0.0013 LR: 0.000100
Epoch [63] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [63] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [63] Batch [130/261] Loss: 0.0006 LR: 0.000100
Epoch [63] Batch [140/261] Loss: 0.0001 LR: 0.000100
Epoch [63] Batch [150/261] Loss: 0.4022 LR: 0.000100
Epoch [63] Batch [160/261] Loss: 0.0001 LR: 0.000100
Epoch [63] Batch [170/261] Loss: 0.0001 LR: 0.000100
Epoch [63] Batch [180/261] Loss: 0.0001 LR: 0.000100
Epoch [63] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [63] Batch [200/261] Loss: 0.0006 LR: 0.000100
Epoch [63] Batch [210/261] Loss: 0.9810 LR: 0.000100
Epoch [63] Batch [220/261] Loss: 0.1786 LR: 0.000100
Epoch [63] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [63] Batch [240/261] Loss: 0.0022 LR: 0.000100
Epoch [63] Batch [250/261] Loss: 0.0812 LR: 0.000100
Epoch [63] Batch [260/261] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [64/350] - Time: 12.69s
Train Loss: 0.0405 | Val Loss: 0.0144
Train Acc: 0.9916 | Val Acc: 0.9972
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.990    Count 8: 0.990    Count 9: 1.000    Count 10: 1.000  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/best_single_image_model.pth
  → New best model! Validation accuracy: 0.9972
新的最佳模型! 验证准确率: 0.9972
Epoch [64] Batch [0/261] Loss: 0.0001 LR: 0.000100
Epoch [64] Batch [10/261] Loss: 0.0001 LR: 0.000100
Epoch [64] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [64] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [64] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [64] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [64] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [64] Batch [70/261] Loss: 0.0007 LR: 0.000100
Epoch [64] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [64] Batch [90/261] Loss: 0.1464 LR: 0.000100
Epoch [64] Batch [100/261] Loss: 0.0681 LR: 0.000100
Epoch [64] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [64] Batch [120/261] Loss: 0.0001 LR: 0.000100
Epoch [64] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [64] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [64] Batch [150/261] Loss: 0.0034 LR: 0.000100
Epoch [64] Batch [160/261] Loss: 0.0657 LR: 0.000100
Epoch [64] Batch [170/261] Loss: 0.0183 LR: 0.000100
Epoch [64] Batch [180/261] Loss: 0.3316 LR: 0.000100
Epoch [64] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [64] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [64] Batch [210/261] Loss: 0.2185 LR: 0.000100
Epoch [64] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [64] Batch [230/261] Loss: 0.5366 LR: 0.000100
Epoch [64] Batch [240/261] Loss: 0.0001 LR: 0.000100
Epoch [64] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [64] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [65/350] - Time: 12.58s
Train Loss: 0.0602 | Val Loss: 0.1104
Train Acc: 0.9894 | Val Acc: 0.9777
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.952    Count 5: 0.892  
  Count 6: 0.990    Count 7: 0.990    Count 8: 0.990    Count 9: 0.967    Count 10: 0.989  
Epoch [65] Batch [0/261] Loss: 0.0001 LR: 0.000100
Epoch [65] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [65] Batch [20/261] Loss: 0.3733 LR: 0.000100
Epoch [65] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [65] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [65] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [65] Batch [60/261] Loss: 0.4119 LR: 0.000100
Epoch [65] Batch [70/261] Loss: 0.0001 LR: 0.000100
Epoch [65] Batch [80/261] Loss: 0.0045 LR: 0.000100
Epoch [65] Batch [90/261] Loss: 0.0082 LR: 0.000100
Epoch [65] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [65] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [65] Batch [120/261] Loss: 0.0014 LR: 0.000100
Epoch [65] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [65] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [65] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [65] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [65] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [65] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [65] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [65] Batch [200/261] Loss: 0.0014 LR: 0.000100
Epoch [65] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [65] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [65] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [65] Batch [240/261] Loss: 0.8951 LR: 0.000100
Epoch [65] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [65] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [66/350] - Time: 12.91s
Train Loss: 0.0272 | Val Loss: 0.0594
Train Acc: 0.9942 | Val Acc: 0.9907
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 0.979    Count 8: 1.000    Count 9: 0.967    Count 10: 0.966  
Epoch [66] Batch [0/261] Loss: 0.0005 LR: 0.000100
Epoch [66] Batch [10/261] Loss: 0.0006 LR: 0.000100
Epoch [66] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [66] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [66] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [66] Batch [50/261] Loss: 0.0003 LR: 0.000100
Epoch [66] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [66] Batch [70/261] Loss: 0.0001 LR: 0.000100
Epoch [66] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [66] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [66] Batch [100/261] Loss: 0.0001 LR: 0.000100
Epoch [66] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [66] Batch [120/261] Loss: 0.0007 LR: 0.000100
Epoch [66] Batch [130/261] Loss: 0.0025 LR: 0.000100
Epoch [66] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [66] Batch [150/261] Loss: 0.0038 LR: 0.000100
Epoch [66] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [66] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [66] Batch [180/261] Loss: 0.0023 LR: 0.000100
Epoch [66] Batch [190/261] Loss: 0.0142 LR: 0.000100
Epoch [66] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [66] Batch [210/261] Loss: 0.0622 LR: 0.000100
Epoch [66] Batch [220/261] Loss: 0.2358 LR: 0.000100
Epoch [66] Batch [230/261] Loss: 0.0004 LR: 0.000100
Epoch [66] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [66] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [66] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [67/350] - Time: 12.96s
Train Loss: 0.0431 | Val Loss: 0.9465
Train Acc: 0.9913 | Val Acc: 0.8333
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.981    Count 4: 0.990    Count 5: 0.814  
  Count 6: 0.806    Count 7: 0.542    Count 8: 0.545    Count 9: 0.789    Count 10: 0.659  
Epoch [67] Batch [0/261] Loss: 0.9594 LR: 0.000100
Epoch [67] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [67] Batch [20/261] Loss: 0.5407 LR: 0.000100
Epoch [67] Batch [30/261] Loss: 0.2935 LR: 0.000100
Epoch [67] Batch [40/261] Loss: 0.0006 LR: 0.000100
Epoch [67] Batch [50/261] Loss: 0.0010 LR: 0.000100
Epoch [67] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [67] Batch [70/261] Loss: 0.0012 LR: 0.000100
Epoch [67] Batch [80/261] Loss: 0.0005 LR: 0.000100
Epoch [67] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [67] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [67] Batch [110/261] Loss: 0.0073 LR: 0.000100
Epoch [67] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [67] Batch [130/261] Loss: 0.0527 LR: 0.000100
Epoch [67] Batch [140/261] Loss: 0.0001 LR: 0.000100
Epoch [67] Batch [150/261] Loss: 0.0746 LR: 0.000100
Epoch [67] Batch [160/261] Loss: 0.0001 LR: 0.000100
Epoch [67] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [67] Batch [180/261] Loss: 0.0036 LR: 0.000100
Epoch [67] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [67] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [67] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [67] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [67] Batch [230/261] Loss: 0.0001 LR: 0.000100
Epoch [67] Batch [240/261] Loss: 0.0004 LR: 0.000100
Epoch [67] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [67] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [68/350] - Time: 12.86s
Train Loss: 0.0382 | Val Loss: 0.0699
Train Acc: 0.9901 | Val Acc: 0.9879
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 0.958    Count 8: 0.980    Count 9: 0.989    Count 10: 0.955  
Epoch [68] Batch [0/261] Loss: 0.0004 LR: 0.000100
Epoch [68] Batch [10/261] Loss: 0.0002 LR: 0.000100
Epoch [68] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [68] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [68] Batch [40/261] Loss: 0.0005 LR: 0.000100
Epoch [68] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [68] Batch [60/261] Loss: 0.0039 LR: 0.000100
Epoch [68] Batch [70/261] Loss: 0.0005 LR: 0.000100
Epoch [68] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [68] Batch [90/261] Loss: 0.0749 LR: 0.000100
Epoch [68] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [68] Batch [110/261] Loss: 0.0095 LR: 0.000100
Epoch [68] Batch [120/261] Loss: 0.0002 LR: 0.000100
Epoch [68] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [68] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [68] Batch [150/261] Loss: 0.0008 LR: 0.000100
Epoch [68] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [68] Batch [170/261] Loss: 0.0004 LR: 0.000100
Epoch [68] Batch [180/261] Loss: 0.0007 LR: 0.000100
Epoch [68] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [68] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [68] Batch [210/261] Loss: 0.0007 LR: 0.000100
Epoch [68] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [68] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [68] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [68] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [68] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [69/350] - Time: 12.73s
Train Loss: 0.0212 | Val Loss: 0.1865
Train Acc: 0.9962 | Val Acc: 0.9674
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.990    Count 5: 0.971  
  Count 6: 0.980    Count 7: 0.979    Count 8: 0.990    Count 9: 0.956    Count 10: 0.761  
Epoch [69] Batch [0/261] Loss: 0.0137 LR: 0.000100
Epoch [69] Batch [10/261] Loss: 0.0038 LR: 0.000100
Epoch [69] Batch [20/261] Loss: 0.0468 LR: 0.000100
Epoch [69] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [69] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [69] Batch [50/261] Loss: 0.0005 LR: 0.000100
Epoch [69] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [69] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [69] Batch [80/261] Loss: 0.0001 LR: 0.000100
Epoch [69] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [69] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [69] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [69] Batch [120/261] Loss: 0.0922 LR: 0.000100
Epoch [69] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [69] Batch [140/261] Loss: 1.1858 LR: 0.000100
Epoch [69] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [69] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [69] Batch [170/261] Loss: 0.9508 LR: 0.000100
Epoch [69] Batch [180/261] Loss: 0.1103 LR: 0.000100
Epoch [69] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [69] Batch [200/261] Loss: 0.0002 LR: 0.000100
Epoch [69] Batch [210/261] Loss: 0.0001 LR: 0.000100
Epoch [69] Batch [220/261] Loss: 0.0002 LR: 0.000100
Epoch [69] Batch [230/261] Loss: 0.3384 LR: 0.000100
Epoch [69] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [69] Batch [250/261] Loss: 0.0004 LR: 0.000100
Epoch [69] Batch [260/261] Loss: 17.5153 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [70/350] - Time: 12.97s
Train Loss: 0.0449 | Val Loss: 0.7253
Train Acc: 0.9906 | Val Acc: 0.8957
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.980  
  Count 6: 0.990    Count 7: 0.990    Count 8: 0.798    Count 9: 0.711    Count 10: 0.295  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/single_image_checkpoint_epoch_69.pth
Epoch [70] Batch [0/261] Loss: 0.7467 LR: 0.000100
Epoch [70] Batch [10/261] Loss: 0.0001 LR: 0.000100
Epoch [70] Batch [20/261] Loss: 0.4957 LR: 0.000100
Epoch [70] Batch [30/261] Loss: 0.0001 LR: 0.000100
Epoch [70] Batch [40/261] Loss: 0.0188 LR: 0.000100
Epoch [70] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [70] Batch [60/261] Loss: 0.0001 LR: 0.000100
Epoch [70] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [70] Batch [80/261] Loss: 0.0099 LR: 0.000100
Epoch [70] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [70] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [70] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [70] Batch [120/261] Loss: 0.0001 LR: 0.000100
Epoch [70] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [70] Batch [140/261] Loss: 0.0107 LR: 0.000100
Epoch [70] Batch [150/261] Loss: 0.0001 LR: 0.000100
Epoch [70] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [70] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [70] Batch [180/261] Loss: 0.0144 LR: 0.000100
Epoch [70] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [70] Batch [200/261] Loss: 0.0001 LR: 0.000100
Epoch [70] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [70] Batch [220/261] Loss: 0.0007 LR: 0.000100
Epoch [70] Batch [230/261] Loss: 0.0001 LR: 0.000100
Epoch [70] Batch [240/261] Loss: 0.0861 LR: 0.000100
Epoch [70] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [70] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [71/350] - Time: 13.02s
Train Loss: 0.0744 | Val Loss: 0.0645
Train Acc: 0.9834 | Val Acc: 0.9795
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 0.979    Count 8: 0.980    Count 9: 0.933    Count 10: 0.875  
Epoch [71] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [71] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [71] Batch [20/261] Loss: 0.0002 LR: 0.000100
Epoch [71] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [71] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [71] Batch [50/261] Loss: 0.0001 LR: 0.000100
Epoch [71] Batch [60/261] Loss: 0.2465 LR: 0.000100
Epoch [71] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [71] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [71] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [71] Batch [100/261] Loss: 0.0041 LR: 0.000100
Epoch [71] Batch [110/261] Loss: 0.0001 LR: 0.000100
Epoch [71] Batch [120/261] Loss: 0.6822 LR: 0.000100
Epoch [71] Batch [130/261] Loss: 0.0002 LR: 0.000100
Epoch [71] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [71] Batch [150/261] Loss: 0.0037 LR: 0.000100
Epoch [71] Batch [160/261] Loss: 0.0042 LR: 0.000100
Epoch [71] Batch [170/261] Loss: 0.0062 LR: 0.000100
Epoch [71] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [71] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [71] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [71] Batch [210/261] Loss: 0.2473 LR: 0.000100
Epoch [71] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [71] Batch [230/261] Loss: 0.0001 LR: 0.000100
Epoch [71] Batch [240/261] Loss: 0.0019 LR: 0.000100
Epoch [71] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [71] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [72/350] - Time: 12.70s
Train Loss: 0.0440 | Val Loss: 0.0052
Train Acc: 0.9918 | Val Acc: 0.9972
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.990    Count 8: 1.000    Count 9: 1.000    Count 10: 0.977  
Epoch [72] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [72] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [72] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [72] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [72] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [72] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [72] Batch [60/261] Loss: 0.0001 LR: 0.000100
Epoch [72] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [72] Batch [80/261] Loss: 0.0001 LR: 0.000100
Epoch [72] Batch [90/261] Loss: 0.0001 LR: 0.000100
Epoch [72] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [72] Batch [110/261] Loss: 0.0001 LR: 0.000100
Epoch [72] Batch [120/261] Loss: 0.0066 LR: 0.000100
Epoch [72] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [72] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [72] Batch [150/261] Loss: 0.0052 LR: 0.000100
Epoch [72] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [72] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [72] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [72] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [72] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [72] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [72] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [72] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [72] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [72] Batch [250/261] Loss: 0.0004 LR: 0.000100
Epoch [72] Batch [260/261] Loss: 0.0004 LR: 0.000100

Epoch [73/350] - Time: 12.61s
Train Loss: 0.0273 | Val Loss: 0.6991
Train Acc: 0.9947 | Val Acc: 0.8687
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.951  
  Count 6: 0.714    Count 7: 0.635    Count 8: 0.616    Count 9: 0.611    Count 10: 1.000  
Epoch [73] Batch [0/261] Loss: 0.0170 LR: 0.000100
Epoch [73] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [73] Batch [20/261] Loss: 0.0001 LR: 0.000100
Epoch [73] Batch [30/261] Loss: 0.0006 LR: 0.000100
Epoch [73] Batch [40/261] Loss: 0.4047 LR: 0.000100
Epoch [73] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [73] Batch [60/261] Loss: 0.0032 LR: 0.000100
Epoch [73] Batch [70/261] Loss: 0.0001 LR: 0.000100
Epoch [73] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [73] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [73] Batch [100/261] Loss: 0.0065 LR: 0.000100
Epoch [73] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [73] Batch [120/261] Loss: 0.0050 LR: 0.000100
Epoch [73] Batch [130/261] Loss: 0.0006 LR: 0.000100
Epoch [73] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [73] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [73] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [73] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [73] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [73] Batch [190/261] Loss: 0.0033 LR: 0.000100
Epoch [73] Batch [200/261] Loss: 0.0989 LR: 0.000100
Epoch [73] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [73] Batch [220/261] Loss: 0.4873 LR: 0.000100
Epoch [73] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [73] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [73] Batch [250/261] Loss: 0.0001 LR: 0.000100
Epoch [73] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [74/350] - Time: 12.98s
Train Loss: 0.0663 | Val Loss: 0.0142
Train Acc: 0.9863 | Val Acc: 0.9972
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 0.990    Count 9: 1.000    Count 10: 0.989  
Epoch [74] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [74] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [74] Batch [20/261] Loss: 0.0006 LR: 0.000100
Epoch [74] Batch [30/261] Loss: 0.2276 LR: 0.000100
Epoch [74] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [74] Batch [50/261] Loss: 0.0417 LR: 0.000100
Epoch [74] Batch [60/261] Loss: 0.0001 LR: 0.000100
Epoch [74] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [74] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [74] Batch [90/261] Loss: 0.0126 LR: 0.000100
Epoch [74] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [74] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [74] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [74] Batch [130/261] Loss: 0.0014 LR: 0.000100
Epoch [74] Batch [140/261] Loss: 0.8070 LR: 0.000100
Epoch [74] Batch [150/261] Loss: 0.0246 LR: 0.000100
Epoch [74] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [74] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [74] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [74] Batch [190/261] Loss: 0.0930 LR: 0.000100
Epoch [74] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [74] Batch [210/261] Loss: 0.7242 LR: 0.000100
Epoch [74] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [74] Batch [230/261] Loss: 0.1979 LR: 0.000100
Epoch [74] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [74] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [74] Batch [260/261] Loss: 0.4453 LR: 0.000100

Epoch [75/350] - Time: 13.05s
Train Loss: 0.0343 | Val Loss: 0.4483
Train Acc: 0.9918 | Val Acc: 0.9358
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 0.990  
  Count 6: 0.980    Count 7: 0.990    Count 8: 0.949    Count 9: 0.889    Count 10: 0.443  
Epoch [75] Batch [0/261] Loss: 0.0125 LR: 0.000100
Epoch [75] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [75] Batch [20/261] Loss: 0.0037 LR: 0.000100
Epoch [75] Batch [30/261] Loss: 0.0011 LR: 0.000100
Epoch [75] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [75] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [75] Batch [60/261] Loss: 0.0001 LR: 0.000100
Epoch [75] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [75] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [75] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [75] Batch [100/261] Loss: 0.5297 LR: 0.000100
Epoch [75] Batch [110/261] Loss: 0.4741 LR: 0.000100
Epoch [75] Batch [120/261] Loss: 0.0002 LR: 0.000100
Epoch [75] Batch [130/261] Loss: 0.0147 LR: 0.000100
Epoch [75] Batch [140/261] Loss: 0.0548 LR: 0.000100
Epoch [75] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [75] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [75] Batch [170/261] Loss: 0.0121 LR: 0.000100
Epoch [75] Batch [180/261] Loss: 0.2948 LR: 0.000100
Epoch [75] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [75] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [75] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [75] Batch [220/261] Loss: 0.0568 LR: 0.000100
Epoch [75] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [75] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [75] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [75] Batch [260/261] Loss: 0.7378 LR: 0.000100

Epoch [76/350] - Time: 12.65s
Train Loss: 0.0462 | Val Loss: 0.1880
Train Acc: 0.9892 | Val Acc: 0.9693
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.980    Count 7: 0.969    Count 8: 0.990    Count 9: 0.933    Count 10: 0.773  
Epoch [76] Batch [0/261] Loss: 0.1179 LR: 0.000100
Epoch [76] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [76] Batch [20/261] Loss: 0.0004 LR: 0.000100
Epoch [76] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [76] Batch [40/261] Loss: 0.1035 LR: 0.000100
Epoch [76] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [76] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [76] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [76] Batch [80/261] Loss: 0.0001 LR: 0.000100
Epoch [76] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [76] Batch [100/261] Loss: 0.0011 LR: 0.000100
Epoch [76] Batch [110/261] Loss: 0.4862 LR: 0.000100
Epoch [76] Batch [120/261] Loss: 0.2498 LR: 0.000100
Epoch [76] Batch [130/261] Loss: 0.0290 LR: 0.000100
Epoch [76] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [76] Batch [150/261] Loss: 0.0042 LR: 0.000100
Epoch [76] Batch [160/261] Loss: 0.0080 LR: 0.000100
Epoch [76] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [76] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [76] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [76] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [76] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [76] Batch [220/261] Loss: 0.0001 LR: 0.000100
Epoch [76] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [76] Batch [240/261] Loss: 0.0350 LR: 0.000100
Epoch [76] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [76] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [77/350] - Time: 12.58s
Train Loss: 0.0381 | Val Loss: 0.5531
Train Acc: 0.9916 | Val Acc: 0.8976
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.776    Count 7: 0.844    Count 8: 0.788    Count 9: 0.922    Count 10: 0.500  
Epoch [77] Batch [0/261] Loss: 0.1400 LR: 0.000100
Epoch [77] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [77] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [77] Batch [30/261] Loss: 0.0437 LR: 0.000100
Epoch [77] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [77] Batch [50/261] Loss: 0.0031 LR: 0.000100
Epoch [77] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [77] Batch [70/261] Loss: 0.0039 LR: 0.000100
Epoch [77] Batch [80/261] Loss: 0.0002 LR: 0.000100
Epoch [77] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [77] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [77] Batch [110/261] Loss: 0.0005 LR: 0.000100
Epoch [77] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [77] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [77] Batch [140/261] Loss: 0.0001 LR: 0.000100
Epoch [77] Batch [150/261] Loss: 0.0004 LR: 0.000100
Epoch [77] Batch [160/261] Loss: 0.0004 LR: 0.000100
Epoch [77] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [77] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [77] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [77] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [77] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [77] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [77] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [77] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [77] Batch [250/261] Loss: 0.0001 LR: 0.000100
Epoch [77] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [78/350] - Time: 12.72s
Train Loss: 0.0202 | Val Loss: 0.1662
Train Acc: 0.9954 | Val Acc: 0.9702
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 0.969    Count 8: 0.929    Count 9: 0.967    Count 10: 0.807  
Epoch [78] Batch [0/261] Loss: 0.0099 LR: 0.000100
Epoch [78] Batch [10/261] Loss: 0.0882 LR: 0.000100
Epoch [78] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [78] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [78] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [78] Batch [50/261] Loss: 0.2305 LR: 0.000100
Epoch [78] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [78] Batch [70/261] Loss: 0.0989 LR: 0.000100
Epoch [78] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [78] Batch [90/261] Loss: 0.3927 LR: 0.000100
Epoch [78] Batch [100/261] Loss: 0.0002 LR: 0.000100
Epoch [78] Batch [110/261] Loss: 0.4784 LR: 0.000100
Epoch [78] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [78] Batch [130/261] Loss: 0.0004 LR: 0.000100
Epoch [78] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [78] Batch [150/261] Loss: 0.0001 LR: 0.000100
Epoch [78] Batch [160/261] Loss: 0.0583 LR: 0.000100
Epoch [78] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [78] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [78] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [78] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [78] Batch [210/261] Loss: 0.4659 LR: 0.000100
Epoch [78] Batch [220/261] Loss: 0.0508 LR: 0.000100
Epoch [78] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [78] Batch [240/261] Loss: 0.3868 LR: 0.000100
Epoch [78] Batch [250/261] Loss: 0.0010 LR: 0.000100
Epoch [78] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [79/350] - Time: 12.66s
Train Loss: 0.0471 | Val Loss: 1.0810
Train Acc: 0.9901 | Val Acc: 0.8622
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.981    Count 5: 0.922  
  Count 6: 0.837    Count 7: 0.562    Count 8: 0.768    Count 9: 0.367    Count 10: 1.000  
Epoch [79] Batch [0/261] Loss: 0.3915 LR: 0.000100
Epoch [79] Batch [10/261] Loss: 0.0039 LR: 0.000100
Epoch [79] Batch [20/261] Loss: 0.0002 LR: 0.000100
Epoch [79] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [79] Batch [40/261] Loss: 0.0013 LR: 0.000100
Epoch [79] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [79] Batch [60/261] Loss: 0.0104 LR: 0.000100
Epoch [79] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [79] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [79] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [79] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [79] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [79] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [79] Batch [130/261] Loss: 0.0001 LR: 0.000100
Epoch [79] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [79] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [79] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [79] Batch [170/261] Loss: 0.0122 LR: 0.000100
Epoch [79] Batch [180/261] Loss: 0.0007 LR: 0.000100
Epoch [79] Batch [190/261] Loss: 0.0021 LR: 0.000100
Epoch [79] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [79] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [79] Batch [220/261] Loss: 0.0006 LR: 0.000100
Epoch [79] Batch [230/261] Loss: 0.0001 LR: 0.000100
Epoch [79] Batch [240/261] Loss: 0.0076 LR: 0.000100
Epoch [79] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [79] Batch [260/261] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [80/350] - Time: 12.66s
Train Loss: 0.0364 | Val Loss: 0.0548
Train Acc: 0.9918 | Val Acc: 0.9926
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.990    Count 5: 1.000  
  Count 6: 0.990    Count 7: 1.000    Count 8: 0.980    Count 9: 0.956    Count 10: 1.000  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/single_image_checkpoint_epoch_79.pth
Epoch [80] Batch [0/261] Loss: 0.0001 LR: 0.000100
Epoch [80] Batch [10/261] Loss: 0.6072 LR: 0.000100
Epoch [80] Batch [20/261] Loss: 0.0001 LR: 0.000100
Epoch [80] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [80] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [80] Batch [50/261] Loss: 0.0002 LR: 0.000100
Epoch [80] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [80] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [80] Batch [80/261] Loss: 0.0006 LR: 0.000100
Epoch [80] Batch [90/261] Loss: 0.0001 LR: 0.000100
Epoch [80] Batch [100/261] Loss: 0.0001 LR: 0.000100
Epoch [80] Batch [110/261] Loss: 0.0711 LR: 0.000100
Epoch [80] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [80] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [80] Batch [140/261] Loss: 0.0638 LR: 0.000100
Epoch [80] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [80] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [80] Batch [170/261] Loss: 0.0062 LR: 0.000100
Epoch [80] Batch [180/261] Loss: 0.0001 LR: 0.000100
Epoch [80] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [80] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [80] Batch [210/261] Loss: 0.0098 LR: 0.000100
Epoch [80] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [80] Batch [230/261] Loss: 0.0552 LR: 0.000100
Epoch [80] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [80] Batch [250/261] Loss: 0.0001 LR: 0.000100
Epoch [80] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [81/350] - Time: 12.77s
Train Loss: 0.0177 | Val Loss: 0.0486
Train Acc: 0.9947 | Val Acc: 0.9916
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.990    Count 8: 0.970    Count 9: 0.967    Count 10: 0.977  
Epoch [81] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [81] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [81] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [81] Batch [30/261] Loss: 0.0002 LR: 0.000100
Epoch [81] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [81] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [81] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [81] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [81] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [81] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [81] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [81] Batch [110/261] Loss: 0.0004 LR: 0.000100
Epoch [81] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [81] Batch [130/261] Loss: 0.0045 LR: 0.000100
Epoch [81] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [81] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [81] Batch [160/261] Loss: 0.0073 LR: 0.000100
Epoch [81] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [81] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [81] Batch [190/261] Loss: 0.0005 LR: 0.000100
Epoch [81] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [81] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [81] Batch [220/261] Loss: 0.0054 LR: 0.000100
Epoch [81] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [81] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [81] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [81] Batch [260/261] Loss: 1.4604 LR: 0.000100

Epoch [82/350] - Time: 12.71s
Train Loss: 0.0202 | Val Loss: 0.0252
Train Acc: 0.9950 | Val Acc: 0.9935
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.959    Count 7: 1.000    Count 8: 0.980    Count 9: 0.989    Count 10: 1.000  
Epoch [82] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [82] Batch [10/261] Loss: 0.6167 LR: 0.000100
Epoch [82] Batch [20/261] Loss: 0.0014 LR: 0.000100
Epoch [82] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [82] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [82] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [82] Batch [60/261] Loss: 0.0001 LR: 0.000100
Epoch [82] Batch [70/261] Loss: 0.0005 LR: 0.000100
Epoch [82] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [82] Batch [90/261] Loss: 0.0005 LR: 0.000100
Epoch [82] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [82] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [82] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [82] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [82] Batch [140/261] Loss: 0.0001 LR: 0.000100
Epoch [82] Batch [150/261] Loss: 0.1844 LR: 0.000100
Epoch [82] Batch [160/261] Loss: 0.0062 LR: 0.000100
Epoch [82] Batch [170/261] Loss: 0.0825 LR: 0.000100
Epoch [82] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [82] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [82] Batch [200/261] Loss: 0.0007 LR: 0.000100
Epoch [82] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [82] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [82] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [82] Batch [240/261] Loss: 0.0001 LR: 0.000100
Epoch [82] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [82] Batch [260/261] Loss: 33.0045 LR: 0.000100

Epoch [83/350] - Time: 12.93s
Train Loss: 0.0363 | Val Loss: 0.0585
Train Acc: 0.9923 | Val Acc: 0.9907
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.990    Count 8: 0.939    Count 9: 0.967    Count 10: 1.000  
Epoch [83] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [83] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [83] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [83] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [83] Batch [40/261] Loss: 0.6808 LR: 0.000100
Epoch [83] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [83] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [83] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [83] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [83] Batch [90/261] Loss: 0.0091 LR: 0.000100
Epoch [83] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [83] Batch [110/261] Loss: 0.0001 LR: 0.000100
Epoch [83] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [83] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [83] Batch [140/261] Loss: 0.6531 LR: 0.000100
Epoch [83] Batch [150/261] Loss: 0.0035 LR: 0.000100
Epoch [83] Batch [160/261] Loss: 0.1026 LR: 0.000100
Epoch [83] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [83] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [83] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [83] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [83] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [83] Batch [220/261] Loss: 0.0188 LR: 0.000100
Epoch [83] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [83] Batch [240/261] Loss: 0.4130 LR: 0.000100
Epoch [83] Batch [250/261] Loss: 0.0025 LR: 0.000100
Epoch [83] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [84/350] - Time: 12.73s
Train Loss: 0.0418 | Val Loss: 0.0349
Train Acc: 0.9916 | Val Acc: 0.9944
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 1.000    Count 8: 0.970    Count 9: 0.989    Count 10: 1.000  
Epoch [84] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [84] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [84] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [84] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [84] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [84] Batch [50/261] Loss: 0.0004 LR: 0.000100
Epoch [84] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [84] Batch [70/261] Loss: 0.0001 LR: 0.000100
Epoch [84] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [84] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [84] Batch [100/261] Loss: 0.0001 LR: 0.000100
Epoch [84] Batch [110/261] Loss: 0.0013 LR: 0.000100
Epoch [84] Batch [120/261] Loss: 0.0002 LR: 0.000100
Epoch [84] Batch [130/261] Loss: 0.0001 LR: 0.000100
Epoch [84] Batch [140/261] Loss: 0.0005 LR: 0.000100
Epoch [84] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [84] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [84] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [84] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [84] Batch [190/261] Loss: 0.0006 LR: 0.000100
Epoch [84] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [84] Batch [210/261] Loss: 0.0016 LR: 0.000100
Epoch [84] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [84] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [84] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [84] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [84] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [85/350] - Time: 12.73s
Train Loss: 0.0289 | Val Loss: 0.0197
Train Acc: 0.9942 | Val Acc: 0.9972
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.980    Count 7: 1.000    Count 8: 1.000    Count 9: 1.000    Count 10: 0.989  
Epoch [85] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [85] Batch [10/261] Loss: 0.0002 LR: 0.000100
Epoch [85] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [85] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [85] Batch [40/261] Loss: 0.4109 LR: 0.000100
Epoch [85] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [85] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [85] Batch [70/261] Loss: 0.5337 LR: 0.000100
Epoch [85] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [85] Batch [90/261] Loss: 0.0002 LR: 0.000100
Epoch [85] Batch [100/261] Loss: 0.0002 LR: 0.000100
Epoch [85] Batch [110/261] Loss: 0.8483 LR: 0.000100
Epoch [85] Batch [120/261] Loss: 0.4615 LR: 0.000100
Epoch [85] Batch [130/261] Loss: 0.2057 LR: 0.000100
Epoch [85] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [85] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [85] Batch [160/261] Loss: 0.0002 LR: 0.000100
Epoch [85] Batch [170/261] Loss: 0.0011 LR: 0.000100
Epoch [85] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [85] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [85] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [85] Batch [210/261] Loss: 0.3999 LR: 0.000100
Epoch [85] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [85] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [85] Batch [240/261] Loss: 0.0001 LR: 0.000100
Epoch [85] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [85] Batch [260/261] Loss: 19.2188 LR: 0.000100

Epoch [86/350] - Time: 12.73s
Train Loss: 0.1124 | Val Loss: 0.0176
Train Acc: 0.9801 | Val Acc: 0.9953
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.990    Count 8: 1.000    Count 9: 1.000    Count 10: 0.955  
Epoch [86] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [86] Batch [10/261] Loss: 0.0008 LR: 0.000100
Epoch [86] Batch [20/261] Loss: 0.0001 LR: 0.000100
Epoch [86] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [86] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [86] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [86] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [86] Batch [70/261] Loss: 0.0001 LR: 0.000100
Epoch [86] Batch [80/261] Loss: 0.0001 LR: 0.000100
Epoch [86] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [86] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [86] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [86] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [86] Batch [130/261] Loss: 0.9455 LR: 0.000100
Epoch [86] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [86] Batch [150/261] Loss: 0.0002 LR: 0.000100
Epoch [86] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [86] Batch [170/261] Loss: 0.0001 LR: 0.000100
Epoch [86] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [86] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [86] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [86] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [86] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [86] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [86] Batch [240/261] Loss: 0.0001 LR: 0.000100
Epoch [86] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [86] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [87/350] - Time: 12.65s
Train Loss: 0.0313 | Val Loss: 0.0409
Train Acc: 0.9947 | Val Acc: 0.9953
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 0.979    Count 8: 1.000    Count 9: 0.989    Count 10: 1.000  
Epoch [87] Batch [0/261] Loss: 0.4451 LR: 0.000100
Epoch [87] Batch [10/261] Loss: 0.0001 LR: 0.000100
Epoch [87] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [87] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [87] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [87] Batch [50/261] Loss: 0.1742 LR: 0.000100
Epoch [87] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [87] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [87] Batch [80/261] Loss: 0.0001 LR: 0.000100
Epoch [87] Batch [90/261] Loss: 0.7041 LR: 0.000100
Epoch [87] Batch [100/261] Loss: 0.0002 LR: 0.000100
Epoch [87] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [87] Batch [120/261] Loss: 0.1088 LR: 0.000100
Epoch [87] Batch [130/261] Loss: 0.0009 LR: 0.000100
Epoch [87] Batch [140/261] Loss: 0.0001 LR: 0.000100
Epoch [87] Batch [150/261] Loss: 0.0001 LR: 0.000100
Epoch [87] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [87] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [87] Batch [180/261] Loss: 0.2991 LR: 0.000100
Epoch [87] Batch [190/261] Loss: 0.0042 LR: 0.000100
Epoch [87] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [87] Batch [210/261] Loss: 0.0001 LR: 0.000100
Epoch [87] Batch [220/261] Loss: 0.0001 LR: 0.000100
Epoch [87] Batch [230/261] Loss: 0.0001 LR: 0.000100
Epoch [87] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [87] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [87] Batch [260/261] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [88/350] - Time: 12.58s
Train Loss: 0.0456 | Val Loss: 0.0020
Train Acc: 0.9892 | Val Acc: 0.9991
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 0.990    Count 9: 1.000    Count 10: 1.000  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/best_single_image_model.pth
  → New best model! Validation accuracy: 0.9991
新的最佳模型! 验证准确率: 0.9991
Epoch [88] Batch [0/261] Loss: 0.0587 LR: 0.000100
Epoch [88] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [88] Batch [20/261] Loss: 0.0164 LR: 0.000100
Epoch [88] Batch [30/261] Loss: 0.0002 LR: 0.000100
Epoch [88] Batch [40/261] Loss: 0.0281 LR: 0.000100
Epoch [88] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [88] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [88] Batch [70/261] Loss: 0.0073 LR: 0.000100
Epoch [88] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [88] Batch [90/261] Loss: 0.1182 LR: 0.000100
Epoch [88] Batch [100/261] Loss: 0.0284 LR: 0.000100
Epoch [88] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [88] Batch [120/261] Loss: 0.0036 LR: 0.000100
Epoch [88] Batch [130/261] Loss: 0.0008 LR: 0.000100
Epoch [88] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [88] Batch [150/261] Loss: 0.0727 LR: 0.000100
Epoch [88] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [88] Batch [170/261] Loss: 0.0001 LR: 0.000100
Epoch [88] Batch [180/261] Loss: 0.0013 LR: 0.000100
Epoch [88] Batch [190/261] Loss: 0.0033 LR: 0.000100
Epoch [88] Batch [200/261] Loss: 0.0003 LR: 0.000100
Epoch [88] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [88] Batch [220/261] Loss: 0.5516 LR: 0.000100
Epoch [88] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [88] Batch [240/261] Loss: 0.0001 LR: 0.000100
Epoch [88] Batch [250/261] Loss: 0.0001 LR: 0.000100
Epoch [88] Batch [260/261] Loss: 0.0381 LR: 0.000100

Epoch [89/350] - Time: 12.78s
Train Loss: 0.0544 | Val Loss: 0.0180
Train Acc: 0.9870 | Val Acc: 0.9944
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 1.000    Count 9: 0.989    Count 10: 0.943  
Epoch [89] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [89] Batch [10/261] Loss: 0.0001 LR: 0.000100
Epoch [89] Batch [20/261] Loss: 0.0011 LR: 0.000100
Epoch [89] Batch [30/261] Loss: 0.0003 LR: 0.000100
Epoch [89] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [89] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [89] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [89] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [89] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [89] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [89] Batch [100/261] Loss: 0.0002 LR: 0.000100
Epoch [89] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [89] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [89] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [89] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [89] Batch [150/261] Loss: 0.7193 LR: 0.000100
Epoch [89] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [89] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [89] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [89] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [89] Batch [200/261] Loss: 0.0004 LR: 0.000100
Epoch [89] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [89] Batch [220/261] Loss: 0.0007 LR: 0.000100
Epoch [89] Batch [230/261] Loss: 0.0001 LR: 0.000100
Epoch [89] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [89] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [89] Batch [260/261] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [90/350] - Time: 12.49s
Train Loss: 0.0221 | Val Loss: 0.0540
Train Acc: 0.9947 | Val Acc: 0.9898
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.990    Count 5: 1.000  
  Count 6: 0.990    Count 7: 0.990    Count 8: 0.990    Count 9: 0.956    Count 10: 0.977  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/single_image_checkpoint_epoch_89.pth
Epoch [90] Batch [0/261] Loss: 0.0001 LR: 0.000100
Epoch [90] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [90] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [90] Batch [30/261] Loss: 0.0253 LR: 0.000100
Epoch [90] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [90] Batch [50/261] Loss: 0.0214 LR: 0.000100
Epoch [90] Batch [60/261] Loss: 0.0191 LR: 0.000100
Epoch [90] Batch [70/261] Loss: 0.0002 LR: 0.000100
Epoch [90] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [90] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [90] Batch [100/261] Loss: 0.0493 LR: 0.000100
Epoch [90] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [90] Batch [120/261] Loss: 0.6258 LR: 0.000100
Epoch [90] Batch [130/261] Loss: 0.0003 LR: 0.000100
Epoch [90] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [90] Batch [150/261] Loss: 0.0001 LR: 0.000100
Epoch [90] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [90] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [90] Batch [180/261] Loss: 0.0920 LR: 0.000100
Epoch [90] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [90] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [90] Batch [210/261] Loss: 0.0003 LR: 0.000100
Epoch [90] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [90] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [90] Batch [240/261] Loss: 0.0001 LR: 0.000100
Epoch [90] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [90] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [91/350] - Time: 12.93s
Train Loss: 0.0609 | Val Loss: 0.0853
Train Acc: 0.9904 | Val Acc: 0.9823
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.990    Count 5: 0.971  
  Count 6: 0.980    Count 7: 0.979    Count 8: 0.980    Count 9: 0.967    Count 10: 0.943  
Epoch [91] Batch [0/261] Loss: 0.0069 LR: 0.000100
Epoch [91] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [91] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [91] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [91] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [91] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [91] Batch [60/261] Loss: 0.0001 LR: 0.000100
Epoch [91] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [91] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [91] Batch [90/261] Loss: 0.0002 LR: 0.000100
Epoch [91] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [91] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [91] Batch [120/261] Loss: 0.1168 LR: 0.000100
Epoch [91] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [91] Batch [140/261] Loss: 0.0008 LR: 0.000100
Epoch [91] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [91] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [91] Batch [170/261] Loss: 0.0001 LR: 0.000100
Epoch [91] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [91] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [91] Batch [200/261] Loss: 0.9952 LR: 0.000100
Epoch [91] Batch [210/261] Loss: 0.0004 LR: 0.000100
Epoch [91] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [91] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [91] Batch [240/261] Loss: 0.0020 LR: 0.000100
Epoch [91] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [91] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [92/350] - Time: 12.75s
Train Loss: 0.0376 | Val Loss: 0.0136
Train Acc: 0.9930 | Val Acc: 0.9963
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.990    Count 8: 0.990    Count 9: 0.989    Count 10: 0.989  
Epoch [92] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [92] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [92] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [92] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [92] Batch [40/261] Loss: 0.0001 LR: 0.000100
Epoch [92] Batch [50/261] Loss: 0.0001 LR: 0.000100
Epoch [92] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [92] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [92] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [92] Batch [90/261] Loss: 0.0004 LR: 0.000100
Epoch [92] Batch [100/261] Loss: 0.3307 LR: 0.000100
Epoch [92] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [92] Batch [120/261] Loss: 0.2074 LR: 0.000100
Epoch [92] Batch [130/261] Loss: 0.0128 LR: 0.000100
Epoch [92] Batch [140/261] Loss: 0.7132 LR: 0.000100
Epoch [92] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [92] Batch [160/261] Loss: 0.9500 LR: 0.000100
Epoch [92] Batch [170/261] Loss: 0.0026 LR: 0.000100
Epoch [92] Batch [180/261] Loss: 0.0001 LR: 0.000100
Epoch [92] Batch [190/261] Loss: 0.0001 LR: 0.000100
Epoch [92] Batch [200/261] Loss: 0.0002 LR: 0.000100
Epoch [92] Batch [210/261] Loss: 0.0021 LR: 0.000100
Epoch [92] Batch [220/261] Loss: 0.3500 LR: 0.000100
Epoch [92] Batch [230/261] Loss: 0.3602 LR: 0.000100
Epoch [92] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [92] Batch [250/261] Loss: 0.7340 LR: 0.000100
Epoch [92] Batch [260/261] Loss: 7.6162 LR: 0.000100

Epoch [93/350] - Time: 12.91s
Train Loss: 0.1106 | Val Loss: 0.0143
Train Acc: 0.9796 | Val Acc: 0.9963
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.992    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.990    Count 8: 0.990    Count 9: 1.000    Count 10: 0.989  
Epoch [93] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [93] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [93] Batch [20/261] Loss: 0.0001 LR: 0.000100
Epoch [93] Batch [30/261] Loss: 0.0015 LR: 0.000100
Epoch [93] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [93] Batch [50/261] Loss: 0.0073 LR: 0.000100
Epoch [93] Batch [60/261] Loss: 0.3969 LR: 0.000100
Epoch [93] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [93] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [93] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [93] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [93] Batch [110/261] Loss: 0.0535 LR: 0.000100
Epoch [93] Batch [120/261] Loss: 0.0196 LR: 0.000100
Epoch [93] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [93] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [93] Batch [150/261] Loss: 0.5477 LR: 0.000100
Epoch [93] Batch [160/261] Loss: 0.0008 LR: 0.000100
Epoch [93] Batch [170/261] Loss: 0.0009 LR: 0.000100
Epoch [93] Batch [180/261] Loss: 0.0001 LR: 0.000100
Epoch [93] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [93] Batch [200/261] Loss: 0.0001 LR: 0.000100
Epoch [93] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [93] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [93] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [93] Batch [240/261] Loss: 0.1200 LR: 0.000100
Epoch [93] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [93] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [94/350] - Time: 12.77s
Train Loss: 0.0286 | Val Loss: 0.0240
Train Acc: 0.9933 | Val Acc: 0.9963
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.990    Count 8: 1.000    Count 9: 1.000    Count 10: 0.966  
Epoch [94] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [94] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [94] Batch [20/261] Loss: 0.0001 LR: 0.000100
Epoch [94] Batch [30/261] Loss: 0.0001 LR: 0.000100
Epoch [94] Batch [40/261] Loss: 0.0002 LR: 0.000100
Epoch [94] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [94] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [94] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [94] Batch [80/261] Loss: 0.0068 LR: 0.000100
Epoch [94] Batch [90/261] Loss: 0.0030 LR: 0.000100
Epoch [94] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [94] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [94] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [94] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [94] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [94] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [94] Batch [160/261] Loss: 0.0651 LR: 0.000100
Epoch [94] Batch [170/261] Loss: 0.0324 LR: 0.000100
Epoch [94] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [94] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [94] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [94] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [94] Batch [220/261] Loss: 0.0229 LR: 0.000100
Epoch [94] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [94] Batch [240/261] Loss: 0.4221 LR: 0.000100
Epoch [94] Batch [250/261] Loss: 0.0001 LR: 0.000100
Epoch [94] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [95/350] - Time: 12.71s
Train Loss: 0.0583 | Val Loss: 0.0116
Train Acc: 0.9892 | Val Acc: 0.9981
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 1.000    Count 9: 0.989    Count 10: 1.000  
Epoch [95] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [95] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [95] Batch [20/261] Loss: 0.0001 LR: 0.000100
Epoch [95] Batch [30/261] Loss: 0.0260 LR: 0.000100
Epoch [95] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [95] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [95] Batch [60/261] Loss: 0.0824 LR: 0.000100
Epoch [95] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [95] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [95] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [95] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [95] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [95] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [95] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [95] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [95] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [95] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [95] Batch [170/261] Loss: 0.0452 LR: 0.000100
Epoch [95] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [95] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [95] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [95] Batch [210/261] Loss: 0.0001 LR: 0.000100
Epoch [95] Batch [220/261] Loss: 0.0002 LR: 0.000100
Epoch [95] Batch [230/261] Loss: 0.0001 LR: 0.000100
Epoch [95] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [95] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [95] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [96/350] - Time: 12.69s
Train Loss: 0.0118 | Val Loss: 0.0337
Train Acc: 0.9966 | Val Acc: 0.9944
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 1.000    Count 8: 1.000    Count 9: 0.989    Count 10: 0.966  
Epoch [96] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [96] Batch [10/261] Loss: 0.2915 LR: 0.000100
Epoch [96] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [96] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [96] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [96] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [96] Batch [60/261] Loss: 0.0003 LR: 0.000100
Epoch [96] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [96] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [96] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [96] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [96] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [96] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [96] Batch [130/261] Loss: 0.1270 LR: 0.000100
Epoch [96] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [96] Batch [150/261] Loss: 0.0001 LR: 0.000100
Epoch [96] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [96] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [96] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [96] Batch [190/261] Loss: 0.2495 LR: 0.000100
Epoch [96] Batch [200/261] Loss: 0.0001 LR: 0.000100
Epoch [96] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [96] Batch [220/261] Loss: 0.0010 LR: 0.000100
Epoch [96] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [96] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [96] Batch [250/261] Loss: 0.0099 LR: 0.000100
Epoch [96] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [97/350] - Time: 12.82s
Train Loss: 0.0308 | Val Loss: 0.0129
Train Acc: 0.9947 | Val Acc: 0.9972
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 0.990    Count 9: 0.989    Count 10: 0.989  
Epoch [97] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [97] Batch [10/261] Loss: 0.0003 LR: 0.000100
Epoch [97] Batch [20/261] Loss: 0.0001 LR: 0.000100
Epoch [97] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [97] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [97] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [97] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [97] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [97] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [97] Batch [90/261] Loss: 0.0002 LR: 0.000100
Epoch [97] Batch [100/261] Loss: 0.0015 LR: 0.000100
Epoch [97] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [97] Batch [120/261] Loss: 0.0003 LR: 0.000100
Epoch [97] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [97] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [97] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [97] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [97] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [97] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [97] Batch [190/261] Loss: 0.0001 LR: 0.000100
Epoch [97] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [97] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [97] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [97] Batch [230/261] Loss: 0.0001 LR: 0.000100
Epoch [97] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [97] Batch [250/261] Loss: 0.0045 LR: 0.000100
Epoch [97] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [98/350] - Time: 12.64s
Train Loss: 0.0181 | Val Loss: 0.0505
Train Acc: 0.9959 | Val Acc: 0.9860
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.969    Count 7: 0.990    Count 8: 0.909    Count 9: 0.978    Count 10: 1.000  
Epoch [98] Batch [0/261] Loss: 0.0001 LR: 0.000100
Epoch [98] Batch [10/261] Loss: 0.6438 LR: 0.000100
Epoch [98] Batch [20/261] Loss: 0.1431 LR: 0.000100
Epoch [98] Batch [30/261] Loss: 0.0006 LR: 0.000100
Epoch [98] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [98] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [98] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [98] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [98] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [98] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [98] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [98] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [98] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [98] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [98] Batch [140/261] Loss: 0.7658 LR: 0.000100
Epoch [98] Batch [150/261] Loss: 0.0017 LR: 0.000100
Epoch [98] Batch [160/261] Loss: 0.0010 LR: 0.000100
Epoch [98] Batch [170/261] Loss: 0.0003 LR: 0.000100
Epoch [98] Batch [180/261] Loss: 0.9585 LR: 0.000100
Epoch [98] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [98] Batch [200/261] Loss: 1.1228 LR: 0.000100
Epoch [98] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [98] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [98] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [98] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [98] Batch [250/261] Loss: 0.0005 LR: 0.000100
Epoch [98] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [99/350] - Time: 12.91s
Train Loss: 0.0457 | Val Loss: 0.1023
Train Acc: 0.9909 | Val Acc: 0.9804
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 1.000    Count 8: 1.000    Count 9: 0.989    Count 10: 0.795  
Epoch [99] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [99] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [99] Batch [20/261] Loss: 0.0515 LR: 0.000100
Epoch [99] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [99] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [99] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [99] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [99] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [99] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [99] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [99] Batch [100/261] Loss: 0.0002 LR: 0.000100
Epoch [99] Batch [110/261] Loss: 0.0001 LR: 0.000100
Epoch [99] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [99] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [99] Batch [140/261] Loss: 0.0002 LR: 0.000100
Epoch [99] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [99] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [99] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [99] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [99] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [99] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [99] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [99] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [99] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [99] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [99] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [99] Batch [260/261] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [100/350] - Time: 13.33s
Train Loss: 0.0194 | Val Loss: 0.1113
Train Acc: 0.9954 | Val Acc: 0.9758
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.979    Count 8: 1.000    Count 9: 0.967    Count 10: 0.773  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/single_image_checkpoint_epoch_99.pth
Epoch [100] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [100] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [100] Batch [20/261] Loss: 0.0001 LR: 0.000100
Epoch [100] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [100] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [100] Batch [50/261] Loss: 0.0001 LR: 0.000100
Epoch [100] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [100] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [100] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [100] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [100] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [100] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [100] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [100] Batch [130/261] Loss: 0.1271 LR: 0.000100
Epoch [100] Batch [140/261] Loss: 0.0001 LR: 0.000100
Epoch [100] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [100] Batch [160/261] Loss: 0.0177 LR: 0.000100
Epoch [100] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [100] Batch [180/261] Loss: 0.2759 LR: 0.000100
Epoch [100] Batch [190/261] Loss: 0.0002 LR: 0.000100
Epoch [100] Batch [200/261] Loss: 0.0010 LR: 0.000100
Epoch [100] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [100] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [100] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [100] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [100] Batch [250/261] Loss: 0.0001 LR: 0.000100
Epoch [100] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [101/350] - Time: 12.79s
Train Loss: 0.0769 | Val Loss: 0.0300
Train Acc: 0.9877 | Val Acc: 0.9916
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 1.000    Count 9: 0.989    Count 10: 0.920  
Epoch [101] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [101] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [101] Batch [20/261] Loss: 0.0004 LR: 0.000100
Epoch [101] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [101] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [101] Batch [50/261] Loss: 0.0402 LR: 0.000100
Epoch [101] Batch [60/261] Loss: 0.0503 LR: 0.000100
Epoch [101] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [101] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [101] Batch [90/261] Loss: 0.0002 LR: 0.000100
Epoch [101] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [101] Batch [110/261] Loss: 0.0004 LR: 0.000100
Epoch [101] Batch [120/261] Loss: 0.0002 LR: 0.000100
Epoch [101] Batch [130/261] Loss: 0.4252 LR: 0.000100
Epoch [101] Batch [140/261] Loss: 0.0001 LR: 0.000100
Epoch [101] Batch [150/261] Loss: 0.0005 LR: 0.000100
Epoch [101] Batch [160/261] Loss: 0.5572 LR: 0.000100
Epoch [101] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [101] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [101] Batch [190/261] Loss: 0.0035 LR: 0.000100
Epoch [101] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [101] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [101] Batch [220/261] Loss: 0.0001 LR: 0.000100
Epoch [101] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [101] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [101] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [101] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [102/350] - Time: 12.80s
Train Loss: 0.0383 | Val Loss: 0.0507
Train Acc: 0.9916 | Val Acc: 0.9907
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.980    Count 7: 0.990    Count 8: 1.000    Count 9: 0.967    Count 10: 0.966  
Epoch [102] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [102] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [102] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [102] Batch [30/261] Loss: 0.5894 LR: 0.000100
Epoch [102] Batch [40/261] Loss: 0.0001 LR: 0.000100
Epoch [102] Batch [50/261] Loss: 0.0013 LR: 0.000100
Epoch [102] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [102] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [102] Batch [80/261] Loss: 0.3095 LR: 0.000100
Epoch [102] Batch [90/261] Loss: 0.0086 LR: 0.000100
Epoch [102] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [102] Batch [110/261] Loss: 0.0064 LR: 0.000100
Epoch [102] Batch [120/261] Loss: 0.0006 LR: 0.000100
Epoch [102] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [102] Batch [140/261] Loss: 0.0021 LR: 0.000100
Epoch [102] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [102] Batch [160/261] Loss: 0.1285 LR: 0.000100
Epoch [102] Batch [170/261] Loss: 0.0175 LR: 0.000100
Epoch [102] Batch [180/261] Loss: 0.5439 LR: 0.000100
Epoch [102] Batch [190/261] Loss: 0.0024 LR: 0.000100
Epoch [102] Batch [200/261] Loss: 0.0003 LR: 0.000100
Epoch [102] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [102] Batch [220/261] Loss: 0.0376 LR: 0.000100
Epoch [102] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [102] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [102] Batch [250/261] Loss: 0.0013 LR: 0.000100
Epoch [102] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [103/350] - Time: 12.81s
Train Loss: 0.0359 | Val Loss: 0.0166
Train Acc: 0.9930 | Val Acc: 0.9963
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.990    Count 8: 1.000    Count 9: 0.989    Count 10: 0.989  
Epoch [103] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [103] Batch [10/261] Loss: 0.0004 LR: 0.000100
Epoch [103] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [103] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [103] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [103] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [103] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [103] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [103] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [103] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [103] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [103] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [103] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [103] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [103] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [103] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [103] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [103] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [103] Batch [180/261] Loss: 0.2846 LR: 0.000100
Epoch [103] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [103] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [103] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [103] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [103] Batch [230/261] Loss: 0.0001 LR: 0.000100
Epoch [103] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [103] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [103] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [104/350] - Time: 12.78s
Train Loss: 0.0122 | Val Loss: 0.4668
Train Acc: 0.9976 | Val Acc: 0.9311
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 0.980  
  Count 6: 0.969    Count 7: 0.958    Count 8: 0.889    Count 9: 0.944    Count 10: 0.455  
Epoch [104] Batch [0/261] Loss: 0.0054 LR: 0.000100
Epoch [104] Batch [10/261] Loss: 0.0009 LR: 0.000100
Epoch [104] Batch [20/261] Loss: 0.3157 LR: 0.000100
Epoch [104] Batch [30/261] Loss: 0.2627 LR: 0.000100
Epoch [104] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [104] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [104] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [104] Batch [70/261] Loss: 0.2024 LR: 0.000100
Epoch [104] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [104] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [104] Batch [100/261] Loss: 0.0010 LR: 0.000100
Epoch [104] Batch [110/261] Loss: 0.0001 LR: 0.000100
Epoch [104] Batch [120/261] Loss: 0.0326 LR: 0.000100
Epoch [104] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [104] Batch [140/261] Loss: 0.0015 LR: 0.000100
Epoch [104] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [104] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [104] Batch [170/261] Loss: 0.0380 LR: 0.000100
Epoch [104] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [104] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [104] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [104] Batch [210/261] Loss: 0.0001 LR: 0.000100
Epoch [104] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [104] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [104] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [104] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [104] Batch [260/261] Loss: 0.9551 LR: 0.000100

Epoch [105/350] - Time: 12.71s
Train Loss: 0.0423 | Val Loss: 0.0576
Train Acc: 0.9918 | Val Acc: 0.9860
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.980    Count 7: 0.917    Count 8: 0.980    Count 9: 0.967    Count 10: 1.000  
Epoch [105] Batch [0/261] Loss: 0.3032 LR: 0.000100
Epoch [105] Batch [10/261] Loss: 0.5908 LR: 0.000100
Epoch [105] Batch [20/261] Loss: 0.0053 LR: 0.000100
Epoch [105] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [105] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [105] Batch [50/261] Loss: 0.0001 LR: 0.000100
Epoch [105] Batch [60/261] Loss: 0.0034 LR: 0.000100
Epoch [105] Batch [70/261] Loss: 0.0385 LR: 0.000100
Epoch [105] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [105] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [105] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [105] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [105] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [105] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [105] Batch [140/261] Loss: 0.0017 LR: 0.000100
Epoch [105] Batch [150/261] Loss: 1.5259 LR: 0.000100
Epoch [105] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [105] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [105] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [105] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [105] Batch [200/261] Loss: 0.3431 LR: 0.000100
Epoch [105] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [105] Batch [220/261] Loss: 0.3996 LR: 0.000100
Epoch [105] Batch [230/261] Loss: 0.0001 LR: 0.000100
Epoch [105] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [105] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [105] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [106/350] - Time: 12.52s
Train Loss: 0.0354 | Val Loss: 0.2546
Train Acc: 0.9928 | Val Acc: 0.9562
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.918    Count 7: 0.958    Count 8: 0.818    Count 9: 0.811    Count 10: 1.000  
Epoch [106] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [106] Batch [10/261] Loss: 0.0001 LR: 0.000100
Epoch [106] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [106] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [106] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [106] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [106] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [106] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [106] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [106] Batch [90/261] Loss: 0.0002 LR: 0.000100
Epoch [106] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [106] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [106] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [106] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [106] Batch [140/261] Loss: 0.0037 LR: 0.000100
Epoch [106] Batch [150/261] Loss: 0.0059 LR: 0.000100
Epoch [106] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [106] Batch [170/261] Loss: 0.0004 LR: 0.000100
Epoch [106] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [106] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [106] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [106] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [106] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [106] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [106] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [106] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [106] Batch [260/261] Loss: 0.0029 LR: 0.000100

Epoch [107/350] - Time: 12.95s
Train Loss: 0.0130 | Val Loss: 0.6516
Train Acc: 0.9971 | Val Acc: 0.8929
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.980  
  Count 6: 0.878    Count 7: 0.667    Count 8: 0.384    Count 9: 0.911    Count 10: 1.000  
Epoch [107] Batch [0/261] Loss: 0.2673 LR: 0.000100
Epoch [107] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [107] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [107] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [107] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [107] Batch [50/261] Loss: 0.0028 LR: 0.000100
Epoch [107] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [107] Batch [70/261] Loss: 0.0001 LR: 0.000100
Epoch [107] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [107] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [107] Batch [100/261] Loss: 0.0002 LR: 0.000100
Epoch [107] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [107] Batch [120/261] Loss: 0.1971 LR: 0.000100
Epoch [107] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [107] Batch [140/261] Loss: 0.2786 LR: 0.000100
Epoch [107] Batch [150/261] Loss: 0.0001 LR: 0.000100
Epoch [107] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [107] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [107] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [107] Batch [190/261] Loss: 0.2396 LR: 0.000100
Epoch [107] Batch [200/261] Loss: 0.0028 LR: 0.000100
Epoch [107] Batch [210/261] Loss: 0.0002 LR: 0.000100
Epoch [107] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [107] Batch [230/261] Loss: 0.0128 LR: 0.000100
Epoch [107] Batch [240/261] Loss: 0.0007 LR: 0.000100
Epoch [107] Batch [250/261] Loss: 0.0001 LR: 0.000100
Epoch [107] Batch [260/261] Loss: 7.8361 LR: 0.000100

Epoch [108/350] - Time: 13.09s
Train Loss: 0.0336 | Val Loss: 0.0323
Train Acc: 0.9925 | Val Acc: 0.9944
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.980    Count 7: 0.990    Count 8: 1.000    Count 9: 0.989    Count 10: 0.989  
Epoch [108] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [108] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [108] Batch [20/261] Loss: 1.3688 LR: 0.000100
Epoch [108] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [108] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [108] Batch [50/261] Loss: 0.0001 LR: 0.000100
Epoch [108] Batch [60/261] Loss: 0.0002 LR: 0.000100
Epoch [108] Batch [70/261] Loss: 0.0002 LR: 0.000100
Epoch [108] Batch [80/261] Loss: 0.2720 LR: 0.000100
Epoch [108] Batch [90/261] Loss: 0.0934 LR: 0.000100
Epoch [108] Batch [100/261] Loss: 0.0001 LR: 0.000100
Epoch [108] Batch [110/261] Loss: 2.2165 LR: 0.000100
Epoch [108] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [108] Batch [130/261] Loss: 0.0013 LR: 0.000100
Epoch [108] Batch [140/261] Loss: 0.1465 LR: 0.000100
Epoch [108] Batch [150/261] Loss: 0.0001 LR: 0.000100
Epoch [108] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [108] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [108] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [108] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [108] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [108] Batch [210/261] Loss: 0.1413 LR: 0.000100
Epoch [108] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [108] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [108] Batch [240/261] Loss: 0.0053 LR: 0.000100
Epoch [108] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [108] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [109/350] - Time: 12.87s
Train Loss: 0.0600 | Val Loss: 0.0383
Train Acc: 0.9882 | Val Acc: 0.9926
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.990    Count 5: 1.000  
  Count 6: 0.980    Count 7: 0.990    Count 8: 1.000    Count 9: 0.978    Count 10: 0.989  
Epoch [109] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [109] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [109] Batch [20/261] Loss: 0.0002 LR: 0.000100
Epoch [109] Batch [30/261] Loss: 0.0001 LR: 0.000100
Epoch [109] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [109] Batch [50/261] Loss: 0.0002 LR: 0.000100
Epoch [109] Batch [60/261] Loss: 0.0683 LR: 0.000100
Epoch [109] Batch [70/261] Loss: 0.0001 LR: 0.000100
Epoch [109] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [109] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [109] Batch [100/261] Loss: 0.0001 LR: 0.000100
Epoch [109] Batch [110/261] Loss: 0.0172 LR: 0.000100
Epoch [109] Batch [120/261] Loss: 0.0003 LR: 0.000100
Epoch [109] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [109] Batch [140/261] Loss: 0.0031 LR: 0.000100
Epoch [109] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [109] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [109] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [109] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [109] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [109] Batch [200/261] Loss: 0.0001 LR: 0.000100
Epoch [109] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [109] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [109] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [109] Batch [240/261] Loss: 0.0105 LR: 0.000100
Epoch [109] Batch [250/261] Loss: 0.0008 LR: 0.000100
Epoch [109] Batch [260/261] Loss: 8.5060 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [110/350] - Time: 12.65s
Train Loss: 0.0233 | Val Loss: 0.1567
Train Acc: 0.9940 | Val Acc: 0.9730
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.981    Count 5: 0.990  
  Count 6: 0.990    Count 7: 1.000    Count 8: 0.990    Count 9: 0.933    Count 10: 0.807  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/single_image_checkpoint_epoch_109.pth
Epoch [110] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [110] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [110] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [110] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [110] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [110] Batch [50/261] Loss: 0.0008 LR: 0.000100
Epoch [110] Batch [60/261] Loss: 0.0001 LR: 0.000100
Epoch [110] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [110] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [110] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [110] Batch [100/261] Loss: 0.0001 LR: 0.000100
Epoch [110] Batch [110/261] Loss: 0.0063 LR: 0.000100
Epoch [110] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [110] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [110] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [110] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [110] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [110] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [110] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [110] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [110] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [110] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [110] Batch [220/261] Loss: 0.0001 LR: 0.000100
Epoch [110] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [110] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [110] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [110] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [111/350] - Time: 12.77s
Train Loss: 0.0047 | Val Loss: 0.0085
Train Acc: 0.9988 | Val Acc: 0.9981
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 1.000    Count 9: 1.000    Count 10: 0.989  
Epoch [111] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [111] Batch [10/261] Loss: 0.0004 LR: 0.000100
Epoch [111] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [111] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [111] Batch [40/261] Loss: 0.0307 LR: 0.000100
Epoch [111] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [111] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [111] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [111] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [111] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [111] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [111] Batch [110/261] Loss: 0.5106 LR: 0.000100
Epoch [111] Batch [120/261] Loss: 0.0001 LR: 0.000100
Epoch [111] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [111] Batch [140/261] Loss: 0.0012 LR: 0.000100
Epoch [111] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [111] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [111] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [111] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [111] Batch [190/261] Loss: 0.0123 LR: 0.000100
Epoch [111] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [111] Batch [210/261] Loss: 0.0038 LR: 0.000100
Epoch [111] Batch [220/261] Loss: 0.0091 LR: 0.000100
Epoch [111] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [111] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [111] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [111] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [112/350] - Time: 12.71s
Train Loss: 0.0183 | Val Loss: 0.0113
Train Acc: 0.9966 | Val Acc: 0.9981
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 1.000    Count 9: 0.989    Count 10: 0.989  
Epoch [112] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [112] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [112] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [112] Batch [30/261] Loss: 0.1217 LR: 0.000100
Epoch [112] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [112] Batch [50/261] Loss: 0.0051 LR: 0.000100
Epoch [112] Batch [60/261] Loss: 0.0039 LR: 0.000100
Epoch [112] Batch [70/261] Loss: 0.5455 LR: 0.000100
Epoch [112] Batch [80/261] Loss: 0.0204 LR: 0.000100
Epoch [112] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [112] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [112] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [112] Batch [120/261] Loss: 0.0004 LR: 0.000100
Epoch [112] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [112] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [112] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [112] Batch [160/261] Loss: 0.0001 LR: 0.000100
Epoch [112] Batch [170/261] Loss: 0.0001 LR: 0.000100
Epoch [112] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [112] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [112] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [112] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [112] Batch [220/261] Loss: 0.9839 LR: 0.000100
Epoch [112] Batch [230/261] Loss: 0.0392 LR: 0.000100
Epoch [112] Batch [240/261] Loss: 0.2696 LR: 0.000100
Epoch [112] Batch [250/261] Loss: 0.8415 LR: 0.000100
Epoch [112] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [113/350] - Time: 12.65s
Train Loss: 0.0674 | Val Loss: 0.0680
Train Acc: 0.9887 | Val Acc: 0.9842
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.972    Count 4: 0.990    Count 5: 0.980  
  Count 6: 0.990    Count 7: 0.969    Count 8: 0.990    Count 9: 0.944    Count 10: 0.989  
Epoch [113] Batch [0/261] Loss: 0.5166 LR: 0.000100
Epoch [113] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [113] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [113] Batch [30/261] Loss: 0.2183 LR: 0.000100
Epoch [113] Batch [40/261] Loss: 0.0001 LR: 0.000100
Epoch [113] Batch [50/261] Loss: 0.0009 LR: 0.000100
Epoch [113] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [113] Batch [70/261] Loss: 0.0015 LR: 0.000100
Epoch [113] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [113] Batch [90/261] Loss: 0.0698 LR: 0.000100
Epoch [113] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [113] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [113] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [113] Batch [130/261] Loss: 0.4920 LR: 0.000100
Epoch [113] Batch [140/261] Loss: 0.3025 LR: 0.000100
Epoch [113] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [113] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [113] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [113] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [113] Batch [190/261] Loss: 0.0011 LR: 0.000100
Epoch [113] Batch [200/261] Loss: 0.0021 LR: 0.000100
Epoch [113] Batch [210/261] Loss: 0.0001 LR: 0.000100
Epoch [113] Batch [220/261] Loss: 0.0002 LR: 0.000100
Epoch [113] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [113] Batch [240/261] Loss: 0.0059 LR: 0.000100
Epoch [113] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [113] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [114/350] - Time: 12.75s
Train Loss: 0.0391 | Val Loss: 0.1776
Train Acc: 0.9918 | Val Acc: 0.9609
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 0.971  
  Count 6: 0.969    Count 7: 0.958    Count 8: 0.828    Count 9: 0.956    Count 10: 0.886  
Epoch [114] Batch [0/261] Loss: 0.0001 LR: 0.000100
Epoch [114] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [114] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [114] Batch [30/261] Loss: 0.0001 LR: 0.000100
Epoch [114] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [114] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [114] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [114] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [114] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [114] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [114] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [114] Batch [110/261] Loss: 0.2500 LR: 0.000100
Epoch [114] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [114] Batch [130/261] Loss: 0.0012 LR: 0.000100
Epoch [114] Batch [140/261] Loss: 0.0001 LR: 0.000100
Epoch [114] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [114] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [114] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [114] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [114] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [114] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [114] Batch [210/261] Loss: 0.0011 LR: 0.000100
Epoch [114] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [114] Batch [230/261] Loss: 0.0003 LR: 0.000100
Epoch [114] Batch [240/261] Loss: 0.9381 LR: 0.000100
Epoch [114] Batch [250/261] Loss: 0.0002 LR: 0.000100
Epoch [114] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [115/350] - Time: 12.66s
Train Loss: 0.0252 | Val Loss: 0.5639
Train Acc: 0.9945 | Val Acc: 0.9125
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.980  
  Count 6: 0.827    Count 7: 0.927    Count 8: 0.808    Count 9: 0.922    Count 10: 0.523  
Epoch [115] Batch [0/261] Loss: 0.0007 LR: 0.000100
Epoch [115] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [115] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [115] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [115] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [115] Batch [50/261] Loss: 0.0004 LR: 0.000100
Epoch [115] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [115] Batch [70/261] Loss: 0.8082 LR: 0.000100
Epoch [115] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [115] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [115] Batch [100/261] Loss: 0.0009 LR: 0.000100
Epoch [115] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [115] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [115] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [115] Batch [140/261] Loss: 0.0016 LR: 0.000100
Epoch [115] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [115] Batch [160/261] Loss: 0.0002 LR: 0.000100
Epoch [115] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [115] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [115] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [115] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [115] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [115] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [115] Batch [230/261] Loss: 0.0084 LR: 0.000100
Epoch [115] Batch [240/261] Loss: 0.0001 LR: 0.000100
Epoch [115] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [115] Batch [260/261] Loss: 18.4053 LR: 0.000100

Epoch [116/350] - Time: 13.03s
Train Loss: 0.0274 | Val Loss: 0.0018
Train Acc: 0.9950 | Val Acc: 0.9991
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 1.000    Count 9: 0.989    Count 10: 1.000  
Epoch [116] Batch [0/261] Loss: 0.0856 LR: 0.000100
Epoch [116] Batch [10/261] Loss: 0.1842 LR: 0.000100
Epoch [116] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [116] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [116] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [116] Batch [50/261] Loss: 1.9164 LR: 0.000100
Epoch [116] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [116] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [116] Batch [80/261] Loss: 0.0007 LR: 0.000100
Epoch [116] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [116] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [116] Batch [110/261] Loss: 0.6051 LR: 0.000100
Epoch [116] Batch [120/261] Loss: 0.0073 LR: 0.000100
Epoch [116] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [116] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [116] Batch [150/261] Loss: 0.0003 LR: 0.000100
Epoch [116] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [116] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [116] Batch [180/261] Loss: 0.0001 LR: 0.000100
Epoch [116] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [116] Batch [200/261] Loss: 0.0403 LR: 0.000100
Epoch [116] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [116] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [116] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [116] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [116] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [116] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [117/350] - Time: 12.53s
Train Loss: 0.0249 | Val Loss: 0.0738
Train Acc: 0.9940 | Val Acc: 0.9898
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 1.000    Count 8: 1.000    Count 9: 0.989    Count 10: 0.909  
Epoch [117] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [117] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [117] Batch [20/261] Loss: 0.0003 LR: 0.000100
Epoch [117] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [117] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [117] Batch [50/261] Loss: 0.0012 LR: 0.000100
Epoch [117] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [117] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [117] Batch [80/261] Loss: 0.0555 LR: 0.000100
Epoch [117] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [117] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [117] Batch [110/261] Loss: 0.0001 LR: 0.000100
Epoch [117] Batch [120/261] Loss: 0.0013 LR: 0.000100
Epoch [117] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [117] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [117] Batch [150/261] Loss: 0.0001 LR: 0.000100
Epoch [117] Batch [160/261] Loss: 0.0042 LR: 0.000100
Epoch [117] Batch [170/261] Loss: 0.0009 LR: 0.000100
Epoch [117] Batch [180/261] Loss: 0.5673 LR: 0.000100
Epoch [117] Batch [190/261] Loss: 0.0001 LR: 0.000100
Epoch [117] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [117] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [117] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [117] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [117] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [117] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [117] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [118/350] - Time: 12.90s
Train Loss: 0.0289 | Val Loss: 0.0203
Train Acc: 0.9928 | Val Acc: 0.9963
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.990    Count 8: 1.000    Count 9: 0.978    Count 10: 0.989  
Epoch [118] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [118] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [118] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [118] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [118] Batch [40/261] Loss: 0.0361 LR: 0.000100
Epoch [118] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [118] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [118] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [118] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [118] Batch [90/261] Loss: 0.0028 LR: 0.000100
Epoch [118] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [118] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [118] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [118] Batch [130/261] Loss: 0.0002 LR: 0.000100
Epoch [118] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [118] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [118] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [118] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [118] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [118] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [118] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [118] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [118] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [118] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [118] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [118] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [118] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [119/350] - Time: 12.67s
Train Loss: 0.0049 | Val Loss: 0.0222
Train Acc: 0.9990 | Val Acc: 0.9963
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.979    Count 8: 0.990    Count 9: 0.989    Count 10: 1.000  
Epoch [119] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [119] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [119] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [119] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [119] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [119] Batch [50/261] Loss: 0.0002 LR: 0.000100
Epoch [119] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [119] Batch [70/261] Loss: 0.0012 LR: 0.000100
Epoch [119] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [119] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [119] Batch [100/261] Loss: 0.0021 LR: 0.000100
Epoch [119] Batch [110/261] Loss: 0.0002 LR: 0.000100
Epoch [119] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [119] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [119] Batch [140/261] Loss: 0.0003 LR: 0.000100
Epoch [119] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [119] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [119] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [119] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [119] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [119] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [119] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [119] Batch [220/261] Loss: 0.0002 LR: 0.000100
Epoch [119] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [119] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [119] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [119] Batch [260/261] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [120/350] - Time: 12.72s
Train Loss: 0.0153 | Val Loss: 0.0049
Train Acc: 0.9971 | Val Acc: 0.9981
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.990    Count 8: 1.000    Count 9: 0.989    Count 10: 1.000  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/single_image_checkpoint_epoch_119.pth
Epoch [120] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [120] Batch [10/261] Loss: 0.0050 LR: 0.000100
Epoch [120] Batch [20/261] Loss: 0.0001 LR: 0.000100
Epoch [120] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [120] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [120] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [120] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [120] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [120] Batch [80/261] Loss: 0.0114 LR: 0.000100
Epoch [120] Batch [90/261] Loss: 0.0002 LR: 0.000100
Epoch [120] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [120] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [120] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [120] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [120] Batch [140/261] Loss: 0.0041 LR: 0.000100
Epoch [120] Batch [150/261] Loss: 0.0002 LR: 0.000100
Epoch [120] Batch [160/261] Loss: 0.1993 LR: 0.000100
Epoch [120] Batch [170/261] Loss: 0.0006 LR: 0.000100
Epoch [120] Batch [180/261] Loss: 0.0001 LR: 0.000100
Epoch [120] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [120] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [120] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [120] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [120] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [120] Batch [240/261] Loss: 0.0004 LR: 0.000100
Epoch [120] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [120] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [121/350] - Time: 12.93s
Train Loss: 0.0381 | Val Loss: 0.0118
Train Acc: 0.9928 | Val Acc: 0.9963
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 1.000    Count 8: 0.980    Count 9: 0.989    Count 10: 1.000  
Epoch [121] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [121] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [121] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [121] Batch [30/261] Loss: 0.0001 LR: 0.000100
Epoch [121] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [121] Batch [50/261] Loss: 0.0001 LR: 0.000100
Epoch [121] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [121] Batch [70/261] Loss: 0.0006 LR: 0.000100
Epoch [121] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [121] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [121] Batch [100/261] Loss: 0.0499 LR: 0.000100
Epoch [121] Batch [110/261] Loss: 0.1818 LR: 0.000100
Epoch [121] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [121] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [121] Batch [140/261] Loss: 0.0006 LR: 0.000100
Epoch [121] Batch [150/261] Loss: 2.3596 LR: 0.000100
Epoch [121] Batch [160/261] Loss: 0.0001 LR: 0.000100
Epoch [121] Batch [170/261] Loss: 0.3386 LR: 0.000100
Epoch [121] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [121] Batch [190/261] Loss: 0.0007 LR: 0.000100
Epoch [121] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [121] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [121] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [121] Batch [230/261] Loss: 0.0001 LR: 0.000100
Epoch [121] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [121] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [121] Batch [260/261] Loss: 0.1324 LR: 0.000100

Epoch [122/350] - Time: 12.97s
Train Loss: 0.0434 | Val Loss: 1.4383
Train Acc: 0.9906 | Val Acc: 0.8222
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.943    Count 5: 0.951  
  Count 6: 0.857    Count 7: 0.406    Count 8: 0.424    Count 9: 0.422    Count 10: 1.000  
Epoch [122] Batch [0/261] Loss: 0.7860 LR: 0.000100
Epoch [122] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [122] Batch [20/261] Loss: 0.0003 LR: 0.000100
Epoch [122] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [122] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [122] Batch [50/261] Loss: 0.0003 LR: 0.000100
Epoch [122] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [122] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [122] Batch [80/261] Loss: 0.0006 LR: 0.000100
Epoch [122] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [122] Batch [100/261] Loss: 0.0001 LR: 0.000100
Epoch [122] Batch [110/261] Loss: 0.0009 LR: 0.000100
Epoch [122] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [122] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [122] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [122] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [122] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [122] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [122] Batch [180/261] Loss: 0.0227 LR: 0.000100
Epoch [122] Batch [190/261] Loss: 0.0001 LR: 0.000100
Epoch [122] Batch [200/261] Loss: 1.0263 LR: 0.000100
Epoch [122] Batch [210/261] Loss: 0.0004 LR: 0.000100
Epoch [122] Batch [220/261] Loss: 0.0001 LR: 0.000100
Epoch [122] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [122] Batch [240/261] Loss: 0.9768 LR: 0.000100
Epoch [122] Batch [250/261] Loss: 0.0001 LR: 0.000100
Epoch [122] Batch [260/261] Loss: 0.0402 LR: 0.000100

Epoch [123/350] - Time: 12.74s
Train Loss: 0.0525 | Val Loss: 0.0183
Train Acc: 0.9911 | Val Acc: 0.9944
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 1.000    Count 9: 0.978    Count 10: 0.955  
Epoch [123] Batch [0/261] Loss: 0.0064 LR: 0.000100
Epoch [123] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [123] Batch [20/261] Loss: 0.0001 LR: 0.000100
Epoch [123] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [123] Batch [40/261] Loss: 0.0014 LR: 0.000100
Epoch [123] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [123] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [123] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [123] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [123] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [123] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [123] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [123] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [123] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [123] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [123] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [123] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [123] Batch [170/261] Loss: 0.0001 LR: 0.000100
Epoch [123] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [123] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [123] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [123] Batch [210/261] Loss: 0.0062 LR: 0.000100
Epoch [123] Batch [220/261] Loss: 0.2127 LR: 0.000100
Epoch [123] Batch [230/261] Loss: 0.0219 LR: 0.000100
Epoch [123] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [123] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [123] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [124/350] - Time: 13.10s
Train Loss: 0.0133 | Val Loss: 0.1428
Train Acc: 0.9964 | Val Acc: 0.9795
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 1.000    Count 8: 0.919    Count 9: 0.856    Count 10: 1.000  
Epoch [124] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [124] Batch [10/261] Loss: 0.0001 LR: 0.000100
Epoch [124] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [124] Batch [30/261] Loss: 0.0065 LR: 0.000100
Epoch [124] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [124] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [124] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [124] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [124] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [124] Batch [90/261] Loss: 0.0002 LR: 0.000100
Epoch [124] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [124] Batch [110/261] Loss: 0.0018 LR: 0.000100
Epoch [124] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [124] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [124] Batch [140/261] Loss: 0.1734 LR: 0.000100
Epoch [124] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [124] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [124] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [124] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [124] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [124] Batch [200/261] Loss: 0.0001 LR: 0.000100
Epoch [124] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [124] Batch [220/261] Loss: 0.0002 LR: 0.000100
Epoch [124] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [124] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [124] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [124] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [125/350] - Time: 12.88s
Train Loss: 0.0358 | Val Loss: 0.0320
Train Acc: 0.9916 | Val Acc: 0.9907
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.990    Count 8: 0.980    Count 9: 0.989    Count 10: 0.943  
Epoch [125] Batch [0/261] Loss: 0.0002 LR: 0.000100
Epoch [125] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [125] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [125] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [125] Batch [40/261] Loss: 0.0001 LR: 0.000100
Epoch [125] Batch [50/261] Loss: 0.0061 LR: 0.000100
Epoch [125] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [125] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [125] Batch [80/261] Loss: 0.3736 LR: 0.000100
Epoch [125] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [125] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [125] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [125] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [125] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [125] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [125] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [125] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [125] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [125] Batch [180/261] Loss: 0.0871 LR: 0.000100
Epoch [125] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [125] Batch [200/261] Loss: 0.0009 LR: 0.000100
Epoch [125] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [125] Batch [220/261] Loss: 0.0002 LR: 0.000100
Epoch [125] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [125] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [125] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [125] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [126/350] - Time: 12.74s
Train Loss: 0.0282 | Val Loss: 0.0296
Train Acc: 0.9947 | Val Acc: 0.9944
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.990    Count 8: 1.000    Count 9: 0.967    Count 10: 0.989  
Epoch [126] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [126] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [126] Batch [20/261] Loss: 0.0003 LR: 0.000100
Epoch [126] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [126] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [126] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [126] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [126] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [126] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [126] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [126] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [126] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [126] Batch [120/261] Loss: 0.0030 LR: 0.000100
Epoch [126] Batch [130/261] Loss: 0.0003 LR: 0.000100
Epoch [126] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [126] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [126] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [126] Batch [170/261] Loss: 0.0001 LR: 0.000100
Epoch [126] Batch [180/261] Loss: 0.0081 LR: 0.000100
Epoch [126] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [126] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [126] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [126] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [126] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [126] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [126] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [126] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [127/350] - Time: 12.65s
Train Loss: 0.0177 | Val Loss: 0.0249
Train Acc: 0.9964 | Val Acc: 0.9944
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 0.990    Count 8: 0.980    Count 9: 0.989    Count 10: 0.989  
Epoch [127] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [127] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [127] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [127] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [127] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [127] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [127] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [127] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [127] Batch [80/261] Loss: 0.0165 LR: 0.000100
Epoch [127] Batch [90/261] Loss: 0.3575 LR: 0.000100
Epoch [127] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [127] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [127] Batch [120/261] Loss: 0.0028 LR: 0.000100
Epoch [127] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [127] Batch [140/261] Loss: 0.0594 LR: 0.000100
Epoch [127] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [127] Batch [160/261] Loss: 1.4139 LR: 0.000100
Epoch [127] Batch [170/261] Loss: 0.1099 LR: 0.000100
Epoch [127] Batch [180/261] Loss: 0.0001 LR: 0.000100
Epoch [127] Batch [190/261] Loss: 0.0012 LR: 0.000100
Epoch [127] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [127] Batch [210/261] Loss: 0.0001 LR: 0.000100
Epoch [127] Batch [220/261] Loss: 0.0003 LR: 0.000100
Epoch [127] Batch [230/261] Loss: 0.0122 LR: 0.000100
Epoch [127] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [127] Batch [250/261] Loss: 0.0099 LR: 0.000100
Epoch [127] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [128/350] - Time: 12.62s
Train Loss: 0.1294 | Val Loss: 0.0378
Train Acc: 0.9781 | Val Acc: 0.9926
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.990    Count 8: 1.000    Count 9: 0.922    Count 10: 1.000  
Epoch [128] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [128] Batch [10/261] Loss: 1.0416 LR: 0.000100
Epoch [128] Batch [20/261] Loss: 0.0025 LR: 0.000100
Epoch [128] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [128] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [128] Batch [50/261] Loss: 0.1357 LR: 0.000100
Epoch [128] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [128] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [128] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [128] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [128] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [128] Batch [110/261] Loss: 0.0002 LR: 0.000100
Epoch [128] Batch [120/261] Loss: 0.0001 LR: 0.000100
Epoch [128] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [128] Batch [140/261] Loss: 0.0007 LR: 0.000100
Epoch [128] Batch [150/261] Loss: 0.0001 LR: 0.000100
Epoch [128] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [128] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [128] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [128] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [128] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [128] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [128] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [128] Batch [230/261] Loss: 0.0117 LR: 0.000100
Epoch [128] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [128] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [128] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [129/350] - Time: 12.96s
Train Loss: 0.0062 | Val Loss: 0.0899
Train Acc: 0.9983 | Val Acc: 0.9832
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 0.990    Count 8: 0.980    Count 9: 1.000    Count 10: 0.852  
Epoch [129] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [129] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [129] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [129] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [129] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [129] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [129] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [129] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [129] Batch [80/261] Loss: 0.0126 LR: 0.000100
Epoch [129] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [129] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [129] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [129] Batch [120/261] Loss: 0.0009 LR: 0.000100
Epoch [129] Batch [130/261] Loss: 0.1341 LR: 0.000100
Epoch [129] Batch [140/261] Loss: 0.0001 LR: 0.000100
Epoch [129] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [129] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [129] Batch [170/261] Loss: 0.7248 LR: 0.000100
Epoch [129] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [129] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [129] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [129] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [129] Batch [220/261] Loss: 0.0002 LR: 0.000100
Epoch [129] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [129] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [129] Batch [250/261] Loss: 0.0001 LR: 0.000100
Epoch [129] Batch [260/261] Loss: 0.0657 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [130/350] - Time: 12.54s
Train Loss: 0.0264 | Val Loss: 0.3749
Train Acc: 0.9942 | Val Acc: 0.9479
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.961  
  Count 6: 0.969    Count 7: 0.896    Count 8: 0.778    Count 9: 0.811    Count 10: 1.000  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/single_image_checkpoint_epoch_129.pth
Epoch [130] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [130] Batch [10/261] Loss: 0.0339 LR: 0.000100
Epoch [130] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [130] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [130] Batch [40/261] Loss: 0.0007 LR: 0.000100
Epoch [130] Batch [50/261] Loss: 0.0001 LR: 0.000100
Epoch [130] Batch [60/261] Loss: 0.0001 LR: 0.000100
Epoch [130] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [130] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [130] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [130] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [130] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [130] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [130] Batch [130/261] Loss: 0.0003 LR: 0.000100
Epoch [130] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [130] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [130] Batch [160/261] Loss: 0.0002 LR: 0.000100
Epoch [130] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [130] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [130] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [130] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [130] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [130] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [130] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [130] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [130] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [130] Batch [260/261] Loss: 0.0004 LR: 0.000100

Epoch [131/350] - Time: 12.71s
Train Loss: 0.0221 | Val Loss: 0.0102
Train Acc: 0.9954 | Val Acc: 0.9981
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.990    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.990    Count 8: 1.000    Count 9: 1.000    Count 10: 1.000  
Epoch [131] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [131] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [131] Batch [20/261] Loss: 0.0004 LR: 0.000100
Epoch [131] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [131] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [131] Batch [50/261] Loss: 0.0101 LR: 0.000100
Epoch [131] Batch [60/261] Loss: 0.1566 LR: 0.000100
Epoch [131] Batch [70/261] Loss: 0.0024 LR: 0.000100
Epoch [131] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [131] Batch [90/261] Loss: 0.0035 LR: 0.000100
Epoch [131] Batch [100/261] Loss: 0.0244 LR: 0.000100
Epoch [131] Batch [110/261] Loss: 0.0090 LR: 0.000100
Epoch [131] Batch [120/261] Loss: 0.1234 LR: 0.000100
Epoch [131] Batch [130/261] Loss: 0.0001 LR: 0.000100
Epoch [131] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [131] Batch [150/261] Loss: 0.2815 LR: 0.000100
Epoch [131] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [131] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [131] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [131] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [131] Batch [200/261] Loss: 0.0040 LR: 0.000100
Epoch [131] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [131] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [131] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [131] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [131] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [131] Batch [260/261] Loss: 13.6083 LR: 0.000100

Epoch [132/350] - Time: 13.01s
Train Loss: 0.0229 | Val Loss: 0.0187
Train Acc: 0.9945 | Val Acc: 0.9972
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.990    Count 8: 1.000    Count 9: 1.000    Count 10: 0.977  
Epoch [132] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [132] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [132] Batch [20/261] Loss: 0.7327 LR: 0.000100
Epoch [132] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [132] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [132] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [132] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [132] Batch [70/261] Loss: 0.0003 LR: 0.000100
Epoch [132] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [132] Batch [90/261] Loss: 0.3247 LR: 0.000100
Epoch [132] Batch [100/261] Loss: 0.3733 LR: 0.000100
Epoch [132] Batch [110/261] Loss: 0.0001 LR: 0.000100
Epoch [132] Batch [120/261] Loss: 0.0017 LR: 0.000100
Epoch [132] Batch [130/261] Loss: 0.0001 LR: 0.000100
Epoch [132] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [132] Batch [150/261] Loss: 0.0001 LR: 0.000100
Epoch [132] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [132] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [132] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [132] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [132] Batch [200/261] Loss: 0.0001 LR: 0.000100
Epoch [132] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [132] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [132] Batch [230/261] Loss: 0.0860 LR: 0.000100
Epoch [132] Batch [240/261] Loss: 0.0002 LR: 0.000100
Epoch [132] Batch [250/261] Loss: 0.0062 LR: 0.000100
Epoch [132] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [133/350] - Time: 12.75s
Train Loss: 0.0349 | Val Loss: 0.1450
Train Acc: 0.9901 | Val Acc: 0.9711
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.980    Count 7: 0.979    Count 8: 0.949    Count 9: 0.956    Count 10: 0.807  
Epoch [133] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [133] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [133] Batch [20/261] Loss: 0.0001 LR: 0.000100
Epoch [133] Batch [30/261] Loss: 0.0113 LR: 0.000100
Epoch [133] Batch [40/261] Loss: 0.0066 LR: 0.000100
Epoch [133] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [133] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [133] Batch [70/261] Loss: 0.1433 LR: 0.000100
Epoch [133] Batch [80/261] Loss: 0.0712 LR: 0.000100
Epoch [133] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [133] Batch [100/261] Loss: 0.1053 LR: 0.000100
Epoch [133] Batch [110/261] Loss: 0.0001 LR: 0.000100
Epoch [133] Batch [120/261] Loss: 0.0006 LR: 0.000100
Epoch [133] Batch [130/261] Loss: 0.0006 LR: 0.000100
Epoch [133] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [133] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [133] Batch [160/261] Loss: 0.0001 LR: 0.000100
Epoch [133] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [133] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [133] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [133] Batch [200/261] Loss: 0.0001 LR: 0.000100
Epoch [133] Batch [210/261] Loss: 0.0728 LR: 0.000100
Epoch [133] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [133] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [133] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [133] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [133] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [134/350] - Time: 13.02s
Train Loss: 0.0428 | Val Loss: 0.0228
Train Acc: 0.9913 | Val Acc: 0.9953
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.979    Count 8: 1.000    Count 9: 0.989    Count 10: 0.989  
Epoch [134] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [134] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [134] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [134] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [134] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [134] Batch [50/261] Loss: 0.0011 LR: 0.000100
Epoch [134] Batch [60/261] Loss: 0.0003 LR: 0.000100
Epoch [134] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [134] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [134] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [134] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [134] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [134] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [134] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [134] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [134] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [134] Batch [160/261] Loss: 0.0002 LR: 0.000100
Epoch [134] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [134] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [134] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [134] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [134] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [134] Batch [220/261] Loss: 0.0056 LR: 0.000100
Epoch [134] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [134] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [134] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [134] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [135/350] - Time: 13.00s
Train Loss: 0.0095 | Val Loss: 0.0210
Train Acc: 0.9983 | Val Acc: 0.9944
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 1.000    Count 8: 0.980    Count 9: 1.000    Count 10: 0.966  
Epoch [135] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [135] Batch [10/261] Loss: 0.0001 LR: 0.000100
Epoch [135] Batch [20/261] Loss: 0.0004 LR: 0.000100
Epoch [135] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [135] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [135] Batch [50/261] Loss: 0.7851 LR: 0.000100
Epoch [135] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [135] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [135] Batch [80/261] Loss: 0.0002 LR: 0.000100
Epoch [135] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [135] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [135] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [135] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [135] Batch [130/261] Loss: 0.0221 LR: 0.000100
Epoch [135] Batch [140/261] Loss: 0.0001 LR: 0.000100
Epoch [135] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [135] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [135] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [135] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [135] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [135] Batch [200/261] Loss: 0.0001 LR: 0.000100
Epoch [135] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [135] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [135] Batch [230/261] Loss: 0.0002 LR: 0.000100
Epoch [135] Batch [240/261] Loss: 0.0001 LR: 0.000100
Epoch [135] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [135] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [136/350] - Time: 12.88s
Train Loss: 0.0182 | Val Loss: 0.1853
Train Acc: 0.9962 | Val Acc: 0.9683
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 0.979    Count 8: 0.980    Count 9: 0.967    Count 10: 0.705  
Epoch [136] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [136] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [136] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [136] Batch [30/261] Loss: 0.0001 LR: 0.000100
Epoch [136] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [136] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [136] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [136] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [136] Batch [80/261] Loss: 0.0003 LR: 0.000100
Epoch [136] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [136] Batch [100/261] Loss: 0.0004 LR: 0.000100
Epoch [136] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [136] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [136] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [136] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [136] Batch [150/261] Loss: 0.3596 LR: 0.000100
Epoch [136] Batch [160/261] Loss: 0.6970 LR: 0.000100
Epoch [136] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [136] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [136] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [136] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [136] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [136] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [136] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [136] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [136] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [136] Batch [260/261] Loss: 0.1878 LR: 0.000100

Epoch [137/350] - Time: 13.04s
Train Loss: 0.0468 | Val Loss: 0.0083
Train Acc: 0.9911 | Val Acc: 0.9991
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 0.990    Count 9: 1.000    Count 10: 1.000  
Epoch [137] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [137] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [137] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [137] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [137] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [137] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [137] Batch [60/261] Loss: 0.0002 LR: 0.000100
Epoch [137] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [137] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [137] Batch [90/261] Loss: 0.0050 LR: 0.000100
Epoch [137] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [137] Batch [110/261] Loss: 1.6143 LR: 0.000100
Epoch [137] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [137] Batch [130/261] Loss: 0.0002 LR: 0.000100
Epoch [137] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [137] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [137] Batch [160/261] Loss: 0.0002 LR: 0.000100
Epoch [137] Batch [170/261] Loss: 0.1235 LR: 0.000100
Epoch [137] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [137] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [137] Batch [200/261] Loss: 0.0013 LR: 0.000100
Epoch [137] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [137] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [137] Batch [230/261] Loss: 0.0001 LR: 0.000100
Epoch [137] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [137] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [137] Batch [260/261] Loss: 0.3254 LR: 0.000100

Epoch [138/350] - Time: 12.93s
Train Loss: 0.0211 | Val Loss: 0.0783
Train Acc: 0.9957 | Val Acc: 0.9851
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.990    Count 5: 1.000  
  Count 6: 0.990    Count 7: 0.979    Count 8: 0.970    Count 9: 0.956    Count 10: 0.955  
Epoch [138] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [138] Batch [10/261] Loss: 0.0001 LR: 0.000100
Epoch [138] Batch [20/261] Loss: 0.0003 LR: 0.000100
Epoch [138] Batch [30/261] Loss: 0.1453 LR: 0.000100
Epoch [138] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [138] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [138] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [138] Batch [70/261] Loss: 0.0002 LR: 0.000100
Epoch [138] Batch [80/261] Loss: 0.0002 LR: 0.000100
Epoch [138] Batch [90/261] Loss: 0.0015 LR: 0.000100
Epoch [138] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [138] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [138] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [138] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [138] Batch [140/261] Loss: 0.0280 LR: 0.000100
Epoch [138] Batch [150/261] Loss: 0.0431 LR: 0.000100
Epoch [138] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [138] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [138] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [138] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [138] Batch [200/261] Loss: 0.1600 LR: 0.000100
Epoch [138] Batch [210/261] Loss: 0.0001 LR: 0.000100
Epoch [138] Batch [220/261] Loss: 0.0011 LR: 0.000100
Epoch [138] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [138] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [138] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [138] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [139/350] - Time: 12.74s
Train Loss: 0.0299 | Val Loss: 0.0431
Train Acc: 0.9928 | Val Acc: 0.9926
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 1.000    Count 8: 0.990    Count 9: 0.956    Count 10: 0.989  
Epoch [139] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [139] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [139] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [139] Batch [30/261] Loss: 0.0001 LR: 0.000100
Epoch [139] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [139] Batch [50/261] Loss: 0.0001 LR: 0.000100
Epoch [139] Batch [60/261] Loss: 0.0001 LR: 0.000100
Epoch [139] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [139] Batch [80/261] Loss: 0.0092 LR: 0.000100
Epoch [139] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [139] Batch [100/261] Loss: 0.0002 LR: 0.000100
Epoch [139] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [139] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [139] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [139] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [139] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [139] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [139] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [139] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [139] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [139] Batch [200/261] Loss: 0.0027 LR: 0.000100
Epoch [139] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [139] Batch [220/261] Loss: 0.0001 LR: 0.000100
Epoch [139] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [139] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [139] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [139] Batch [260/261] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [140/350] - Time: 12.81s
Train Loss: 0.0138 | Val Loss: 0.0043
Train Acc: 0.9971 | Val Acc: 0.9981
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 1.000    Count 9: 1.000    Count 10: 0.989  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/single_image_checkpoint_epoch_139.pth
Epoch [140] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [140] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [140] Batch [20/261] Loss: 0.2708 LR: 0.000100
Epoch [140] Batch [30/261] Loss: 0.1547 LR: 0.000100
Epoch [140] Batch [40/261] Loss: 0.1086 LR: 0.000100
Epoch [140] Batch [50/261] Loss: 0.0840 LR: 0.000100
Epoch [140] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [140] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [140] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [140] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [140] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [140] Batch [110/261] Loss: 0.0001 LR: 0.000100
Epoch [140] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [140] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [140] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [140] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [140] Batch [160/261] Loss: 0.0222 LR: 0.000100
Epoch [140] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [140] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [140] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [140] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [140] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [140] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [140] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [140] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [140] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [140] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [141/350] - Time: 12.95s
Train Loss: 0.0283 | Val Loss: 0.0381
Train Acc: 0.9928 | Val Acc: 0.9879
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 0.990    Count 8: 1.000    Count 9: 0.978    Count 10: 0.898  
Epoch [141] Batch [0/261] Loss: 0.0001 LR: 0.000100
Epoch [141] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [141] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [141] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [141] Batch [40/261] Loss: 0.0003 LR: 0.000100
Epoch [141] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [141] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [141] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [141] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [141] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [141] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [141] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [141] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [141] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [141] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [141] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [141] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [141] Batch [170/261] Loss: 0.0003 LR: 0.000100
Epoch [141] Batch [180/261] Loss: 0.0468 LR: 0.000100
Epoch [141] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [141] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [141] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [141] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [141] Batch [230/261] Loss: 0.2919 LR: 0.000100
Epoch [141] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [141] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [141] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [142/350] - Time: 12.81s
Train Loss: 0.0277 | Val Loss: 0.0426
Train Acc: 0.9938 | Val Acc: 0.9888
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 1.000    Count 8: 1.000    Count 9: 0.967    Count 10: 0.909  
Epoch [142] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [142] Batch [10/261] Loss: 0.0015 LR: 0.000100
Epoch [142] Batch [20/261] Loss: 0.0054 LR: 0.000100
Epoch [142] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [142] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [142] Batch [50/261] Loss: 0.0001 LR: 0.000100
Epoch [142] Batch [60/261] Loss: 0.0100 LR: 0.000100
Epoch [142] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [142] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [142] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [142] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [142] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [142] Batch [120/261] Loss: 0.0001 LR: 0.000100
Epoch [142] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [142] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [142] Batch [150/261] Loss: 0.0001 LR: 0.000100
Epoch [142] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [142] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [142] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [142] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [142] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [142] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [142] Batch [220/261] Loss: 0.0126 LR: 0.000100
Epoch [142] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [142] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [142] Batch [250/261] Loss: 0.0003 LR: 0.000100
Epoch [142] Batch [260/261] Loss: 13.0446 LR: 0.000100

Epoch [143/350] - Time: 12.85s
Train Loss: 0.0152 | Val Loss: 0.0113
Train Acc: 0.9964 | Val Acc: 0.9963
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.990    Count 5: 1.000  
  Count 6: 0.990    Count 7: 1.000    Count 8: 1.000    Count 9: 0.989    Count 10: 1.000  
Epoch [143] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [143] Batch [10/261] Loss: 0.0013 LR: 0.000100
Epoch [143] Batch [20/261] Loss: 0.0018 LR: 0.000100
Epoch [143] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [143] Batch [40/261] Loss: 0.0123 LR: 0.000100
Epoch [143] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [143] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [143] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [143] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [143] Batch [90/261] Loss: 0.0001 LR: 0.000100
Epoch [143] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [143] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [143] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [143] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [143] Batch [140/261] Loss: 0.0012 LR: 0.000100
Epoch [143] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [143] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [143] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [143] Batch [180/261] Loss: 0.0021 LR: 0.000100
Epoch [143] Batch [190/261] Loss: 0.1855 LR: 0.000100
Epoch [143] Batch [200/261] Loss: 0.0001 LR: 0.000100
Epoch [143] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [143] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [143] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [143] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [143] Batch [250/261] Loss: 0.0001 LR: 0.000100
Epoch [143] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [144/350] - Time: 12.84s
Train Loss: 0.0252 | Val Loss: 0.0548
Train Acc: 0.9954 | Val Acc: 0.9898
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 0.979    Count 8: 0.990    Count 9: 0.978    Count 10: 0.955  
Epoch [144] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [144] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [144] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [144] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [144] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [144] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [144] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [144] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [144] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [144] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [144] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [144] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [144] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [144] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [144] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [144] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [144] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [144] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [144] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [144] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [144] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [144] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [144] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [144] Batch [230/261] Loss: 0.0001 LR: 0.000100
Epoch [144] Batch [240/261] Loss: 0.0026 LR: 0.000100
Epoch [144] Batch [250/261] Loss: 0.0010 LR: 0.000100
Epoch [144] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [145/350] - Time: 12.63s
Train Loss: 0.0095 | Val Loss: 0.0681
Train Acc: 0.9981 | Val Acc: 0.9879
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.980    Count 7: 1.000    Count 8: 1.000    Count 9: 0.900    Count 10: 0.989  
Epoch [145] Batch [0/261] Loss: 0.0073 LR: 0.000100
Epoch [145] Batch [10/261] Loss: 0.0001 LR: 0.000100
Epoch [145] Batch [20/261] Loss: 0.0055 LR: 0.000100
Epoch [145] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [145] Batch [40/261] Loss: 0.0001 LR: 0.000100
Epoch [145] Batch [50/261] Loss: 0.0003 LR: 0.000100
Epoch [145] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [145] Batch [70/261] Loss: 0.3710 LR: 0.000100
Epoch [145] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [145] Batch [90/261] Loss: 0.0391 LR: 0.000100
Epoch [145] Batch [100/261] Loss: 0.4011 LR: 0.000100
Epoch [145] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [145] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [145] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [145] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [145] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [145] Batch [160/261] Loss: 0.0008 LR: 0.000100
Epoch [145] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [145] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [145] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [145] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [145] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [145] Batch [220/261] Loss: 0.0003 LR: 0.000100
Epoch [145] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [145] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [145] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [145] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [146/350] - Time: 12.70s
Train Loss: 0.0552 | Val Loss: 0.0495
Train Acc: 0.9899 | Val Acc: 0.9898
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.990    Count 5: 1.000  
  Count 6: 0.980    Count 7: 0.969    Count 8: 1.000    Count 9: 0.989    Count 10: 0.955  
Epoch [146] Batch [0/261] Loss: 0.0002 LR: 0.000100
Epoch [146] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [146] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [146] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [146] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [146] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [146] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [146] Batch [70/261] Loss: 0.0001 LR: 0.000100
Epoch [146] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [146] Batch [90/261] Loss: 0.0002 LR: 0.000100
Epoch [146] Batch [100/261] Loss: 0.0057 LR: 0.000100
Epoch [146] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [146] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [146] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [146] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [146] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [146] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [146] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [146] Batch [180/261] Loss: 0.1356 LR: 0.000100
Epoch [146] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [146] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [146] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [146] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [146] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [146] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [146] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [146] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [147/350] - Time: 13.07s
Train Loss: 0.0307 | Val Loss: 0.0305
Train Acc: 0.9935 | Val Acc: 0.9944
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.979    Count 8: 0.980    Count 9: 0.978    Count 10: 1.000  
Epoch [147] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [147] Batch [10/261] Loss: 0.0033 LR: 0.000100
Epoch [147] Batch [20/261] Loss: 0.0376 LR: 0.000100
Epoch [147] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [147] Batch [40/261] Loss: 0.0158 LR: 0.000100
Epoch [147] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [147] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [147] Batch [70/261] Loss: 0.2567 LR: 0.000100
Epoch [147] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [147] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [147] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [147] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [147] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [147] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [147] Batch [140/261] Loss: 0.0014 LR: 0.000100
Epoch [147] Batch [150/261] Loss: 0.0024 LR: 0.000100
Epoch [147] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [147] Batch [170/261] Loss: 0.0026 LR: 0.000100
Epoch [147] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [147] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [147] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [147] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [147] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [147] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [147] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [147] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [147] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [148/350] - Time: 13.13s
Train Loss: 0.0190 | Val Loss: 0.0348
Train Acc: 0.9959 | Val Acc: 0.9926
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 0.980    Count 9: 0.933    Count 10: 1.000  
Epoch [148] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [148] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [148] Batch [20/261] Loss: 0.1642 LR: 0.000100
Epoch [148] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [148] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [148] Batch [50/261] Loss: 0.0004 LR: 0.000100
Epoch [148] Batch [60/261] Loss: 0.0446 LR: 0.000100
Epoch [148] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [148] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [148] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [148] Batch [100/261] Loss: 0.0001 LR: 0.000100
Epoch [148] Batch [110/261] Loss: 0.0031 LR: 0.000100
Epoch [148] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [148] Batch [130/261] Loss: 0.0023 LR: 0.000100
Epoch [148] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [148] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [148] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [148] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [148] Batch [180/261] Loss: 0.0131 LR: 0.000100
Epoch [148] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [148] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [148] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [148] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [148] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [148] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [148] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [148] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [149/350] - Time: 12.97s
Train Loss: 0.0166 | Val Loss: 0.1126
Train Acc: 0.9974 | Val Acc: 0.9730
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 0.939    Count 9: 0.989    Count 10: 0.761  
Epoch [149] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [149] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [149] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [149] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [149] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [149] Batch [50/261] Loss: 0.7427 LR: 0.000100
Epoch [149] Batch [60/261] Loss: 0.0004 LR: 0.000100
Epoch [149] Batch [70/261] Loss: 0.0013 LR: 0.000100
Epoch [149] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [149] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [149] Batch [100/261] Loss: 0.0001 LR: 0.000100
Epoch [149] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [149] Batch [120/261] Loss: 0.0072 LR: 0.000100
Epoch [149] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [149] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [149] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [149] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [149] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [149] Batch [180/261] Loss: 0.0001 LR: 0.000100
Epoch [149] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [149] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [149] Batch [210/261] Loss: 0.0002 LR: 0.000100
Epoch [149] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [149] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [149] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [149] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [149] Batch [260/261] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [150/350] - Time: 12.60s
Train Loss: 0.0084 | Val Loss: 0.0612
Train Acc: 0.9983 | Val Acc: 0.9832
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.979    Count 8: 0.980    Count 9: 0.989    Count 10: 0.852  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/single_image_checkpoint_epoch_149.pth
Epoch [150] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [150] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [150] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [150] Batch [30/261] Loss: 0.0001 LR: 0.000100
Epoch [150] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [150] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [150] Batch [60/261] Loss: 0.0062 LR: 0.000100
Epoch [150] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [150] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [150] Batch [90/261] Loss: 0.0008 LR: 0.000100
Epoch [150] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [150] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [150] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [150] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [150] Batch [140/261] Loss: 0.1535 LR: 0.000100
Epoch [150] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [150] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [150] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [150] Batch [180/261] Loss: 0.4594 LR: 0.000100
Epoch [150] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [150] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [150] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [150] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [150] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [150] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [150] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [150] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [151/350] - Time: 13.17s
Train Loss: 0.0144 | Val Loss: 1.1201
Train Acc: 0.9969 | Val Acc: 0.8818
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.941  
  Count 6: 0.969    Count 7: 0.938    Count 8: 0.808    Count 9: 0.778    Count 10: 0.170  
Epoch [151] Batch [0/261] Loss: 0.2311 LR: 0.000100
Epoch [151] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [151] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [151] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [151] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [151] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [151] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [151] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [151] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [151] Batch [90/261] Loss: 0.0022 LR: 0.000100
Epoch [151] Batch [100/261] Loss: 0.2475 LR: 0.000100
Epoch [151] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [151] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [151] Batch [130/261] Loss: 0.1486 LR: 0.000100
Epoch [151] Batch [140/261] Loss: 0.0515 LR: 0.000100
Epoch [151] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [151] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [151] Batch [170/261] Loss: 0.3111 LR: 0.000100
Epoch [151] Batch [180/261] Loss: 0.1329 LR: 0.000100
Epoch [151] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [151] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [151] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [151] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [151] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [151] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [151] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [151] Batch [260/261] Loss: 35.5969 LR: 0.000100

Epoch [152/350] - Time: 12.96s
Train Loss: 0.0424 | Val Loss: 0.0268
Train Acc: 0.9916 | Val Acc: 0.9963
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 0.990    Count 8: 0.980    Count 9: 1.000    Count 10: 1.000  
Epoch [152] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [152] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [152] Batch [20/261] Loss: 0.1073 LR: 0.000100
Epoch [152] Batch [30/261] Loss: 0.0039 LR: 0.000100
Epoch [152] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [152] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [152] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [152] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [152] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [152] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [152] Batch [100/261] Loss: 0.0001 LR: 0.000100
Epoch [152] Batch [110/261] Loss: 0.0041 LR: 0.000100
Epoch [152] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [152] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [152] Batch [140/261] Loss: 0.3516 LR: 0.000100
Epoch [152] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [152] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [152] Batch [170/261] Loss: 0.0002 LR: 0.000100
Epoch [152] Batch [180/261] Loss: 0.0004 LR: 0.000100
Epoch [152] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [152] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [152] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [152] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [152] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [152] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [152] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [152] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [153/350] - Time: 12.87s
Train Loss: 0.0202 | Val Loss: 0.0643
Train Acc: 0.9947 | Val Acc: 0.9888
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 1.000    Count 9: 0.978    Count 10: 0.886  
Epoch [153] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [153] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [153] Batch [20/261] Loss: 0.0040 LR: 0.000100
Epoch [153] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [153] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [153] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [153] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [153] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [153] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [153] Batch [90/261] Loss: 0.0009 LR: 0.000100
Epoch [153] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [153] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [153] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [153] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [153] Batch [140/261] Loss: 0.5890 LR: 0.000100
Epoch [153] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [153] Batch [160/261] Loss: 0.0005 LR: 0.000100
Epoch [153] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [153] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [153] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [153] Batch [200/261] Loss: 0.0001 LR: 0.000100
Epoch [153] Batch [210/261] Loss: 0.0561 LR: 0.000100
Epoch [153] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [153] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [153] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [153] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [153] Batch [260/261] Loss: 0.0001 LR: 0.000100

Epoch [154/350] - Time: 12.62s
Train Loss: 0.0361 | Val Loss: 0.8309
Train Acc: 0.9933 | Val Acc: 0.8966
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.990    Count 5: 1.000  
  Count 6: 0.980    Count 7: 0.958    Count 8: 0.747    Count 9: 0.811    Count 10: 0.295  
Epoch [154] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [154] Batch [10/261] Loss: 0.0002 LR: 0.000100
Epoch [154] Batch [20/261] Loss: 0.0159 LR: 0.000100
Epoch [154] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [154] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [154] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [154] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [154] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [154] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [154] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [154] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [154] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [154] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [154] Batch [130/261] Loss: 0.0004 LR: 0.000100
Epoch [154] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [154] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [154] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [154] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [154] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [154] Batch [190/261] Loss: 0.0004 LR: 0.000100
Epoch [154] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [154] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [154] Batch [220/261] Loss: 0.0013 LR: 0.000100
Epoch [154] Batch [230/261] Loss: 0.0001 LR: 0.000100
Epoch [154] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [154] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [154] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [155/350] - Time: 12.95s
Train Loss: 0.0157 | Val Loss: 0.0669
Train Acc: 0.9962 | Val Acc: 0.9879
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 0.958    Count 8: 0.960    Count 9: 0.978    Count 10: 0.989  
Epoch [155] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [155] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [155] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [155] Batch [30/261] Loss: 0.0015 LR: 0.000100
Epoch [155] Batch [40/261] Loss: 0.0022 LR: 0.000100
Epoch [155] Batch [50/261] Loss: 0.4943 LR: 0.000100
Epoch [155] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [155] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [155] Batch [80/261] Loss: 0.0115 LR: 0.000100
Epoch [155] Batch [90/261] Loss: 0.0001 LR: 0.000100
Epoch [155] Batch [100/261] Loss: 0.0079 LR: 0.000100
Epoch [155] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [155] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [155] Batch [130/261] Loss: 0.0012 LR: 0.000100
Epoch [155] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [155] Batch [150/261] Loss: 0.0001 LR: 0.000100
Epoch [155] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [155] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [155] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [155] Batch [190/261] Loss: 0.0001 LR: 0.000100
Epoch [155] Batch [200/261] Loss: 0.0094 LR: 0.000100
Epoch [155] Batch [210/261] Loss: 0.0004 LR: 0.000100
Epoch [155] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [155] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [155] Batch [240/261] Loss: 0.1782 LR: 0.000100
Epoch [155] Batch [250/261] Loss: 0.0002 LR: 0.000100
Epoch [155] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [156/350] - Time: 13.30s
Train Loss: 0.0336 | Val Loss: 0.0577
Train Acc: 0.9923 | Val Acc: 0.9926
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.990    Count 5: 1.000  
  Count 6: 0.980    Count 7: 1.000    Count 8: 1.000    Count 9: 0.956    Count 10: 1.000  
Epoch [156] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [156] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [156] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [156] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [156] Batch [40/261] Loss: 0.0017 LR: 0.000100
Epoch [156] Batch [50/261] Loss: 0.0005 LR: 0.000100
Epoch [156] Batch [60/261] Loss: 0.0107 LR: 0.000100
Epoch [156] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [156] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [156] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [156] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [156] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [156] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [156] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [156] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [156] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [156] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [156] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [156] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [156] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [156] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [156] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [156] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [156] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [156] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [156] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [156] Batch [260/261] Loss: 4.4237 LR: 0.000100

Epoch [157/350] - Time: 12.56s
Train Loss: 0.0091 | Val Loss: 0.0198
Train Acc: 0.9981 | Val Acc: 0.9981
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 1.000    Count 9: 0.989    Count 10: 1.000  
Epoch [157] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [157] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [157] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [157] Batch [30/261] Loss: 0.0005 LR: 0.000100
Epoch [157] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [157] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [157] Batch [60/261] Loss: 0.0142 LR: 0.000100
Epoch [157] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [157] Batch [80/261] Loss: 0.0008 LR: 0.000100
Epoch [157] Batch [90/261] Loss: 0.0003 LR: 0.000100
Epoch [157] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [157] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [157] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [157] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [157] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [157] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [157] Batch [160/261] Loss: 0.0002 LR: 0.000100
Epoch [157] Batch [170/261] Loss: 0.0007 LR: 0.000100
Epoch [157] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [157] Batch [190/261] Loss: 0.0001 LR: 0.000100
Epoch [157] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [157] Batch [210/261] Loss: 0.0002 LR: 0.000100
Epoch [157] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [157] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [157] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [157] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [157] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [158/350] - Time: 12.53s
Train Loss: 0.0304 | Val Loss: 0.0158
Train Acc: 0.9942 | Val Acc: 0.9963
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 0.990    Count 8: 1.000    Count 9: 0.989    Count 10: 0.989  
Epoch [158] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [158] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [158] Batch [20/261] Loss: 0.1823 LR: 0.000100
Epoch [158] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [158] Batch [40/261] Loss: 0.0468 LR: 0.000100
Epoch [158] Batch [50/261] Loss: 1.1261 LR: 0.000100
Epoch [158] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [158] Batch [70/261] Loss: 0.0006 LR: 0.000100
Epoch [158] Batch [80/261] Loss: 0.0001 LR: 0.000100
Epoch [158] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [158] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [158] Batch [110/261] Loss: 0.0001 LR: 0.000100
Epoch [158] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [158] Batch [130/261] Loss: 0.0079 LR: 0.000100
Epoch [158] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [158] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [158] Batch [160/261] Loss: 0.1437 LR: 0.000100
Epoch [158] Batch [170/261] Loss: 0.0003 LR: 0.000100
Epoch [158] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [158] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [158] Batch [200/261] Loss: 0.0002 LR: 0.000100
Epoch [158] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [158] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [158] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [158] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [158] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [158] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [159/350] - Time: 12.66s
Train Loss: 0.0285 | Val Loss: 0.0033
Train Acc: 0.9952 | Val Acc: 0.9991
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 1.000    Count 9: 1.000    Count 10: 0.989  
Epoch [159] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [159] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [159] Batch [20/261] Loss: 0.0001 LR: 0.000100
Epoch [159] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [159] Batch [40/261] Loss: 0.0002 LR: 0.000100
Epoch [159] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [159] Batch [60/261] Loss: 0.0001 LR: 0.000100
Epoch [159] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [159] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [159] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [159] Batch [100/261] Loss: 0.0009 LR: 0.000100
Epoch [159] Batch [110/261] Loss: 0.0049 LR: 0.000100
Epoch [159] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [159] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [159] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [159] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [159] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [159] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [159] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [159] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [159] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [159] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [159] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [159] Batch [230/261] Loss: 0.3302 LR: 0.000100
Epoch [159] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [159] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [159] Batch [260/261] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [160/350] - Time: 12.76s
Train Loss: 0.0144 | Val Loss: 0.0008
Train Acc: 0.9966 | Val Acc: 0.9991
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 1.000    Count 9: 1.000    Count 10: 0.989  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/single_image_checkpoint_epoch_159.pth
Epoch [160] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [160] Batch [10/261] Loss: 0.0001 LR: 0.000100
Epoch [160] Batch [20/261] Loss: 0.0032 LR: 0.000100
Epoch [160] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [160] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [160] Batch [50/261] Loss: 0.0001 LR: 0.000100
Epoch [160] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [160] Batch [70/261] Loss: 0.0001 LR: 0.000100
Epoch [160] Batch [80/261] Loss: 0.0001 LR: 0.000100
Epoch [160] Batch [90/261] Loss: 0.0002 LR: 0.000100
Epoch [160] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [160] Batch [110/261] Loss: 0.0002 LR: 0.000100
Epoch [160] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [160] Batch [130/261] Loss: 0.2901 LR: 0.000100
Epoch [160] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [160] Batch [150/261] Loss: 0.0009 LR: 0.000100
Epoch [160] Batch [160/261] Loss: 0.0003 LR: 0.000100
Epoch [160] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [160] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [160] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [160] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [160] Batch [210/261] Loss: 0.0003 LR: 0.000100
Epoch [160] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [160] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [160] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [160] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [160] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [161/350] - Time: 12.70s
Train Loss: 0.0189 | Val Loss: 0.0034
Train Acc: 0.9964 | Val Acc: 0.9981
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 1.000    Count 9: 0.978    Count 10: 1.000  
Epoch [161] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [161] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [161] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [161] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [161] Batch [40/261] Loss: 0.0001 LR: 0.000100
Epoch [161] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [161] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [161] Batch [70/261] Loss: 0.0002 LR: 0.000100
Epoch [161] Batch [80/261] Loss: 0.0773 LR: 0.000100
Epoch [161] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [161] Batch [100/261] Loss: 0.0812 LR: 0.000100
Epoch [161] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [161] Batch [120/261] Loss: 0.0010 LR: 0.000100
Epoch [161] Batch [130/261] Loss: 0.1015 LR: 0.000100
Epoch [161] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [161] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [161] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [161] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [161] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [161] Batch [190/261] Loss: 0.4455 LR: 0.000100
Epoch [161] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [161] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [161] Batch [220/261] Loss: 0.2676 LR: 0.000100
Epoch [161] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [161] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [161] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [161] Batch [260/261] Loss: 0.0001 LR: 0.000100

Epoch [162/350] - Time: 12.64s
Train Loss: 0.0368 | Val Loss: 0.0312
Train Acc: 0.9909 | Val Acc: 0.9944
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.990    Count 5: 1.000  
  Count 6: 0.990    Count 7: 0.979    Count 8: 0.990    Count 9: 0.989    Count 10: 1.000  
Epoch [162] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [162] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [162] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [162] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [162] Batch [40/261] Loss: 0.0006 LR: 0.000100
Epoch [162] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [162] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [162] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [162] Batch [80/261] Loss: 0.0001 LR: 0.000100
Epoch [162] Batch [90/261] Loss: 0.2608 LR: 0.000100
Epoch [162] Batch [100/261] Loss: 0.0002 LR: 0.000100
Epoch [162] Batch [110/261] Loss: 0.0020 LR: 0.000100
Epoch [162] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [162] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [162] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [162] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [162] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [162] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [162] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [162] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [162] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [162] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [162] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [162] Batch [230/261] Loss: 0.4546 LR: 0.000100
Epoch [162] Batch [240/261] Loss: 0.0003 LR: 0.000100
Epoch [162] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [162] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [163/350] - Time: 12.69s
Train Loss: 0.0271 | Val Loss: 0.1984
Train Acc: 0.9952 | Val Acc: 0.9730
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.981    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 0.979    Count 8: 0.889    Count 9: 0.856    Count 10: 1.000  
Epoch [163] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [163] Batch [10/261] Loss: 3.8002 LR: 0.000100
Epoch [163] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [163] Batch [30/261] Loss: 0.6436 LR: 0.000100
Epoch [163] Batch [40/261] Loss: 0.0001 LR: 0.000100
Epoch [163] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [163] Batch [60/261] Loss: 0.0008 LR: 0.000100
Epoch [163] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [163] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [163] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [163] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [163] Batch [110/261] Loss: 0.0001 LR: 0.000100
Epoch [163] Batch [120/261] Loss: 0.0001 LR: 0.000100
Epoch [163] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [163] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [163] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [163] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [163] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [163] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [163] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [163] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [163] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [163] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [163] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [163] Batch [240/261] Loss: 0.0970 LR: 0.000100
Epoch [163] Batch [250/261] Loss: 0.0002 LR: 0.000100
Epoch [163] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [164/350] - Time: 13.46s
Train Loss: 0.0397 | Val Loss: 0.0289
Train Acc: 0.9930 | Val Acc: 0.9944
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 0.990    Count 9: 0.956    Count 10: 1.000  
Epoch [164] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [164] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [164] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [164] Batch [30/261] Loss: 0.0027 LR: 0.000100
Epoch [164] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [164] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [164] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [164] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [164] Batch [80/261] Loss: 0.3515 LR: 0.000100
Epoch [164] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [164] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [164] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [164] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [164] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [164] Batch [140/261] Loss: 0.0001 LR: 0.000100
Epoch [164] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [164] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [164] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [164] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [164] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [164] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [164] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [164] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [164] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [164] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [164] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [164] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [165/350] - Time: 12.62s
Train Loss: 0.0031 | Val Loss: 0.0164
Train Acc: 0.9993 | Val Acc: 0.9972
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.990    Count 8: 1.000    Count 9: 1.000    Count 10: 0.989  
Epoch [165] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [165] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [165] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [165] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [165] Batch [40/261] Loss: 0.0032 LR: 0.000100
Epoch [165] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [165] Batch [60/261] Loss: 0.0026 LR: 0.000100
Epoch [165] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [165] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [165] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [165] Batch [100/261] Loss: 0.4726 LR: 0.000100
Epoch [165] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [165] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [165] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [165] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [165] Batch [150/261] Loss: 0.0001 LR: 0.000100
Epoch [165] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [165] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [165] Batch [180/261] Loss: 0.0002 LR: 0.000100
Epoch [165] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [165] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [165] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [165] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [165] Batch [230/261] Loss: 0.0017 LR: 0.000100
Epoch [165] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [165] Batch [250/261] Loss: 0.5905 LR: 0.000100
Epoch [165] Batch [260/261] Loss: 0.0035 LR: 0.000100

Epoch [166/350] - Time: 12.70s
Train Loss: 0.0230 | Val Loss: 0.0352
Train Acc: 0.9957 | Val Acc: 0.9944
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.980    Count 7: 0.979    Count 8: 1.000    Count 9: 0.989    Count 10: 1.000  
Epoch [166] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [166] Batch [10/261] Loss: 0.0055 LR: 0.000100
Epoch [166] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [166] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [166] Batch [40/261] Loss: 0.0083 LR: 0.000100
Epoch [166] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [166] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [166] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [166] Batch [80/261] Loss: 0.0007 LR: 0.000100
Epoch [166] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [166] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [166] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [166] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [166] Batch [130/261] Loss: 0.0002 LR: 0.000100
Epoch [166] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [166] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [166] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [166] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [166] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [166] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [166] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [166] Batch [210/261] Loss: 0.0008 LR: 0.000100
Epoch [166] Batch [220/261] Loss: 0.0038 LR: 0.000100
Epoch [166] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [166] Batch [240/261] Loss: 0.2923 LR: 0.000100
Epoch [166] Batch [250/261] Loss: 0.0004 LR: 0.000100
Epoch [166] Batch [260/261] Loss: 0.0004 LR: 0.000100

Epoch [167/350] - Time: 12.72s
Train Loss: 0.0307 | Val Loss: 0.3886
Train Acc: 0.9933 | Val Acc: 0.9469
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.990    Count 5: 1.000  
  Count 6: 0.980    Count 7: 0.979    Count 8: 1.000    Count 9: 0.944    Count 10: 0.466  
Epoch [167] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [167] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [167] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [167] Batch [30/261] Loss: 0.0013 LR: 0.000100
Epoch [167] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [167] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [167] Batch [60/261] Loss: 0.0039 LR: 0.000100
Epoch [167] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [167] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [167] Batch [90/261] Loss: 0.1157 LR: 0.000100
Epoch [167] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [167] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [167] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [167] Batch [130/261] Loss: 0.0001 LR: 0.000100
Epoch [167] Batch [140/261] Loss: 0.3365 LR: 0.000100
Epoch [167] Batch [150/261] Loss: 0.0016 LR: 0.000100
Epoch [167] Batch [160/261] Loss: 0.5199 LR: 0.000100
Epoch [167] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [167] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [167] Batch [190/261] Loss: 0.0002 LR: 0.000100
Epoch [167] Batch [200/261] Loss: 0.0003 LR: 0.000100
Epoch [167] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [167] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [167] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [167] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [167] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [167] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [168/350] - Time: 12.69s
Train Loss: 0.0685 | Val Loss: 0.0205
Train Acc: 0.9870 | Val Acc: 0.9944
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 1.000    Count 8: 1.000    Count 9: 0.989    Count 10: 0.966  
Epoch [168] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [168] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [168] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [168] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [168] Batch [40/261] Loss: 0.8713 LR: 0.000100
Epoch [168] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [168] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [168] Batch [70/261] Loss: 0.0002 LR: 0.000100
Epoch [168] Batch [80/261] Loss: 0.0092 LR: 0.000100
Epoch [168] Batch [90/261] Loss: 0.2574 LR: 0.000100
Epoch [168] Batch [100/261] Loss: 0.0006 LR: 0.000100
Epoch [168] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [168] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [168] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [168] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [168] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [168] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [168] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [168] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [168] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [168] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [168] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [168] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [168] Batch [230/261] Loss: 0.0007 LR: 0.000100
Epoch [168] Batch [240/261] Loss: 0.0004 LR: 0.000100
Epoch [168] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [168] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [169/350] - Time: 12.85s
Train Loss: 0.0147 | Val Loss: 0.0342
Train Acc: 0.9971 | Val Acc: 0.9926
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.980    Count 7: 1.000    Count 8: 0.970    Count 9: 0.978    Count 10: 1.000  
Epoch [169] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [169] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [169] Batch [20/261] Loss: 0.0001 LR: 0.000100
Epoch [169] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [169] Batch [40/261] Loss: 0.0060 LR: 0.000100
Epoch [169] Batch [50/261] Loss: 0.0012 LR: 0.000100
Epoch [169] Batch [60/261] Loss: 0.0050 LR: 0.000100
Epoch [169] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [169] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [169] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [169] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [169] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [169] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [169] Batch [130/261] Loss: 0.0458 LR: 0.000100
Epoch [169] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [169] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [169] Batch [160/261] Loss: 0.0001 LR: 0.000100
Epoch [169] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [169] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [169] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [169] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [169] Batch [210/261] Loss: 0.0001 LR: 0.000100
Epoch [169] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [169] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [169] Batch [240/261] Loss: 0.7951 LR: 0.000100
Epoch [169] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [169] Batch [260/261] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [170/350] - Time: 12.71s
Train Loss: 0.0184 | Val Loss: 0.5138
Train Acc: 0.9962 | Val Acc: 0.9292
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 0.980  
  Count 6: 0.980    Count 7: 0.969    Count 8: 0.919    Count 9: 0.856    Count 10: 0.466  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/single_image_checkpoint_epoch_169.pth
Epoch [170] Batch [0/261] Loss: 0.8497 LR: 0.000100
Epoch [170] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [170] Batch [20/261] Loss: 0.0198 LR: 0.000100
Epoch [170] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [170] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [170] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [170] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [170] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [170] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [170] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [170] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [170] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [170] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [170] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [170] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [170] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [170] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [170] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [170] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [170] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [170] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [170] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [170] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [170] Batch [230/261] Loss: 0.0001 LR: 0.000100
Epoch [170] Batch [240/261] Loss: 0.0084 LR: 0.000100
Epoch [170] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [170] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [171/350] - Time: 12.85s
Train Loss: 0.0172 | Val Loss: 0.0414
Train Acc: 0.9974 | Val Acc: 0.9879
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 0.990    Count 9: 1.000    Count 10: 0.875  
Epoch [171] Batch [0/261] Loss: 0.0006 LR: 0.000100
Epoch [171] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [171] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [171] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [171] Batch [40/261] Loss: 0.0001 LR: 0.000100
Epoch [171] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [171] Batch [60/261] Loss: 0.0006 LR: 0.000100
Epoch [171] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [171] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [171] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [171] Batch [100/261] Loss: 0.0001 LR: 0.000100
Epoch [171] Batch [110/261] Loss: 0.0023 LR: 0.000100
Epoch [171] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [171] Batch [130/261] Loss: 0.0018 LR: 0.000100
Epoch [171] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [171] Batch [150/261] Loss: 0.0002 LR: 0.000100
Epoch [171] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [171] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [171] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [171] Batch [190/261] Loss: 0.0002 LR: 0.000100
Epoch [171] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [171] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [171] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [171] Batch [230/261] Loss: 0.0001 LR: 0.000100
Epoch [171] Batch [240/261] Loss: 0.0020 LR: 0.000100
Epoch [171] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [171] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [172/350] - Time: 12.97s
Train Loss: 0.0214 | Val Loss: 0.0450
Train Acc: 0.9933 | Val Acc: 0.9907
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 0.980    Count 9: 0.911    Count 10: 1.000  
Epoch [172] Batch [0/261] Loss: 0.3062 LR: 0.000100
Epoch [172] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [172] Batch [20/261] Loss: 0.0621 LR: 0.000100
Epoch [172] Batch [30/261] Loss: 0.0004 LR: 0.000100
Epoch [172] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [172] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [172] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [172] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [172] Batch [80/261] Loss: 1.4782 LR: 0.000100
Epoch [172] Batch [90/261] Loss: 0.0001 LR: 0.000100
Epoch [172] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [172] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [172] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [172] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [172] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [172] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [172] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [172] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [172] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [172] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [172] Batch [200/261] Loss: 0.0001 LR: 0.000100
Epoch [172] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [172] Batch [220/261] Loss: 0.2538 LR: 0.000100
Epoch [172] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [172] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [172] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [172] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [173/350] - Time: 12.91s
Train Loss: 0.0312 | Val Loss: 0.4177
Train Acc: 0.9942 | Val Acc: 0.9451
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 0.979    Count 8: 0.960    Count 9: 0.978    Count 10: 0.443  
Epoch [173] Batch [0/261] Loss: 0.0001 LR: 0.000100
Epoch [173] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [173] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [173] Batch [30/261] Loss: 0.0012 LR: 0.000100
Epoch [173] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [173] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [173] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [173] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [173] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [173] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [173] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [173] Batch [110/261] Loss: 0.0102 LR: 0.000100
Epoch [173] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [173] Batch [130/261] Loss: 0.0003 LR: 0.000100
Epoch [173] Batch [140/261] Loss: 0.0001 LR: 0.000100
Epoch [173] Batch [150/261] Loss: 0.1075 LR: 0.000100
Epoch [173] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [173] Batch [170/261] Loss: 0.8073 LR: 0.000100
Epoch [173] Batch [180/261] Loss: 0.1686 LR: 0.000100
Epoch [173] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [173] Batch [200/261] Loss: 0.0001 LR: 0.000100
Epoch [173] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [173] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [173] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [173] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [173] Batch [250/261] Loss: 0.0001 LR: 0.000100
Epoch [173] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [174/350] - Time: 12.52s
Train Loss: 0.0462 | Val Loss: 0.0134
Train Acc: 0.9897 | Val Acc: 0.9981
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 0.990    Count 9: 1.000    Count 10: 1.000  
Epoch [174] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [174] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [174] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [174] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [174] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [174] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [174] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [174] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [174] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [174] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [174] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [174] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [174] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [174] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [174] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [174] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [174] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [174] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [174] Batch [180/261] Loss: 0.0036 LR: 0.000100
Epoch [174] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [174] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [174] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [174] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [174] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [174] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [174] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [174] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [175/350] - Time: 12.66s
Train Loss: 0.0071 | Val Loss: 0.0055
Train Acc: 0.9990 | Val Acc: 0.9991
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 1.000    Count 9: 1.000    Count 10: 1.000  
Epoch [175] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [175] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [175] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [175] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [175] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [175] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [175] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [175] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [175] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [175] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [175] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [175] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [175] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [175] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [175] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [175] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [175] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [175] Batch [170/261] Loss: 0.0003 LR: 0.000100
Epoch [175] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [175] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [175] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [175] Batch [210/261] Loss: 0.0017 LR: 0.000100
Epoch [175] Batch [220/261] Loss: 0.0003 LR: 0.000100
Epoch [175] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [175] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [175] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [175] Batch [260/261] Loss: 20.4079 LR: 0.000100

Epoch [176/350] - Time: 12.61s
Train Loss: 0.0159 | Val Loss: 0.0923
Train Acc: 0.9971 | Val Acc: 0.9851
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 0.979    Count 8: 1.000    Count 9: 0.989    Count 10: 0.875  
Epoch [176] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [176] Batch [10/261] Loss: 0.0106 LR: 0.000100
Epoch [176] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [176] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [176] Batch [40/261] Loss: 0.0517 LR: 0.000100
Epoch [176] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [176] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [176] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [176] Batch [80/261] Loss: 0.0071 LR: 0.000100
Epoch [176] Batch [90/261] Loss: 0.0017 LR: 0.000100
Epoch [176] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [176] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [176] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [176] Batch [130/261] Loss: 0.0001 LR: 0.000100
Epoch [176] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [176] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [176] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [176] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [176] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [176] Batch [190/261] Loss: 0.0011 LR: 0.000100
Epoch [176] Batch [200/261] Loss: 0.0018 LR: 0.000100
Epoch [176] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [176] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [176] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [176] Batch [240/261] Loss: 0.0108 LR: 0.000100
Epoch [176] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [176] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [177/350] - Time: 13.18s
Train Loss: 0.0224 | Val Loss: 0.1964
Train Acc: 0.9940 | Val Acc: 0.9786
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 0.960    Count 9: 0.800    Count 10: 1.000  
Epoch [177] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [177] Batch [10/261] Loss: 0.0001 LR: 0.000100
Epoch [177] Batch [20/261] Loss: 0.2644 LR: 0.000100
Epoch [177] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [177] Batch [40/261] Loss: 0.2857 LR: 0.000100
Epoch [177] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [177] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [177] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [177] Batch [80/261] Loss: 0.0002 LR: 0.000100
Epoch [177] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [177] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [177] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [177] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [177] Batch [130/261] Loss: 0.0007 LR: 0.000100
Epoch [177] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [177] Batch [150/261] Loss: 0.0001 LR: 0.000100
Epoch [177] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [177] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [177] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [177] Batch [190/261] Loss: 0.0001 LR: 0.000100
Epoch [177] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [177] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [177] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [177] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [177] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [177] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [177] Batch [260/261] Loss: 0.0002 LR: 0.000100

Epoch [178/350] - Time: 12.78s
Train Loss: 0.0260 | Val Loss: 0.0172
Train Acc: 0.9950 | Val Acc: 0.9972
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.990    Count 8: 1.000    Count 9: 1.000    Count 10: 0.989  
Epoch [178] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [178] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [178] Batch [20/261] Loss: 0.0007 LR: 0.000100
Epoch [178] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [178] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [178] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [178] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [178] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [178] Batch [80/261] Loss: 0.0001 LR: 0.000100
Epoch [178] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [178] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [178] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [178] Batch [120/261] Loss: 0.0001 LR: 0.000100
Epoch [178] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [178] Batch [140/261] Loss: 0.0251 LR: 0.000100
Epoch [178] Batch [150/261] Loss: 0.0002 LR: 0.000100
Epoch [178] Batch [160/261] Loss: 0.0002 LR: 0.000100
Epoch [178] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [178] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [178] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [178] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [178] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [178] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [178] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [178] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [178] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [178] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [179/350] - Time: 12.95s
Train Loss: 0.0168 | Val Loss: 0.0392
Train Acc: 0.9964 | Val Acc: 0.9953
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.990    Count 5: 1.000  
  Count 6: 0.990    Count 7: 0.979    Count 8: 1.000    Count 9: 1.000    Count 10: 1.000  
Epoch [179] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [179] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [179] Batch [20/261] Loss: 0.0094 LR: 0.000100
Epoch [179] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [179] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [179] Batch [50/261] Loss: 0.0013 LR: 0.000100
Epoch [179] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [179] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [179] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [179] Batch [90/261] Loss: 0.0604 LR: 0.000100
Epoch [179] Batch [100/261] Loss: 0.0001 LR: 0.000100
Epoch [179] Batch [110/261] Loss: 1.1230 LR: 0.000100
Epoch [179] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [179] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [179] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [179] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [179] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [179] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [179] Batch [180/261] Loss: 0.0012 LR: 0.000100
Epoch [179] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [179] Batch [200/261] Loss: 0.0032 LR: 0.000100
Epoch [179] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [179] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [179] Batch [230/261] Loss: 0.0001 LR: 0.000100
Epoch [179] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [179] Batch [250/261] Loss: 0.0001 LR: 0.000100
Epoch [179] Batch [260/261] Loss: 0.0057 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [180/350] - Time: 13.11s
Train Loss: 0.0112 | Val Loss: 0.0018
Train Acc: 0.9974 | Val Acc: 0.9991
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 1.000    Count 9: 0.989    Count 10: 1.000  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/single_image_checkpoint_epoch_179.pth
Epoch [180] Batch [0/261] Loss: 0.0434 LR: 0.000100
Epoch [180] Batch [10/261] Loss: 0.1372 LR: 0.000100
Epoch [180] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [180] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [180] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [180] Batch [50/261] Loss: 0.0133 LR: 0.000100
Epoch [180] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [180] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [180] Batch [80/261] Loss: 0.0149 LR: 0.000100
Epoch [180] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [180] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [180] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [180] Batch [120/261] Loss: 0.0028 LR: 0.000100
Epoch [180] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [180] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [180] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [180] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [180] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [180] Batch [180/261] Loss: 0.0321 LR: 0.000100
Epoch [180] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [180] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [180] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [180] Batch [220/261] Loss: 0.1817 LR: 0.000100
Epoch [180] Batch [230/261] Loss: 0.5484 LR: 0.000100
Epoch [180] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [180] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [180] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [181/350] - Time: 12.65s
Train Loss: 0.0384 | Val Loss: 1.1583
Train Acc: 0.9918 | Val Acc: 0.8603
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.929    Count 7: 0.562    Count 8: 0.495    Count 9: 0.433    Count 10: 1.000  
Epoch [181] Batch [0/261] Loss: 0.0029 LR: 0.000100
Epoch [181] Batch [10/261] Loss: 0.0001 LR: 0.000100
Epoch [181] Batch [20/261] Loss: 0.0078 LR: 0.000100
Epoch [181] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [181] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [181] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [181] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [181] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [181] Batch [80/261] Loss: 0.0001 LR: 0.000100
Epoch [181] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [181] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [181] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [181] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [181] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [181] Batch [140/261] Loss: 0.4211 LR: 0.000100
Epoch [181] Batch [150/261] Loss: 0.0222 LR: 0.000100
Epoch [181] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [181] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [181] Batch [180/261] Loss: 0.0063 LR: 0.000100
Epoch [181] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [181] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [181] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [181] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [181] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [181] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [181] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [181] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [182/350] - Time: 12.76s
Train Loss: 0.0175 | Val Loss: 0.0528
Train Acc: 0.9964 | Val Acc: 0.9907
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.990    Count 5: 0.980  
  Count 6: 0.990    Count 7: 0.979    Count 8: 1.000    Count 9: 1.000    Count 10: 0.966  
Epoch [182] Batch [0/261] Loss: 0.0002 LR: 0.000100
Epoch [182] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [182] Batch [20/261] Loss: 0.0002 LR: 0.000100
Epoch [182] Batch [30/261] Loss: 0.5805 LR: 0.000100
Epoch [182] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [182] Batch [50/261] Loss: 0.0002 LR: 0.000100
Epoch [182] Batch [60/261] Loss: 0.0002 LR: 0.000100
Epoch [182] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [182] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [182] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [182] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [182] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [182] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [182] Batch [130/261] Loss: 0.0018 LR: 0.000100
Epoch [182] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [182] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [182] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [182] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [182] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [182] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [182] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [182] Batch [210/261] Loss: 0.0002 LR: 0.000100
Epoch [182] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [182] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [182] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [182] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [182] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [183/350] - Time: 12.71s
Train Loss: 0.0126 | Val Loss: 0.0883
Train Acc: 0.9974 | Val Acc: 0.9879
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 0.958    Count 8: 0.970    Count 9: 0.944    Count 10: 1.000  
Epoch [183] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [183] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [183] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [183] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [183] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [183] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [183] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [183] Batch [70/261] Loss: 0.0002 LR: 0.000100
Epoch [183] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [183] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [183] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [183] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [183] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [183] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [183] Batch [140/261] Loss: 0.0001 LR: 0.000100
Epoch [183] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [183] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [183] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [183] Batch [180/261] Loss: 0.0001 LR: 0.000100
Epoch [183] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [183] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [183] Batch [210/261] Loss: 0.0012 LR: 0.000100
Epoch [183] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [183] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [183] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [183] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [183] Batch [260/261] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [184/350] - Time: 12.91s
Train Loss: 0.0196 | Val Loss: 0.0009
Train Acc: 0.9966 | Val Acc: 1.0000
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 1.000    Count 9: 1.000    Count 10: 1.000  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/best_single_image_model.pth
  → New best model! Validation accuracy: 1.0000
新的最佳模型! 验证准确率: 1.0000
Epoch [184] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [184] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [184] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [184] Batch [30/261] Loss: 0.0002 LR: 0.000100
Epoch [184] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [184] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [184] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [184] Batch [70/261] Loss: 0.5083 LR: 0.000100
Epoch [184] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [184] Batch [90/261] Loss: 0.0043 LR: 0.000100
Epoch [184] Batch [100/261] Loss: 0.0006 LR: 0.000100
Epoch [184] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [184] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [184] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [184] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [184] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [184] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [184] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [184] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [184] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [184] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [184] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [184] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [184] Batch [230/261] Loss: 0.0001 LR: 0.000100
Epoch [184] Batch [240/261] Loss: 0.0002 LR: 0.000100
Epoch [184] Batch [250/261] Loss: 0.0800 LR: 0.000100
Epoch [184] Batch [260/261] Loss: 0.7484 LR: 0.000100

Epoch [185/350] - Time: 12.77s
Train Loss: 0.0097 | Val Loss: 0.0314
Train Acc: 0.9971 | Val Acc: 0.9935
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 0.979    Count 8: 1.000    Count 9: 1.000    Count 10: 0.966  
Epoch [185] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [185] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [185] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [185] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [185] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [185] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [185] Batch [60/261] Loss: 0.0016 LR: 0.000100
Epoch [185] Batch [70/261] Loss: 0.0001 LR: 0.000100
Epoch [185] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [185] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [185] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [185] Batch [110/261] Loss: 0.0031 LR: 0.000100
Epoch [185] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [185] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [185] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [185] Batch [150/261] Loss: 0.0705 LR: 0.000100
Epoch [185] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [185] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [185] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [185] Batch [190/261] Loss: 0.0010 LR: 0.000100
Epoch [185] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [185] Batch [210/261] Loss: 1.9331 LR: 0.000100
Epoch [185] Batch [220/261] Loss: 0.0001 LR: 0.000100
Epoch [185] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [185] Batch [240/261] Loss: 0.0001 LR: 0.000100
Epoch [185] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [185] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [186/350] - Time: 12.80s
Train Loss: 0.0656 | Val Loss: 0.0281
Train Acc: 0.9904 | Val Acc: 0.9953
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 1.000    Count 8: 0.970    Count 9: 0.989    Count 10: 1.000  
Epoch [186] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [186] Batch [10/261] Loss: 0.0003 LR: 0.000100
Epoch [186] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [186] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [186] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [186] Batch [50/261] Loss: 0.0001 LR: 0.000100
Epoch [186] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [186] Batch [70/261] Loss: 0.1246 LR: 0.000100
Epoch [186] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [186] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [186] Batch [100/261] Loss: 0.0001 LR: 0.000100
Epoch [186] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [186] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [186] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [186] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [186] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [186] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [186] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [186] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [186] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [186] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [186] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [186] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [186] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [186] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [186] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [186] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [187/350] - Time: 12.70s
Train Loss: 0.0052 | Val Loss: 0.0215
Train Acc: 0.9988 | Val Acc: 0.9935
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 1.000    Count 8: 1.000    Count 9: 1.000    Count 10: 0.943  
Epoch [187] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [187] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [187] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [187] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [187] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [187] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [187] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [187] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [187] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [187] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [187] Batch [100/261] Loss: 0.0288 LR: 0.000100
Epoch [187] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [187] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [187] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [187] Batch [140/261] Loss: 0.0929 LR: 0.000100
Epoch [187] Batch [150/261] Loss: 0.0002 LR: 0.000100
Epoch [187] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [187] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [187] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [187] Batch [190/261] Loss: 0.0001 LR: 0.000100
Epoch [187] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [187] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [187] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [187] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [187] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [187] Batch [250/261] Loss: 0.0002 LR: 0.000100
Epoch [187] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [188/350] - Time: 13.12s
Train Loss: 0.0123 | Val Loss: 0.0808
Train Acc: 0.9971 | Val Acc: 0.9842
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.959    Count 7: 0.990    Count 8: 0.929    Count 9: 0.944    Count 10: 1.000  
Epoch [188] Batch [0/261] Loss: 0.0020 LR: 0.000100
Epoch [188] Batch [10/261] Loss: 0.1764 LR: 0.000100
Epoch [188] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [188] Batch [30/261] Loss: 0.0001 LR: 0.000100
Epoch [188] Batch [40/261] Loss: 0.0001 LR: 0.000100
Epoch [188] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [188] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [188] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [188] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [188] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [188] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [188] Batch [110/261] Loss: 0.0995 LR: 0.000100
Epoch [188] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [188] Batch [130/261] Loss: 0.0001 LR: 0.000100
Epoch [188] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [188] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [188] Batch [160/261] Loss: 0.0027 LR: 0.000100
Epoch [188] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [188] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [188] Batch [190/261] Loss: 0.0002 LR: 0.000100
Epoch [188] Batch [200/261] Loss: 0.0005 LR: 0.000100
Epoch [188] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [188] Batch [220/261] Loss: 0.0001 LR: 0.000100
Epoch [188] Batch [230/261] Loss: 0.0003 LR: 0.000100
Epoch [188] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [188] Batch [250/261] Loss: 0.0518 LR: 0.000100
Epoch [188] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [189/350] - Time: 12.76s
Train Loss: 0.0314 | Val Loss: 0.1086
Train Acc: 0.9925 | Val Acc: 0.9795
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 0.990    Count 9: 1.000    Count 10: 0.773  
Epoch [189] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [189] Batch [10/261] Loss: 0.0001 LR: 0.000100
Epoch [189] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [189] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [189] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [189] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [189] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [189] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [189] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [189] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [189] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [189] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [189] Batch [120/261] Loss: 0.0008 LR: 0.000100
Epoch [189] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [189] Batch [140/261] Loss: 0.0001 LR: 0.000100
Epoch [189] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [189] Batch [160/261] Loss: 0.0287 LR: 0.000100
Epoch [189] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [189] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [189] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [189] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [189] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [189] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [189] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [189] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [189] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [189] Batch [260/261] Loss: 0.0016 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [190/350] - Time: 12.73s
Train Loss: 0.0136 | Val Loss: 0.0734
Train Acc: 0.9969 | Val Acc: 0.9767
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.867    Count 7: 0.917    Count 8: 0.970    Count 9: 1.000    Count 10: 0.989  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/single_image_checkpoint_epoch_189.pth
Epoch [190] Batch [0/261] Loss: 0.0003 LR: 0.000100
Epoch [190] Batch [10/261] Loss: 0.0003 LR: 0.000100
Epoch [190] Batch [20/261] Loss: 0.0001 LR: 0.000100
Epoch [190] Batch [30/261] Loss: 0.0005 LR: 0.000100
Epoch [190] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [190] Batch [50/261] Loss: 0.0206 LR: 0.000100
Epoch [190] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [190] Batch [70/261] Loss: 0.0002 LR: 0.000100
Epoch [190] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [190] Batch [90/261] Loss: 0.0690 LR: 0.000100
Epoch [190] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [190] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [190] Batch [120/261] Loss: 0.0021 LR: 0.000100
Epoch [190] Batch [130/261] Loss: 0.0079 LR: 0.000100
Epoch [190] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [190] Batch [150/261] Loss: 0.0179 LR: 0.000100
Epoch [190] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [190] Batch [170/261] Loss: 0.0001 LR: 0.000100
Epoch [190] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [190] Batch [190/261] Loss: 0.0008 LR: 0.000100
Epoch [190] Batch [200/261] Loss: 0.0001 LR: 0.000100
Epoch [190] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [190] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [190] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [190] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [190] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [190] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [191/350] - Time: 12.72s
Train Loss: 0.0223 | Val Loss: 0.0870
Train Acc: 0.9947 | Val Acc: 0.9823
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 0.879    Count 9: 0.989    Count 10: 0.943  
Epoch [191] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [191] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [191] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [191] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [191] Batch [40/261] Loss: 0.0003 LR: 0.000100
Epoch [191] Batch [50/261] Loss: 0.0014 LR: 0.000100
Epoch [191] Batch [60/261] Loss: 0.0697 LR: 0.000100
Epoch [191] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [191] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [191] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [191] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [191] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [191] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [191] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [191] Batch [140/261] Loss: 0.0011 LR: 0.000100
Epoch [191] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [191] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [191] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [191] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [191] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [191] Batch [200/261] Loss: 0.0009 LR: 0.000100
Epoch [191] Batch [210/261] Loss: 0.0017 LR: 0.000100
Epoch [191] Batch [220/261] Loss: 0.0002 LR: 0.000100
Epoch [191] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [191] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [191] Batch [250/261] Loss: 0.0017 LR: 0.000100
Epoch [191] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [192/350] - Time: 12.61s
Train Loss: 0.0304 | Val Loss: 0.0313
Train Acc: 0.9928 | Val Acc: 0.9907
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 0.990  
  Count 6: 0.980    Count 7: 0.979    Count 8: 0.990    Count 9: 0.967    Count 10: 1.000  
Epoch [192] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [192] Batch [10/261] Loss: 0.0003 LR: 0.000100
Epoch [192] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [192] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [192] Batch [40/261] Loss: 0.0091 LR: 0.000100
Epoch [192] Batch [50/261] Loss: 0.0002 LR: 0.000100
Epoch [192] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [192] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [192] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [192] Batch [90/261] Loss: 0.0001 LR: 0.000100
Epoch [192] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [192] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [192] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [192] Batch [130/261] Loss: 0.0004 LR: 0.000100
Epoch [192] Batch [140/261] Loss: 0.0005 LR: 0.000100
Epoch [192] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [192] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [192] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [192] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [192] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [192] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [192] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [192] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [192] Batch [230/261] Loss: 0.0001 LR: 0.000100
Epoch [192] Batch [240/261] Loss: 0.0973 LR: 0.000100
Epoch [192] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [192] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [193/350] - Time: 12.86s
Train Loss: 0.0207 | Val Loss: 0.0404
Train Acc: 0.9962 | Val Acc: 0.9926
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.990    Count 8: 0.970    Count 9: 0.967    Count 10: 1.000  
Epoch [193] Batch [0/261] Loss: 0.0004 LR: 0.000100
Epoch [193] Batch [10/261] Loss: 0.0001 LR: 0.000100
Epoch [193] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [193] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [193] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [193] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [193] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [193] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [193] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [193] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [193] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [193] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [193] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [193] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [193] Batch [140/261] Loss: 0.0002 LR: 0.000100
Epoch [193] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [193] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [193] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [193] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [193] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [193] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [193] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [193] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [193] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [193] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [193] Batch [250/261] Loss: 0.0235 LR: 0.000100
Epoch [193] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [194/350] - Time: 12.72s
Train Loss: 0.0324 | Val Loss: 0.0035
Train Acc: 0.9942 | Val Acc: 0.9981
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 1.000    Count 9: 1.000    Count 10: 0.989  
Epoch [194] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [194] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [194] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [194] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [194] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [194] Batch [50/261] Loss: 0.0001 LR: 0.000100
Epoch [194] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [194] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [194] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [194] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [194] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [194] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [194] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [194] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [194] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [194] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [194] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [194] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [194] Batch [180/261] Loss: 0.0001 LR: 0.000100
Epoch [194] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [194] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [194] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [194] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [194] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [194] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [194] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [194] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [195/350] - Time: 12.62s
Train Loss: 0.0184 | Val Loss: 1.2405
Train Acc: 0.9954 | Val Acc: 0.8557
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 0.990    Count 8: 0.566    Count 9: 0.611    Count 10: 0.148  
Epoch [195] Batch [0/261] Loss: 1.7099 LR: 0.000100
Epoch [195] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [195] Batch [20/261] Loss: 0.0001 LR: 0.000100
Epoch [195] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [195] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [195] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [195] Batch [60/261] Loss: 0.0001 LR: 0.000100
Epoch [195] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [195] Batch [80/261] Loss: 0.8410 LR: 0.000100
Epoch [195] Batch [90/261] Loss: 0.0001 LR: 0.000100
Epoch [195] Batch [100/261] Loss: 0.2205 LR: 0.000100
Epoch [195] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [195] Batch [120/261] Loss: 0.0004 LR: 0.000100
Epoch [195] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [195] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [195] Batch [150/261] Loss: 0.0001 LR: 0.000100
Epoch [195] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [195] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [195] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [195] Batch [190/261] Loss: 0.0762 LR: 0.000100
Epoch [195] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [195] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [195] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [195] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [195] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [195] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [195] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [196/350] - Time: 12.95s
Train Loss: 0.0208 | Val Loss: 0.0475
Train Acc: 0.9954 | Val Acc: 0.9907
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.969    Count 7: 0.979    Count 8: 0.980    Count 9: 0.967    Count 10: 1.000  
Epoch [196] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [196] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [196] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [196] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [196] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [196] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [196] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [196] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [196] Batch [80/261] Loss: 0.0072 LR: 0.000100
Epoch [196] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [196] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [196] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [196] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [196] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [196] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [196] Batch [150/261] Loss: 0.5402 LR: 0.000100
Epoch [196] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [196] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [196] Batch [180/261] Loss: 0.2027 LR: 0.000100
Epoch [196] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [196] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [196] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [196] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [196] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [196] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [196] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [196] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [197/350] - Time: 12.80s
Train Loss: 0.0052 | Val Loss: 0.0752
Train Acc: 0.9981 | Val Acc: 0.9870
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 1.000    Count 9: 1.000    Count 10: 0.852  
Epoch [197] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [197] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [197] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [197] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [197] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [197] Batch [50/261] Loss: 0.0006 LR: 0.000100
Epoch [197] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [197] Batch [70/261] Loss: 0.0001 LR: 0.000100
Epoch [197] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [197] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [197] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [197] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [197] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [197] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [197] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [197] Batch [150/261] Loss: 0.0003 LR: 0.000100
Epoch [197] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [197] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [197] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [197] Batch [190/261] Loss: 0.0003 LR: 0.000100
Epoch [197] Batch [200/261] Loss: 0.0001 LR: 0.000100
Epoch [197] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [197] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [197] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [197] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [197] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [197] Batch [260/261] Loss: 0.0069 LR: 0.000100

Epoch [198/350] - Time: 12.64s
Train Loss: 0.0179 | Val Loss: 0.0953
Train Acc: 0.9969 | Val Acc: 0.9907
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 0.990    Count 9: 0.911    Count 10: 1.000  
Epoch [198] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [198] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [198] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [198] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [198] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [198] Batch [50/261] Loss: 0.0074 LR: 0.000100
Epoch [198] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [198] Batch [70/261] Loss: 0.0003 LR: 0.000100
Epoch [198] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [198] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [198] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [198] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [198] Batch [120/261] Loss: 0.0013 LR: 0.000100
Epoch [198] Batch [130/261] Loss: 0.7644 LR: 0.000100
Epoch [198] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [198] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [198] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [198] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [198] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [198] Batch [190/261] Loss: 0.0001 LR: 0.000100
Epoch [198] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [198] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [198] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [198] Batch [230/261] Loss: 0.0003 LR: 0.000100
Epoch [198] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [198] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [198] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [199/350] - Time: 12.71s
Train Loss: 0.0242 | Val Loss: 0.0648
Train Acc: 0.9950 | Val Acc: 0.9898
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 0.899    Count 9: 0.989    Count 10: 1.000  
Epoch [199] Batch [0/261] Loss: 0.0082 LR: 0.000100
Epoch [199] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [199] Batch [20/261] Loss: 0.0001 LR: 0.000100
Epoch [199] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [199] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [199] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [199] Batch [60/261] Loss: 0.0001 LR: 0.000100
Epoch [199] Batch [70/261] Loss: 0.0001 LR: 0.000100
Epoch [199] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [199] Batch [90/261] Loss: 0.0003 LR: 0.000100
Epoch [199] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [199] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [199] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [199] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [199] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [199] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [199] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [199] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [199] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [199] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [199] Batch [200/261] Loss: 0.0001 LR: 0.000100
Epoch [199] Batch [210/261] Loss: 0.0001 LR: 0.000100
Epoch [199] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [199] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [199] Batch [240/261] Loss: 0.3462 LR: 0.000100
Epoch [199] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [199] Batch [260/261] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [200/350] - Time: 12.74s
Train Loss: 0.0109 | Val Loss: 0.5053
Train Acc: 0.9969 | Val Acc: 0.9358
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 1.000    Count 8: 0.899    Count 9: 0.922    Count 10: 0.432  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/single_image_checkpoint_epoch_199.pth
Epoch [200] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [200] Batch [10/261] Loss: 0.0001 LR: 0.000100
Epoch [200] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [200] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [200] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [200] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [200] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [200] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [200] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [200] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [200] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [200] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [200] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [200] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [200] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [200] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [200] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [200] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [200] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [200] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [200] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [200] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [200] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [200] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [200] Batch [240/261] Loss: 0.3970 LR: 0.000100
Epoch [200] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [200] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [201/350] - Time: 12.56s
Train Loss: 0.0038 | Val Loss: 0.1273
Train Acc: 0.9993 | Val Acc: 0.9842
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 0.970    Count 9: 0.844    Count 10: 1.000  
Epoch [201] Batch [0/261] Loss: 0.2319 LR: 0.000100
Epoch [201] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [201] Batch [20/261] Loss: 0.0001 LR: 0.000100
Epoch [201] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [201] Batch [40/261] Loss: 0.0001 LR: 0.000100
Epoch [201] Batch [50/261] Loss: 0.2134 LR: 0.000100
Epoch [201] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [201] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [201] Batch [80/261] Loss: 0.6177 LR: 0.000100
Epoch [201] Batch [90/261] Loss: 0.0001 LR: 0.000100
Epoch [201] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [201] Batch [110/261] Loss: 0.0007 LR: 0.000100
Epoch [201] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [201] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [201] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [201] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [201] Batch [160/261] Loss: 0.0016 LR: 0.000100
Epoch [201] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [201] Batch [180/261] Loss: 0.0001 LR: 0.000100
Epoch [201] Batch [190/261] Loss: 0.0004 LR: 0.000100
Epoch [201] Batch [200/261] Loss: 0.0030 LR: 0.000100
Epoch [201] Batch [210/261] Loss: 0.4787 LR: 0.000100
Epoch [201] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [201] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [201] Batch [240/261] Loss: 0.4390 LR: 0.000100
Epoch [201] Batch [250/261] Loss: 0.0005 LR: 0.000100
Epoch [201] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [202/350] - Time: 12.76s
Train Loss: 0.0332 | Val Loss: 0.0190
Train Acc: 0.9935 | Val Acc: 0.9953
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 0.990    Count 9: 1.000    Count 10: 0.966  
Epoch [202] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [202] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [202] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [202] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [202] Batch [40/261] Loss: 0.0001 LR: 0.000100
Epoch [202] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [202] Batch [60/261] Loss: 0.0002 LR: 0.000100
Epoch [202] Batch [70/261] Loss: 0.2422 LR: 0.000100
Epoch [202] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [202] Batch [90/261] Loss: 0.0001 LR: 0.000100
Epoch [202] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [202] Batch [110/261] Loss: 0.1037 LR: 0.000100
Epoch [202] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [202] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [202] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [202] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [202] Batch [160/261] Loss: 0.2151 LR: 0.000100
Epoch [202] Batch [170/261] Loss: 0.0001 LR: 0.000100
Epoch [202] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [202] Batch [190/261] Loss: 0.0015 LR: 0.000100
Epoch [202] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [202] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [202] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [202] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [202] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [202] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [202] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [203/350] - Time: 12.84s
Train Loss: 0.0096 | Val Loss: 0.0002
Train Acc: 0.9969 | Val Acc: 1.0000
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 1.000    Count 9: 1.000    Count 10: 1.000  
Epoch [203] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [203] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [203] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [203] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [203] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [203] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [203] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [203] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [203] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [203] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [203] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [203] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [203] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [203] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [203] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [203] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [203] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [203] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [203] Batch [180/261] Loss: 0.0004 LR: 0.000100
Epoch [203] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [203] Batch [200/261] Loss: 0.0032 LR: 0.000100
Epoch [203] Batch [210/261] Loss: 0.0016 LR: 0.000100
Epoch [203] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [203] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [203] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [203] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [203] Batch [260/261] Loss: 7.0518 LR: 0.000100

Epoch [204/350] - Time: 13.27s
Train Loss: 0.0025 | Val Loss: 0.0635
Train Acc: 0.9993 | Val Acc: 0.9888
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.990    Count 5: 0.980  
  Count 6: 0.980    Count 7: 0.979    Count 8: 0.970    Count 9: 1.000    Count 10: 0.989  
Epoch [204] Batch [0/261] Loss: 0.0007 LR: 0.000100
Epoch [204] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [204] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [204] Batch [30/261] Loss: 0.0030 LR: 0.000100
Epoch [204] Batch [40/261] Loss: 0.8467 LR: 0.000100
Epoch [204] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [204] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [204] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [204] Batch [80/261] Loss: 0.0004 LR: 0.000100
Epoch [204] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [204] Batch [100/261] Loss: 0.1207 LR: 0.000100
Epoch [204] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [204] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [204] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [204] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [204] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [204] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [204] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [204] Batch [180/261] Loss: 0.0013 LR: 0.000100
Epoch [204] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [204] Batch [200/261] Loss: 0.0014 LR: 0.000100
Epoch [204] Batch [210/261] Loss: 0.0031 LR: 0.000100
Epoch [204] Batch [220/261] Loss: 0.0001 LR: 0.000100
Epoch [204] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [204] Batch [240/261] Loss: 0.0037 LR: 0.000100
Epoch [204] Batch [250/261] Loss: 0.0114 LR: 0.000100
Epoch [204] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [205/350] - Time: 12.65s
Train Loss: 0.0295 | Val Loss: 0.0283
Train Acc: 0.9925 | Val Acc: 0.9953
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.990    Count 5: 1.000  
  Count 6: 0.990    Count 7: 1.000    Count 8: 1.000    Count 9: 0.989    Count 10: 0.989  
Epoch [205] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [205] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [205] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [205] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [205] Batch [40/261] Loss: 0.0056 LR: 0.000100
Epoch [205] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [205] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [205] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [205] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [205] Batch [90/261] Loss: 0.0209 LR: 0.000100
Epoch [205] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [205] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [205] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [205] Batch [130/261] Loss: 0.0001 LR: 0.000100
Epoch [205] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [205] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [205] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [205] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [205] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [205] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [205] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [205] Batch [210/261] Loss: 0.0002 LR: 0.000100
Epoch [205] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [205] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [205] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [205] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [205] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [206/350] - Time: 12.59s
Train Loss: 0.0104 | Val Loss: 0.1190
Train Acc: 0.9978 | Val Acc: 0.9758
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 0.990    Count 8: 1.000    Count 9: 0.989    Count 10: 0.750  
Epoch [206] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [206] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [206] Batch [20/261] Loss: 0.0046 LR: 0.000100
Epoch [206] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [206] Batch [40/261] Loss: 0.0996 LR: 0.000100
Epoch [206] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [206] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [206] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [206] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [206] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [206] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [206] Batch [110/261] Loss: 0.2074 LR: 0.000100
Epoch [206] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [206] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [206] Batch [140/261] Loss: 0.0010 LR: 0.000100
Epoch [206] Batch [150/261] Loss: 0.0003 LR: 0.000100
Epoch [206] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [206] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [206] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [206] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [206] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [206] Batch [210/261] Loss: 0.0001 LR: 0.000100
Epoch [206] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [206] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [206] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [206] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [206] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [207/350] - Time: 12.74s
Train Loss: 0.0105 | Val Loss: 0.0543
Train Acc: 0.9978 | Val Acc: 0.9907
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.990    Count 8: 0.990    Count 9: 0.911    Count 10: 1.000  
Epoch [207] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [207] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [207] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [207] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [207] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [207] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [207] Batch [60/261] Loss: 0.0007 LR: 0.000100
Epoch [207] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [207] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [207] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [207] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [207] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [207] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [207] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [207] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [207] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [207] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [207] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [207] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [207] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [207] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [207] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [207] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [207] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [207] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [207] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [207] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [208/350] - Time: 12.64s
Train Loss: 0.0002 | Val Loss: 0.0000
Train Acc: 1.0000 | Val Acc: 1.0000
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 1.000    Count 9: 1.000    Count 10: 1.000  
Epoch [208] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [208] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [208] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [208] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [208] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [208] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [208] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [208] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [208] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [208] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [208] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [208] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [208] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [208] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [208] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [208] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [208] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [208] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [208] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [208] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [208] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [208] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [208] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [208] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [208] Batch [240/261] Loss: 0.0022 LR: 0.000100
Epoch [208] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [208] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [209/350] - Time: 12.71s
Train Loss: 0.0000 | Val Loss: 0.0483
Train Acc: 1.0000 | Val Acc: 0.9926
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.980    Count 7: 0.979    Count 8: 0.990    Count 9: 0.989    Count 10: 0.989  
Epoch [209] Batch [0/261] Loss: 0.0001 LR: 0.000100
Epoch [209] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [209] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [209] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [209] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [209] Batch [50/261] Loss: 0.0001 LR: 0.000100
Epoch [209] Batch [60/261] Loss: 0.0022 LR: 0.000100
Epoch [209] Batch [70/261] Loss: 0.4825 LR: 0.000100
Epoch [209] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [209] Batch [90/261] Loss: 0.0001 LR: 0.000100
Epoch [209] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [209] Batch [110/261] Loss: 0.3027 LR: 0.000100
Epoch [209] Batch [120/261] Loss: 1.9890 LR: 0.000100
Epoch [209] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [209] Batch [140/261] Loss: 0.0079 LR: 0.000100
Epoch [209] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [209] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [209] Batch [170/261] Loss: 0.0009 LR: 0.000100
Epoch [209] Batch [180/261] Loss: 0.0043 LR: 0.000100
Epoch [209] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [209] Batch [200/261] Loss: 0.0046 LR: 0.000100
Epoch [209] Batch [210/261] Loss: 0.0006 LR: 0.000100
Epoch [209] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [209] Batch [230/261] Loss: 0.7137 LR: 0.000100
Epoch [209] Batch [240/261] Loss: 0.0001 LR: 0.000100
Epoch [209] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [209] Batch [260/261] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [210/350] - Time: 12.73s
Train Loss: 0.0393 | Val Loss: 0.6520
Train Acc: 0.9911 | Val Acc: 0.9190
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.980    Count 7: 0.833    Count 8: 0.687    Count 9: 0.578    Count 10: 1.000  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/single_image_checkpoint_epoch_209.pth
Epoch [210] Batch [0/261] Loss: 0.0281 LR: 0.000100
Epoch [210] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [210] Batch [20/261] Loss: 0.0227 LR: 0.000100
Epoch [210] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [210] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [210] Batch [50/261] Loss: 0.0002 LR: 0.000100
Epoch [210] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [210] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [210] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [210] Batch [90/261] Loss: 0.0001 LR: 0.000100
Epoch [210] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [210] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [210] Batch [120/261] Loss: 0.0667 LR: 0.000100
Epoch [210] Batch [130/261] Loss: 0.2703 LR: 0.000100
Epoch [210] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [210] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [210] Batch [160/261] Loss: 0.0013 LR: 0.000100
Epoch [210] Batch [170/261] Loss: 0.0876 LR: 0.000100
Epoch [210] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [210] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [210] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [210] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [210] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [210] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [210] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [210] Batch [250/261] Loss: 0.0006 LR: 0.000100
Epoch [210] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [211/350] - Time: 12.84s
Train Loss: 0.0222 | Val Loss: 0.0690
Train Acc: 0.9945 | Val Acc: 0.9916
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 1.000    Count 9: 0.911    Count 10: 1.000  
Epoch [211] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [211] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [211] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [211] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [211] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [211] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [211] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [211] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [211] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [211] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [211] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [211] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [211] Batch [120/261] Loss: 0.0478 LR: 0.000100
Epoch [211] Batch [130/261] Loss: 0.0003 LR: 0.000100
Epoch [211] Batch [140/261] Loss: 0.0037 LR: 0.000100
Epoch [211] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [211] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [211] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [211] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [211] Batch [190/261] Loss: 0.0555 LR: 0.000100
Epoch [211] Batch [200/261] Loss: 0.0001 LR: 0.000100
Epoch [211] Batch [210/261] Loss: 0.1790 LR: 0.000100
Epoch [211] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [211] Batch [230/261] Loss: 0.0479 LR: 0.000100
Epoch [211] Batch [240/261] Loss: 0.0218 LR: 0.000100
Epoch [211] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [211] Batch [260/261] Loss: 0.0003 LR: 0.000100

Epoch [212/350] - Time: 13.08s
Train Loss: 0.0323 | Val Loss: 0.1311
Train Acc: 0.9935 | Val Acc: 0.9739
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.981    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 1.000    Count 8: 1.000    Count 9: 1.000    Count 10: 0.716  
Epoch [212] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [212] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [212] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [212] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [212] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [212] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [212] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [212] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [212] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [212] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [212] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [212] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [212] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [212] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [212] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [212] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [212] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [212] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [212] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [212] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [212] Batch [200/261] Loss: 0.0001 LR: 0.000100
Epoch [212] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [212] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [212] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [212] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [212] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [212] Batch [260/261] Loss: 5.0166 LR: 0.000100

Epoch [213/350] - Time: 12.55s
Train Loss: 0.0116 | Val Loss: 0.0083
Train Acc: 0.9978 | Val Acc: 0.9981
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 1.000    Count 9: 1.000    Count 10: 0.977  
Epoch [213] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [213] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [213] Batch [20/261] Loss: 0.0005 LR: 0.000100
Epoch [213] Batch [30/261] Loss: 0.0002 LR: 0.000100
Epoch [213] Batch [40/261] Loss: 0.0009 LR: 0.000100
Epoch [213] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [213] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [213] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [213] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [213] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [213] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [213] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [213] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [213] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [213] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [213] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [213] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [213] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [213] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [213] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [213] Batch [200/261] Loss: 0.0004 LR: 0.000100
Epoch [213] Batch [210/261] Loss: 0.7715 LR: 0.000100
Epoch [213] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [213] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [213] Batch [240/261] Loss: 0.0001 LR: 0.000100
Epoch [213] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [213] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [214/350] - Time: 12.92s
Train Loss: 0.0406 | Val Loss: 0.0220
Train Acc: 0.9928 | Val Acc: 0.9953
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 1.000    Count 8: 1.000    Count 9: 1.000    Count 10: 0.966  
Epoch [214] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [214] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [214] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [214] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [214] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [214] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [214] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [214] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [214] Batch [80/261] Loss: 0.0002 LR: 0.000100
Epoch [214] Batch [90/261] Loss: 0.0002 LR: 0.000100
Epoch [214] Batch [100/261] Loss: 0.6570 LR: 0.000100
Epoch [214] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [214] Batch [120/261] Loss: 0.0069 LR: 0.000100
Epoch [214] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [214] Batch [140/261] Loss: 0.1764 LR: 0.000100
Epoch [214] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [214] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [214] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [214] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [214] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [214] Batch [200/261] Loss: 0.0131 LR: 0.000100
Epoch [214] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [214] Batch [220/261] Loss: 0.3573 LR: 0.000100
Epoch [214] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [214] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [214] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [214] Batch [260/261] Loss: 9.5024 LR: 0.000100

Epoch [215/350] - Time: 12.67s
Train Loss: 0.0576 | Val Loss: 0.0649
Train Acc: 0.9904 | Val Acc: 0.9879
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 0.990    Count 8: 1.000    Count 9: 0.989    Count 10: 0.886  
Epoch [215] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [215] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [215] Batch [20/261] Loss: 0.0007 LR: 0.000100
Epoch [215] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [215] Batch [40/261] Loss: 0.0007 LR: 0.000100
Epoch [215] Batch [50/261] Loss: 0.0001 LR: 0.000100
Epoch [215] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [215] Batch [70/261] Loss: 0.0001 LR: 0.000100
Epoch [215] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [215] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [215] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [215] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [215] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [215] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [215] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [215] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [215] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [215] Batch [170/261] Loss: 0.0001 LR: 0.000100
Epoch [215] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [215] Batch [190/261] Loss: 0.0001 LR: 0.000100
Epoch [215] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [215] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [215] Batch [220/261] Loss: 0.0001 LR: 0.000100
Epoch [215] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [215] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [215] Batch [250/261] Loss: 0.0073 LR: 0.000100
Epoch [215] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [216/350] - Time: 12.75s
Train Loss: 0.0171 | Val Loss: 0.0169
Train Acc: 0.9974 | Val Acc: 0.9963
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 0.980    Count 9: 1.000    Count 10: 0.977  
Epoch [216] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [216] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [216] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [216] Batch [30/261] Loss: 0.0003 LR: 0.000100
Epoch [216] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [216] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [216] Batch [60/261] Loss: 0.0002 LR: 0.000100
Epoch [216] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [216] Batch [80/261] Loss: 0.0018 LR: 0.000100
Epoch [216] Batch [90/261] Loss: 0.5644 LR: 0.000100
Epoch [216] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [216] Batch [110/261] Loss: 0.0183 LR: 0.000100
Epoch [216] Batch [120/261] Loss: 0.0005 LR: 0.000100
Epoch [216] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [216] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [216] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [216] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [216] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [216] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [216] Batch [190/261] Loss: 0.0002 LR: 0.000100
Epoch [216] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [216] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [216] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [216] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [216] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [216] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [216] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [217/350] - Time: 12.89s
Train Loss: 0.0369 | Val Loss: 0.0489
Train Acc: 0.9916 | Val Acc: 0.9907
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 0.990    Count 9: 1.000    Count 10: 0.898  
Epoch [217] Batch [0/261] Loss: 0.0011 LR: 0.000100
Epoch [217] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [217] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [217] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [217] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [217] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [217] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [217] Batch [70/261] Loss: 0.0021 LR: 0.000100
Epoch [217] Batch [80/261] Loss: 0.0094 LR: 0.000100
Epoch [217] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [217] Batch [100/261] Loss: 0.0001 LR: 0.000100
Epoch [217] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [217] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [217] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [217] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [217] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [217] Batch [160/261] Loss: 0.0136 LR: 0.000100
Epoch [217] Batch [170/261] Loss: 0.0003 LR: 0.000100
Epoch [217] Batch [180/261] Loss: 0.2624 LR: 0.000100
Epoch [217] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [217] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [217] Batch [210/261] Loss: 0.0893 LR: 0.000100
Epoch [217] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [217] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [217] Batch [240/261] Loss: 0.0355 LR: 0.000100
Epoch [217] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [217] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [218/350] - Time: 12.75s
Train Loss: 0.0061 | Val Loss: 0.0195
Train Acc: 0.9988 | Val Acc: 0.9972
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.980    Count 7: 1.000    Count 8: 1.000    Count 9: 1.000    Count 10: 1.000  
Epoch [218] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [218] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [218] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [218] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [218] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [218] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [218] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [218] Batch [70/261] Loss: 0.0024 LR: 0.000100
Epoch [218] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [218] Batch [90/261] Loss: 0.0018 LR: 0.000100
Epoch [218] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [218] Batch [110/261] Loss: 0.0042 LR: 0.000100
Epoch [218] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [218] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [218] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [218] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [218] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [218] Batch [170/261] Loss: 0.0887 LR: 0.000100
Epoch [218] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [218] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [218] Batch [200/261] Loss: 0.0002 LR: 0.000100
Epoch [218] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [218] Batch [220/261] Loss: 0.0079 LR: 0.000100
Epoch [218] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [218] Batch [240/261] Loss: 0.0003 LR: 0.000100
Epoch [218] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [218] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [219/350] - Time: 12.70s
Train Loss: 0.0189 | Val Loss: 0.0070
Train Acc: 0.9966 | Val Acc: 0.9981
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 1.000    Count 9: 0.989    Count 10: 0.989  
Epoch [219] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [219] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [219] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [219] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [219] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [219] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [219] Batch [60/261] Loss: 0.0001 LR: 0.000100
Epoch [219] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [219] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [219] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [219] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [219] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [219] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [219] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [219] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [219] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [219] Batch [160/261] Loss: 0.0001 LR: 0.000100
Epoch [219] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [219] Batch [180/261] Loss: 0.0242 LR: 0.000100
Epoch [219] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [219] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [219] Batch [210/261] Loss: 0.0002 LR: 0.000100
Epoch [219] Batch [220/261] Loss: 0.0001 LR: 0.000100
Epoch [219] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [219] Batch [240/261] Loss: 0.0011 LR: 0.000100
Epoch [219] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [219] Batch [260/261] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [220/350] - Time: 13.00s
Train Loss: 0.0317 | Val Loss: 0.0124
Train Acc: 0.9945 | Val Acc: 0.9963
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 0.990    Count 9: 1.000    Count 10: 0.977  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/single_image_checkpoint_epoch_219.pth
Epoch [220] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [220] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [220] Batch [20/261] Loss: 0.0001 LR: 0.000100
Epoch [220] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [220] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [220] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [220] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [220] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [220] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [220] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [220] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [220] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [220] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [220] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [220] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [220] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [220] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [220] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [220] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [220] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [220] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [220] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [220] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [220] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [220] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [220] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [220] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [221/350] - Time: 12.58s
Train Loss: 0.0001 | Val Loss: 0.0538
Train Acc: 1.0000 | Val Acc: 0.9888
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 1.000    Count 9: 1.000    Count 10: 0.875  
Epoch [221] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [221] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [221] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [221] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [221] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [221] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [221] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [221] Batch [70/261] Loss: 0.0002 LR: 0.000100
Epoch [221] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [221] Batch [90/261] Loss: 0.0002 LR: 0.000100
Epoch [221] Batch [100/261] Loss: 0.0127 LR: 0.000100
Epoch [221] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [221] Batch [120/261] Loss: 0.0076 LR: 0.000100
Epoch [221] Batch [130/261] Loss: 0.0011 LR: 0.000100
Epoch [221] Batch [140/261] Loss: 0.0002 LR: 0.000100
Epoch [221] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [221] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [221] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [221] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [221] Batch [190/261] Loss: 0.0632 LR: 0.000100
Epoch [221] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [221] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [221] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [221] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [221] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [221] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [221] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [222/350] - Time: 12.78s
Train Loss: 0.0101 | Val Loss: 0.0657
Train Acc: 0.9966 | Val Acc: 0.9823
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 0.969    Count 8: 0.980    Count 9: 1.000    Count 10: 0.864  
Epoch [222] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [222] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [222] Batch [20/261] Loss: 0.0002 LR: 0.000100
Epoch [222] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [222] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [222] Batch [50/261] Loss: 0.0003 LR: 0.000100
Epoch [222] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [222] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [222] Batch [80/261] Loss: 0.0001 LR: 0.000100
Epoch [222] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [222] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [222] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [222] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [222] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [222] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [222] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [222] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [222] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [222] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [222] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [222] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [222] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [222] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [222] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [222] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [222] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [222] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [223/350] - Time: 12.81s
Train Loss: 0.0049 | Val Loss: 0.0408
Train Acc: 0.9993 | Val Acc: 0.9916
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 0.970    Count 9: 0.944    Count 10: 1.000  
Epoch [223] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [223] Batch [10/261] Loss: 0.0002 LR: 0.000100
Epoch [223] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [223] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [223] Batch [40/261] Loss: 0.0002 LR: 0.000100
Epoch [223] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [223] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [223] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [223] Batch [80/261] Loss: 0.0001 LR: 0.000100
Epoch [223] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [223] Batch [100/261] Loss: 0.0700 LR: 0.000100
Epoch [223] Batch [110/261] Loss: 0.0020 LR: 0.000100
Epoch [223] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [223] Batch [130/261] Loss: 0.0199 LR: 0.000100
Epoch [223] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [223] Batch [150/261] Loss: 0.0002 LR: 0.000100
Epoch [223] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [223] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [223] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [223] Batch [190/261] Loss: 0.0002 LR: 0.000100
Epoch [223] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [223] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [223] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [223] Batch [230/261] Loss: 0.0026 LR: 0.000100
Epoch [223] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [223] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [223] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [224/350] - Time: 12.65s
Train Loss: 0.0359 | Val Loss: 0.0293
Train Acc: 0.9940 | Val Acc: 0.9953
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 0.990    Count 8: 1.000    Count 9: 0.978    Count 10: 1.000  
Epoch [224] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [224] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [224] Batch [20/261] Loss: 0.2999 LR: 0.000100
Epoch [224] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [224] Batch [40/261] Loss: 0.0026 LR: 0.000100
Epoch [224] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [224] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [224] Batch [70/261] Loss: 0.0120 LR: 0.000100
Epoch [224] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [224] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [224] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [224] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [224] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [224] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [224] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [224] Batch [150/261] Loss: 0.0011 LR: 0.000100
Epoch [224] Batch [160/261] Loss: 0.0209 LR: 0.000100
Epoch [224] Batch [170/261] Loss: 0.0038 LR: 0.000100
Epoch [224] Batch [180/261] Loss: 0.0098 LR: 0.000100
Epoch [224] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [224] Batch [200/261] Loss: 0.0707 LR: 0.000100
Epoch [224] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [224] Batch [220/261] Loss: 0.0083 LR: 0.000100
Epoch [224] Batch [230/261] Loss: 0.4740 LR: 0.000100
Epoch [224] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [224] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [224] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [225/350] - Time: 12.81s
Train Loss: 0.0367 | Val Loss: 0.1092
Train Acc: 0.9916 | Val Acc: 0.9795
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.990    Count 5: 1.000  
  Count 6: 0.980    Count 7: 0.958    Count 8: 0.990    Count 9: 0.989    Count 10: 0.852  
Epoch [225] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [225] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [225] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [225] Batch [30/261] Loss: 1.0290 LR: 0.000100
Epoch [225] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [225] Batch [50/261] Loss: 0.0028 LR: 0.000100
Epoch [225] Batch [60/261] Loss: 0.0001 LR: 0.000100
Epoch [225] Batch [70/261] Loss: 0.0004 LR: 0.000100
Epoch [225] Batch [80/261] Loss: 0.1541 LR: 0.000100
Epoch [225] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [225] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [225] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [225] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [225] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [225] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [225] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [225] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [225] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [225] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [225] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [225] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [225] Batch [210/261] Loss: 0.0001 LR: 0.000100
Epoch [225] Batch [220/261] Loss: 0.0001 LR: 0.000100
Epoch [225] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [225] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [225] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [225] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [226/350] - Time: 12.77s
Train Loss: 0.0280 | Val Loss: 0.0114
Train Acc: 0.9945 | Val Acc: 0.9963
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 0.990    Count 9: 0.989    Count 10: 0.989  
Epoch [226] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [226] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [226] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [226] Batch [30/261] Loss: 0.0034 LR: 0.000100
Epoch [226] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [226] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [226] Batch [60/261] Loss: 0.0087 LR: 0.000100
Epoch [226] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [226] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [226] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [226] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [226] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [226] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [226] Batch [130/261] Loss: 0.0076 LR: 0.000100
Epoch [226] Batch [140/261] Loss: 0.2744 LR: 0.000100
Epoch [226] Batch [150/261] Loss: 0.0012 LR: 0.000100
Epoch [226] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [226] Batch [170/261] Loss: 0.0001 LR: 0.000100
Epoch [226] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [226] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [226] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [226] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [226] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [226] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [226] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [226] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [226] Batch [260/261] Loss: 3.0311 LR: 0.000100

Epoch [227/350] - Time: 12.66s
Train Loss: 0.0112 | Val Loss: 0.0271
Train Acc: 0.9969 | Val Acc: 0.9944
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.990    Count 5: 1.000  
  Count 6: 0.980    Count 7: 1.000    Count 8: 0.990    Count 9: 0.989    Count 10: 1.000  
Epoch [227] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [227] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [227] Batch [20/261] Loss: 0.0048 LR: 0.000100
Epoch [227] Batch [30/261] Loss: 0.0565 LR: 0.000100
Epoch [227] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [227] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [227] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [227] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [227] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [227] Batch [90/261] Loss: 0.0018 LR: 0.000100
Epoch [227] Batch [100/261] Loss: 0.0002 LR: 0.000100
Epoch [227] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [227] Batch [120/261] Loss: 0.0001 LR: 0.000100
Epoch [227] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [227] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [227] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [227] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [227] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [227] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [227] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [227] Batch [200/261] Loss: 0.0012 LR: 0.000100
Epoch [227] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [227] Batch [220/261] Loss: 0.0010 LR: 0.000100
Epoch [227] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [227] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [227] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [227] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [228/350] - Time: 13.12s
Train Loss: 0.0048 | Val Loss: 0.3148
Train Acc: 0.9981 | Val Acc: 0.9618
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.990    Count 5: 1.000  
  Count 6: 0.929    Count 7: 0.969    Count 8: 0.788    Count 9: 0.900    Count 10: 1.000  
Epoch [228] Batch [0/261] Loss: 0.0002 LR: 0.000100
Epoch [228] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [228] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [228] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [228] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [228] Batch [50/261] Loss: 0.0242 LR: 0.000100
Epoch [228] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [228] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [228] Batch [80/261] Loss: 0.0027 LR: 0.000100
Epoch [228] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [228] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [228] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [228] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [228] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [228] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [228] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [228] Batch [160/261] Loss: 0.0002 LR: 0.000100
Epoch [228] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [228] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [228] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [228] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [228] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [228] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [228] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [228] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [228] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [228] Batch [260/261] Loss: 0.0001 LR: 0.000100

Epoch [229/350] - Time: 12.64s
Train Loss: 0.0024 | Val Loss: 0.8902
Train Acc: 0.9990 | Val Acc: 0.8920
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.882  
  Count 6: 0.939    Count 7: 0.740    Count 8: 0.566    Count 9: 0.667    Count 10: 1.000  
Epoch [229] Batch [0/261] Loss: 0.9479 LR: 0.000100
Epoch [229] Batch [10/261] Loss: 0.0035 LR: 0.000100
Epoch [229] Batch [20/261] Loss: 0.0009 LR: 0.000100
Epoch [229] Batch [30/261] Loss: 0.1511 LR: 0.000100
Epoch [229] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [229] Batch [50/261] Loss: 0.0001 LR: 0.000100
Epoch [229] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [229] Batch [70/261] Loss: 0.0001 LR: 0.000100
Epoch [229] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [229] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [229] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [229] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [229] Batch [120/261] Loss: 0.0021 LR: 0.000100
Epoch [229] Batch [130/261] Loss: 0.0002 LR: 0.000100
Epoch [229] Batch [140/261] Loss: 0.0006 LR: 0.000100
Epoch [229] Batch [150/261] Loss: 0.4272 LR: 0.000100
Epoch [229] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [229] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [229] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [229] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [229] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [229] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [229] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [229] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [229] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [229] Batch [250/261] Loss: 0.0006 LR: 0.000100
Epoch [229] Batch [260/261] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [230/350] - Time: 12.66s
Train Loss: 0.0304 | Val Loss: 0.0215
Train Acc: 0.9930 | Val Acc: 0.9935
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 1.000    Count 8: 0.970    Count 9: 0.989    Count 10: 0.989  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/single_image_checkpoint_epoch_229.pth
Epoch [230] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [230] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [230] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [230] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [230] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [230] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [230] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [230] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [230] Batch [80/261] Loss: 0.0014 LR: 0.000100
Epoch [230] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [230] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [230] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [230] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [230] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [230] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [230] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [230] Batch [160/261] Loss: 0.0002 LR: 0.000100
Epoch [230] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [230] Batch [180/261] Loss: 0.1093 LR: 0.000100
Epoch [230] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [230] Batch [200/261] Loss: 0.0018 LR: 0.000100
Epoch [230] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [230] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [230] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [230] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [230] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [230] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [231/350] - Time: 12.83s
Train Loss: 0.0218 | Val Loss: 0.0261
Train Acc: 0.9938 | Val Acc: 0.9953
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 0.979    Count 8: 1.000    Count 9: 1.000    Count 10: 0.977  
Epoch [231] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [231] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [231] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [231] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [231] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [231] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [231] Batch [60/261] Loss: 0.0002 LR: 0.000100
Epoch [231] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [231] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [231] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [231] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [231] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [231] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [231] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [231] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [231] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [231] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [231] Batch [170/261] Loss: 0.0003 LR: 0.000100
Epoch [231] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [231] Batch [190/261] Loss: 0.1924 LR: 0.000100
Epoch [231] Batch [200/261] Loss: 0.8604 LR: 0.000100
Epoch [231] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [231] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [231] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [231] Batch [240/261] Loss: 0.5140 LR: 0.000100
Epoch [231] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [231] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [232/350] - Time: 12.76s
Train Loss: 0.0190 | Val Loss: 0.0418
Train Acc: 0.9971 | Val Acc: 0.9944
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 0.970    Count 9: 0.978    Count 10: 1.000  
Epoch [232] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [232] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [232] Batch [20/261] Loss: 0.0006 LR: 0.000100
Epoch [232] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [232] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [232] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [232] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [232] Batch [70/261] Loss: 0.0005 LR: 0.000100
Epoch [232] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [232] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [232] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [232] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [232] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [232] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [232] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [232] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [232] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [232] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [232] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [232] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [232] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [232] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [232] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [232] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [232] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [232] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [232] Batch [260/261] Loss: 0.8846 LR: 0.000100

Epoch [233/350] - Time: 12.61s
Train Loss: 0.0010 | Val Loss: 0.0187
Train Acc: 0.9995 | Val Acc: 0.9972
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 0.990    Count 9: 0.989    Count 10: 1.000  
Epoch [233] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [233] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [233] Batch [20/261] Loss: 0.2718 LR: 0.000100
Epoch [233] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [233] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [233] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [233] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [233] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [233] Batch [80/261] Loss: 0.4019 LR: 0.000100
Epoch [233] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [233] Batch [100/261] Loss: 0.0002 LR: 0.000100
Epoch [233] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [233] Batch [120/261] Loss: 0.5281 LR: 0.000100
Epoch [233] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [233] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [233] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [233] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [233] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [233] Batch [180/261] Loss: 0.0389 LR: 0.000100
Epoch [233] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [233] Batch [200/261] Loss: 0.0001 LR: 0.000100
Epoch [233] Batch [210/261] Loss: 0.1948 LR: 0.000100
Epoch [233] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [233] Batch [230/261] Loss: 0.0001 LR: 0.000100
Epoch [233] Batch [240/261] Loss: 0.0001 LR: 0.000100
Epoch [233] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [233] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [234/350] - Time: 12.80s
Train Loss: 0.0329 | Val Loss: 0.2651
Train Acc: 0.9923 | Val Acc: 0.9609
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.990  
  Count 6: 0.939    Count 7: 0.958    Count 8: 0.889    Count 9: 0.778    Count 10: 1.000  
Epoch [234] Batch [0/261] Loss: 0.0938 LR: 0.000100
Epoch [234] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [234] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [234] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [234] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [234] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [234] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [234] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [234] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [234] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [234] Batch [100/261] Loss: 0.0002 LR: 0.000100
Epoch [234] Batch [110/261] Loss: 0.0001 LR: 0.000100
Epoch [234] Batch [120/261] Loss: 0.0050 LR: 0.000100
Epoch [234] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [234] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [234] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [234] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [234] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [234] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [234] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [234] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [234] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [234] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [234] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [234] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [234] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [234] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [235/350] - Time: 12.61s
Train Loss: 0.0048 | Val Loss: 0.0465
Train Acc: 0.9986 | Val Acc: 0.9888
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.979    Count 8: 1.000    Count 9: 0.989    Count 10: 0.909  
Epoch [235] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [235] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [235] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [235] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [235] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [235] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [235] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [235] Batch [70/261] Loss: 0.0003 LR: 0.000100
Epoch [235] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [235] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [235] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [235] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [235] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [235] Batch [130/261] Loss: 0.0001 LR: 0.000100
Epoch [235] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [235] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [235] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [235] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [235] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [235] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [235] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [235] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [235] Batch [220/261] Loss: 0.0002 LR: 0.000100
Epoch [235] Batch [230/261] Loss: 0.0001 LR: 0.000100
Epoch [235] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [235] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [235] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [236/350] - Time: 13.07s
Train Loss: 0.0087 | Val Loss: 0.0333
Train Acc: 0.9978 | Val Acc: 0.9916
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.958    Count 8: 0.970    Count 9: 0.978    Count 10: 1.000  
Epoch [236] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [236] Batch [10/261] Loss: 0.0001 LR: 0.000100
Epoch [236] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [236] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [236] Batch [40/261] Loss: 0.0001 LR: 0.000100
Epoch [236] Batch [50/261] Loss: 0.0001 LR: 0.000100
Epoch [236] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [236] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [236] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [236] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [236] Batch [100/261] Loss: 0.5179 LR: 0.000100
Epoch [236] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [236] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [236] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [236] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [236] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [236] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [236] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [236] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [236] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [236] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [236] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [236] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [236] Batch [230/261] Loss: 0.0001 LR: 0.000100
Epoch [236] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [236] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [236] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [237/350] - Time: 12.65s
Train Loss: 0.0092 | Val Loss: 0.0219
Train Acc: 0.9990 | Val Acc: 0.9953
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 0.980    Count 9: 0.978    Count 10: 1.000  
Epoch [237] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [237] Batch [10/261] Loss: 0.0003 LR: 0.000100
Epoch [237] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [237] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [237] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [237] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [237] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [237] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [237] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [237] Batch [90/261] Loss: 0.0026 LR: 0.000100
Epoch [237] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [237] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [237] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [237] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [237] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [237] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [237] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [237] Batch [170/261] Loss: 0.0436 LR: 0.000100
Epoch [237] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [237] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [237] Batch [200/261] Loss: 0.0002 LR: 0.000100
Epoch [237] Batch [210/261] Loss: 0.0001 LR: 0.000100
Epoch [237] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [237] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [237] Batch [240/261] Loss: 0.0002 LR: 0.000100
Epoch [237] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [237] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [238/350] - Time: 12.73s
Train Loss: 0.0121 | Val Loss: 0.3711
Train Acc: 0.9969 | Val Acc: 0.9376
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.990    Count 5: 0.990  
  Count 6: 0.959    Count 7: 0.958    Count 8: 0.909    Count 9: 0.900    Count 10: 0.568  
Epoch [238] Batch [0/261] Loss: 0.2889 LR: 0.000100
Epoch [238] Batch [10/261] Loss: 0.0001 LR: 0.000100
Epoch [238] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [238] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [238] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [238] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [238] Batch [60/261] Loss: 0.0084 LR: 0.000100
Epoch [238] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [238] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [238] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [238] Batch [100/261] Loss: 0.0015 LR: 0.000100
Epoch [238] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [238] Batch [120/261] Loss: 0.0001 LR: 0.000100
Epoch [238] Batch [130/261] Loss: 0.0001 LR: 0.000100
Epoch [238] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [238] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [238] Batch [160/261] Loss: 0.0001 LR: 0.000100
Epoch [238] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [238] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [238] Batch [190/261] Loss: 0.0034 LR: 0.000100
Epoch [238] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [238] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [238] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [238] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [238] Batch [240/261] Loss: 0.0179 LR: 0.000100
Epoch [238] Batch [250/261] Loss: 0.0044 LR: 0.000100
Epoch [238] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [239/350] - Time: 12.62s
Train Loss: 0.0306 | Val Loss: 0.1446
Train Acc: 0.9947 | Val Acc: 0.9777
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.979    Count 8: 0.960    Count 9: 0.800    Count 10: 1.000  
Epoch [239] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [239] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [239] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [239] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [239] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [239] Batch [50/261] Loss: 0.0001 LR: 0.000100
Epoch [239] Batch [60/261] Loss: 0.7003 LR: 0.000100
Epoch [239] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [239] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [239] Batch [90/261] Loss: 0.0001 LR: 0.000100
Epoch [239] Batch [100/261] Loss: 0.1942 LR: 0.000100
Epoch [239] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [239] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [239] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [239] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [239] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [239] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [239] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [239] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [239] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [239] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [239] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [239] Batch [220/261] Loss: 0.0002 LR: 0.000100
Epoch [239] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [239] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [239] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [239] Batch [260/261] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [240/350] - Time: 12.63s
Train Loss: 0.0144 | Val Loss: 0.0056
Train Acc: 0.9974 | Val Acc: 0.9991
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.990    Count 8: 1.000    Count 9: 1.000    Count 10: 1.000  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/single_image_checkpoint_epoch_239.pth
Epoch [240] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [240] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [240] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [240] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [240] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [240] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [240] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [240] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [240] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [240] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [240] Batch [100/261] Loss: 0.0007 LR: 0.000100
Epoch [240] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [240] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [240] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [240] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [240] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [240] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [240] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [240] Batch [180/261] Loss: 0.0081 LR: 0.000100
Epoch [240] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [240] Batch [200/261] Loss: 0.0011 LR: 0.000100
Epoch [240] Batch [210/261] Loss: 0.0006 LR: 0.000100
Epoch [240] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [240] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [240] Batch [240/261] Loss: 0.0009 LR: 0.000100
Epoch [240] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [240] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [241/350] - Time: 12.66s
Train Loss: 0.0140 | Val Loss: 0.0072
Train Acc: 0.9969 | Val Acc: 0.9972
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 0.980    Count 9: 0.989    Count 10: 1.000  
Epoch [241] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [241] Batch [10/261] Loss: 0.0002 LR: 0.000100
Epoch [241] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [241] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [241] Batch [40/261] Loss: 0.0004 LR: 0.000100
Epoch [241] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [241] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [241] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [241] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [241] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [241] Batch [100/261] Loss: 0.0001 LR: 0.000100
Epoch [241] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [241] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [241] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [241] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [241] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [241] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [241] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [241] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [241] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [241] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [241] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [241] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [241] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [241] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [241] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [241] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [242/350] - Time: 12.54s
Train Loss: 0.0079 | Val Loss: 0.0119
Train Acc: 0.9986 | Val Acc: 0.9972
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 0.980    Count 9: 0.989    Count 10: 1.000  
Epoch [242] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [242] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [242] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [242] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [242] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [242] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [242] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [242] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [242] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [242] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [242] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [242] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [242] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [242] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [242] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [242] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [242] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [242] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [242] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [242] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [242] Batch [200/261] Loss: 0.0001 LR: 0.000100
Epoch [242] Batch [210/261] Loss: 0.2596 LR: 0.000100
Epoch [242] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [242] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [242] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [242] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [242] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [243/350] - Time: 12.73s
Train Loss: 0.0186 | Val Loss: 0.0235
Train Acc: 0.9962 | Val Acc: 0.9963
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.990    Count 8: 1.000    Count 9: 0.967    Count 10: 1.000  
Epoch [243] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [243] Batch [10/261] Loss: 0.4359 LR: 0.000100
Epoch [243] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [243] Batch [30/261] Loss: 0.0374 LR: 0.000100
Epoch [243] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [243] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [243] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [243] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [243] Batch [80/261] Loss: 0.1963 LR: 0.000100
Epoch [243] Batch [90/261] Loss: 0.2826 LR: 0.000100
Epoch [243] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [243] Batch [110/261] Loss: 0.0005 LR: 0.000100
Epoch [243] Batch [120/261] Loss: 0.6509 LR: 0.000100
Epoch [243] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [243] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [243] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [243] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [243] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [243] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [243] Batch [190/261] Loss: 0.0317 LR: 0.000100
Epoch [243] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [243] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [243] Batch [220/261] Loss: 0.0001 LR: 0.000100
Epoch [243] Batch [230/261] Loss: 0.0003 LR: 0.000100
Epoch [243] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [243] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [243] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [244/350] - Time: 12.86s
Train Loss: 0.0415 | Val Loss: 0.0017
Train Acc: 0.9918 | Val Acc: 0.9991
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 1.000    Count 8: 1.000    Count 9: 1.000    Count 10: 1.000  
Epoch [244] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [244] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [244] Batch [20/261] Loss: 0.0001 LR: 0.000100
Epoch [244] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [244] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [244] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [244] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [244] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [244] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [244] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [244] Batch [100/261] Loss: 0.0572 LR: 0.000100
Epoch [244] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [244] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [244] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [244] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [244] Batch [150/261] Loss: 0.0662 LR: 0.000100
Epoch [244] Batch [160/261] Loss: 0.0002 LR: 0.000100
Epoch [244] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [244] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [244] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [244] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [244] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [244] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [244] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [244] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [244] Batch [250/261] Loss: 0.0289 LR: 0.000100
Epoch [244] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [245/350] - Time: 12.71s
Train Loss: 0.0131 | Val Loss: 0.0120
Train Acc: 0.9966 | Val Acc: 0.9972
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.980    Count 7: 1.000    Count 8: 0.990    Count 9: 1.000    Count 10: 1.000  
Epoch [245] Batch [0/261] Loss: 0.0001 LR: 0.000100
Epoch [245] Batch [10/261] Loss: 0.1160 LR: 0.000100
Epoch [245] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [245] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [245] Batch [40/261] Loss: 0.0001 LR: 0.000100
Epoch [245] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [245] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [245] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [245] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [245] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [245] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [245] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [245] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [245] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [245] Batch [140/261] Loss: 0.0028 LR: 0.000100
Epoch [245] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [245] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [245] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [245] Batch [180/261] Loss: 0.3873 LR: 0.000100
Epoch [245] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [245] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [245] Batch [210/261] Loss: 0.0001 LR: 0.000100
Epoch [245] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [245] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [245] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [245] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [245] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [246/350] - Time: 12.67s
Train Loss: 0.0053 | Val Loss: 0.0602
Train Acc: 0.9988 | Val Acc: 0.9926
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.990    Count 5: 1.000  
  Count 6: 0.990    Count 7: 0.979    Count 8: 0.980    Count 9: 0.989    Count 10: 1.000  
Epoch [246] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [246] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [246] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [246] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [246] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [246] Batch [50/261] Loss: 0.0001 LR: 0.000100
Epoch [246] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [246] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [246] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [246] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [246] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [246] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [246] Batch [120/261] Loss: 0.0001 LR: 0.000100
Epoch [246] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [246] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [246] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [246] Batch [160/261] Loss: 0.0013 LR: 0.000100
Epoch [246] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [246] Batch [180/261] Loss: 0.0096 LR: 0.000100
Epoch [246] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [246] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [246] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [246] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [246] Batch [230/261] Loss: 0.0265 LR: 0.000100
Epoch [246] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [246] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [246] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [247/350] - Time: 12.90s
Train Loss: 0.0050 | Val Loss: 0.0694
Train Acc: 0.9983 | Val Acc: 0.9907
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 0.970    Count 9: 0.922    Count 10: 1.000  
Epoch [247] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [247] Batch [10/261] Loss: 0.0397 LR: 0.000100
Epoch [247] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [247] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [247] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [247] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [247] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [247] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [247] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [247] Batch [90/261] Loss: 0.0120 LR: 0.000100
Epoch [247] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [247] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [247] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [247] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [247] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [247] Batch [150/261] Loss: 0.0001 LR: 0.000100
Epoch [247] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [247] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [247] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [247] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [247] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [247] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [247] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [247] Batch [230/261] Loss: 0.0005 LR: 0.000100
Epoch [247] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [247] Batch [250/261] Loss: 0.0001 LR: 0.000100
Epoch [247] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [248/350] - Time: 12.85s
Train Loss: 0.0259 | Val Loss: 0.0475
Train Acc: 0.9940 | Val Acc: 0.9926
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.979    Count 8: 0.980    Count 9: 0.956    Count 10: 1.000  
Epoch [248] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [248] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [248] Batch [20/261] Loss: 0.0001 LR: 0.000100
Epoch [248] Batch [30/261] Loss: 0.0031 LR: 0.000100
Epoch [248] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [248] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [248] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [248] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [248] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [248] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [248] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [248] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [248] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [248] Batch [130/261] Loss: 0.0003 LR: 0.000100
Epoch [248] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [248] Batch [150/261] Loss: 0.0014 LR: 0.000100
Epoch [248] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [248] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [248] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [248] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [248] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [248] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [248] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [248] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [248] Batch [240/261] Loss: 0.0001 LR: 0.000100
Epoch [248] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [248] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [249/350] - Time: 12.80s
Train Loss: 0.0080 | Val Loss: 0.0005
Train Acc: 0.9974 | Val Acc: 1.0000
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 1.000    Count 9: 1.000    Count 10: 1.000  
Epoch [249] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [249] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [249] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [249] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [249] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [249] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [249] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [249] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [249] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [249] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [249] Batch [100/261] Loss: 0.0021 LR: 0.000100
Epoch [249] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [249] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [249] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [249] Batch [140/261] Loss: 0.0098 LR: 0.000100
Epoch [249] Batch [150/261] Loss: 0.0028 LR: 0.000100
Epoch [249] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [249] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [249] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [249] Batch [190/261] Loss: 0.0001 LR: 0.000100
Epoch [249] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [249] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [249] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [249] Batch [230/261] Loss: 0.0001 LR: 0.000100
Epoch [249] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [249] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [249] Batch [260/261] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [250/350] - Time: 12.71s
Train Loss: 0.0134 | Val Loss: 0.1126
Train Acc: 0.9969 | Val Acc: 0.9870
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 0.970    Count 9: 0.889    Count 10: 1.000  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/single_image_checkpoint_epoch_249.pth
Epoch [250] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [250] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [250] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [250] Batch [30/261] Loss: 0.0001 LR: 0.000100
Epoch [250] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [250] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [250] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [250] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [250] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [250] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [250] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [250] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [250] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [250] Batch [130/261] Loss: 0.0001 LR: 0.000100
Epoch [250] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [250] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [250] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [250] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [250] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [250] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [250] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [250] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [250] Batch [220/261] Loss: 1.2497 LR: 0.000100
Epoch [250] Batch [230/261] Loss: 0.0069 LR: 0.000100
Epoch [250] Batch [240/261] Loss: 0.0001 LR: 0.000100
Epoch [250] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [250] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [251/350] - Time: 12.74s
Train Loss: 0.0106 | Val Loss: 0.0081
Train Acc: 0.9976 | Val Acc: 0.9981
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 0.980    Count 9: 1.000    Count 10: 1.000  
Epoch [251] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [251] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [251] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [251] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [251] Batch [40/261] Loss: 0.1468 LR: 0.000100
Epoch [251] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [251] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [251] Batch [70/261] Loss: 0.0002 LR: 0.000100
Epoch [251] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [251] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [251] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [251] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [251] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [251] Batch [130/261] Loss: 0.0001 LR: 0.000100
Epoch [251] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [251] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [251] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [251] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [251] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [251] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [251] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [251] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [251] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [251] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [251] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [251] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [251] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [252/350] - Time: 13.12s
Train Loss: 0.0024 | Val Loss: 0.0180
Train Acc: 0.9988 | Val Acc: 0.9963
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 1.000    Count 9: 0.967    Count 10: 1.000  
Epoch [252] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [252] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [252] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [252] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [252] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [252] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [252] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [252] Batch [70/261] Loss: 0.0002 LR: 0.000100
Epoch [252] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [252] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [252] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [252] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [252] Batch [120/261] Loss: 0.3701 LR: 0.000100
Epoch [252] Batch [130/261] Loss: 0.0001 LR: 0.000100
Epoch [252] Batch [140/261] Loss: 0.3628 LR: 0.000100
Epoch [252] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [252] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [252] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [252] Batch [180/261] Loss: 0.0001 LR: 0.000100
Epoch [252] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [252] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [252] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [252] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [252] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [252] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [252] Batch [250/261] Loss: 0.0018 LR: 0.000100
Epoch [252] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [253/350] - Time: 12.66s
Train Loss: 0.0266 | Val Loss: 0.0359
Train Acc: 0.9959 | Val Acc: 0.9935
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 0.979    Count 8: 0.980    Count 9: 0.978    Count 10: 1.000  
Epoch [253] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [253] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [253] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [253] Batch [30/261] Loss: 0.0002 LR: 0.000100
Epoch [253] Batch [40/261] Loss: 0.7088 LR: 0.000100
Epoch [253] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [253] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [253] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [253] Batch [80/261] Loss: 0.2626 LR: 0.000100
Epoch [253] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [253] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [253] Batch [110/261] Loss: 0.0001 LR: 0.000100
Epoch [253] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [253] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [253] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [253] Batch [150/261] Loss: 0.0001 LR: 0.000100
Epoch [253] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [253] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [253] Batch [180/261] Loss: 0.3005 LR: 0.000100
Epoch [253] Batch [190/261] Loss: 0.0270 LR: 0.000100
Epoch [253] Batch [200/261] Loss: 0.0001 LR: 0.000100
Epoch [253] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [253] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [253] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [253] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [253] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [253] Batch [260/261] Loss: 0.0001 LR: 0.000100

Epoch [254/350] - Time: 12.68s
Train Loss: 0.0182 | Val Loss: 0.0423
Train Acc: 0.9959 | Val Acc: 0.9935
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.981    Count 4: 0.981    Count 5: 1.000  
  Count 6: 0.990    Count 7: 0.990    Count 8: 1.000    Count 9: 1.000    Count 10: 0.989  
Epoch [254] Batch [0/261] Loss: 0.0001 LR: 0.000100
Epoch [254] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [254] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [254] Batch [30/261] Loss: 0.0001 LR: 0.000100
Epoch [254] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [254] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [254] Batch [60/261] Loss: 0.8562 LR: 0.000100
Epoch [254] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [254] Batch [80/261] Loss: 0.0026 LR: 0.000100
Epoch [254] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [254] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [254] Batch [110/261] Loss: 0.0927 LR: 0.000100
Epoch [254] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [254] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [254] Batch [140/261] Loss: 0.1626 LR: 0.000100
Epoch [254] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [254] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [254] Batch [170/261] Loss: 0.0146 LR: 0.000100
Epoch [254] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [254] Batch [190/261] Loss: 0.0013 LR: 0.000100
Epoch [254] Batch [200/261] Loss: 0.0555 LR: 0.000100
Epoch [254] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [254] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [254] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [254] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [254] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [254] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [255/350] - Time: 12.63s
Train Loss: 0.0293 | Val Loss: 0.0300
Train Acc: 0.9940 | Val Acc: 0.9963
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 1.000    Count 8: 0.990    Count 9: 1.000    Count 10: 0.989  
Epoch [255] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [255] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [255] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [255] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [255] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [255] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [255] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [255] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [255] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [255] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [255] Batch [100/261] Loss: 0.0001 LR: 0.000100
Epoch [255] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [255] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [255] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [255] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [255] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [255] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [255] Batch [170/261] Loss: 0.0001 LR: 0.000100
Epoch [255] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [255] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [255] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [255] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [255] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [255] Batch [230/261] Loss: 0.0017 LR: 0.000100
Epoch [255] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [255] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [255] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [256/350] - Time: 12.95s
Train Loss: 0.0058 | Val Loss: 0.0186
Train Acc: 0.9983 | Val Acc: 0.9963
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 1.000    Count 8: 1.000    Count 9: 0.989    Count 10: 0.989  
Epoch [256] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [256] Batch [10/261] Loss: 0.2598 LR: 0.000100
Epoch [256] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [256] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [256] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [256] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [256] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [256] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [256] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [256] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [256] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [256] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [256] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [256] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [256] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [256] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [256] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [256] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [256] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [256] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [256] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [256] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [256] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [256] Batch [230/261] Loss: 0.0008 LR: 0.000100
Epoch [256] Batch [240/261] Loss: 0.1939 LR: 0.000100
Epoch [256] Batch [250/261] Loss: 0.1273 LR: 0.000100
Epoch [256] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [257/350] - Time: 12.95s
Train Loss: 0.0111 | Val Loss: 0.2445
Train Acc: 0.9966 | Val Acc: 0.9618
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 0.939    Count 9: 0.767    Count 10: 0.841  
Epoch [257] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [257] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [257] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [257] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [257] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [257] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [257] Batch [60/261] Loss: 0.0007 LR: 0.000100
Epoch [257] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [257] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [257] Batch [90/261] Loss: 0.0142 LR: 0.000100
Epoch [257] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [257] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [257] Batch [120/261] Loss: 0.0157 LR: 0.000100
Epoch [257] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [257] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [257] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [257] Batch [160/261] Loss: 0.0001 LR: 0.000100
Epoch [257] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [257] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [257] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [257] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [257] Batch [210/261] Loss: 0.0001 LR: 0.000100
Epoch [257] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [257] Batch [230/261] Loss: 0.0001 LR: 0.000100
Epoch [257] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [257] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [257] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [258/350] - Time: 12.72s
Train Loss: 0.0117 | Val Loss: 0.0224
Train Acc: 0.9971 | Val Acc: 0.9972
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.990    Count 8: 1.000    Count 9: 0.978    Count 10: 1.000  
Epoch [258] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [258] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [258] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [258] Batch [30/261] Loss: 0.0148 LR: 0.000100
Epoch [258] Batch [40/261] Loss: 0.0020 LR: 0.000100
Epoch [258] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [258] Batch [60/261] Loss: 0.0114 LR: 0.000100
Epoch [258] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [258] Batch [80/261] Loss: 0.0001 LR: 0.000100
Epoch [258] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [258] Batch [100/261] Loss: 0.2481 LR: 0.000100
Epoch [258] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [258] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [258] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [258] Batch [140/261] Loss: 0.0007 LR: 0.000100
Epoch [258] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [258] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [258] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [258] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [258] Batch [190/261] Loss: 0.0727 LR: 0.000100
Epoch [258] Batch [200/261] Loss: 0.0014 LR: 0.000100
Epoch [258] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [258] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [258] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [258] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [258] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [258] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [259/350] - Time: 12.64s
Train Loss: 0.0032 | Val Loss: 0.0907
Train Acc: 0.9983 | Val Acc: 0.9814
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 1.000    Count 9: 1.000    Count 10: 0.773  
Epoch [259] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [259] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [259] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [259] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [259] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [259] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [259] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [259] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [259] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [259] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [259] Batch [100/261] Loss: 0.0009 LR: 0.000100
Epoch [259] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [259] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [259] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [259] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [259] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [259] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [259] Batch [170/261] Loss: 0.1025 LR: 0.000100
Epoch [259] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [259] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [259] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [259] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [259] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [259] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [259] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [259] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [259] Batch [260/261] Loss: 0.0003 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [260/350] - Time: 13.28s
Train Loss: 0.0238 | Val Loss: 0.1135
Train Acc: 0.9954 | Val Acc: 0.9767
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 1.000    Count 8: 0.960    Count 9: 0.967    Count 10: 0.818  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/single_image_checkpoint_epoch_259.pth
Epoch [260] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [260] Batch [10/261] Loss: 0.0001 LR: 0.000100
Epoch [260] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [260] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [260] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [260] Batch [50/261] Loss: 0.0085 LR: 0.000100
Epoch [260] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [260] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [260] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [260] Batch [90/261] Loss: 0.0001 LR: 0.000100
Epoch [260] Batch [100/261] Loss: 0.2225 LR: 0.000100
Epoch [260] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [260] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [260] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [260] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [260] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [260] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [260] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [260] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [260] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [260] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [260] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [260] Batch [220/261] Loss: 0.0026 LR: 0.000100
Epoch [260] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [260] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [260] Batch [250/261] Loss: 0.0008 LR: 0.000100
Epoch [260] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [261/350] - Time: 13.03s
Train Loss: 0.0080 | Val Loss: 0.0886
Train Acc: 0.9974 | Val Acc: 0.9851
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.990  
  Count 6: 0.990    Count 7: 0.917    Count 8: 0.990    Count 9: 0.956    Count 10: 0.989  
Epoch [261] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [261] Batch [10/261] Loss: 0.4463 LR: 0.000100
Epoch [261] Batch [20/261] Loss: 0.0008 LR: 0.000100
Epoch [261] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [261] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [261] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [261] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [261] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [261] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [261] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [261] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [261] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [261] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [261] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [261] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [261] Batch [150/261] Loss: 0.0007 LR: 0.000100
Epoch [261] Batch [160/261] Loss: 0.0097 LR: 0.000100
Epoch [261] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [261] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [261] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [261] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [261] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [261] Batch [220/261] Loss: 0.0001 LR: 0.000100
Epoch [261] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [261] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [261] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [261] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [262/350] - Time: 12.74s
Train Loss: 0.0216 | Val Loss: 0.0299
Train Acc: 0.9952 | Val Acc: 0.9935
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 0.970    Count 9: 0.967    Count 10: 1.000  
Epoch [262] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [262] Batch [10/261] Loss: 0.4299 LR: 0.000100
Epoch [262] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [262] Batch [30/261] Loss: 0.0106 LR: 0.000100
Epoch [262] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [262] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [262] Batch [60/261] Loss: 0.0024 LR: 0.000100
Epoch [262] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [262] Batch [80/261] Loss: 0.0002 LR: 0.000100
Epoch [262] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [262] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [262] Batch [110/261] Loss: 0.0055 LR: 0.000100
Epoch [262] Batch [120/261] Loss: 0.6075 LR: 0.000100
Epoch [262] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [262] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [262] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [262] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [262] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [262] Batch [180/261] Loss: 2.3137 LR: 0.000100
Epoch [262] Batch [190/261] Loss: 0.0009 LR: 0.000100
Epoch [262] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [262] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [262] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [262] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [262] Batch [240/261] Loss: 0.1279 LR: 0.000100
Epoch [262] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [262] Batch [260/261] Loss: 42.7623 LR: 0.000100

Epoch [263/350] - Time: 12.57s
Train Loss: 0.0639 | Val Loss: 0.2251
Train Acc: 0.9918 | Val Acc: 0.9683
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 1.000    Count 9: 0.989    Count 10: 0.625  
Epoch [263] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [263] Batch [10/261] Loss: 0.0032 LR: 0.000100
Epoch [263] Batch [20/261] Loss: 0.6182 LR: 0.000100
Epoch [263] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [263] Batch [40/261] Loss: 0.0001 LR: 0.000100
Epoch [263] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [263] Batch [60/261] Loss: 0.0002 LR: 0.000100
Epoch [263] Batch [70/261] Loss: 0.0001 LR: 0.000100
Epoch [263] Batch [80/261] Loss: 0.0671 LR: 0.000100
Epoch [263] Batch [90/261] Loss: 0.0001 LR: 0.000100
Epoch [263] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [263] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [263] Batch [120/261] Loss: 0.9028 LR: 0.000100
Epoch [263] Batch [130/261] Loss: 0.0009 LR: 0.000100
Epoch [263] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [263] Batch [150/261] Loss: 0.0001 LR: 0.000100
Epoch [263] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [263] Batch [170/261] Loss: 0.0001 LR: 0.000100
Epoch [263] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [263] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [263] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [263] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [263] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [263] Batch [230/261] Loss: 0.0003 LR: 0.000100
Epoch [263] Batch [240/261] Loss: 0.0128 LR: 0.000100
Epoch [263] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [263] Batch [260/261] Loss: 8.7759 LR: 0.000100

Epoch [264/350] - Time: 12.89s
Train Loss: 0.0235 | Val Loss: 0.0547
Train Acc: 0.9945 | Val Acc: 0.9926
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 0.960    Count 9: 0.967    Count 10: 1.000  
Epoch [264] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [264] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [264] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [264] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [264] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [264] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [264] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [264] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [264] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [264] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [264] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [264] Batch [110/261] Loss: 0.0071 LR: 0.000100
Epoch [264] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [264] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [264] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [264] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [264] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [264] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [264] Batch [180/261] Loss: 0.0003 LR: 0.000100
Epoch [264] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [264] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [264] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [264] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [264] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [264] Batch [240/261] Loss: 0.0052 LR: 0.000100
Epoch [264] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [264] Batch [260/261] Loss: 0.0003 LR: 0.000100

Epoch [265/350] - Time: 12.77s
Train Loss: 0.0085 | Val Loss: 0.0689
Train Acc: 0.9988 | Val Acc: 0.9870
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.927    Count 8: 0.980    Count 9: 0.944    Count 10: 1.000  
Epoch [265] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [265] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [265] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [265] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [265] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [265] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [265] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [265] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [265] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [265] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [265] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [265] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [265] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [265] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [265] Batch [140/261] Loss: 0.0008 LR: 0.000100
Epoch [265] Batch [150/261] Loss: 0.0020 LR: 0.000100
Epoch [265] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [265] Batch [170/261] Loss: 0.0014 LR: 0.000100
Epoch [265] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [265] Batch [190/261] Loss: 0.0002 LR: 0.000100
Epoch [265] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [265] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [265] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [265] Batch [230/261] Loss: 0.0001 LR: 0.000100
Epoch [265] Batch [240/261] Loss: 0.0001 LR: 0.000100
Epoch [265] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [265] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [266/350] - Time: 12.73s
Train Loss: 0.0038 | Val Loss: 0.0431
Train Acc: 0.9993 | Val Acc: 0.9935
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.990    Count 8: 0.990    Count 9: 0.956    Count 10: 1.000  
Epoch [266] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [266] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [266] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [266] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [266] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [266] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [266] Batch [60/261] Loss: 0.0133 LR: 0.000100
Epoch [266] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [266] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [266] Batch [90/261] Loss: 0.0021 LR: 0.000100
Epoch [266] Batch [100/261] Loss: 0.0002 LR: 0.000100
Epoch [266] Batch [110/261] Loss: 0.0167 LR: 0.000100
Epoch [266] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [266] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [266] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [266] Batch [150/261] Loss: 0.0002 LR: 0.000100
Epoch [266] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [266] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [266] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [266] Batch [190/261] Loss: 0.0106 LR: 0.000100
Epoch [266] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [266] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [266] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [266] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [266] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [266] Batch [250/261] Loss: 0.0187 LR: 0.000100
Epoch [266] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [267/350] - Time: 12.94s
Train Loss: 0.0089 | Val Loss: 0.2793
Train Acc: 0.9981 | Val Acc: 0.9544
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 0.971  
  Count 6: 0.959    Count 7: 0.927    Count 8: 0.919    Count 9: 0.711    Count 10: 1.000  
Epoch [267] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [267] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [267] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [267] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [267] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [267] Batch [50/261] Loss: 0.0001 LR: 0.000100
Epoch [267] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [267] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [267] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [267] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [267] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [267] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [267] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [267] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [267] Batch [140/261] Loss: 0.0005 LR: 0.000100
Epoch [267] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [267] Batch [160/261] Loss: 0.4264 LR: 0.000100
Epoch [267] Batch [170/261] Loss: 0.0005 LR: 0.000100
Epoch [267] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [267] Batch [190/261] Loss: 0.7245 LR: 0.000100
Epoch [267] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [267] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [267] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [267] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [267] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [267] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [267] Batch [260/261] Loss: 0.9421 LR: 0.000100

Epoch [268/350] - Time: 13.08s
Train Loss: 0.0158 | Val Loss: 0.0222
Train Acc: 0.9971 | Val Acc: 0.9944
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 0.990    Count 9: 1.000    Count 10: 0.943  
Epoch [268] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [268] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [268] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [268] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [268] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [268] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [268] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [268] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [268] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [268] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [268] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [268] Batch [110/261] Loss: 0.0001 LR: 0.000100
Epoch [268] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [268] Batch [130/261] Loss: 0.0850 LR: 0.000100
Epoch [268] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [268] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [268] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [268] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [268] Batch [180/261] Loss: 0.0368 LR: 0.000100
Epoch [268] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [268] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [268] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [268] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [268] Batch [230/261] Loss: 0.0054 LR: 0.000100
Epoch [268] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [268] Batch [250/261] Loss: 0.0125 LR: 0.000100
Epoch [268] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [269/350] - Time: 12.92s
Train Loss: 0.0108 | Val Loss: 0.6629
Train Acc: 0.9964 | Val Acc: 0.9283
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 1.000    Count 9: 0.878    Count 10: 0.250  
Epoch [269] Batch [0/261] Loss: 0.0049 LR: 0.000100
Epoch [269] Batch [10/261] Loss: 0.0029 LR: 0.000100
Epoch [269] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [269] Batch [30/261] Loss: 0.0027 LR: 0.000100
Epoch [269] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [269] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [269] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [269] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [269] Batch [80/261] Loss: 0.0005 LR: 0.000100
Epoch [269] Batch [90/261] Loss: 0.0001 LR: 0.000100
Epoch [269] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [269] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [269] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [269] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [269] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [269] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [269] Batch [160/261] Loss: 0.0164 LR: 0.000100
Epoch [269] Batch [170/261] Loss: 0.0006 LR: 0.000100
Epoch [269] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [269] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [269] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [269] Batch [210/261] Loss: 0.0001 LR: 0.000100
Epoch [269] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [269] Batch [230/261] Loss: 0.0001 LR: 0.000100
Epoch [269] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [269] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [269] Batch [260/261] Loss: 0.0001 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [270/350] - Time: 13.00s
Train Loss: 0.0174 | Val Loss: 0.0804
Train Acc: 0.9966 | Val Acc: 0.9870
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.990    Count 5: 1.000  
  Count 6: 0.980    Count 7: 0.969    Count 8: 0.990    Count 9: 0.978    Count 10: 0.955  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/single_image_checkpoint_epoch_269.pth
Epoch [270] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [270] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [270] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [270] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [270] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [270] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [270] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [270] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [270] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [270] Batch [90/261] Loss: 0.0583 LR: 0.000100
Epoch [270] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [270] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [270] Batch [120/261] Loss: 0.0014 LR: 0.000100
Epoch [270] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [270] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [270] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [270] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [270] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [270] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [270] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [270] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [270] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [270] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [270] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [270] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [270] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [270] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [271/350] - Time: 12.81s
Train Loss: 0.0153 | Val Loss: 0.0028
Train Acc: 0.9966 | Val Acc: 0.9991
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 1.000    Count 9: 1.000    Count 10: 0.989  
Epoch [271] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [271] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [271] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [271] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [271] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [271] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [271] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [271] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [271] Batch [80/261] Loss: 0.7274 LR: 0.000100
Epoch [271] Batch [90/261] Loss: 0.0001 LR: 0.000100
Epoch [271] Batch [100/261] Loss: 0.0004 LR: 0.000100
Epoch [271] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [271] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [271] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [271] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [271] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [271] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [271] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [271] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [271] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [271] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [271] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [271] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [271] Batch [230/261] Loss: 0.0005 LR: 0.000100
Epoch [271] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [271] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [271] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [272/350] - Time: 12.79s
Train Loss: 0.0033 | Val Loss: 0.1422
Train Acc: 0.9995 | Val Acc: 0.9786
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.990    Count 8: 0.980    Count 9: 0.778    Count 10: 1.000  
Epoch [272] Batch [0/261] Loss: 0.0002 LR: 0.000100
Epoch [272] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [272] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [272] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [272] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [272] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [272] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [272] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [272] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [272] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [272] Batch [100/261] Loss: 0.0001 LR: 0.000100
Epoch [272] Batch [110/261] Loss: 0.3377 LR: 0.000100
Epoch [272] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [272] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [272] Batch [140/261] Loss: 0.0013 LR: 0.000100
Epoch [272] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [272] Batch [160/261] Loss: 0.0022 LR: 0.000100
Epoch [272] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [272] Batch [180/261] Loss: 0.6370 LR: 0.000100
Epoch [272] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [272] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [272] Batch [210/261] Loss: 0.1105 LR: 0.000100
Epoch [272] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [272] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [272] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [272] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [272] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [273/350] - Time: 12.99s
Train Loss: 0.0229 | Val Loss: 0.1103
Train Acc: 0.9952 | Val Acc: 0.9851
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 0.958    Count 8: 0.939    Count 9: 0.944    Count 10: 1.000  
Epoch [273] Batch [0/261] Loss: 0.1232 LR: 0.000100
Epoch [273] Batch [10/261] Loss: 0.0008 LR: 0.000100
Epoch [273] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [273] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [273] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [273] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [273] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [273] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [273] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [273] Batch [90/261] Loss: 0.1579 LR: 0.000100
Epoch [273] Batch [100/261] Loss: 0.0001 LR: 0.000100
Epoch [273] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [273] Batch [120/261] Loss: 0.0003 LR: 0.000100
Epoch [273] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [273] Batch [140/261] Loss: 0.0001 LR: 0.000100
Epoch [273] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [273] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [273] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [273] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [273] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [273] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [273] Batch [210/261] Loss: 0.0232 LR: 0.000100
Epoch [273] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [273] Batch [230/261] Loss: 0.0001 LR: 0.000100
Epoch [273] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [273] Batch [250/261] Loss: 0.0453 LR: 0.000100
Epoch [273] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [274/350] - Time: 12.71s
Train Loss: 0.0209 | Val Loss: 0.0641
Train Acc: 0.9954 | Val Acc: 0.9907
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.990    Count 5: 1.000  
  Count 6: 0.990    Count 7: 1.000    Count 8: 1.000    Count 9: 1.000    Count 10: 0.920  
Epoch [274] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [274] Batch [10/261] Loss: 0.0002 LR: 0.000100
Epoch [274] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [274] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [274] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [274] Batch [50/261] Loss: 0.0016 LR: 0.000100
Epoch [274] Batch [60/261] Loss: 0.0064 LR: 0.000100
Epoch [274] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [274] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [274] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [274] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [274] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [274] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [274] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [274] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [274] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [274] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [274] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [274] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [274] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [274] Batch [200/261] Loss: 0.0001 LR: 0.000100
Epoch [274] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [274] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [274] Batch [230/261] Loss: 0.0030 LR: 0.000100
Epoch [274] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [274] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [274] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [275/350] - Time: 12.85s
Train Loss: 0.0059 | Val Loss: 0.0151
Train Acc: 0.9988 | Val Acc: 0.9972
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 0.990    Count 9: 0.989    Count 10: 1.000  
Epoch [275] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [275] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [275] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [275] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [275] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [275] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [275] Batch [60/261] Loss: 0.0005 LR: 0.000100
Epoch [275] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [275] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [275] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [275] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [275] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [275] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [275] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [275] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [275] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [275] Batch [160/261] Loss: 0.0002 LR: 0.000100
Epoch [275] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [275] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [275] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [275] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [275] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [275] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [275] Batch [230/261] Loss: 0.0594 LR: 0.000100
Epoch [275] Batch [240/261] Loss: 0.0003 LR: 0.000100
Epoch [275] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [275] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [276/350] - Time: 12.94s
Train Loss: 0.0044 | Val Loss: 0.0582
Train Acc: 0.9986 | Val Acc: 0.9916
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.990    Count 5: 1.000  
  Count 6: 0.990    Count 7: 0.979    Count 8: 0.980    Count 9: 0.978    Count 10: 1.000  
Epoch [276] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [276] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [276] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [276] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [276] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [276] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [276] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [276] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [276] Batch [80/261] Loss: 0.0001 LR: 0.000100
Epoch [276] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [276] Batch [100/261] Loss: 0.0001 LR: 0.000100
Epoch [276] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [276] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [276] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [276] Batch [140/261] Loss: 0.0001 LR: 0.000100
Epoch [276] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [276] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [276] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [276] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [276] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [276] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [276] Batch [210/261] Loss: 0.2116 LR: 0.000100
Epoch [276] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [276] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [276] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [276] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [276] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [277/350] - Time: 12.85s
Train Loss: 0.0035 | Val Loss: 0.0145
Train Acc: 0.9981 | Val Acc: 0.9972
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 1.000    Count 9: 0.989    Count 10: 0.989  
Epoch [277] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [277] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [277] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [277] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [277] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [277] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [277] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [277] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [277] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [277] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [277] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [277] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [277] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [277] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [277] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [277] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [277] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [277] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [277] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [277] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [277] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [277] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [277] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [277] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [277] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [277] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [277] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [278/350] - Time: 12.60s
Train Loss: 0.0002 | Val Loss: 0.0080
Train Acc: 1.0000 | Val Acc: 0.9981
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 0.990    Count 9: 1.000    Count 10: 1.000  
Epoch [278] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [278] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [278] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [278] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [278] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [278] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [278] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [278] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [278] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [278] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [278] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [278] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [278] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [278] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [278] Batch [140/261] Loss: 0.0023 LR: 0.000100
Epoch [278] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [278] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [278] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [278] Batch [180/261] Loss: 0.0003 LR: 0.000100
Epoch [278] Batch [190/261] Loss: 0.0001 LR: 0.000100
Epoch [278] Batch [200/261] Loss: 0.0001 LR: 0.000100
Epoch [278] Batch [210/261] Loss: 0.0479 LR: 0.000100
Epoch [278] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [278] Batch [230/261] Loss: 0.4775 LR: 0.000100
Epoch [278] Batch [240/261] Loss: 0.0795 LR: 0.000100
Epoch [278] Batch [250/261] Loss: 0.0417 LR: 0.000100
Epoch [278] Batch [260/261] Loss: 0.0012 LR: 0.000100

Epoch [279/350] - Time: 12.59s
Train Loss: 0.0296 | Val Loss: 0.4614
Train Acc: 0.9935 | Val Acc: 0.9292
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.972    Count 4: 0.952    Count 5: 0.961  
  Count 6: 0.949    Count 7: 0.938    Count 8: 0.798    Count 9: 0.978    Count 10: 0.648  
Epoch [279] Batch [0/261] Loss: 0.7140 LR: 0.000100
Epoch [279] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [279] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [279] Batch [30/261] Loss: 0.0002 LR: 0.000100
Epoch [279] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [279] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [279] Batch [60/261] Loss: 0.0004 LR: 0.000100
Epoch [279] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [279] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [279] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [279] Batch [100/261] Loss: 0.0001 LR: 0.000100
Epoch [279] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [279] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [279] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [279] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [279] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [279] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [279] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [279] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [279] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [279] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [279] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [279] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [279] Batch [230/261] Loss: 0.0006 LR: 0.000100
Epoch [279] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [279] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [279] Batch [260/261] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [280/350] - Time: 12.58s
Train Loss: 0.0128 | Val Loss: 0.7274
Train Acc: 0.9971 | Val Acc: 0.9143
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.888    Count 7: 0.833    Count 8: 0.808    Count 9: 0.489    Count 10: 1.000  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/single_image_checkpoint_epoch_279.pth
Epoch [280] Batch [0/261] Loss: 0.3439 LR: 0.000100
Epoch [280] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [280] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [280] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [280] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [280] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [280] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [280] Batch [70/261] Loss: 0.0002 LR: 0.000100
Epoch [280] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [280] Batch [90/261] Loss: 0.1349 LR: 0.000100
Epoch [280] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [280] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [280] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [280] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [280] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [280] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [280] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [280] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [280] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [280] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [280] Batch [200/261] Loss: 0.0685 LR: 0.000100
Epoch [280] Batch [210/261] Loss: 0.0011 LR: 0.000100
Epoch [280] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [280] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [280] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [280] Batch [250/261] Loss: 0.0001 LR: 0.000100
Epoch [280] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [281/350] - Time: 12.66s
Train Loss: 0.0162 | Val Loss: 0.0284
Train Acc: 0.9957 | Val Acc: 0.9916
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 1.000    Count 9: 1.000    Count 10: 0.909  
Epoch [281] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [281] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [281] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [281] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [281] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [281] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [281] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [281] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [281] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [281] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [281] Batch [100/261] Loss: 0.0357 LR: 0.000100
Epoch [281] Batch [110/261] Loss: 0.3625 LR: 0.000100
Epoch [281] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [281] Batch [130/261] Loss: 0.0001 LR: 0.000100
Epoch [281] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [281] Batch [150/261] Loss: 0.0772 LR: 0.000100
Epoch [281] Batch [160/261] Loss: 0.0001 LR: 0.000100
Epoch [281] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [281] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [281] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [281] Batch [200/261] Loss: 0.0011 LR: 0.000100
Epoch [281] Batch [210/261] Loss: 0.0001 LR: 0.000100
Epoch [281] Batch [220/261] Loss: 0.4443 LR: 0.000100
Epoch [281] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [281] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [281] Batch [250/261] Loss: 0.0021 LR: 0.000100
Epoch [281] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [282/350] - Time: 12.74s
Train Loss: 0.0393 | Val Loss: 0.0060
Train Acc: 0.9930 | Val Acc: 0.9972
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 1.000    Count 9: 0.989    Count 10: 0.989  
Epoch [282] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [282] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [282] Batch [20/261] Loss: 0.0732 LR: 0.000100
Epoch [282] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [282] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [282] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [282] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [282] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [282] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [282] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [282] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [282] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [282] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [282] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [282] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [282] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [282] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [282] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [282] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [282] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [282] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [282] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [282] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [282] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [282] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [282] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [282] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [283/350] - Time: 12.68s
Train Loss: 0.0102 | Val Loss: 0.0561
Train Acc: 0.9981 | Val Acc: 0.9907
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.979    Count 8: 0.949    Count 9: 0.967    Count 10: 1.000  
Epoch [283] Batch [0/261] Loss: 0.0001 LR: 0.000100
Epoch [283] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [283] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [283] Batch [30/261] Loss: 0.0011 LR: 0.000100
Epoch [283] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [283] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [283] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [283] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [283] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [283] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [283] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [283] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [283] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [283] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [283] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [283] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [283] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [283] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [283] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [283] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [283] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [283] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [283] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [283] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [283] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [283] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [283] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [284/350] - Time: 13.07s
Train Loss: 0.0002 | Val Loss: 0.0057
Train Acc: 1.0000 | Val Acc: 0.9991
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 0.990    Count 9: 1.000    Count 10: 1.000  
Epoch [284] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [284] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [284] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [284] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [284] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [284] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [284] Batch [60/261] Loss: 0.0028 LR: 0.000100
Epoch [284] Batch [70/261] Loss: 0.0001 LR: 0.000100
Epoch [284] Batch [80/261] Loss: 0.0001 LR: 0.000100
Epoch [284] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [284] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [284] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [284] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [284] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [284] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [284] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [284] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [284] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [284] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [284] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [284] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [284] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [284] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [284] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [284] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [284] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [284] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [285/350] - Time: 12.88s
Train Loss: 0.0080 | Val Loss: 0.4658
Train Acc: 0.9988 | Val Acc: 0.9376
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 0.979    Count 8: 0.909    Count 9: 0.889    Count 10: 0.500  
Epoch [285] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [285] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [285] Batch [20/261] Loss: 0.0003 LR: 0.000100
Epoch [285] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [285] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [285] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [285] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [285] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [285] Batch [80/261] Loss: 0.0002 LR: 0.000100
Epoch [285] Batch [90/261] Loss: 0.0004 LR: 0.000100
Epoch [285] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [285] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [285] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [285] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [285] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [285] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [285] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [285] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [285] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [285] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [285] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [285] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [285] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [285] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [285] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [285] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [285] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [286/350] - Time: 12.68s
Train Loss: 0.0071 | Val Loss: 0.1014
Train Acc: 0.9981 | Val Acc: 0.9842
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.906    Count 8: 0.970    Count 9: 0.944    Count 10: 1.000  
Epoch [286] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [286] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [286] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [286] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [286] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [286] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [286] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [286] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [286] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [286] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [286] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [286] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [286] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [286] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [286] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [286] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [286] Batch [160/261] Loss: 0.0001 LR: 0.000100
Epoch [286] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [286] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [286] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [286] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [286] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [286] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [286] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [286] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [286] Batch [250/261] Loss: 0.0002 LR: 0.000100
Epoch [286] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [287/350] - Time: 12.73s
Train Loss: 0.0141 | Val Loss: 0.0507
Train Acc: 0.9974 | Val Acc: 0.9916
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 0.980    Count 9: 0.944    Count 10: 0.989  
Epoch [287] Batch [0/261] Loss: 0.0017 LR: 0.000100
Epoch [287] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [287] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [287] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [287] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [287] Batch [50/261] Loss: 0.0001 LR: 0.000100
Epoch [287] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [287] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [287] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [287] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [287] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [287] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [287] Batch [120/261] Loss: 0.0006 LR: 0.000100
Epoch [287] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [287] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [287] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [287] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [287] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [287] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [287] Batch [190/261] Loss: 0.0003 LR: 0.000100
Epoch [287] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [287] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [287] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [287] Batch [230/261] Loss: 0.0587 LR: 0.000100
Epoch [287] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [287] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [287] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [288/350] - Time: 12.67s
Train Loss: 0.0216 | Val Loss: 0.0796
Train Acc: 0.9959 | Val Acc: 0.9888
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 0.969    Count 8: 0.990    Count 9: 0.989    Count 10: 0.932  
Epoch [288] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [288] Batch [10/261] Loss: 0.0002 LR: 0.000100
Epoch [288] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [288] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [288] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [288] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [288] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [288] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [288] Batch [80/261] Loss: 0.0001 LR: 0.000100
Epoch [288] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [288] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [288] Batch [110/261] Loss: 0.0001 LR: 0.000100
Epoch [288] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [288] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [288] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [288] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [288] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [288] Batch [170/261] Loss: 0.0001 LR: 0.000100
Epoch [288] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [288] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [288] Batch [200/261] Loss: 0.0240 LR: 0.000100
Epoch [288] Batch [210/261] Loss: 0.0574 LR: 0.000100
Epoch [288] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [288] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [288] Batch [240/261] Loss: 0.0005 LR: 0.000100
Epoch [288] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [288] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [289/350] - Time: 12.76s
Train Loss: 0.0206 | Val Loss: 0.0032
Train Acc: 0.9959 | Val Acc: 0.9981
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 0.990    Count 9: 1.000    Count 10: 0.989  
Epoch [289] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [289] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [289] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [289] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [289] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [289] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [289] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [289] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [289] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [289] Batch [90/261] Loss: 0.0001 LR: 0.000100
Epoch [289] Batch [100/261] Loss: 0.0306 LR: 0.000100
Epoch [289] Batch [110/261] Loss: 0.0003 LR: 0.000100
Epoch [289] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [289] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [289] Batch [140/261] Loss: 0.0482 LR: 0.000100
Epoch [289] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [289] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [289] Batch [170/261] Loss: 0.0708 LR: 0.000100
Epoch [289] Batch [180/261] Loss: 0.0036 LR: 0.000100
Epoch [289] Batch [190/261] Loss: 0.0500 LR: 0.000100
Epoch [289] Batch [200/261] Loss: 0.0003 LR: 0.000100
Epoch [289] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [289] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [289] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [289] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [289] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [289] Batch [260/261] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [290/350] - Time: 12.86s
Train Loss: 0.0261 | Val Loss: 1.0634
Train Acc: 0.9942 | Val Acc: 0.8706
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.981    Count 4: 0.990    Count 5: 1.000  
  Count 6: 0.990    Count 7: 0.844    Count 8: 0.848    Count 9: 0.678    Count 10: 0.136  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/single_image_checkpoint_epoch_289.pth
Epoch [290] Batch [0/261] Loss: 0.1014 LR: 0.000100
Epoch [290] Batch [10/261] Loss: 0.0031 LR: 0.000100
Epoch [290] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [290] Batch [30/261] Loss: 0.0001 LR: 0.000100
Epoch [290] Batch [40/261] Loss: 0.0038 LR: 0.000100
Epoch [290] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [290] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [290] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [290] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [290] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [290] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [290] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [290] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [290] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [290] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [290] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [290] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [290] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [290] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [290] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [290] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [290] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [290] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [290] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [290] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [290] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [290] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [291/350] - Time: 12.69s
Train Loss: 0.0061 | Val Loss: 0.0161
Train Acc: 0.9978 | Val Acc: 0.9972
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 1.000    Count 9: 1.000    Count 10: 0.977  
Epoch [291] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [291] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [291] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [291] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [291] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [291] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [291] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [291] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [291] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [291] Batch [90/261] Loss: 0.0001 LR: 0.000100
Epoch [291] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [291] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [291] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [291] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [291] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [291] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [291] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [291] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [291] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [291] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [291] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [291] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [291] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [291] Batch [230/261] Loss: 0.0002 LR: 0.000100
Epoch [291] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [291] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [291] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [292/350] - Time: 13.04s
Train Loss: 0.0000 | Val Loss: 0.0557
Train Acc: 1.0000 | Val Acc: 0.9860
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 1.000    Count 8: 0.990    Count 9: 0.989    Count 10: 0.875  
Epoch [292] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [292] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [292] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [292] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [292] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [292] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [292] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [292] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [292] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [292] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [292] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [292] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [292] Batch [120/261] Loss: 0.0591 LR: 0.000100
Epoch [292] Batch [130/261] Loss: 0.0157 LR: 0.000100
Epoch [292] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [292] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [292] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [292] Batch [170/261] Loss: 0.0037 LR: 0.000100
Epoch [292] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [292] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [292] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [292] Batch [210/261] Loss: 0.0003 LR: 0.000100
Epoch [292] Batch [220/261] Loss: 0.3366 LR: 0.000100
Epoch [292] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [292] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [292] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [292] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [293/350] - Time: 12.82s
Train Loss: 0.0098 | Val Loss: 0.0464
Train Acc: 0.9971 | Val Acc: 0.9860
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.969    Count 8: 0.990    Count 9: 1.000    Count 10: 0.875  
Epoch [293] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [293] Batch [10/261] Loss: 0.0002 LR: 0.000100
Epoch [293] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [293] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [293] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [293] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [293] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [293] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [293] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [293] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [293] Batch [100/261] Loss: 0.0021 LR: 0.000100
Epoch [293] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [293] Batch [120/261] Loss: 0.0001 LR: 0.000100
Epoch [293] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [293] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [293] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [293] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [293] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [293] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [293] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [293] Batch [200/261] Loss: 0.0025 LR: 0.000100
Epoch [293] Batch [210/261] Loss: 0.0001 LR: 0.000100
Epoch [293] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [293] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [293] Batch [240/261] Loss: 1.0799 LR: 0.000100
Epoch [293] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [293] Batch [260/261] Loss: 1.1128 LR: 0.000100

Epoch [294/350] - Time: 13.10s
Train Loss: 0.0173 | Val Loss: 0.0716
Train Acc: 0.9966 | Val Acc: 0.9879
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.980    Count 7: 1.000    Count 8: 1.000    Count 9: 1.000    Count 10: 0.886  
Epoch [294] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [294] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [294] Batch [20/261] Loss: 0.0004 LR: 0.000100
Epoch [294] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [294] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [294] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [294] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [294] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [294] Batch [80/261] Loss: 0.0001 LR: 0.000100
Epoch [294] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [294] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [294] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [294] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [294] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [294] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [294] Batch [150/261] Loss: 0.0001 LR: 0.000100
Epoch [294] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [294] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [294] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [294] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [294] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [294] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [294] Batch [220/261] Loss: 0.3383 LR: 0.000100
Epoch [294] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [294] Batch [240/261] Loss: 0.4451 LR: 0.000100
Epoch [294] Batch [250/261] Loss: 0.0015 LR: 0.000100
Epoch [294] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [295/350] - Time: 12.81s
Train Loss: 0.0140 | Val Loss: 0.0626
Train Acc: 0.9974 | Val Acc: 0.9907
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 1.000    Count 9: 1.000    Count 10: 0.898  
Epoch [295] Batch [0/261] Loss: 0.0002 LR: 0.000100
Epoch [295] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [295] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [295] Batch [30/261] Loss: 0.0714 LR: 0.000100
Epoch [295] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [295] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [295] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [295] Batch [70/261] Loss: 0.0007 LR: 0.000100
Epoch [295] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [295] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [295] Batch [100/261] Loss: 0.0006 LR: 0.000100
Epoch [295] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [295] Batch [120/261] Loss: 0.3440 LR: 0.000100
Epoch [295] Batch [130/261] Loss: 0.4106 LR: 0.000100
Epoch [295] Batch [140/261] Loss: 0.0001 LR: 0.000100
Epoch [295] Batch [150/261] Loss: 0.1084 LR: 0.000100
Epoch [295] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [295] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [295] Batch [180/261] Loss: 0.0113 LR: 0.000100
Epoch [295] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [295] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [295] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [295] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [295] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [295] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [295] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [295] Batch [260/261] Loss: 0.0005 LR: 0.000100

Epoch [296/350] - Time: 12.75s
Train Loss: 0.0136 | Val Loss: 0.0322
Train Acc: 0.9971 | Val Acc: 0.9963
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 1.000    Count 8: 0.990    Count 9: 0.978    Count 10: 1.000  
Epoch [296] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [296] Batch [10/261] Loss: 0.0004 LR: 0.000100
Epoch [296] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [296] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [296] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [296] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [296] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [296] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [296] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [296] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [296] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [296] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [296] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [296] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [296] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [296] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [296] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [296] Batch [170/261] Loss: 0.0001 LR: 0.000100
Epoch [296] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [296] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [296] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [296] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [296] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [296] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [296] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [296] Batch [250/261] Loss: 0.0003 LR: 0.000100
Epoch [296] Batch [260/261] Loss: 17.5997 LR: 0.000100

Epoch [297/350] - Time: 12.88s
Train Loss: 0.0156 | Val Loss: 0.0039
Train Acc: 0.9981 | Val Acc: 0.9991
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 1.000    Count 9: 0.989    Count 10: 1.000  
Epoch [297] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [297] Batch [10/261] Loss: 0.0003 LR: 0.000100
Epoch [297] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [297] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [297] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [297] Batch [50/261] Loss: 0.0064 LR: 0.000100
Epoch [297] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [297] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [297] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [297] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [297] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [297] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [297] Batch [120/261] Loss: 0.0687 LR: 0.000100
Epoch [297] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [297] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [297] Batch [150/261] Loss: 0.0001 LR: 0.000100
Epoch [297] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [297] Batch [170/261] Loss: 0.0001 LR: 0.000100
Epoch [297] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [297] Batch [190/261] Loss: 0.0001 LR: 0.000100
Epoch [297] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [297] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [297] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [297] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [297] Batch [240/261] Loss: 0.0003 LR: 0.000100
Epoch [297] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [297] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [298/350] - Time: 12.83s
Train Loss: 0.0177 | Val Loss: 0.0062
Train Acc: 0.9954 | Val Acc: 0.9991
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 1.000    Count 8: 1.000    Count 9: 1.000    Count 10: 1.000  
Epoch [298] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [298] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [298] Batch [20/261] Loss: 0.2989 LR: 0.000100
Epoch [298] Batch [30/261] Loss: 0.0013 LR: 0.000100
Epoch [298] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [298] Batch [50/261] Loss: 0.2094 LR: 0.000100
Epoch [298] Batch [60/261] Loss: 0.0183 LR: 0.000100
Epoch [298] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [298] Batch [80/261] Loss: 0.0193 LR: 0.000100
Epoch [298] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [298] Batch [100/261] Loss: 0.0001 LR: 0.000100
Epoch [298] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [298] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [298] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [298] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [298] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [298] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [298] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [298] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [298] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [298] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [298] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [298] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [298] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [298] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [298] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [298] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [299/350] - Time: 12.86s
Train Loss: 0.0153 | Val Loss: 0.0095
Train Acc: 0.9971 | Val Acc: 0.9953
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 0.990    Count 9: 1.000    Count 10: 0.955  
Epoch [299] Batch [0/261] Loss: 0.0003 LR: 0.000100
Epoch [299] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [299] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [299] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [299] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [299] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [299] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [299] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [299] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [299] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [299] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [299] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [299] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [299] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [299] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [299] Batch [150/261] Loss: 0.0009 LR: 0.000100
Epoch [299] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [299] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [299] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [299] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [299] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [299] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [299] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [299] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [299] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [299] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [299] Batch [260/261] Loss: 0.0002 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [300/350] - Time: 13.25s
Train Loss: 0.0081 | Val Loss: 0.0095
Train Acc: 0.9976 | Val Acc: 0.9972
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 1.000    Count 9: 0.989    Count 10: 0.989  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/single_image_checkpoint_epoch_299.pth
Epoch [300] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [300] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [300] Batch [20/261] Loss: 0.0001 LR: 0.000100
Epoch [300] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [300] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [300] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [300] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [300] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [300] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [300] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [300] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [300] Batch [110/261] Loss: 0.0049 LR: 0.000100
Epoch [300] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [300] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [300] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [300] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [300] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [300] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [300] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [300] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [300] Batch [200/261] Loss: 0.0054 LR: 0.000100
Epoch [300] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [300] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [300] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [300] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [300] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [300] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [301/350] - Time: 13.09s
Train Loss: 0.0002 | Val Loss: 0.0102
Train Acc: 1.0000 | Val Acc: 0.9972
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.990    Count 8: 0.990    Count 9: 0.989    Count 10: 1.000  
Epoch [301] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [301] Batch [10/261] Loss: 0.0003 LR: 0.000100
Epoch [301] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [301] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [301] Batch [40/261] Loss: 0.0004 LR: 0.000100
Epoch [301] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [301] Batch [60/261] Loss: 0.0001 LR: 0.000100
Epoch [301] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [301] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [301] Batch [90/261] Loss: 0.0020 LR: 0.000100
Epoch [301] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [301] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [301] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [301] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [301] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [301] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [301] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [301] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [301] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [301] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [301] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [301] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [301] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [301] Batch [230/261] Loss: 0.0003 LR: 0.000100
Epoch [301] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [301] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [301] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [302/350] - Time: 12.82s
Train Loss: 0.0066 | Val Loss: 0.0522
Train Acc: 0.9986 | Val Acc: 0.9916
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 0.939    Count 9: 0.967    Count 10: 1.000  
Epoch [302] Batch [0/261] Loss: 0.0022 LR: 0.000100
Epoch [302] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [302] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [302] Batch [30/261] Loss: 0.0039 LR: 0.000100
Epoch [302] Batch [40/261] Loss: 0.0001 LR: 0.000100
Epoch [302] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [302] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [302] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [302] Batch [80/261] Loss: 0.0019 LR: 0.000100
Epoch [302] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [302] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [302] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [302] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [302] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [302] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [302] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [302] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [302] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [302] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [302] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [302] Batch [200/261] Loss: 0.0883 LR: 0.000100
Epoch [302] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [302] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [302] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [302] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [302] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [302] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [303/350] - Time: 12.87s
Train Loss: 0.0277 | Val Loss: 0.0445
Train Acc: 0.9952 | Val Acc: 0.9907
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 1.000    Count 9: 0.989    Count 10: 0.898  
Epoch [303] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [303] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [303] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [303] Batch [30/261] Loss: 0.0001 LR: 0.000100
Epoch [303] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [303] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [303] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [303] Batch [70/261] Loss: 0.0004 LR: 0.000100
Epoch [303] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [303] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [303] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [303] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [303] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [303] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [303] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [303] Batch [150/261] Loss: 0.0009 LR: 0.000100
Epoch [303] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [303] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [303] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [303] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [303] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [303] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [303] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [303] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [303] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [303] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [303] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [304/350] - Time: 12.92s
Train Loss: 0.0039 | Val Loss: 0.0351
Train Acc: 0.9988 | Val Acc: 0.9926
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 0.990    Count 9: 0.922    Count 10: 1.000  
Epoch [304] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [304] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [304] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [304] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [304] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [304] Batch [50/261] Loss: 0.0001 LR: 0.000100
Epoch [304] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [304] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [304] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [304] Batch [90/261] Loss: 0.3347 LR: 0.000100
Epoch [304] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [304] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [304] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [304] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [304] Batch [140/261] Loss: 0.0040 LR: 0.000100
Epoch [304] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [304] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [304] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [304] Batch [180/261] Loss: 0.1229 LR: 0.000100
Epoch [304] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [304] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [304] Batch [210/261] Loss: 0.0001 LR: 0.000100
Epoch [304] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [304] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [304] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [304] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [304] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [305/350] - Time: 12.70s
Train Loss: 0.0117 | Val Loss: 0.0077
Train Acc: 0.9962 | Val Acc: 0.9972
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 0.990    Count 9: 1.000    Count 10: 0.989  
Epoch [305] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [305] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [305] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [305] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [305] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [305] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [305] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [305] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [305] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [305] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [305] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [305] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [305] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [305] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [305] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [305] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [305] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [305] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [305] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [305] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [305] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [305] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [305] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [305] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [305] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [305] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [305] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [306/350] - Time: 12.75s
Train Loss: 0.0002 | Val Loss: 0.0080
Train Acc: 1.0000 | Val Acc: 0.9981
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 1.000    Count 9: 0.989    Count 10: 1.000  
Epoch [306] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [306] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [306] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [306] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [306] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [306] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [306] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [306] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [306] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [306] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [306] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [306] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [306] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [306] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [306] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [306] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [306] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [306] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [306] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [306] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [306] Batch [200/261] Loss: 0.0022 LR: 0.000100
Epoch [306] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [306] Batch [220/261] Loss: 0.0006 LR: 0.000100
Epoch [306] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [306] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [306] Batch [250/261] Loss: 0.0019 LR: 0.000100
Epoch [306] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [307/350] - Time: 12.71s
Train Loss: 0.0004 | Val Loss: 0.1097
Train Acc: 0.9998 | Val Acc: 0.9749
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.990    Count 5: 0.892  
  Count 6: 0.918    Count 7: 0.979    Count 8: 0.960    Count 9: 0.989    Count 10: 1.000  
Epoch [307] Batch [0/261] Loss: 0.0007 LR: 0.000100
Epoch [307] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [307] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [307] Batch [30/261] Loss: 0.0443 LR: 0.000100
Epoch [307] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [307] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [307] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [307] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [307] Batch [80/261] Loss: 0.0054 LR: 0.000100
Epoch [307] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [307] Batch [100/261] Loss: 0.0106 LR: 0.000100
Epoch [307] Batch [110/261] Loss: 0.0086 LR: 0.000100
Epoch [307] Batch [120/261] Loss: 0.0458 LR: 0.000100
Epoch [307] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [307] Batch [140/261] Loss: 0.0004 LR: 0.000100
Epoch [307] Batch [150/261] Loss: 0.0001 LR: 0.000100
Epoch [307] Batch [160/261] Loss: 0.0002 LR: 0.000100
Epoch [307] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [307] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [307] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [307] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [307] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [307] Batch [220/261] Loss: 1.8869 LR: 0.000100
Epoch [307] Batch [230/261] Loss: 0.5308 LR: 0.000100
Epoch [307] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [307] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [307] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [308/350] - Time: 12.97s
Train Loss: 0.0415 | Val Loss: 0.0132
Train Acc: 0.9918 | Val Acc: 0.9972
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 0.980    Count 9: 1.000    Count 10: 1.000  
Epoch [308] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [308] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [308] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [308] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [308] Batch [40/261] Loss: 0.0002 LR: 0.000100
Epoch [308] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [308] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [308] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [308] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [308] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [308] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [308] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [308] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [308] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [308] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [308] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [308] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [308] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [308] Batch [180/261] Loss: 0.0001 LR: 0.000100
Epoch [308] Batch [190/261] Loss: 0.0001 LR: 0.000100
Epoch [308] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [308] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [308] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [308] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [308] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [308] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [308] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [309/350] - Time: 12.69s
Train Loss: 0.0030 | Val Loss: 0.0662
Train Acc: 0.9990 | Val Acc: 0.9888
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 1.000    Count 9: 1.000    Count 10: 0.864  
Epoch [309] Batch [0/261] Loss: 0.0003 LR: 0.000100
Epoch [309] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [309] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [309] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [309] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [309] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [309] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [309] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [309] Batch [80/261] Loss: 0.0012 LR: 0.000100
Epoch [309] Batch [90/261] Loss: 0.0006 LR: 0.000100
Epoch [309] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [309] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [309] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [309] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [309] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [309] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [309] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [309] Batch [170/261] Loss: 0.0002 LR: 0.000100
Epoch [309] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [309] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [309] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [309] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [309] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [309] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [309] Batch [240/261] Loss: 0.0001 LR: 0.000100
Epoch [309] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [309] Batch [260/261] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [310/350] - Time: 12.66s
Train Loss: 0.0141 | Val Loss: 0.0337
Train Acc: 0.9969 | Val Acc: 0.9926
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 1.000    Count 9: 0.933    Count 10: 0.989  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/single_image_checkpoint_epoch_309.pth
Epoch [310] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [310] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [310] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [310] Batch [30/261] Loss: 0.0989 LR: 0.000100
Epoch [310] Batch [40/261] Loss: 0.0005 LR: 0.000100
Epoch [310] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [310] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [310] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [310] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [310] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [310] Batch [100/261] Loss: 0.0004 LR: 0.000100
Epoch [310] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [310] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [310] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [310] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [310] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [310] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [310] Batch [170/261] Loss: 0.4212 LR: 0.000100
Epoch [310] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [310] Batch [190/261] Loss: 0.0004 LR: 0.000100
Epoch [310] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [310] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [310] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [310] Batch [230/261] Loss: 0.0002 LR: 0.000100
Epoch [310] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [310] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [310] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [311/350] - Time: 12.80s
Train Loss: 0.0177 | Val Loss: 0.0246
Train Acc: 0.9964 | Val Acc: 0.9944
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 0.990    Count 9: 0.956    Count 10: 1.000  
Epoch [311] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [311] Batch [10/261] Loss: 0.0002 LR: 0.000100
Epoch [311] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [311] Batch [30/261] Loss: 0.7761 LR: 0.000100
Epoch [311] Batch [40/261] Loss: 0.0025 LR: 0.000100
Epoch [311] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [311] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [311] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [311] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [311] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [311] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [311] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [311] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [311] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [311] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [311] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [311] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [311] Batch [170/261] Loss: 0.0002 LR: 0.000100
Epoch [311] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [311] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [311] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [311] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [311] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [311] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [311] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [311] Batch [250/261] Loss: 0.0001 LR: 0.000100
Epoch [311] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [312/350] - Time: 12.64s
Train Loss: 0.0237 | Val Loss: 0.2320
Train Acc: 0.9947 | Val Acc: 0.9562
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 1.000    Count 8: 0.990    Count 9: 0.933    Count 10: 0.568  
Epoch [312] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [312] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [312] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [312] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [312] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [312] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [312] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [312] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [312] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [312] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [312] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [312] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [312] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [312] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [312] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [312] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [312] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [312] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [312] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [312] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [312] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [312] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [312] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [312] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [312] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [312] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [312] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [313/350] - Time: 12.78s
Train Loss: 0.0009 | Val Loss: 0.0068
Train Acc: 0.9998 | Val Acc: 0.9972
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 1.000    Count 9: 1.000    Count 10: 0.977  
Epoch [313] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [313] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [313] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [313] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [313] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [313] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [313] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [313] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [313] Batch [80/261] Loss: 0.2045 LR: 0.000100
Epoch [313] Batch [90/261] Loss: 0.0440 LR: 0.000100
Epoch [313] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [313] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [313] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [313] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [313] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [313] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [313] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [313] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [313] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [313] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [313] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [313] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [313] Batch [220/261] Loss: 0.0005 LR: 0.000100
Epoch [313] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [313] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [313] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [313] Batch [260/261] Loss: 0.1554 LR: 0.000100

Epoch [314/350] - Time: 12.74s
Train Loss: 0.0028 | Val Loss: 0.0077
Train Acc: 0.9988 | Val Acc: 0.9981
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.990    Count 8: 1.000    Count 9: 1.000    Count 10: 1.000  
Epoch [314] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [314] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [314] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [314] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [314] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [314] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [314] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [314] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [314] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [314] Batch [90/261] Loss: 0.0001 LR: 0.000100
Epoch [314] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [314] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [314] Batch [120/261] Loss: 0.0023 LR: 0.000100
Epoch [314] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [314] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [314] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [314] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [314] Batch [170/261] Loss: 0.0002 LR: 0.000100
Epoch [314] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [314] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [314] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [314] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [314] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [314] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [314] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [314] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [314] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [315/350] - Time: 12.79s
Train Loss: 0.0143 | Val Loss: 0.0146
Train Acc: 0.9976 | Val Acc: 0.9972
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 1.000    Count 9: 1.000    Count 10: 0.966  
Epoch [315] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [315] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [315] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [315] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [315] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [315] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [315] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [315] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [315] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [315] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [315] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [315] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [315] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [315] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [315] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [315] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [315] Batch [160/261] Loss: 0.0001 LR: 0.000100
Epoch [315] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [315] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [315] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [315] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [315] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [315] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [315] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [315] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [315] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [315] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [316/350] - Time: 13.06s
Train Loss: 0.0000 | Val Loss: 0.0160
Train Acc: 1.0000 | Val Acc: 0.9963
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 0.990    Count 9: 0.978    Count 10: 1.000  
Epoch [316] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [316] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [316] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [316] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [316] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [316] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [316] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [316] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [316] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [316] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [316] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [316] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [316] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [316] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [316] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [316] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [316] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [316] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [316] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [316] Batch [190/261] Loss: 0.0017 LR: 0.000100
Epoch [316] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [316] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [316] Batch [220/261] Loss: 0.0003 LR: 0.000100
Epoch [316] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [316] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [316] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [316] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [317/350] - Time: 12.82s
Train Loss: 0.0116 | Val Loss: 0.0186
Train Acc: 0.9978 | Val Acc: 0.9953
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 1.000    Count 8: 1.000    Count 9: 0.989    Count 10: 0.977  
Epoch [317] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [317] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [317] Batch [20/261] Loss: 0.0001 LR: 0.000100
Epoch [317] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [317] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [317] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [317] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [317] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [317] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [317] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [317] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [317] Batch [110/261] Loss: 0.0001 LR: 0.000100
Epoch [317] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [317] Batch [130/261] Loss: 0.0035 LR: 0.000100
Epoch [317] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [317] Batch [150/261] Loss: 0.0002 LR: 0.000100
Epoch [317] Batch [160/261] Loss: 2.3342 LR: 0.000100
Epoch [317] Batch [170/261] Loss: 0.0187 LR: 0.000100
Epoch [317] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [317] Batch [190/261] Loss: 0.0001 LR: 0.000100
Epoch [317] Batch [200/261] Loss: 0.0006 LR: 0.000100
Epoch [317] Batch [210/261] Loss: 0.0003 LR: 0.000100
Epoch [317] Batch [220/261] Loss: 0.0002 LR: 0.000100
Epoch [317] Batch [230/261] Loss: 0.0038 LR: 0.000100
Epoch [317] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [317] Batch [250/261] Loss: 0.2830 LR: 0.000100
Epoch [317] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [318/350] - Time: 12.95s
Train Loss: 0.0250 | Val Loss: 0.0617
Train Acc: 0.9959 | Val Acc: 0.9926
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.990    Count 8: 0.980    Count 9: 0.944    Count 10: 1.000  
Epoch [318] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [318] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [318] Batch [20/261] Loss: 0.1058 LR: 0.000100
Epoch [318] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [318] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [318] Batch [50/261] Loss: 0.0005 LR: 0.000100
Epoch [318] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [318] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [318] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [318] Batch [90/261] Loss: 0.0001 LR: 0.000100
Epoch [318] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [318] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [318] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [318] Batch [130/261] Loss: 0.0003 LR: 0.000100
Epoch [318] Batch [140/261] Loss: 0.0184 LR: 0.000100
Epoch [318] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [318] Batch [160/261] Loss: 0.4308 LR: 0.000100
Epoch [318] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [318] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [318] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [318] Batch [200/261] Loss: 0.6102 LR: 0.000100
Epoch [318] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [318] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [318] Batch [230/261] Loss: 0.0001 LR: 0.000100
Epoch [318] Batch [240/261] Loss: 0.2423 LR: 0.000100
Epoch [318] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [318] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [319/350] - Time: 12.70s
Train Loss: 0.0445 | Val Loss: 0.0566
Train Acc: 0.9909 | Val Acc: 0.9926
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 0.979    Count 8: 0.990    Count 9: 0.967    Count 10: 1.000  
Epoch [319] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [319] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [319] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [319] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [319] Batch [40/261] Loss: 0.0001 LR: 0.000100
Epoch [319] Batch [50/261] Loss: 0.0001 LR: 0.000100
Epoch [319] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [319] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [319] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [319] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [319] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [319] Batch [110/261] Loss: 0.0020 LR: 0.000100
Epoch [319] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [319] Batch [130/261] Loss: 0.0017 LR: 0.000100
Epoch [319] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [319] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [319] Batch [160/261] Loss: 0.0002 LR: 0.000100
Epoch [319] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [319] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [319] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [319] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [319] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [319] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [319] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [319] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [319] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [319] Batch [260/261] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [320/350] - Time: 12.53s
Train Loss: 0.0102 | Val Loss: 0.0368
Train Acc: 0.9978 | Val Acc: 0.9953
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 1.000    Count 8: 1.000    Count 9: 0.967    Count 10: 1.000  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/single_image_checkpoint_epoch_319.pth
Epoch [320] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [320] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [320] Batch [20/261] Loss: 0.0001 LR: 0.000100
Epoch [320] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [320] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [320] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [320] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [320] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [320] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [320] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [320] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [320] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [320] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [320] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [320] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [320] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [320] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [320] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [320] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [320] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [320] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [320] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [320] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [320] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [320] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [320] Batch [250/261] Loss: 0.0005 LR: 0.000100
Epoch [320] Batch [260/261] Loss: 0.0001 LR: 0.000100

Epoch [321/350] - Time: 12.78s
Train Loss: 0.0089 | Val Loss: 0.3779
Train Acc: 0.9986 | Val Acc: 0.9423
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.990    Count 5: 0.990  
  Count 6: 0.969    Count 7: 0.969    Count 8: 0.909    Count 9: 0.900    Count 10: 0.602  
Epoch [321] Batch [0/261] Loss: 0.0050 LR: 0.000100
Epoch [321] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [321] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [321] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [321] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [321] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [321] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [321] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [321] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [321] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [321] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [321] Batch [110/261] Loss: 0.2313 LR: 0.000100
Epoch [321] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [321] Batch [130/261] Loss: 0.0001 LR: 0.000100
Epoch [321] Batch [140/261] Loss: 0.0002 LR: 0.000100
Epoch [321] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [321] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [321] Batch [170/261] Loss: 0.0064 LR: 0.000100
Epoch [321] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [321] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [321] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [321] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [321] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [321] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [321] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [321] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [321] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [322/350] - Time: 12.74s
Train Loss: 0.0199 | Val Loss: 0.1448
Train Acc: 0.9966 | Val Acc: 0.9665
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.917    Count 8: 0.838    Count 9: 0.878    Count 10: 1.000  
Epoch [322] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [322] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [322] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [322] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [322] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [322] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [322] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [322] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [322] Batch [80/261] Loss: 0.0001 LR: 0.000100
Epoch [322] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [322] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [322] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [322] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [322] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [322] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [322] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [322] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [322] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [322] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [322] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [322] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [322] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [322] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [322] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [322] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [322] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [322] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [323/350] - Time: 12.86s
Train Loss: 0.0012 | Val Loss: 0.0042
Train Acc: 0.9998 | Val Acc: 0.9981
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 1.000    Count 9: 1.000    Count 10: 0.989  
Epoch [323] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [323] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [323] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [323] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [323] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [323] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [323] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [323] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [323] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [323] Batch [90/261] Loss: 0.0899 LR: 0.000100
Epoch [323] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [323] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [323] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [323] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [323] Batch [140/261] Loss: 0.0017 LR: 0.000100
Epoch [323] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [323] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [323] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [323] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [323] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [323] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [323] Batch [210/261] Loss: 0.0001 LR: 0.000100
Epoch [323] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [323] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [323] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [323] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [323] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [324/350] - Time: 12.86s
Train Loss: 0.0006 | Val Loss: 0.0216
Train Acc: 0.9998 | Val Acc: 0.9944
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 1.000    Count 8: 0.990    Count 9: 1.000    Count 10: 0.966  
Epoch [324] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [324] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [324] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [324] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [324] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [324] Batch [50/261] Loss: 0.0001 LR: 0.000100
Epoch [324] Batch [60/261] Loss: 0.0001 LR: 0.000100
Epoch [324] Batch [70/261] Loss: 0.0008 LR: 0.000100
Epoch [324] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [324] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [324] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [324] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [324] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [324] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [324] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [324] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [324] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [324] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [324] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [324] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [324] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [324] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [324] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [324] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [324] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [324] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [324] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [325/350] - Time: 12.57s
Train Loss: 0.0078 | Val Loss: 0.0536
Train Acc: 0.9978 | Val Acc: 0.9888
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 1.000    Count 8: 0.990    Count 9: 0.978    Count 10: 0.920  
Epoch [325] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [325] Batch [10/261] Loss: 0.0032 LR: 0.000100
Epoch [325] Batch [20/261] Loss: 0.0053 LR: 0.000100
Epoch [325] Batch [30/261] Loss: 0.0167 LR: 0.000100
Epoch [325] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [325] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [325] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [325] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [325] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [325] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [325] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [325] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [325] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [325] Batch [130/261] Loss: 0.0253 LR: 0.000100
Epoch [325] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [325] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [325] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [325] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [325] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [325] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [325] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [325] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [325] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [325] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [325] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [325] Batch [250/261] Loss: 0.0591 LR: 0.000100
Epoch [325] Batch [260/261] Loss: 4.6812 LR: 0.000100

Epoch [326/350] - Time: 12.71s
Train Loss: 0.0273 | Val Loss: 0.0048
Train Acc: 0.9945 | Val Acc: 0.9972
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.981    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 1.000    Count 9: 1.000    Count 10: 1.000  
Epoch [326] Batch [0/261] Loss: 0.0006 LR: 0.000100
Epoch [326] Batch [10/261] Loss: 0.0001 LR: 0.000100
Epoch [326] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [326] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [326] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [326] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [326] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [326] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [326] Batch [80/261] Loss: 0.0001 LR: 0.000100
Epoch [326] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [326] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [326] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [326] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [326] Batch [130/261] Loss: 0.0001 LR: 0.000100
Epoch [326] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [326] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [326] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [326] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [326] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [326] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [326] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [326] Batch [210/261] Loss: 0.0001 LR: 0.000100
Epoch [326] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [326] Batch [230/261] Loss: 0.2213 LR: 0.000100
Epoch [326] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [326] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [326] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [327/350] - Time: 12.79s
Train Loss: 0.0320 | Val Loss: 0.0103
Train Acc: 0.9945 | Val Acc: 0.9972
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 0.970    Count 9: 1.000    Count 10: 1.000  
Epoch [327] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [327] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [327] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [327] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [327] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [327] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [327] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [327] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [327] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [327] Batch [90/261] Loss: 0.0003 LR: 0.000100
Epoch [327] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [327] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [327] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [327] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [327] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [327] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [327] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [327] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [327] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [327] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [327] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [327] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [327] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [327] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [327] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [327] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [327] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [328/350] - Time: 12.86s
Train Loss: 0.0067 | Val Loss: 0.1130
Train Acc: 0.9981 | Val Acc: 0.9786
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 1.000    Count 9: 1.000    Count 10: 0.739  
Epoch [328] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [328] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [328] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [328] Batch [30/261] Loss: 0.0002 LR: 0.000100
Epoch [328] Batch [40/261] Loss: 0.0160 LR: 0.000100
Epoch [328] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [328] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [328] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [328] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [328] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [328] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [328] Batch [110/261] Loss: 0.0003 LR: 0.000100
Epoch [328] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [328] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [328] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [328] Batch [150/261] Loss: 0.0001 LR: 0.000100
Epoch [328] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [328] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [328] Batch [180/261] Loss: 0.0002 LR: 0.000100
Epoch [328] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [328] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [328] Batch [210/261] Loss: 0.0056 LR: 0.000100
Epoch [328] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [328] Batch [230/261] Loss: 0.3307 LR: 0.000100
Epoch [328] Batch [240/261] Loss: 0.0001 LR: 0.000100
Epoch [328] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [328] Batch [260/261] Loss: 0.1026 LR: 0.000100

Epoch [329/350] - Time: 12.77s
Train Loss: 0.0101 | Val Loss: 0.0260
Train Acc: 0.9978 | Val Acc: 0.9944
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.980    Count 7: 0.990    Count 8: 0.970    Count 9: 1.000    Count 10: 1.000  
Epoch [329] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [329] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [329] Batch [20/261] Loss: 0.0001 LR: 0.000100
Epoch [329] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [329] Batch [40/261] Loss: 0.0001 LR: 0.000100
Epoch [329] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [329] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [329] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [329] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [329] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [329] Batch [100/261] Loss: 0.0015 LR: 0.000100
Epoch [329] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [329] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [329] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [329] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [329] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [329] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [329] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [329] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [329] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [329] Batch [200/261] Loss: 0.0010 LR: 0.000100
Epoch [329] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [329] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [329] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [329] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [329] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [329] Batch [260/261] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [330/350] - Time: 12.83s
Train Loss: 0.0149 | Val Loss: 0.0188
Train Acc: 0.9971 | Val Acc: 0.9944
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 0.979    Count 8: 1.000    Count 9: 1.000    Count 10: 0.977  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/single_image_checkpoint_epoch_329.pth
Epoch [330] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [330] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [330] Batch [20/261] Loss: 0.0001 LR: 0.000100
Epoch [330] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [330] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [330] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [330] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [330] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [330] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [330] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [330] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [330] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [330] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [330] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [330] Batch [140/261] Loss: 0.0006 LR: 0.000100
Epoch [330] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [330] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [330] Batch [170/261] Loss: 0.0001 LR: 0.000100
Epoch [330] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [330] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [330] Batch [200/261] Loss: 0.0001 LR: 0.000100
Epoch [330] Batch [210/261] Loss: 0.0173 LR: 0.000100
Epoch [330] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [330] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [330] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [330] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [330] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [331/350] - Time: 12.69s
Train Loss: 0.0110 | Val Loss: 0.1497
Train Acc: 0.9978 | Val Acc: 0.9823
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.990    Count 8: 0.970    Count 9: 0.844    Count 10: 1.000  
Epoch [331] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [331] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [331] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [331] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [331] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [331] Batch [50/261] Loss: 0.3110 LR: 0.000100
Epoch [331] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [331] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [331] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [331] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [331] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [331] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [331] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [331] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [331] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [331] Batch [150/261] Loss: 0.0002 LR: 0.000100
Epoch [331] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [331] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [331] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [331] Batch [190/261] Loss: 0.2346 LR: 0.000100
Epoch [331] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [331] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [331] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [331] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [331] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [331] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [331] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [332/350] - Time: 12.91s
Train Loss: 0.0089 | Val Loss: 0.0166
Train Acc: 0.9978 | Val Acc: 0.9981
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 1.000    Count 8: 1.000    Count 9: 1.000    Count 10: 1.000  
Epoch [332] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [332] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [332] Batch [20/261] Loss: 0.1161 LR: 0.000100
Epoch [332] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [332] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [332] Batch [50/261] Loss: 0.1198 LR: 0.000100
Epoch [332] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [332] Batch [70/261] Loss: 0.0002 LR: 0.000100
Epoch [332] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [332] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [332] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [332] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [332] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [332] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [332] Batch [140/261] Loss: 0.0001 LR: 0.000100
Epoch [332] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [332] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [332] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [332] Batch [180/261] Loss: 0.0006 LR: 0.000100
Epoch [332] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [332] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [332] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [332] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [332] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [332] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [332] Batch [250/261] Loss: 0.0002 LR: 0.000100
Epoch [332] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [333/350] - Time: 12.64s
Train Loss: 0.0247 | Val Loss: 0.1500
Train Acc: 0.9942 | Val Acc: 0.9730
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 0.990    Count 9: 1.000    Count 10: 0.682  
Epoch [333] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [333] Batch [10/261] Loss: 0.0004 LR: 0.000100
Epoch [333] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [333] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [333] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [333] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [333] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [333] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [333] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [333] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [333] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [333] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [333] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [333] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [333] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [333] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [333] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [333] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [333] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [333] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [333] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [333] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [333] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [333] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [333] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [333] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [333] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [334/350] - Time: 12.72s
Train Loss: 0.0000 | Val Loss: 0.0104
Train Acc: 1.0000 | Val Acc: 0.9944
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.990    Count 8: 0.990    Count 9: 0.989    Count 10: 0.977  
Epoch [334] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [334] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [334] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [334] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [334] Batch [40/261] Loss: 0.0001 LR: 0.000100
Epoch [334] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [334] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [334] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [334] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [334] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [334] Batch [100/261] Loss: 0.0142 LR: 0.000100
Epoch [334] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [334] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [334] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [334] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [334] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [334] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [334] Batch [170/261] Loss: 0.0040 LR: 0.000100
Epoch [334] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [334] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [334] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [334] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [334] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [334] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [334] Batch [240/261] Loss: 0.0002 LR: 0.000100
Epoch [334] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [334] Batch [260/261] Loss: 1.0906 LR: 0.000100

Epoch [335/350] - Time: 12.69s
Train Loss: 0.0043 | Val Loss: 0.0096
Train Acc: 0.9983 | Val Acc: 0.9981
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.979    Count 8: 1.000    Count 9: 1.000    Count 10: 1.000  
Epoch [335] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [335] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [335] Batch [20/261] Loss: 0.0143 LR: 0.000100
Epoch [335] Batch [30/261] Loss: 0.0023 LR: 0.000100
Epoch [335] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [335] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [335] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [335] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [335] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [335] Batch [90/261] Loss: 0.6868 LR: 0.000100
Epoch [335] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [335] Batch [110/261] Loss: 0.0006 LR: 0.000100
Epoch [335] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [335] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [335] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [335] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [335] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [335] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [335] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [335] Batch [190/261] Loss: 0.0115 LR: 0.000100
Epoch [335] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [335] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [335] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [335] Batch [230/261] Loss: 0.0001 LR: 0.000100
Epoch [335] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [335] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [335] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [336/350] - Time: 12.67s
Train Loss: 0.0227 | Val Loss: 0.0376
Train Acc: 0.9947 | Val Acc: 0.9935
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.979    Count 8: 0.990    Count 9: 0.967    Count 10: 1.000  
Epoch [336] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [336] Batch [10/261] Loss: 0.0001 LR: 0.000100
Epoch [336] Batch [20/261] Loss: 0.5466 LR: 0.000100
Epoch [336] Batch [30/261] Loss: 0.0014 LR: 0.000100
Epoch [336] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [336] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [336] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [336] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [336] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [336] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [336] Batch [100/261] Loss: 0.0008 LR: 0.000100
Epoch [336] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [336] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [336] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [336] Batch [140/261] Loss: 0.0002 LR: 0.000100
Epoch [336] Batch [150/261] Loss: 0.0002 LR: 0.000100
Epoch [336] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [336] Batch [170/261] Loss: 0.0029 LR: 0.000100
Epoch [336] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [336] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [336] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [336] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [336] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [336] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [336] Batch [240/261] Loss: 0.0522 LR: 0.000100
Epoch [336] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [336] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [337/350] - Time: 12.85s
Train Loss: 0.0152 | Val Loss: 0.1846
Train Acc: 0.9966 | Val Acc: 0.9749
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 0.927    Count 8: 0.970    Count 9: 0.822    Count 10: 1.000  
Epoch [337] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [337] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [337] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [337] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [337] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [337] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [337] Batch [60/261] Loss: 0.0002 LR: 0.000100
Epoch [337] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [337] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [337] Batch [90/261] Loss: 0.0001 LR: 0.000100
Epoch [337] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [337] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [337] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [337] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [337] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [337] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [337] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [337] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [337] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [337] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [337] Batch [200/261] Loss: 0.3847 LR: 0.000100
Epoch [337] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [337] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [337] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [337] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [337] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [337] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [338/350] - Time: 12.65s
Train Loss: 0.0067 | Val Loss: 0.0153
Train Acc: 0.9986 | Val Acc: 0.9944
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.990    Count 5: 1.000  
  Count 6: 0.990    Count 7: 0.990    Count 8: 0.990    Count 9: 0.989    Count 10: 1.000  
Epoch [338] Batch [0/261] Loss: 0.0001 LR: 0.000100
Epoch [338] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [338] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [338] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [338] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [338] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [338] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [338] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [338] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [338] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [338] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [338] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [338] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [338] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [338] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [338] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [338] Batch [160/261] Loss: 0.1210 LR: 0.000100
Epoch [338] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [338] Batch [180/261] Loss: 0.0003 LR: 0.000100
Epoch [338] Batch [190/261] Loss: 0.0001 LR: 0.000100
Epoch [338] Batch [200/261] Loss: 0.0004 LR: 0.000100
Epoch [338] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [338] Batch [220/261] Loss: 0.0004 LR: 0.000100
Epoch [338] Batch [230/261] Loss: 0.0002 LR: 0.000100
Epoch [338] Batch [240/261] Loss: 0.0001 LR: 0.000100
Epoch [338] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [338] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [339/350] - Time: 12.87s
Train Loss: 0.0099 | Val Loss: 0.0929
Train Acc: 0.9983 | Val Acc: 0.9898
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.992    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.938    Count 8: 0.980    Count 9: 0.978    Count 10: 1.000  
Epoch [339] Batch [0/261] Loss: 0.0190 LR: 0.000100
Epoch [339] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [339] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [339] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [339] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [339] Batch [50/261] Loss: 0.0005 LR: 0.000100
Epoch [339] Batch [60/261] Loss: 0.0001 LR: 0.000100
Epoch [339] Batch [70/261] Loss: 0.0001 LR: 0.000100
Epoch [339] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [339] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [339] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [339] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [339] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [339] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [339] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [339] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [339] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [339] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [339] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [339] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [339] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [339] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [339] Batch [220/261] Loss: 0.0001 LR: 0.000100
Epoch [339] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [339] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [339] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [339] Batch [260/261] Loss: 0.0041 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [340/350] - Time: 12.88s
Train Loss: 0.0004 | Val Loss: 0.0134
Train Acc: 0.9998 | Val Acc: 0.9981
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.990    Count 8: 0.990    Count 9: 1.000    Count 10: 1.000  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/single_image_checkpoint_epoch_339.pth
Epoch [340] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [340] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [340] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [340] Batch [30/261] Loss: 0.0001 LR: 0.000100
Epoch [340] Batch [40/261] Loss: 0.0463 LR: 0.000100
Epoch [340] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [340] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [340] Batch [70/261] Loss: 0.5132 LR: 0.000100
Epoch [340] Batch [80/261] Loss: 0.0001 LR: 0.000100
Epoch [340] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [340] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [340] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [340] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [340] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [340] Batch [140/261] Loss: 0.0001 LR: 0.000100
Epoch [340] Batch [150/261] Loss: 0.0001 LR: 0.000100
Epoch [340] Batch [160/261] Loss: 0.3264 LR: 0.000100
Epoch [340] Batch [170/261] Loss: 0.3841 LR: 0.000100
Epoch [340] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [340] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [340] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [340] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [340] Batch [220/261] Loss: 0.0843 LR: 0.000100
Epoch [340] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [340] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [340] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [340] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [341/350] - Time: 12.68s
Train Loss: 0.0400 | Val Loss: 0.0925
Train Acc: 0.9916 | Val Acc: 0.9860
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 1.000    Count 8: 0.990    Count 9: 0.989    Count 10: 0.864  
Epoch [341] Batch [0/261] Loss: 0.0105 LR: 0.000100
Epoch [341] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [341] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [341] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [341] Batch [40/261] Loss: 0.0007 LR: 0.000100
Epoch [341] Batch [50/261] Loss: 0.0010 LR: 0.000100
Epoch [341] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [341] Batch [70/261] Loss: 0.0695 LR: 0.000100
Epoch [341] Batch [80/261] Loss: 0.0001 LR: 0.000100
Epoch [341] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [341] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [341] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [341] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [341] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [341] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [341] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [341] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [341] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [341] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [341] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [341] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [341] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [341] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [341] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [341] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [341] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [341] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [342/350] - Time: 12.80s
Train Loss: 0.0083 | Val Loss: 0.0195
Train Acc: 0.9976 | Val Acc: 0.9944
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 0.990    Count 9: 1.000    Count 10: 0.943  
Epoch [342] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [342] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [342] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [342] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [342] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [342] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [342] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [342] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [342] Batch [80/261] Loss: 0.0057 LR: 0.000100
Epoch [342] Batch [90/261] Loss: 0.0003 LR: 0.000100
Epoch [342] Batch [100/261] Loss: 0.0001 LR: 0.000100
Epoch [342] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [342] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [342] Batch [130/261] Loss: 0.0011 LR: 0.000100
Epoch [342] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [342] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [342] Batch [160/261] Loss: 0.0038 LR: 0.000100
Epoch [342] Batch [170/261] Loss: 0.0001 LR: 0.000100
Epoch [342] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [342] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [342] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [342] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [342] Batch [220/261] Loss: 0.0003 LR: 0.000100
Epoch [342] Batch [230/261] Loss: 0.0004 LR: 0.000100
Epoch [342] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [342] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [342] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [343/350] - Time: 12.66s
Train Loss: 0.0038 | Val Loss: 0.6559
Train Acc: 0.9990 | Val Acc: 0.9255
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.990    Count 7: 1.000    Count 8: 0.949    Count 9: 0.933    Count 10: 0.239  
Epoch [343] Batch [0/261] Loss: 0.0387 LR: 0.000100
Epoch [343] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [343] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [343] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [343] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [343] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [343] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [343] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [343] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [343] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [343] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [343] Batch [110/261] Loss: 0.0001 LR: 0.000100
Epoch [343] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [343] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [343] Batch [140/261] Loss: 0.0001 LR: 0.000100
Epoch [343] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [343] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [343] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [343] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [343] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [343] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [343] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [343] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [343] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [343] Batch [240/261] Loss: 0.0099 LR: 0.000100
Epoch [343] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [343] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [344/350] - Time: 12.79s
Train Loss: 0.0186 | Val Loss: 0.0143
Train Acc: 0.9959 | Val Acc: 0.9963
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 1.000    Count 9: 0.989    Count 10: 0.977  
Epoch [344] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [344] Batch [10/261] Loss: 0.3593 LR: 0.000100
Epoch [344] Batch [20/261] Loss: 0.0120 LR: 0.000100
Epoch [344] Batch [30/261] Loss: 0.0147 LR: 0.000100
Epoch [344] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [344] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [344] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [344] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [344] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [344] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [344] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [344] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [344] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [344] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [344] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [344] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [344] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [344] Batch [170/261] Loss: 0.0004 LR: 0.000100
Epoch [344] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [344] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [344] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [344] Batch [210/261] Loss: 0.9681 LR: 0.000100
Epoch [344] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [344] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [344] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [344] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [344] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [345/350] - Time: 12.69s
Train Loss: 0.0053 | Val Loss: 0.0113
Train Acc: 0.9990 | Val Acc: 0.9981
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 1.000    Count 9: 1.000    Count 10: 0.989  
Epoch [345] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [345] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [345] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [345] Batch [30/261] Loss: 0.0001 LR: 0.000100
Epoch [345] Batch [40/261] Loss: 0.3574 LR: 0.000100
Epoch [345] Batch [50/261] Loss: 0.0004 LR: 0.000100
Epoch [345] Batch [60/261] Loss: 0.0007 LR: 0.000100
Epoch [345] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [345] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [345] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [345] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [345] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [345] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [345] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [345] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [345] Batch [150/261] Loss: 0.0002 LR: 0.000100
Epoch [345] Batch [160/261] Loss: 0.0001 LR: 0.000100
Epoch [345] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [345] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [345] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [345] Batch [200/261] Loss: 0.0242 LR: 0.000100
Epoch [345] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [345] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [345] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [345] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [345] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [345] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [346/350] - Time: 12.75s
Train Loss: 0.0134 | Val Loss: 0.0326
Train Acc: 0.9974 | Val Acc: 0.9944
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.990    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.979    Count 8: 1.000    Count 9: 0.978    Count 10: 1.000  
Epoch [346] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [346] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [346] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [346] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [346] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [346] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [346] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [346] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [346] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [346] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [346] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [346] Batch [110/261] Loss: 0.1364 LR: 0.000100
Epoch [346] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [346] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [346] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [346] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [346] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [346] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [346] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [346] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [346] Batch [200/261] Loss: 0.0596 LR: 0.000100
Epoch [346] Batch [210/261] Loss: 0.0231 LR: 0.000100
Epoch [346] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [346] Batch [230/261] Loss: 0.0527 LR: 0.000100
Epoch [346] Batch [240/261] Loss: 0.0259 LR: 0.000100
Epoch [346] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [346] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [347/350] - Time: 13.58s
Train Loss: 0.0177 | Val Loss: 0.2372
Train Acc: 0.9959 | Val Acc: 0.9693
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.990    Count 8: 0.909    Count 9: 0.756    Count 10: 1.000  
Epoch [347] Batch [0/261] Loss: 0.0004 LR: 0.000100
Epoch [347] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [347] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [347] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [347] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [347] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [347] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [347] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [347] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [347] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [347] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [347] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [347] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [347] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [347] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [347] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [347] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [347] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [347] Batch [180/261] Loss: 0.0014 LR: 0.000100
Epoch [347] Batch [190/261] Loss: 0.0003 LR: 0.000100
Epoch [347] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [347] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [347] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [347] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [347] Batch [240/261] Loss: 0.0003 LR: 0.000100
Epoch [347] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [347] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [348/350] - Time: 12.84s
Train Loss: 0.0066 | Val Loss: 0.0532
Train Acc: 0.9988 | Val Acc: 0.9870
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 0.958    Count 8: 0.990    Count 9: 0.989    Count 10: 0.909  
Epoch [348] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [348] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [348] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [348] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [348] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [348] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [348] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [348] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [348] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [348] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [348] Batch [100/261] Loss: 0.0000 LR: 0.000100
Epoch [348] Batch [110/261] Loss: 0.0001 LR: 0.000100
Epoch [348] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [348] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [348] Batch [140/261] Loss: 0.0013 LR: 0.000100
Epoch [348] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [348] Batch [160/261] Loss: 0.0000 LR: 0.000100
Epoch [348] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [348] Batch [180/261] Loss: 0.0000 LR: 0.000100
Epoch [348] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [348] Batch [200/261] Loss: 0.0000 LR: 0.000100
Epoch [348] Batch [210/261] Loss: 0.0001 LR: 0.000100
Epoch [348] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [348] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [348] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [348] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [348] Batch [260/261] Loss: 0.0000 LR: 0.000100

Epoch [349/350] - Time: 12.70s
Train Loss: 0.0172 | Val Loss: 0.0092
Train Acc: 0.9971 | Val Acc: 0.9981
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 1.000    Count 7: 1.000    Count 8: 1.000    Count 9: 1.000    Count 10: 0.989  
Epoch [349] Batch [0/261] Loss: 0.0000 LR: 0.000100
Epoch [349] Batch [10/261] Loss: 0.0000 LR: 0.000100
Epoch [349] Batch [20/261] Loss: 0.0000 LR: 0.000100
Epoch [349] Batch [30/261] Loss: 0.0000 LR: 0.000100
Epoch [349] Batch [40/261] Loss: 0.0000 LR: 0.000100
Epoch [349] Batch [50/261] Loss: 0.0000 LR: 0.000100
Epoch [349] Batch [60/261] Loss: 0.0000 LR: 0.000100
Epoch [349] Batch [70/261] Loss: 0.0000 LR: 0.000100
Epoch [349] Batch [80/261] Loss: 0.0000 LR: 0.000100
Epoch [349] Batch [90/261] Loss: 0.0000 LR: 0.000100
Epoch [349] Batch [100/261] Loss: 0.0663 LR: 0.000100
Epoch [349] Batch [110/261] Loss: 0.0000 LR: 0.000100
Epoch [349] Batch [120/261] Loss: 0.0000 LR: 0.000100
Epoch [349] Batch [130/261] Loss: 0.0000 LR: 0.000100
Epoch [349] Batch [140/261] Loss: 0.0000 LR: 0.000100
Epoch [349] Batch [150/261] Loss: 0.0000 LR: 0.000100
Epoch [349] Batch [160/261] Loss: 0.0001 LR: 0.000100
Epoch [349] Batch [170/261] Loss: 0.0000 LR: 0.000100
Epoch [349] Batch [180/261] Loss: 0.0121 LR: 0.000100
Epoch [349] Batch [190/261] Loss: 0.0000 LR: 0.000100
Epoch [349] Batch [200/261] Loss: 0.4444 LR: 0.000100
Epoch [349] Batch [210/261] Loss: 0.0000 LR: 0.000100
Epoch [349] Batch [220/261] Loss: 0.0000 LR: 0.000100
Epoch [349] Batch [230/261] Loss: 0.0000 LR: 0.000100
Epoch [349] Batch [240/261] Loss: 0.0000 LR: 0.000100
Epoch [349] Batch [250/261] Loss: 0.0000 LR: 0.000100
Epoch [349] Batch [260/261] Loss: 0.0003 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [350/350] - Time: 12.83s
Train Loss: 0.0118 | Val Loss: 0.0368
Train Acc: 0.9969 | Val Acc: 0.9953
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.980    Count 7: 1.000    Count 8: 0.990    Count 9: 0.989    Count 10: 1.000  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/single_image_checkpoint_epoch_349.pth
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/final_single_image_model.pth

训练完成!
最佳验证准确率: 1.0000
最佳验证损失: 0.0009
最终混淆矩阵保存到: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/final_confusion_matrix.png
详细报告保存到: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/100data/check_points/classification_report.txt

训练成功完成！
程序结束。
=== 完成 ===
结束时间: Sun 20 Jul 02:50:27 BST 2025
