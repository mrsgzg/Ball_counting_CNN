=== 作业信息 ===
节点: node802
GPU: 1 (ID: 0)
CPU核心: 4
内存: 20480 MB
开始时间: Sun 20 Jul 00:45:39 BST 2025
=== 激活环境 ===
/var/spool/slurmd/job3583392/slurm_script: line 25: --version: command not found
Python版本: 
当前环境: cgtest
=== 开始训练 ===
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:279: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(test_path, map_location=self.device)
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')
============================================================
单图像分类模型训练配置
============================================================
基础配置:
  device: cuda
  batch_size: 16
  learning_rate: 0.0001
  total_epochs: 350
  image_mode: rgb

数据配置:
  data_root: /mnt/iusers01/fatpou01/compsci01/k09562zs/scratch/Ball_counting_CNN/ball_data_collection
  train_csv: scratch/Ball_counting_CNN/Tools_script/ball_counting_dataset_train_50.csv
  val_csv: scratch/Ball_counting_CNN/Tools_script/ball_counting_dataset_val.csv

模型配置:
  use_attention: True
  cnn_layers: 3
  cnn_channels: [64, 128, 256]
  feature_dim: 256
  attention_heads: 1
  dropout: 0.1

训练配置:
  scheduler_type: none
  label_smoothing: 0.0
  grad_clip_norm: 1.0

保存配置:
  save_dir: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points
  log_dir: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/logs
  save_every: 10
============================================================
所有路径验证通过
配置保存到: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/single_image_config.json

正在初始化单图像分类模型训练器...
图像模式: RGB
使用注意力机制: True
标签范围: 1-10 (对应10个类别)
SingleImageClassifier初始化:
  CNN层数: 3
  CNN通道: [64, 128, 256]
  输入通道: 3
  输出类别: 10 (对应标签1-10)
  特征维度: 256
  隐藏维度: 256
  使用注意力: True
  注意力头数: 1
创建带注意力机制的单图像分类模型 (标签1-10)
✓ Model initialization validation passed
=== 创建单图像数据加载器 - 图像模式: RGB ===
标签: 直接使用ball_count
单图像数据集构建完成:
  原始序列数: 472
  提取的单图像样本数: 2076
  图像模式: rgb
  标签: 直接使用ball_count
单图像数据集构建完成:
  原始序列数: 242
  提取的单图像样本数: 1074
  图像模式: rgb
  标签: 直接使用ball_count

训练集类别分布:
  球数 1: 324 样本
  球数 2: 246 样本
  球数 3: 212 样本
  球数 4: 200 样本
  球数 5: 192 样本
  球数 6: 182 样本
  球数 7: 184 样本
  球数 8: 180 样本
  球数 9: 180 样本
  球数 10: 176 样本

验证集类别分布:
  球数 1: 162 样本
  球数 2: 126 样本
  球数 3: 108 样本
  球数 4: 105 样本
  球数 5: 102 样本
  球数 6: 98 样本
  球数 7: 96 样本
  球数 8: 99 样本
  球数 9: 90 样本
  球数 10: 88 样本
SingleImageTrainer initialized:
  Model parameters: 734,858
  Training samples: 2,076
  Validation samples: 1,074
  Image mode: RGB
  Use attention: True
  Label mapping: 1-10 -> 0-9 (for loss calculation)

开始训练单图像分类模型...
目标: 作为具身计数模型的对比基线
使用所有帧图像，标签为对应序列的ball_count

开始训练单图像分类模型
总计 350 个epoch
设备: cuda
图像模式: rgb
Testing model save/load functionality...
✓ Model save/load test passed
Saving initial model state...
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/initial_single_image_model.pth

Performing initial validation...
Initial validation - Loss: 2.3060, Accuracy: 0.1006
Epoch [0] Batch [0/130] Loss: 2.2976 LR: 0.000100
Epoch [0] Batch [10/130] Loss: 2.2744 LR: 0.000100
Epoch [0] Batch [20/130] Loss: 2.2338 LR: 0.000100
Epoch [0] Batch [30/130] Loss: 2.2067 LR: 0.000100
Epoch [0] Batch [40/130] Loss: 2.1680 LR: 0.000100
Epoch [0] Batch [50/130] Loss: 2.0236 LR: 0.000100
Epoch [0] Batch [60/130] Loss: 2.0434 LR: 0.000100
Epoch [0] Batch [70/130] Loss: 1.9245 LR: 0.000100
Epoch [0] Batch [80/130] Loss: 1.8128 LR: 0.000100
Epoch [0] Batch [90/130] Loss: 1.8224 LR: 0.000100
Epoch [0] Batch [100/130] Loss: 1.6943 LR: 0.000100
Epoch [0] Batch [110/130] Loss: 1.6637 LR: 0.000100
Epoch [0] Batch [120/130] Loss: 1.7339 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [1/350] - Time: 8.57s
Train Loss: 1.9567 | Val Loss: 1.5668
Train Acc: 0.2563 | Val Acc: 0.3166
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.008    Count 3: 0.009    Count 4: 0.000    Count 5: 0.853  
  Count 6: 0.000    Count 7: 0.000    Count 8: 0.000    Count 9: 1.000    Count 10: 0.000  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/best_single_image_model.pth
  → New best model! Validation accuracy: 0.3166
新的最佳模型! 验证准确率: 0.3166
Epoch [1] Batch [0/130] Loss: 1.5135 LR: 0.000100
Epoch [1] Batch [10/130] Loss: 1.6187 LR: 0.000100
Epoch [1] Batch [20/130] Loss: 1.7364 LR: 0.000100
Epoch [1] Batch [30/130] Loss: 1.6179 LR: 0.000100
Epoch [1] Batch [40/130] Loss: 1.1824 LR: 0.000100
Epoch [1] Batch [50/130] Loss: 1.7774 LR: 0.000100
Epoch [1] Batch [60/130] Loss: 1.7893 LR: 0.000100
Epoch [1] Batch [70/130] Loss: 1.4170 LR: 0.000100
Epoch [1] Batch [80/130] Loss: 1.4038 LR: 0.000100
Epoch [1] Batch [90/130] Loss: 1.4104 LR: 0.000100
Epoch [1] Batch [100/130] Loss: 1.3665 LR: 0.000100
Epoch [1] Batch [110/130] Loss: 1.4620 LR: 0.000100
Epoch [1] Batch [120/130] Loss: 1.4724 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [2/350] - Time: 8.38s
Train Loss: 1.4678 | Val Loss: 1.2327
Train Acc: 0.3839 | Val Acc: 0.4702
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.981    Count 2: 0.151    Count 3: 0.222    Count 4: 0.457    Count 5: 0.882  
  Count 6: 0.245    Count 7: 0.552    Count 8: 0.000    Count 9: 0.000    Count 10: 1.000  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/best_single_image_model.pth
  → New best model! Validation accuracy: 0.4702
新的最佳模型! 验证准确率: 0.4702
Epoch [2] Batch [0/130] Loss: 1.4435 LR: 0.000100
Epoch [2] Batch [10/130] Loss: 1.3589 LR: 0.000100
Epoch [2] Batch [20/130] Loss: 1.3846 LR: 0.000100
Epoch [2] Batch [30/130] Loss: 1.2513 LR: 0.000100
Epoch [2] Batch [40/130] Loss: 1.4817 LR: 0.000100
Epoch [2] Batch [50/130] Loss: 1.2259 LR: 0.000100
Epoch [2] Batch [60/130] Loss: 1.2107 LR: 0.000100
Epoch [2] Batch [70/130] Loss: 1.8286 LR: 0.000100
Epoch [2] Batch [80/130] Loss: 1.1205 LR: 0.000100
Epoch [2] Batch [90/130] Loss: 1.1312 LR: 0.000100
Epoch [2] Batch [100/130] Loss: 1.3129 LR: 0.000100
Epoch [2] Batch [110/130] Loss: 0.8568 LR: 0.000100
Epoch [2] Batch [120/130] Loss: 1.2677 LR: 0.000100

Epoch [3/350] - Time: 8.26s
Train Loss: 1.2466 | Val Loss: 1.3640
Train Acc: 0.4672 | Val Acc: 0.2588
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.079    Count 3: 0.111    Count 4: 0.238    Count 5: 0.059  
  Count 6: 0.173    Count 7: 0.031    Count 8: 0.000    Count 9: 0.489    Count 10: 0.000  
Epoch [3] Batch [0/130] Loss: 1.7797 LR: 0.000100
Epoch [3] Batch [10/130] Loss: 1.3234 LR: 0.000100
Epoch [3] Batch [20/130] Loss: 1.3769 LR: 0.000100
Epoch [3] Batch [30/130] Loss: 0.9115 LR: 0.000100
Epoch [3] Batch [40/130] Loss: 1.0072 LR: 0.000100
Epoch [3] Batch [50/130] Loss: 2.2400 LR: 0.000100
Epoch [3] Batch [60/130] Loss: 1.2265 LR: 0.000100
Epoch [3] Batch [70/130] Loss: 1.1037 LR: 0.000100
Epoch [3] Batch [80/130] Loss: 1.4443 LR: 0.000100
Epoch [3] Batch [90/130] Loss: 1.1457 LR: 0.000100
Epoch [3] Batch [100/130] Loss: 0.8895 LR: 0.000100
Epoch [3] Batch [110/130] Loss: 1.8558 LR: 0.000100
Epoch [3] Batch [120/130] Loss: 1.8070 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [4/350] - Time: 8.23s
Train Loss: 1.1061 | Val Loss: 0.8847
Train Acc: 0.5303 | Val Acc: 0.6387
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.981    Count 2: 0.833    Count 3: 0.722    Count 4: 0.733    Count 5: 0.588  
  Count 6: 0.041    Count 7: 0.833    Count 8: 0.141    Count 9: 0.467    Count 10: 0.761  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/best_single_image_model.pth
  → New best model! Validation accuracy: 0.6387
新的最佳模型! 验证准确率: 0.6387
Epoch [4] Batch [0/130] Loss: 1.0178 LR: 0.000100
Epoch [4] Batch [10/130] Loss: 0.9117 LR: 0.000100
Epoch [4] Batch [20/130] Loss: 0.8058 LR: 0.000100
Epoch [4] Batch [30/130] Loss: 0.7605 LR: 0.000100
Epoch [4] Batch [40/130] Loss: 0.7494 LR: 0.000100
Epoch [4] Batch [50/130] Loss: 0.8600 LR: 0.000100
Epoch [4] Batch [60/130] Loss: 0.9852 LR: 0.000100
Epoch [4] Batch [70/130] Loss: 0.8939 LR: 0.000100
Epoch [4] Batch [80/130] Loss: 1.2772 LR: 0.000100
Epoch [4] Batch [90/130] Loss: 1.3921 LR: 0.000100
Epoch [4] Batch [100/130] Loss: 0.7604 LR: 0.000100
Epoch [4] Batch [110/130] Loss: 0.7301 LR: 0.000100
Epoch [4] Batch [120/130] Loss: 0.6748 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [5/350] - Time: 8.30s
Train Loss: 0.9275 | Val Loss: 0.8190
Train Acc: 0.6031 | Val Acc: 0.6397
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.984    Count 3: 0.926    Count 4: 0.657    Count 5: 0.627  
  Count 6: 0.378    Count 7: 0.385    Count 8: 0.000    Count 9: 0.544    Count 10: 0.511  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/best_single_image_model.pth
  → New best model! Validation accuracy: 0.6397
新的最佳模型! 验证准确率: 0.6397
Epoch [5] Batch [0/130] Loss: 0.6677 LR: 0.000100
Epoch [5] Batch [10/130] Loss: 0.6535 LR: 0.000100
Epoch [5] Batch [20/130] Loss: 0.9785 LR: 0.000100
Epoch [5] Batch [30/130] Loss: 1.5440 LR: 0.000100
Epoch [5] Batch [40/130] Loss: 1.2172 LR: 0.000100
Epoch [5] Batch [50/130] Loss: 1.1567 LR: 0.000100
Epoch [5] Batch [60/130] Loss: 1.0190 LR: 0.000100
Epoch [5] Batch [70/130] Loss: 0.4942 LR: 0.000100
Epoch [5] Batch [80/130] Loss: 1.3604 LR: 0.000100
Epoch [5] Batch [90/130] Loss: 0.8167 LR: 0.000100
Epoch [5] Batch [100/130] Loss: 0.7866 LR: 0.000100
Epoch [5] Batch [110/130] Loss: 0.6099 LR: 0.000100
Epoch [5] Batch [120/130] Loss: 0.7926 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [6/350] - Time: 8.22s
Train Loss: 0.8402 | Val Loss: 0.8048
Train Acc: 0.6329 | Val Acc: 0.6713
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.969    Count 2: 0.889    Count 3: 0.852    Count 4: 0.590    Count 5: 0.412  
  Count 6: 0.316    Count 7: 0.573    Count 8: 0.455    Count 9: 0.500    Count 10: 0.909  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/best_single_image_model.pth
  → New best model! Validation accuracy: 0.6713
新的最佳模型! 验证准确率: 0.6713
Epoch [6] Batch [0/130] Loss: 0.9340 LR: 0.000100
Epoch [6] Batch [10/130] Loss: 0.8796 LR: 0.000100
Epoch [6] Batch [20/130] Loss: 0.7280 LR: 0.000100
Epoch [6] Batch [30/130] Loss: 0.4632 LR: 0.000100
Epoch [6] Batch [40/130] Loss: 1.8966 LR: 0.000100
Epoch [6] Batch [50/130] Loss: 0.5027 LR: 0.000100
Epoch [6] Batch [60/130] Loss: 0.7004 LR: 0.000100
Epoch [6] Batch [70/130] Loss: 0.8979 LR: 0.000100
Epoch [6] Batch [80/130] Loss: 1.2712 LR: 0.000100
Epoch [6] Batch [90/130] Loss: 0.4356 LR: 0.000100
Epoch [6] Batch [100/130] Loss: 0.6375 LR: 0.000100
Epoch [6] Batch [110/130] Loss: 1.4119 LR: 0.000100
Epoch [6] Batch [120/130] Loss: 0.7816 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [7/350] - Time: 8.29s
Train Loss: 0.7625 | Val Loss: 0.6285
Train Acc: 0.6749 | Val Acc: 0.7663
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.960    Count 3: 0.954    Count 4: 0.838    Count 5: 0.931  
  Count 6: 0.602    Count 7: 0.406    Count 8: 0.525    Count 9: 0.278    Count 10: 0.909  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/best_single_image_model.pth
  → New best model! Validation accuracy: 0.7663
新的最佳模型! 验证准确率: 0.7663
Epoch [7] Batch [0/130] Loss: 0.5840 LR: 0.000100
Epoch [7] Batch [10/130] Loss: 0.6184 LR: 0.000100
Epoch [7] Batch [20/130] Loss: 0.4909 LR: 0.000100
Epoch [7] Batch [30/130] Loss: 0.5104 LR: 0.000100
Epoch [7] Batch [40/130] Loss: 0.6230 LR: 0.000100
Epoch [7] Batch [50/130] Loss: 0.7957 LR: 0.000100
Epoch [7] Batch [60/130] Loss: 1.1938 LR: 0.000100
Epoch [7] Batch [70/130] Loss: 0.7357 LR: 0.000100
Epoch [7] Batch [80/130] Loss: 0.7908 LR: 0.000100
Epoch [7] Batch [90/130] Loss: 0.7727 LR: 0.000100
Epoch [7] Batch [100/130] Loss: 0.8222 LR: 0.000100
Epoch [7] Batch [110/130] Loss: 0.6497 LR: 0.000100
Epoch [7] Batch [120/130] Loss: 0.2984 LR: 0.000100

Epoch [8/350] - Time: 8.27s
Train Loss: 0.6974 | Val Loss: 0.7785
Train Acc: 0.7042 | Val Acc: 0.6471
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.905    Count 3: 0.926    Count 4: 0.686    Count 5: 0.824  
  Count 6: 0.398    Count 7: 0.302    Count 8: 0.030    Count 9: 0.511    Count 10: 0.523  
Epoch [8] Batch [0/130] Loss: 1.4284 LR: 0.000100
Epoch [8] Batch [10/130] Loss: 0.4280 LR: 0.000100
Epoch [8] Batch [20/130] Loss: 0.3422 LR: 0.000100
Epoch [8] Batch [30/130] Loss: 0.4019 LR: 0.000100
Epoch [8] Batch [40/130] Loss: 0.4482 LR: 0.000100
Epoch [8] Batch [50/130] Loss: 0.5136 LR: 0.000100
Epoch [8] Batch [60/130] Loss: 0.7651 LR: 0.000100
Epoch [8] Batch [70/130] Loss: 0.3891 LR: 0.000100
Epoch [8] Batch [80/130] Loss: 1.0659 LR: 0.000100
Epoch [8] Batch [90/130] Loss: 0.4278 LR: 0.000100
Epoch [8] Batch [100/130] Loss: 0.4429 LR: 0.000100
Epoch [8] Batch [110/130] Loss: 0.4222 LR: 0.000100
Epoch [8] Batch [120/130] Loss: 0.1922 LR: 0.000100

Epoch [9/350] - Time: 8.29s
Train Loss: 0.6721 | Val Loss: 0.5937
Train Acc: 0.7134 | Val Acc: 0.7449
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.988    Count 2: 0.960    Count 3: 0.843    Count 4: 0.933    Count 5: 0.520  
  Count 6: 0.827    Count 7: 0.323    Count 8: 0.626    Count 9: 0.344    Count 10: 0.818  
Epoch [9] Batch [0/130] Loss: 0.4801 LR: 0.000100
Epoch [9] Batch [10/130] Loss: 0.4751 LR: 0.000100
Epoch [9] Batch [20/130] Loss: 1.4315 LR: 0.000100
Epoch [9] Batch [30/130] Loss: 1.2925 LR: 0.000100
Epoch [9] Batch [40/130] Loss: 0.5152 LR: 0.000100
Epoch [9] Batch [50/130] Loss: 1.3690 LR: 0.000100
Epoch [9] Batch [60/130] Loss: 0.4046 LR: 0.000100
Epoch [9] Batch [70/130] Loss: 0.5982 LR: 0.000100
Epoch [9] Batch [80/130] Loss: 0.7490 LR: 0.000100
Epoch [9] Batch [90/130] Loss: 0.6531 LR: 0.000100
Epoch [9] Batch [100/130] Loss: 0.6583 LR: 0.000100
Epoch [9] Batch [110/130] Loss: 0.6090 LR: 0.000100
Epoch [9] Batch [120/130] Loss: 0.4851 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [10/350] - Time: 8.45s
Train Loss: 0.6260 | Val Loss: 0.7816
Train Acc: 0.7384 | Val Acc: 0.6415
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.992    Count 3: 0.972    Count 4: 0.590    Count 5: 0.833  
  Count 6: 0.408    Count 7: 0.302    Count 8: 0.242    Count 9: 0.289    Count 10: 0.364  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/single_image_checkpoint_epoch_9.pth
Epoch [10] Batch [0/130] Loss: 0.2747 LR: 0.000100
Epoch [10] Batch [10/130] Loss: 1.3815 LR: 0.000100
Epoch [10] Batch [20/130] Loss: 0.3662 LR: 0.000100
Epoch [10] Batch [30/130] Loss: 0.4214 LR: 0.000100
Epoch [10] Batch [40/130] Loss: 0.3854 LR: 0.000100
Epoch [10] Batch [50/130] Loss: 1.0232 LR: 0.000100
Epoch [10] Batch [60/130] Loss: 0.5200 LR: 0.000100
Epoch [10] Batch [70/130] Loss: 0.8120 LR: 0.000100
Epoch [10] Batch [80/130] Loss: 0.8184 LR: 0.000100
Epoch [10] Batch [90/130] Loss: 0.5552 LR: 0.000100
Epoch [10] Batch [100/130] Loss: 1.2114 LR: 0.000100
Epoch [10] Batch [110/130] Loss: 0.5843 LR: 0.000100
Epoch [10] Batch [120/130] Loss: 0.6151 LR: 0.000100

Epoch [11/350] - Time: 8.28s
Train Loss: 0.6733 | Val Loss: 0.5911
Train Acc: 0.7076 | Val Acc: 0.7626
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.984    Count 3: 0.954    Count 4: 0.924    Count 5: 0.882  
  Count 6: 0.561    Count 7: 0.385    Count 8: 0.687    Count 9: 0.289    Count 10: 0.648  
Epoch [11] Batch [0/130] Loss: 0.5388 LR: 0.000100
Epoch [11] Batch [10/130] Loss: 0.2000 LR: 0.000100
Epoch [11] Batch [20/130] Loss: 0.5001 LR: 0.000100
Epoch [11] Batch [30/130] Loss: 0.4406 LR: 0.000100
Epoch [11] Batch [40/130] Loss: 0.8457 LR: 0.000100
Epoch [11] Batch [50/130] Loss: 1.0865 LR: 0.000100
Epoch [11] Batch [60/130] Loss: 0.4254 LR: 0.000100
Epoch [11] Batch [70/130] Loss: 0.8496 LR: 0.000100
Epoch [11] Batch [80/130] Loss: 0.3669 LR: 0.000100
Epoch [11] Batch [90/130] Loss: 0.6559 LR: 0.000100
Epoch [11] Batch [100/130] Loss: 0.3369 LR: 0.000100
Epoch [11] Batch [110/130] Loss: 0.3304 LR: 0.000100
Epoch [11] Batch [120/130] Loss: 0.2360 LR: 0.000100

Epoch [12/350] - Time: 8.13s
Train Loss: 0.5309 | Val Loss: 1.3364
Train Acc: 0.7794 | Val Acc: 0.4935
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.802    Count 2: 0.873    Count 3: 0.630    Count 4: 0.210    Count 5: 0.216  
  Count 6: 0.153    Count 7: 0.365    Count 8: 0.222    Count 9: 0.267    Count 10: 0.932  
Epoch [12] Batch [0/130] Loss: 1.7140 LR: 0.000100
Epoch [12] Batch [10/130] Loss: 0.5362 LR: 0.000100
Epoch [12] Batch [20/130] Loss: 0.3227 LR: 0.000100
Epoch [12] Batch [30/130] Loss: 0.7236 LR: 0.000100
Epoch [12] Batch [40/130] Loss: 0.5327 LR: 0.000100
Epoch [12] Batch [50/130] Loss: 0.3569 LR: 0.000100
Epoch [12] Batch [60/130] Loss: 0.5994 LR: 0.000100
Epoch [12] Batch [70/130] Loss: 0.5102 LR: 0.000100
Epoch [12] Batch [80/130] Loss: 0.3932 LR: 0.000100
Epoch [12] Batch [90/130] Loss: 0.4903 LR: 0.000100
Epoch [12] Batch [100/130] Loss: 0.6218 LR: 0.000100
Epoch [12] Batch [110/130] Loss: 0.3118 LR: 0.000100
Epoch [12] Batch [120/130] Loss: 0.2495 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [13/350] - Time: 8.35s
Train Loss: 0.4809 | Val Loss: 0.4944
Train Acc: 0.8015 | Val Acc: 0.8091
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.976    Count 3: 0.991    Count 4: 0.952    Count 5: 0.892  
  Count 6: 0.684    Count 7: 0.667    Count 8: 0.687    Count 9: 0.556    Count 10: 0.432  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/best_single_image_model.pth
  → New best model! Validation accuracy: 0.8091
新的最佳模型! 验证准确率: 0.8091
Epoch [13] Batch [0/130] Loss: 0.3855 LR: 0.000100
Epoch [13] Batch [10/130] Loss: 0.7921 LR: 0.000100
Epoch [13] Batch [20/130] Loss: 0.2451 LR: 0.000100
Epoch [13] Batch [30/130] Loss: 0.3078 LR: 0.000100
Epoch [13] Batch [40/130] Loss: 0.2713 LR: 0.000100
Epoch [13] Batch [50/130] Loss: 0.4052 LR: 0.000100
Epoch [13] Batch [60/130] Loss: 0.3356 LR: 0.000100
Epoch [13] Batch [70/130] Loss: 0.3297 LR: 0.000100
Epoch [13] Batch [80/130] Loss: 0.2685 LR: 0.000100
Epoch [13] Batch [90/130] Loss: 0.5447 LR: 0.000100
Epoch [13] Batch [100/130] Loss: 0.6970 LR: 0.000100
Epoch [13] Batch [110/130] Loss: 0.3791 LR: 0.000100
Epoch [13] Batch [120/130] Loss: 0.3488 LR: 0.000100

Epoch [14/350] - Time: 8.33s
Train Loss: 0.4023 | Val Loss: 0.5646
Train Acc: 0.8425 | Val Acc: 0.7980
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.984    Count 3: 0.944    Count 4: 0.924    Count 5: 0.951  
  Count 6: 0.827    Count 7: 0.604    Count 8: 0.677    Count 9: 0.356    Count 10: 0.420  
Epoch [14] Batch [0/130] Loss: 0.1455 LR: 0.000100
Epoch [14] Batch [10/130] Loss: 0.2521 LR: 0.000100
Epoch [14] Batch [20/130] Loss: 0.4444 LR: 0.000100
Epoch [14] Batch [30/130] Loss: 0.2112 LR: 0.000100
Epoch [14] Batch [40/130] Loss: 0.2548 LR: 0.000100
Epoch [14] Batch [50/130] Loss: 0.0783 LR: 0.000100
Epoch [14] Batch [60/130] Loss: 0.3452 LR: 0.000100
Epoch [14] Batch [70/130] Loss: 0.3156 LR: 0.000100
Epoch [14] Batch [80/130] Loss: 0.3577 LR: 0.000100
Epoch [14] Batch [90/130] Loss: 0.1696 LR: 0.000100
Epoch [14] Batch [100/130] Loss: 0.4100 LR: 0.000100
Epoch [14] Batch [110/130] Loss: 0.1839 LR: 0.000100
Epoch [14] Batch [120/130] Loss: 0.2973 LR: 0.000100

Epoch [15/350] - Time: 8.23s
Train Loss: 0.3453 | Val Loss: 0.8075
Train Acc: 0.8656 | Val Acc: 0.7458
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.952    Count 3: 0.954    Count 4: 0.829    Count 5: 0.647  
  Count 6: 0.531    Count 7: 0.625    Count 8: 0.535    Count 9: 0.500    Count 10: 0.602  
Epoch [15] Batch [0/130] Loss: 0.3638 LR: 0.000100
Epoch [15] Batch [10/130] Loss: 0.3199 LR: 0.000100
Epoch [15] Batch [20/130] Loss: 0.2361 LR: 0.000100
Epoch [15] Batch [30/130] Loss: 0.2323 LR: 0.000100
Epoch [15] Batch [40/130] Loss: 0.6264 LR: 0.000100
Epoch [15] Batch [50/130] Loss: 0.0762 LR: 0.000100
Epoch [15] Batch [60/130] Loss: 0.3010 LR: 0.000100
Epoch [15] Batch [70/130] Loss: 0.2566 LR: 0.000100
Epoch [15] Batch [80/130] Loss: 0.2953 LR: 0.000100
Epoch [15] Batch [90/130] Loss: 0.2853 LR: 0.000100
Epoch [15] Batch [100/130] Loss: 0.3379 LR: 0.000100
Epoch [15] Batch [110/130] Loss: 0.2077 LR: 0.000100
Epoch [15] Batch [120/130] Loss: 0.4651 LR: 0.000100

Epoch [16/350] - Time: 8.39s
Train Loss: 0.3013 | Val Loss: 1.0513
Train Acc: 0.8796 | Val Acc: 0.6406
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.921    Count 3: 0.954    Count 4: 0.724    Count 5: 0.804  
  Count 6: 0.602    Count 7: 0.271    Count 8: 0.141    Count 9: 0.244    Count 10: 0.318  
Epoch [16] Batch [0/130] Loss: 0.3293 LR: 0.000100
Epoch [16] Batch [10/130] Loss: 1.3699 LR: 0.000100
Epoch [16] Batch [20/130] Loss: 0.2385 LR: 0.000100
Epoch [16] Batch [30/130] Loss: 0.7368 LR: 0.000100
Epoch [16] Batch [40/130] Loss: 0.3200 LR: 0.000100
Epoch [16] Batch [50/130] Loss: 0.1572 LR: 0.000100
Epoch [16] Batch [60/130] Loss: 0.9748 LR: 0.000100
Epoch [16] Batch [70/130] Loss: 0.6308 LR: 0.000100
Epoch [16] Batch [80/130] Loss: 0.1763 LR: 0.000100
Epoch [16] Batch [90/130] Loss: 0.2397 LR: 0.000100
Epoch [16] Batch [100/130] Loss: 0.1790 LR: 0.000100
Epoch [16] Batch [110/130] Loss: 0.2035 LR: 0.000100
Epoch [16] Batch [120/130] Loss: 0.2970 LR: 0.000100

Epoch [17/350] - Time: 8.26s
Train Loss: 0.3776 | Val Loss: 1.2704
Train Acc: 0.8512 | Val Acc: 0.5708
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.992    Count 3: 0.898    Count 4: 0.543    Count 5: 0.618  
  Count 6: 0.316    Count 7: 0.229    Count 8: 0.162    Count 9: 0.167    Count 10: 0.284  
Epoch [17] Batch [0/130] Loss: 0.5882 LR: 0.000100
Epoch [17] Batch [10/130] Loss: 0.1979 LR: 0.000100
Epoch [17] Batch [20/130] Loss: 0.3260 LR: 0.000100
Epoch [17] Batch [30/130] Loss: 0.6514 LR: 0.000100
Epoch [17] Batch [40/130] Loss: 0.2857 LR: 0.000100
Epoch [17] Batch [50/130] Loss: 0.2221 LR: 0.000100
Epoch [17] Batch [60/130] Loss: 0.1258 LR: 0.000100
Epoch [17] Batch [70/130] Loss: 0.5465 LR: 0.000100
Epoch [17] Batch [80/130] Loss: 0.3469 LR: 0.000100
Epoch [17] Batch [90/130] Loss: 0.1920 LR: 0.000100
Epoch [17] Batch [100/130] Loss: 0.2598 LR: 0.000100
Epoch [17] Batch [110/130] Loss: 0.2364 LR: 0.000100
Epoch [17] Batch [120/130] Loss: 0.6241 LR: 0.000100

Epoch [18/350] - Time: 8.52s
Train Loss: 0.3286 | Val Loss: 0.6423
Train Acc: 0.8709 | Val Acc: 0.7998
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.981    Count 4: 0.981    Count 5: 0.873  
  Count 6: 0.806    Count 7: 0.656    Count 8: 0.576    Count 9: 0.422    Count 10: 0.409  
Epoch [18] Batch [0/130] Loss: 0.1385 LR: 0.000100
Epoch [18] Batch [10/130] Loss: 0.0726 LR: 0.000100
Epoch [18] Batch [20/130] Loss: 0.4174 LR: 0.000100
Epoch [18] Batch [30/130] Loss: 0.1577 LR: 0.000100
Epoch [18] Batch [40/130] Loss: 0.0531 LR: 0.000100
Epoch [18] Batch [50/130] Loss: 0.5594 LR: 0.000100
Epoch [18] Batch [60/130] Loss: 0.2857 LR: 0.000100
Epoch [18] Batch [70/130] Loss: 0.0973 LR: 0.000100
Epoch [18] Batch [80/130] Loss: 0.1942 LR: 0.000100
Epoch [18] Batch [90/130] Loss: 0.2175 LR: 0.000100
Epoch [18] Batch [100/130] Loss: 0.3941 LR: 0.000100
Epoch [18] Batch [110/130] Loss: 0.3939 LR: 0.000100
Epoch [18] Batch [120/130] Loss: 0.4495 LR: 0.000100

Epoch [19/350] - Time: 8.25s
Train Loss: 0.2809 | Val Loss: 0.7560
Train Acc: 0.8911 | Val Acc: 0.7430
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.992    Count 3: 0.963    Count 4: 0.905    Count 5: 0.971  
  Count 6: 0.541    Count 7: 0.719    Count 8: 0.212    Count 9: 0.311    Count 10: 0.477  
Epoch [19] Batch [0/130] Loss: 0.3038 LR: 0.000100
Epoch [19] Batch [10/130] Loss: 0.2972 LR: 0.000100
Epoch [19] Batch [20/130] Loss: 0.1979 LR: 0.000100
Epoch [19] Batch [30/130] Loss: 0.2899 LR: 0.000100
Epoch [19] Batch [40/130] Loss: 0.4215 LR: 0.000100
Epoch [19] Batch [50/130] Loss: 0.4960 LR: 0.000100
Epoch [19] Batch [60/130] Loss: 0.0716 LR: 0.000100
Epoch [19] Batch [70/130] Loss: 0.1061 LR: 0.000100
Epoch [19] Batch [80/130] Loss: 0.9815 LR: 0.000100
Epoch [19] Batch [90/130] Loss: 0.7040 LR: 0.000100
Epoch [19] Batch [100/130] Loss: 0.1250 LR: 0.000100
Epoch [19] Batch [110/130] Loss: 0.0499 LR: 0.000100
Epoch [19] Batch [120/130] Loss: 0.1147 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [20/350] - Time: 8.31s
Train Loss: 0.2484 | Val Loss: 0.7559
Train Acc: 0.9051 | Val Acc: 0.7691
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.954    Count 4: 0.952    Count 5: 0.941  
  Count 6: 0.765    Count 7: 0.333    Count 8: 0.596    Count 9: 0.333    Count 10: 0.489  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/single_image_checkpoint_epoch_19.pth
Epoch [20] Batch [0/130] Loss: 0.0830 LR: 0.000100
Epoch [20] Batch [10/130] Loss: 0.3186 LR: 0.000100
Epoch [20] Batch [20/130] Loss: 0.1890 LR: 0.000100
Epoch [20] Batch [30/130] Loss: 0.3658 LR: 0.000100
Epoch [20] Batch [40/130] Loss: 0.4242 LR: 0.000100
Epoch [20] Batch [50/130] Loss: 0.4731 LR: 0.000100
Epoch [20] Batch [60/130] Loss: 0.5342 LR: 0.000100
Epoch [20] Batch [70/130] Loss: 0.0689 LR: 0.000100
Epoch [20] Batch [80/130] Loss: 0.1184 LR: 0.000100
Epoch [20] Batch [90/130] Loss: 0.1793 LR: 0.000100
Epoch [20] Batch [100/130] Loss: 0.2019 LR: 0.000100
Epoch [20] Batch [110/130] Loss: 0.2700 LR: 0.000100
Epoch [20] Batch [120/130] Loss: 0.1883 LR: 0.000100

Epoch [21/350] - Time: 8.13s
Train Loss: 0.2749 | Val Loss: 0.9718
Train Acc: 0.8931 | Val Acc: 0.7328
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.984    Count 3: 0.954    Count 4: 0.857    Count 5: 0.971  
  Count 6: 0.633    Count 7: 0.271    Count 8: 0.495    Count 9: 0.233    Count 10: 0.580  
Epoch [21] Batch [0/130] Loss: 0.1066 LR: 0.000100
Epoch [21] Batch [10/130] Loss: 0.1832 LR: 0.000100
Epoch [21] Batch [20/130] Loss: 0.0473 LR: 0.000100
Epoch [21] Batch [30/130] Loss: 0.0935 LR: 0.000100
Epoch [21] Batch [40/130] Loss: 0.6335 LR: 0.000100
Epoch [21] Batch [50/130] Loss: 0.0565 LR: 0.000100
Epoch [21] Batch [60/130] Loss: 0.2779 LR: 0.000100
Epoch [21] Batch [70/130] Loss: 0.7220 LR: 0.000100
Epoch [21] Batch [80/130] Loss: 0.3280 LR: 0.000100
Epoch [21] Batch [90/130] Loss: 0.1002 LR: 0.000100
Epoch [21] Batch [100/130] Loss: 0.0336 LR: 0.000100
Epoch [21] Batch [110/130] Loss: 0.5969 LR: 0.000100
Epoch [21] Batch [120/130] Loss: 0.0140 LR: 0.000100

Epoch [22/350] - Time: 8.39s
Train Loss: 0.2100 | Val Loss: 0.7558
Train Acc: 0.9253 | Val Acc: 0.7980
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.933    Count 5: 0.922  
  Count 6: 0.796    Count 7: 0.604    Count 8: 0.606    Count 9: 0.356    Count 10: 0.477  
Epoch [22] Batch [0/130] Loss: 0.0183 LR: 0.000100
Epoch [22] Batch [10/130] Loss: 0.0267 LR: 0.000100
Epoch [22] Batch [20/130] Loss: 0.0242 LR: 0.000100
Epoch [22] Batch [30/130] Loss: 0.0541 LR: 0.000100
Epoch [22] Batch [40/130] Loss: 0.2035 LR: 0.000100
Epoch [22] Batch [50/130] Loss: 0.2348 LR: 0.000100
Epoch [22] Batch [60/130] Loss: 0.1833 LR: 0.000100
Epoch [22] Batch [70/130] Loss: 0.5829 LR: 0.000100
Epoch [22] Batch [80/130] Loss: 0.2301 LR: 0.000100
Epoch [22] Batch [90/130] Loss: 0.2787 LR: 0.000100
Epoch [22] Batch [100/130] Loss: 0.5673 LR: 0.000100
Epoch [22] Batch [110/130] Loss: 0.2349 LR: 0.000100
Epoch [22] Batch [120/130] Loss: 0.0815 LR: 0.000100

Epoch [23/350] - Time: 8.23s
Train Loss: 0.2112 | Val Loss: 1.0574
Train Acc: 0.9186 | Val Acc: 0.7225
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.952    Count 3: 0.963    Count 4: 0.857    Count 5: 0.912  
  Count 6: 0.796    Count 7: 0.427    Count 8: 0.293    Count 9: 0.244    Count 10: 0.420  
Epoch [23] Batch [0/130] Loss: 0.2297 LR: 0.000100
Epoch [23] Batch [10/130] Loss: 0.1289 LR: 0.000100
Epoch [23] Batch [20/130] Loss: 0.2175 LR: 0.000100
Epoch [23] Batch [30/130] Loss: 0.0415 LR: 0.000100
Epoch [23] Batch [40/130] Loss: 0.2209 LR: 0.000100
Epoch [23] Batch [50/130] Loss: 0.0890 LR: 0.000100
Epoch [23] Batch [60/130] Loss: 0.1112 LR: 0.000100
Epoch [23] Batch [70/130] Loss: 0.1162 LR: 0.000100
Epoch [23] Batch [80/130] Loss: 0.0253 LR: 0.000100
Epoch [23] Batch [90/130] Loss: 0.0252 LR: 0.000100
Epoch [23] Batch [100/130] Loss: 0.1596 LR: 0.000100
Epoch [23] Batch [110/130] Loss: 0.0785 LR: 0.000100
Epoch [23] Batch [120/130] Loss: 0.0556 LR: 0.000100

Epoch [24/350] - Time: 8.28s
Train Loss: 0.2294 | Val Loss: 1.6065
Train Acc: 0.9109 | Val Acc: 0.6397
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.992    Count 3: 0.963    Count 4: 0.829    Count 5: 0.657  
  Count 6: 0.490    Count 7: 0.292    Count 8: 0.232    Count 9: 0.156    Count 10: 0.330  
Epoch [24] Batch [0/130] Loss: 0.9618 LR: 0.000100
Epoch [24] Batch [10/130] Loss: 0.0969 LR: 0.000100
Epoch [24] Batch [20/130] Loss: 0.3845 LR: 0.000100
Epoch [24] Batch [30/130] Loss: 0.2798 LR: 0.000100
Epoch [24] Batch [40/130] Loss: 0.6330 LR: 0.000100
Epoch [24] Batch [50/130] Loss: 0.1564 LR: 0.000100
Epoch [24] Batch [60/130] Loss: 0.2685 LR: 0.000100
Epoch [24] Batch [70/130] Loss: 0.1790 LR: 0.000100
Epoch [24] Batch [80/130] Loss: 0.0344 LR: 0.000100
Epoch [24] Batch [90/130] Loss: 0.2708 LR: 0.000100
Epoch [24] Batch [100/130] Loss: 0.0249 LR: 0.000100
Epoch [24] Batch [110/130] Loss: 0.1215 LR: 0.000100
Epoch [24] Batch [120/130] Loss: 0.2090 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [25/350] - Time: 8.32s
Train Loss: 0.1971 | Val Loss: 0.7987
Train Acc: 0.9210 | Val Acc: 0.8128
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.972    Count 4: 0.971    Count 5: 0.971  
  Count 6: 0.694    Count 7: 0.625    Count 8: 0.566    Count 9: 0.433    Count 10: 0.636  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/best_single_image_model.pth
  → New best model! Validation accuracy: 0.8128
新的最佳模型! 验证准确率: 0.8128
Epoch [25] Batch [0/130] Loss: 0.1660 LR: 0.000100
Epoch [25] Batch [10/130] Loss: 0.0388 LR: 0.000100
Epoch [25] Batch [20/130] Loss: 0.0852 LR: 0.000100
Epoch [25] Batch [30/130] Loss: 0.2344 LR: 0.000100
Epoch [25] Batch [40/130] Loss: 0.3112 LR: 0.000100
Epoch [25] Batch [50/130] Loss: 0.2871 LR: 0.000100
Epoch [25] Batch [60/130] Loss: 0.2107 LR: 0.000100
Epoch [25] Batch [70/130] Loss: 0.0533 LR: 0.000100
Epoch [25] Batch [80/130] Loss: 0.0792 LR: 0.000100
Epoch [25] Batch [90/130] Loss: 0.3091 LR: 0.000100
Epoch [25] Batch [100/130] Loss: 0.1454 LR: 0.000100
Epoch [25] Batch [110/130] Loss: 0.0834 LR: 0.000100
Epoch [25] Batch [120/130] Loss: 0.0468 LR: 0.000100

Epoch [26/350] - Time: 8.56s
Train Loss: 0.1850 | Val Loss: 1.1029
Train Acc: 0.9364 | Val Acc: 0.7672
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.992    Count 3: 0.963    Count 4: 0.962    Count 5: 0.990  
  Count 6: 0.816    Count 7: 0.573    Count 8: 0.273    Count 9: 0.289    Count 10: 0.489  
Epoch [26] Batch [0/130] Loss: 0.1123 LR: 0.000100
Epoch [26] Batch [10/130] Loss: 0.1696 LR: 0.000100
Epoch [26] Batch [20/130] Loss: 0.0918 LR: 0.000100
Epoch [26] Batch [30/130] Loss: 0.0189 LR: 0.000100
Epoch [26] Batch [40/130] Loss: 0.0109 LR: 0.000100
Epoch [26] Batch [50/130] Loss: 0.3256 LR: 0.000100
Epoch [26] Batch [60/130] Loss: 0.0386 LR: 0.000100
Epoch [26] Batch [70/130] Loss: 0.0243 LR: 0.000100
Epoch [26] Batch [80/130] Loss: 0.0107 LR: 0.000100
Epoch [26] Batch [90/130] Loss: 0.0145 LR: 0.000100
Epoch [26] Batch [100/130] Loss: 0.0193 LR: 0.000100
Epoch [26] Batch [110/130] Loss: 0.2441 LR: 0.000100
Epoch [26] Batch [120/130] Loss: 0.1576 LR: 0.000100

Epoch [27/350] - Time: 8.17s
Train Loss: 0.1504 | Val Loss: 0.8686
Train Acc: 0.9417 | Val Acc: 0.8091
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.992    Count 3: 0.972    Count 4: 0.981    Count 5: 0.941  
  Count 6: 0.806    Count 7: 0.594    Count 8: 0.636    Count 9: 0.467    Count 10: 0.420  
Epoch [27] Batch [0/130] Loss: 0.2525 LR: 0.000100
Epoch [27] Batch [10/130] Loss: 0.0990 LR: 0.000100
Epoch [27] Batch [20/130] Loss: 0.1778 LR: 0.000100
Epoch [27] Batch [30/130] Loss: 0.0300 LR: 0.000100
Epoch [27] Batch [40/130] Loss: 0.1872 LR: 0.000100
Epoch [27] Batch [50/130] Loss: 0.0778 LR: 0.000100
Epoch [27] Batch [60/130] Loss: 0.1808 LR: 0.000100
Epoch [27] Batch [70/130] Loss: 0.0230 LR: 0.000100
Epoch [27] Batch [80/130] Loss: 0.0085 LR: 0.000100
Epoch [27] Batch [90/130] Loss: 0.0753 LR: 0.000100
Epoch [27] Batch [100/130] Loss: 0.0169 LR: 0.000100
Epoch [27] Batch [110/130] Loss: 0.1754 LR: 0.000100
Epoch [27] Batch [120/130] Loss: 0.0582 LR: 0.000100

Epoch [28/350] - Time: 8.28s
Train Loss: 0.1470 | Val Loss: 1.0087
Train Acc: 0.9528 | Val Acc: 0.8017
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.992    Count 3: 0.981    Count 4: 0.952    Count 5: 0.892  
  Count 6: 0.827    Count 7: 0.771    Count 8: 0.505    Count 9: 0.378    Count 10: 0.432  
Epoch [28] Batch [0/130] Loss: 0.0324 LR: 0.000100
Epoch [28] Batch [10/130] Loss: 0.0450 LR: 0.000100
Epoch [28] Batch [20/130] Loss: 0.1121 LR: 0.000100
Epoch [28] Batch [30/130] Loss: 0.0158 LR: 0.000100
Epoch [28] Batch [40/130] Loss: 0.0226 LR: 0.000100
Epoch [28] Batch [50/130] Loss: 0.3368 LR: 0.000100
Epoch [28] Batch [60/130] Loss: 0.0121 LR: 0.000100
Epoch [28] Batch [70/130] Loss: 0.0299 LR: 0.000100
Epoch [28] Batch [80/130] Loss: 0.0765 LR: 0.000100
Epoch [28] Batch [90/130] Loss: 0.1162 LR: 0.000100
Epoch [28] Batch [100/130] Loss: 0.0562 LR: 0.000100
Epoch [28] Batch [110/130] Loss: 0.1916 LR: 0.000100
Epoch [28] Batch [120/130] Loss: 0.0653 LR: 0.000100

Epoch [29/350] - Time: 8.27s
Train Loss: 0.1828 | Val Loss: 0.9499
Train Acc: 0.9321 | Val Acc: 0.8119
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.981    Count 4: 0.876    Count 5: 0.814  
  Count 6: 0.755    Count 7: 0.750    Count 8: 0.657    Count 9: 0.300    Count 10: 0.739  
Epoch [29] Batch [0/130] Loss: 0.1945 LR: 0.000100
Epoch [29] Batch [10/130] Loss: 0.3548 LR: 0.000100
Epoch [29] Batch [20/130] Loss: 0.0078 LR: 0.000100
Epoch [29] Batch [30/130] Loss: 0.0352 LR: 0.000100
Epoch [29] Batch [40/130] Loss: 0.1654 LR: 0.000100
Epoch [29] Batch [50/130] Loss: 0.3334 LR: 0.000100
Epoch [29] Batch [60/130] Loss: 0.1687 LR: 0.000100
Epoch [29] Batch [70/130] Loss: 0.3735 LR: 0.000100
Epoch [29] Batch [80/130] Loss: 0.1759 LR: 0.000100
Epoch [29] Batch [90/130] Loss: 0.2894 LR: 0.000100
Epoch [29] Batch [100/130] Loss: 0.0932 LR: 0.000100
Epoch [29] Batch [110/130] Loss: 0.0964 LR: 0.000100
Epoch [29] Batch [120/130] Loss: 0.0535 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [30/350] - Time: 8.23s
Train Loss: 0.1317 | Val Loss: 1.2432
Train Acc: 0.9494 | Val Acc: 0.7756
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.984    Count 3: 0.972    Count 4: 0.962    Count 5: 0.931  
  Count 6: 0.857    Count 7: 0.479    Count 8: 0.475    Count 9: 0.211    Count 10: 0.568  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/single_image_checkpoint_epoch_29.pth
Epoch [30] Batch [0/130] Loss: 0.0189 LR: 0.000100
Epoch [30] Batch [10/130] Loss: 0.0289 LR: 0.000100
Epoch [30] Batch [20/130] Loss: 0.6016 LR: 0.000100
Epoch [30] Batch [30/130] Loss: 0.3515 LR: 0.000100
Epoch [30] Batch [40/130] Loss: 0.0288 LR: 0.000100
Epoch [30] Batch [50/130] Loss: 0.0574 LR: 0.000100
Epoch [30] Batch [60/130] Loss: 0.1137 LR: 0.000100
Epoch [30] Batch [70/130] Loss: 0.3284 LR: 0.000100
Epoch [30] Batch [80/130] Loss: 0.0356 LR: 0.000100
Epoch [30] Batch [90/130] Loss: 0.0346 LR: 0.000100
Epoch [30] Batch [100/130] Loss: 0.0547 LR: 0.000100
Epoch [30] Batch [110/130] Loss: 0.1744 LR: 0.000100
Epoch [30] Batch [120/130] Loss: 0.0018 LR: 0.000100

Epoch [31/350] - Time: 8.24s
Train Loss: 0.1395 | Val Loss: 1.0388
Train Acc: 0.9485 | Val Acc: 0.8007
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.963    Count 4: 0.981    Count 5: 0.931  
  Count 6: 0.847    Count 7: 0.552    Count 8: 0.636    Count 9: 0.367    Count 10: 0.432  
Epoch [31] Batch [0/130] Loss: 0.0055 LR: 0.000100
Epoch [31] Batch [10/130] Loss: 0.1173 LR: 0.000100
Epoch [31] Batch [20/130] Loss: 0.0470 LR: 0.000100
Epoch [31] Batch [30/130] Loss: 0.0268 LR: 0.000100
Epoch [31] Batch [40/130] Loss: 0.0088 LR: 0.000100
Epoch [31] Batch [50/130] Loss: 0.3780 LR: 0.000100
Epoch [31] Batch [60/130] Loss: 0.0010 LR: 0.000100
Epoch [31] Batch [70/130] Loss: 0.0190 LR: 0.000100
Epoch [31] Batch [80/130] Loss: 0.0679 LR: 0.000100
Epoch [31] Batch [90/130] Loss: 0.0165 LR: 0.000100
Epoch [31] Batch [100/130] Loss: 0.0303 LR: 0.000100
Epoch [31] Batch [110/130] Loss: 0.2401 LR: 0.000100
Epoch [31] Batch [120/130] Loss: 0.2491 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [32/350] - Time: 8.23s
Train Loss: 0.1169 | Val Loss: 1.0030
Train Acc: 0.9605 | Val Acc: 0.8147
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.963    Count 4: 0.971    Count 5: 0.951  
  Count 6: 0.796    Count 7: 0.635    Count 8: 0.626    Count 9: 0.511    Count 10: 0.420  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/best_single_image_model.pth
  → New best model! Validation accuracy: 0.8147
新的最佳模型! 验证准确率: 0.8147
Epoch [32] Batch [0/130] Loss: 0.1197 LR: 0.000100
Epoch [32] Batch [10/130] Loss: 0.1479 LR: 0.000100
Epoch [32] Batch [20/130] Loss: 0.0892 LR: 0.000100
Epoch [32] Batch [30/130] Loss: 0.0845 LR: 0.000100
Epoch [32] Batch [40/130] Loss: 0.0278 LR: 0.000100
Epoch [32] Batch [50/130] Loss: 0.0864 LR: 0.000100
Epoch [32] Batch [60/130] Loss: 0.0558 LR: 0.000100
Epoch [32] Batch [70/130] Loss: 0.0239 LR: 0.000100
Epoch [32] Batch [80/130] Loss: 0.0663 LR: 0.000100
Epoch [32] Batch [90/130] Loss: 0.0056 LR: 0.000100
Epoch [32] Batch [100/130] Loss: 0.0142 LR: 0.000100
Epoch [32] Batch [110/130] Loss: 0.1564 LR: 0.000100
Epoch [32] Batch [120/130] Loss: 0.0089 LR: 0.000100

Epoch [33/350] - Time: 8.16s
Train Loss: 0.1189 | Val Loss: 1.4461
Train Acc: 0.9653 | Val Acc: 0.7775
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.984    Count 3: 1.000    Count 4: 0.962    Count 5: 0.971  
  Count 6: 0.837    Count 7: 0.531    Count 8: 0.616    Count 9: 0.256    Count 10: 0.273  
Epoch [33] Batch [0/130] Loss: 0.3241 LR: 0.000100
Epoch [33] Batch [10/130] Loss: 0.0150 LR: 0.000100
Epoch [33] Batch [20/130] Loss: 0.2892 LR: 0.000100
Epoch [33] Batch [30/130] Loss: 0.0271 LR: 0.000100
Epoch [33] Batch [40/130] Loss: 0.1783 LR: 0.000100
Epoch [33] Batch [50/130] Loss: 0.0948 LR: 0.000100
Epoch [33] Batch [60/130] Loss: 0.5609 LR: 0.000100
Epoch [33] Batch [70/130] Loss: 0.0231 LR: 0.000100
Epoch [33] Batch [80/130] Loss: 0.0148 LR: 0.000100
Epoch [33] Batch [90/130] Loss: 0.2933 LR: 0.000100
Epoch [33] Batch [100/130] Loss: 0.4511 LR: 0.000100
Epoch [33] Batch [110/130] Loss: 0.1010 LR: 0.000100
Epoch [33] Batch [120/130] Loss: 0.0319 LR: 0.000100

Epoch [34/350] - Time: 8.54s
Train Loss: 0.1311 | Val Loss: 1.6680
Train Acc: 0.9557 | Val Acc: 0.7747
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 1.000    Count 3: 1.000    Count 4: 0.981    Count 5: 0.961  
  Count 6: 0.857    Count 7: 0.438    Count 8: 0.646    Count 9: 0.256    Count 10: 0.261  
Epoch [34] Batch [0/130] Loss: 0.0706 LR: 0.000100
Epoch [34] Batch [10/130] Loss: 0.0136 LR: 0.000100
Epoch [34] Batch [20/130] Loss: 0.2982 LR: 0.000100
Epoch [34] Batch [30/130] Loss: 0.4903 LR: 0.000100
Epoch [34] Batch [40/130] Loss: 0.0947 LR: 0.000100
Epoch [34] Batch [50/130] Loss: 0.0051 LR: 0.000100
Epoch [34] Batch [60/130] Loss: 0.4793 LR: 0.000100
Epoch [34] Batch [70/130] Loss: 0.0029 LR: 0.000100
Epoch [34] Batch [80/130] Loss: 0.0027 LR: 0.000100
Epoch [34] Batch [90/130] Loss: 0.1019 LR: 0.000100
Epoch [34] Batch [100/130] Loss: 0.0766 LR: 0.000100
Epoch [34] Batch [110/130] Loss: 0.0282 LR: 0.000100
Epoch [34] Batch [120/130] Loss: 0.1086 LR: 0.000100

Epoch [35/350] - Time: 8.24s
Train Loss: 0.1558 | Val Loss: 1.2329
Train Acc: 0.9509 | Val Acc: 0.8147
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.971    Count 5: 0.882  
  Count 6: 0.857    Count 7: 0.656    Count 8: 0.667    Count 9: 0.433    Count 10: 0.409  
Epoch [35] Batch [0/130] Loss: 0.1740 LR: 0.000100
Epoch [35] Batch [10/130] Loss: 0.0430 LR: 0.000100
Epoch [35] Batch [20/130] Loss: 0.0012 LR: 0.000100
Epoch [35] Batch [30/130] Loss: 0.0006 LR: 0.000100
Epoch [35] Batch [40/130] Loss: 0.0320 LR: 0.000100
Epoch [35] Batch [50/130] Loss: 0.0177 LR: 0.000100
Epoch [35] Batch [60/130] Loss: 0.1388 LR: 0.000100
Epoch [35] Batch [70/130] Loss: 0.0044 LR: 0.000100
Epoch [35] Batch [80/130] Loss: 0.0078 LR: 0.000100
Epoch [35] Batch [90/130] Loss: 0.0073 LR: 0.000100
Epoch [35] Batch [100/130] Loss: 0.0012 LR: 0.000100
Epoch [35] Batch [110/130] Loss: 0.0024 LR: 0.000100
Epoch [35] Batch [120/130] Loss: 0.0256 LR: 0.000100

Epoch [36/350] - Time: 8.18s
Train Loss: 0.0816 | Val Loss: 1.2235
Train Acc: 0.9740 | Val Acc: 0.8073
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.921    Count 3: 1.000    Count 4: 0.905    Count 5: 0.843  
  Count 6: 0.857    Count 7: 0.708    Count 8: 0.606    Count 9: 0.378    Count 10: 0.614  
Epoch [36] Batch [0/130] Loss: 0.0117 LR: 0.000100
Epoch [36] Batch [10/130] Loss: 0.2851 LR: 0.000100
Epoch [36] Batch [20/130] Loss: 0.1402 LR: 0.000100
Epoch [36] Batch [30/130] Loss: 0.0250 LR: 0.000100
Epoch [36] Batch [40/130] Loss: 0.0030 LR: 0.000100
Epoch [36] Batch [50/130] Loss: 0.0334 LR: 0.000100
Epoch [36] Batch [60/130] Loss: 0.0078 LR: 0.000100
Epoch [36] Batch [70/130] Loss: 0.4550 LR: 0.000100
Epoch [36] Batch [80/130] Loss: 0.1607 LR: 0.000100
Epoch [36] Batch [90/130] Loss: 0.1317 LR: 0.000100
Epoch [36] Batch [100/130] Loss: 0.0480 LR: 0.000100
Epoch [36] Batch [110/130] Loss: 0.0068 LR: 0.000100
Epoch [36] Batch [120/130] Loss: 0.0118 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [37/350] - Time: 8.26s
Train Loss: 0.1187 | Val Loss: 1.0308
Train Acc: 0.9610 | Val Acc: 0.8287
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.971    Count 5: 0.931  
  Count 6: 0.857    Count 7: 0.656    Count 8: 0.636    Count 9: 0.400    Count 10: 0.591  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/best_single_image_model.pth
  → New best model! Validation accuracy: 0.8287
新的最佳模型! 验证准确率: 0.8287
Epoch [37] Batch [0/130] Loss: 0.0151 LR: 0.000100
Epoch [37] Batch [10/130] Loss: 0.0452 LR: 0.000100
Epoch [37] Batch [20/130] Loss: 0.0182 LR: 0.000100
Epoch [37] Batch [30/130] Loss: 0.0293 LR: 0.000100
Epoch [37] Batch [40/130] Loss: 0.0004 LR: 0.000100
Epoch [37] Batch [50/130] Loss: 0.0062 LR: 0.000100
Epoch [37] Batch [60/130] Loss: 0.0148 LR: 0.000100
Epoch [37] Batch [70/130] Loss: 0.0057 LR: 0.000100
Epoch [37] Batch [80/130] Loss: 1.4827 LR: 0.000100
Epoch [37] Batch [90/130] Loss: 0.2964 LR: 0.000100
Epoch [37] Batch [100/130] Loss: 0.0005 LR: 0.000100
Epoch [37] Batch [110/130] Loss: 0.3243 LR: 0.000100
Epoch [37] Batch [120/130] Loss: 0.0209 LR: 0.000100

Epoch [38/350] - Time: 8.28s
Train Loss: 0.0774 | Val Loss: 1.6575
Train Acc: 0.9754 | Val Acc: 0.7421
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.907    Count 4: 0.829    Count 5: 0.755  
  Count 6: 0.408    Count 7: 0.677    Count 8: 0.495    Count 9: 0.556    Count 10: 0.489  
Epoch [38] Batch [0/130] Loss: 0.0053 LR: 0.000100
Epoch [38] Batch [10/130] Loss: 0.0005 LR: 0.000100
Epoch [38] Batch [20/130] Loss: 0.0007 LR: 0.000100
Epoch [38] Batch [30/130] Loss: 0.0064 LR: 0.000100
Epoch [38] Batch [40/130] Loss: 0.0338 LR: 0.000100
Epoch [38] Batch [50/130] Loss: 0.0051 LR: 0.000100
Epoch [38] Batch [60/130] Loss: 0.5952 LR: 0.000100
Epoch [38] Batch [70/130] Loss: 0.0650 LR: 0.000100
Epoch [38] Batch [80/130] Loss: 0.0129 LR: 0.000100
Epoch [38] Batch [90/130] Loss: 0.4533 LR: 0.000100
Epoch [38] Batch [100/130] Loss: 0.3440 LR: 0.000100
Epoch [38] Batch [110/130] Loss: 0.4385 LR: 0.000100
Epoch [38] Batch [120/130] Loss: 0.0085 LR: 0.000100

Epoch [39/350] - Time: 8.24s
Train Loss: 0.1157 | Val Loss: 2.0975
Train Acc: 0.9619 | Val Acc: 0.6788
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.988    Count 2: 1.000    Count 3: 0.981    Count 4: 0.800    Count 5: 0.431  
  Count 6: 0.286    Count 7: 0.406    Count 8: 0.505    Count 9: 0.567    Count 10: 0.466  
Epoch [39] Batch [0/130] Loss: 0.0848 LR: 0.000100
Epoch [39] Batch [10/130] Loss: 0.0006 LR: 0.000100
Epoch [39] Batch [20/130] Loss: 0.0323 LR: 0.000100
Epoch [39] Batch [30/130] Loss: 0.0183 LR: 0.000100
Epoch [39] Batch [40/130] Loss: 0.0213 LR: 0.000100
Epoch [39] Batch [50/130] Loss: 0.0011 LR: 0.000100
Epoch [39] Batch [60/130] Loss: 0.0247 LR: 0.000100
Epoch [39] Batch [70/130] Loss: 0.0063 LR: 0.000100
Epoch [39] Batch [80/130] Loss: 0.0026 LR: 0.000100
Epoch [39] Batch [90/130] Loss: 0.0015 LR: 0.000100
Epoch [39] Batch [100/130] Loss: 0.0013 LR: 0.000100
Epoch [39] Batch [110/130] Loss: 0.0291 LR: 0.000100
Epoch [39] Batch [120/130] Loss: 0.0016 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [40/350] - Time: 8.27s
Train Loss: 0.0959 | Val Loss: 1.6291
Train Acc: 0.9692 | Val Acc: 0.7998
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.981    Count 4: 0.962    Count 5: 0.873  
  Count 6: 0.796    Count 7: 0.583    Count 8: 0.556    Count 9: 0.256    Count 10: 0.716  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/single_image_checkpoint_epoch_39.pth
Epoch [40] Batch [0/130] Loss: 0.2797 LR: 0.000100
Epoch [40] Batch [10/130] Loss: 0.0067 LR: 0.000100
Epoch [40] Batch [20/130] Loss: 0.0010 LR: 0.000100
Epoch [40] Batch [30/130] Loss: 0.0572 LR: 0.000100
Epoch [40] Batch [40/130] Loss: 0.0009 LR: 0.000100
Epoch [40] Batch [50/130] Loss: 0.1261 LR: 0.000100
Epoch [40] Batch [60/130] Loss: 0.0138 LR: 0.000100
Epoch [40] Batch [70/130] Loss: 0.1146 LR: 0.000100
Epoch [40] Batch [80/130] Loss: 0.0247 LR: 0.000100
Epoch [40] Batch [90/130] Loss: 0.1620 LR: 0.000100
Epoch [40] Batch [100/130] Loss: 0.0003 LR: 0.000100
Epoch [40] Batch [110/130] Loss: 0.0024 LR: 0.000100
Epoch [40] Batch [120/130] Loss: 0.0132 LR: 0.000100

Epoch [41/350] - Time: 8.39s
Train Loss: 0.0659 | Val Loss: 2.4241
Train Acc: 0.9798 | Val Acc: 0.7439
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.981    Count 5: 0.941  
  Count 6: 0.847    Count 7: 0.427    Count 8: 0.414    Count 9: 0.211    Count 10: 0.239  
Epoch [41] Batch [0/130] Loss: 0.0415 LR: 0.000100
Epoch [41] Batch [10/130] Loss: 0.0288 LR: 0.000100
Epoch [41] Batch [20/130] Loss: 0.0002 LR: 0.000100
Epoch [41] Batch [30/130] Loss: 0.0262 LR: 0.000100
Epoch [41] Batch [40/130] Loss: 0.0651 LR: 0.000100
Epoch [41] Batch [50/130] Loss: 0.0054 LR: 0.000100
Epoch [41] Batch [60/130] Loss: 0.0011 LR: 0.000100
Epoch [41] Batch [70/130] Loss: 0.0037 LR: 0.000100
Epoch [41] Batch [80/130] Loss: 0.0007 LR: 0.000100
Epoch [41] Batch [90/130] Loss: 0.0365 LR: 0.000100
Epoch [41] Batch [100/130] Loss: 0.1646 LR: 0.000100
Epoch [41] Batch [110/130] Loss: 0.1117 LR: 0.000100
Epoch [41] Batch [120/130] Loss: 0.0076 LR: 0.000100

Epoch [42/350] - Time: 8.44s
Train Loss: 0.0743 | Val Loss: 1.5838
Train Acc: 0.9730 | Val Acc: 0.8128
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.972    Count 4: 0.990    Count 5: 0.990  
  Count 6: 0.878    Count 7: 0.656    Count 8: 0.657    Count 9: 0.311    Count 10: 0.375  
Epoch [42] Batch [0/130] Loss: 0.1193 LR: 0.000100
Epoch [42] Batch [10/130] Loss: 0.0043 LR: 0.000100
Epoch [42] Batch [20/130] Loss: 0.0005 LR: 0.000100
Epoch [42] Batch [30/130] Loss: 0.0003 LR: 0.000100
Epoch [42] Batch [40/130] Loss: 0.3299 LR: 0.000100
Epoch [42] Batch [50/130] Loss: 0.0027 LR: 0.000100
Epoch [42] Batch [60/130] Loss: 0.0017 LR: 0.000100
Epoch [42] Batch [70/130] Loss: 0.6086 LR: 0.000100
Epoch [42] Batch [80/130] Loss: 0.0030 LR: 0.000100
Epoch [42] Batch [90/130] Loss: 0.0170 LR: 0.000100
Epoch [42] Batch [100/130] Loss: 0.0235 LR: 0.000100
Epoch [42] Batch [110/130] Loss: 0.0514 LR: 0.000100
Epoch [42] Batch [120/130] Loss: 0.3157 LR: 0.000100

Epoch [43/350] - Time: 8.79s
Train Loss: 0.0682 | Val Loss: 2.5022
Train Acc: 0.9764 | Val Acc: 0.7551
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 1.000    Count 3: 0.991    Count 4: 0.990    Count 5: 0.990  
  Count 6: 0.847    Count 7: 0.500    Count 8: 0.192    Count 9: 0.156    Count 10: 0.545  
Epoch [43] Batch [0/130] Loss: 0.0382 LR: 0.000100
Epoch [43] Batch [10/130] Loss: 0.3342 LR: 0.000100
Epoch [43] Batch [20/130] Loss: 0.0008 LR: 0.000100
Epoch [43] Batch [30/130] Loss: 0.0002 LR: 0.000100
Epoch [43] Batch [40/130] Loss: 0.0003 LR: 0.000100
Epoch [43] Batch [50/130] Loss: 0.0009 LR: 0.000100
Epoch [43] Batch [60/130] Loss: 0.0429 LR: 0.000100
Epoch [43] Batch [70/130] Loss: 0.0120 LR: 0.000100
Epoch [43] Batch [80/130] Loss: 0.4212 LR: 0.000100
Epoch [43] Batch [90/130] Loss: 0.0006 LR: 0.000100
Epoch [43] Batch [100/130] Loss: 0.0961 LR: 0.000100
Epoch [43] Batch [110/130] Loss: 0.0640 LR: 0.000100
Epoch [43] Batch [120/130] Loss: 0.0166 LR: 0.000100

Epoch [44/350] - Time: 8.38s
Train Loss: 0.0935 | Val Loss: 2.4599
Train Acc: 0.9759 | Val Acc: 0.7412
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.963    Count 4: 0.981    Count 5: 0.971  
  Count 6: 0.765    Count 7: 0.333    Count 8: 0.313    Count 9: 0.267    Count 10: 0.455  
Epoch [44] Batch [0/130] Loss: 0.1199 LR: 0.000100
Epoch [44] Batch [10/130] Loss: 0.0026 LR: 0.000100
Epoch [44] Batch [20/130] Loss: 0.0060 LR: 0.000100
Epoch [44] Batch [30/130] Loss: 0.0007 LR: 0.000100
Epoch [44] Batch [40/130] Loss: 0.0295 LR: 0.000100
Epoch [44] Batch [50/130] Loss: 0.0007 LR: 0.000100
Epoch [44] Batch [60/130] Loss: 0.0281 LR: 0.000100
Epoch [44] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [44] Batch [80/130] Loss: 0.0570 LR: 0.000100
Epoch [44] Batch [90/130] Loss: 0.0002 LR: 0.000100
Epoch [44] Batch [100/130] Loss: 0.0003 LR: 0.000100
Epoch [44] Batch [110/130] Loss: 0.0929 LR: 0.000100
Epoch [44] Batch [120/130] Loss: 0.0016 LR: 0.000100

Epoch [45/350] - Time: 8.30s
Train Loss: 0.0653 | Val Loss: 2.1022
Train Acc: 0.9812 | Val Acc: 0.7840
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.990    Count 5: 0.922  
  Count 6: 0.847    Count 7: 0.531    Count 8: 0.283    Count 9: 0.267    Count 10: 0.716  
Epoch [45] Batch [0/130] Loss: 0.0240 LR: 0.000100
Epoch [45] Batch [10/130] Loss: 0.0011 LR: 0.000100
Epoch [45] Batch [20/130] Loss: 0.4398 LR: 0.000100
Epoch [45] Batch [30/130] Loss: 0.0016 LR: 0.000100
Epoch [45] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [45] Batch [50/130] Loss: 0.0644 LR: 0.000100
Epoch [45] Batch [60/130] Loss: 0.0074 LR: 0.000100
Epoch [45] Batch [70/130] Loss: 0.0025 LR: 0.000100
Epoch [45] Batch [80/130] Loss: 0.0020 LR: 0.000100
Epoch [45] Batch [90/130] Loss: 0.0009 LR: 0.000100
Epoch [45] Batch [100/130] Loss: 0.0025 LR: 0.000100
Epoch [45] Batch [110/130] Loss: 0.1074 LR: 0.000100
Epoch [45] Batch [120/130] Loss: 0.0088 LR: 0.000100

Epoch [46/350] - Time: 8.36s
Train Loss: 0.0709 | Val Loss: 1.7664
Train Acc: 0.9827 | Val Acc: 0.8128
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.992    Count 3: 0.991    Count 4: 0.981    Count 5: 0.951  
  Count 6: 0.867    Count 7: 0.625    Count 8: 0.596    Count 9: 0.244    Count 10: 0.602  
Epoch [46] Batch [0/130] Loss: 0.0110 LR: 0.000100
Epoch [46] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [46] Batch [20/130] Loss: 0.0016 LR: 0.000100
Epoch [46] Batch [30/130] Loss: 0.0001 LR: 0.000100
Epoch [46] Batch [40/130] Loss: 0.0004 LR: 0.000100
Epoch [46] Batch [50/130] Loss: 0.6507 LR: 0.000100
Epoch [46] Batch [60/130] Loss: 0.0003 LR: 0.000100
Epoch [46] Batch [70/130] Loss: 0.2695 LR: 0.000100
Epoch [46] Batch [80/130] Loss: 0.0089 LR: 0.000100
Epoch [46] Batch [90/130] Loss: 0.1395 LR: 0.000100
Epoch [46] Batch [100/130] Loss: 0.0045 LR: 0.000100
Epoch [46] Batch [110/130] Loss: 0.0001 LR: 0.000100
Epoch [46] Batch [120/130] Loss: 0.0049 LR: 0.000100

Epoch [47/350] - Time: 8.14s
Train Loss: 0.0799 | Val Loss: 2.2732
Train Acc: 0.9788 | Val Acc: 0.7644
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 1.000    Count 3: 0.981    Count 4: 0.971    Count 5: 0.941  
  Count 6: 0.878    Count 7: 0.417    Count 8: 0.455    Count 9: 0.178    Count 10: 0.489  
Epoch [47] Batch [0/130] Loss: 0.0076 LR: 0.000100
Epoch [47] Batch [10/130] Loss: 0.6739 LR: 0.000100
Epoch [47] Batch [20/130] Loss: 0.0601 LR: 0.000100
Epoch [47] Batch [30/130] Loss: 0.0432 LR: 0.000100
Epoch [47] Batch [40/130] Loss: 0.0052 LR: 0.000100
Epoch [47] Batch [50/130] Loss: 0.0146 LR: 0.000100
Epoch [47] Batch [60/130] Loss: 0.0002 LR: 0.000100
Epoch [47] Batch [70/130] Loss: 0.0072 LR: 0.000100
Epoch [47] Batch [80/130] Loss: 0.0002 LR: 0.000100
Epoch [47] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [47] Batch [100/130] Loss: 0.0016 LR: 0.000100
Epoch [47] Batch [110/130] Loss: 0.0036 LR: 0.000100
Epoch [47] Batch [120/130] Loss: 0.0011 LR: 0.000100

Epoch [48/350] - Time: 8.27s
Train Loss: 0.0793 | Val Loss: 1.9883
Train Acc: 0.9798 | Val Acc: 0.7849
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.981    Count 5: 0.980  
  Count 6: 0.847    Count 7: 0.469    Count 8: 0.424    Count 9: 0.256    Count 10: 0.591  
Epoch [48] Batch [0/130] Loss: 0.0026 LR: 0.000100
Epoch [48] Batch [10/130] Loss: 0.0003 LR: 0.000100
Epoch [48] Batch [20/130] Loss: 0.0002 LR: 0.000100
Epoch [48] Batch [30/130] Loss: 0.0052 LR: 0.000100
Epoch [48] Batch [40/130] Loss: 0.2271 LR: 0.000100
Epoch [48] Batch [50/130] Loss: 0.0002 LR: 0.000100
Epoch [48] Batch [60/130] Loss: 0.0725 LR: 0.000100
Epoch [48] Batch [70/130] Loss: 0.0459 LR: 0.000100
Epoch [48] Batch [80/130] Loss: 0.0103 LR: 0.000100
Epoch [48] Batch [90/130] Loss: 0.0001 LR: 0.000100
Epoch [48] Batch [100/130] Loss: 0.0001 LR: 0.000100
Epoch [48] Batch [110/130] Loss: 0.1052 LR: 0.000100
Epoch [48] Batch [120/130] Loss: 0.0399 LR: 0.000100

Epoch [49/350] - Time: 8.32s
Train Loss: 0.0442 | Val Loss: 2.1715
Train Acc: 0.9836 | Val Acc: 0.7942
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 1.000    Count 3: 1.000    Count 4: 0.990    Count 5: 0.980  
  Count 6: 0.847    Count 7: 0.531    Count 8: 0.636    Count 9: 0.311    Count 10: 0.330  
Epoch [49] Batch [0/130] Loss: 0.0006 LR: 0.000100
Epoch [49] Batch [10/130] Loss: 0.0045 LR: 0.000100
Epoch [49] Batch [20/130] Loss: 0.0665 LR: 0.000100
Epoch [49] Batch [30/130] Loss: 0.0019 LR: 0.000100
Epoch [49] Batch [40/130] Loss: 0.0190 LR: 0.000100
Epoch [49] Batch [50/130] Loss: 0.0025 LR: 0.000100
Epoch [49] Batch [60/130] Loss: 0.0008 LR: 0.000100
Epoch [49] Batch [70/130] Loss: 0.0218 LR: 0.000100
Epoch [49] Batch [80/130] Loss: 0.1387 LR: 0.000100
Epoch [49] Batch [90/130] Loss: 0.0146 LR: 0.000100
Epoch [49] Batch [100/130] Loss: 0.0005 LR: 0.000100
Epoch [49] Batch [110/130] Loss: 0.0018 LR: 0.000100
Epoch [49] Batch [120/130] Loss: 0.2188 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [50/350] - Time: 8.28s
Train Loss: 0.0768 | Val Loss: 2.2576
Train Acc: 0.9788 | Val Acc: 0.7635
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.972    Count 4: 0.933    Count 5: 0.922  
  Count 6: 0.786    Count 7: 0.448    Count 8: 0.434    Count 9: 0.289    Count 10: 0.523  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/single_image_checkpoint_epoch_49.pth
Epoch [50] Batch [0/130] Loss: 0.0275 LR: 0.000100
Epoch [50] Batch [10/130] Loss: 0.0367 LR: 0.000100
Epoch [50] Batch [20/130] Loss: 0.0728 LR: 0.000100
Epoch [50] Batch [30/130] Loss: 0.0017 LR: 0.000100
Epoch [50] Batch [40/130] Loss: 0.0002 LR: 0.000100
Epoch [50] Batch [50/130] Loss: 0.0001 LR: 0.000100
Epoch [50] Batch [60/130] Loss: 0.0021 LR: 0.000100
Epoch [50] Batch [70/130] Loss: 0.1787 LR: 0.000100
Epoch [50] Batch [80/130] Loss: 0.3569 LR: 0.000100
Epoch [50] Batch [90/130] Loss: 0.0044 LR: 0.000100
Epoch [50] Batch [100/130] Loss: 0.0001 LR: 0.000100
Epoch [50] Batch [110/130] Loss: 0.0007 LR: 0.000100
Epoch [50] Batch [120/130] Loss: 0.0011 LR: 0.000100

Epoch [51/350] - Time: 8.59s
Train Loss: 0.0832 | Val Loss: 2.2116
Train Acc: 0.9793 | Val Acc: 0.7840
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.972    Count 4: 0.971    Count 5: 0.990  
  Count 6: 0.857    Count 7: 0.521    Count 8: 0.657    Count 9: 0.267    Count 10: 0.261  
Epoch [51] Batch [0/130] Loss: 0.0001 LR: 0.000100
Epoch [51] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [51] Batch [20/130] Loss: 0.2481 LR: 0.000100
Epoch [51] Batch [30/130] Loss: 0.0859 LR: 0.000100
Epoch [51] Batch [40/130] Loss: 0.3928 LR: 0.000100
Epoch [51] Batch [50/130] Loss: 1.1407 LR: 0.000100
Epoch [51] Batch [60/130] Loss: 0.0001 LR: 0.000100
Epoch [51] Batch [70/130] Loss: 0.0001 LR: 0.000100
Epoch [51] Batch [80/130] Loss: 0.0285 LR: 0.000100
Epoch [51] Batch [90/130] Loss: 0.0008 LR: 0.000100
Epoch [51] Batch [100/130] Loss: 0.0027 LR: 0.000100
Epoch [51] Batch [110/130] Loss: 0.0042 LR: 0.000100
Epoch [51] Batch [120/130] Loss: 0.0126 LR: 0.000100

Epoch [52/350] - Time: 8.20s
Train Loss: 0.0417 | Val Loss: 2.1015
Train Acc: 0.9889 | Val Acc: 0.8119
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.981    Count 5: 0.922  
  Count 6: 0.827    Count 7: 0.667    Count 8: 0.485    Count 9: 0.389    Count 10: 0.580  
Epoch [52] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [52] Batch [10/130] Loss: 0.0002 LR: 0.000100
Epoch [52] Batch [20/130] Loss: 0.0089 LR: 0.000100
Epoch [52] Batch [30/130] Loss: 0.0002 LR: 0.000100
Epoch [52] Batch [40/130] Loss: 0.3723 LR: 0.000100
Epoch [52] Batch [50/130] Loss: 0.0192 LR: 0.000100
Epoch [52] Batch [60/130] Loss: 0.0121 LR: 0.000100
Epoch [52] Batch [70/130] Loss: 0.1506 LR: 0.000100
Epoch [52] Batch [80/130] Loss: 0.0010 LR: 0.000100
Epoch [52] Batch [90/130] Loss: 0.1528 LR: 0.000100
Epoch [52] Batch [100/130] Loss: 0.0003 LR: 0.000100
Epoch [52] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [52] Batch [120/130] Loss: 0.0005 LR: 0.000100

Epoch [53/350] - Time: 8.37s
Train Loss: 0.0507 | Val Loss: 2.1208
Train Acc: 0.9851 | Val Acc: 0.8175
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.981    Count 5: 0.912  
  Count 6: 0.898    Count 7: 0.521    Count 8: 0.636    Count 9: 0.500    Count 10: 0.455  
Epoch [53] Batch [0/130] Loss: 0.0654 LR: 0.000100
Epoch [53] Batch [10/130] Loss: 0.0008 LR: 0.000100
Epoch [53] Batch [20/130] Loss: 0.0015 LR: 0.000100
Epoch [53] Batch [30/130] Loss: 0.0003 LR: 0.000100
Epoch [53] Batch [40/130] Loss: 0.0119 LR: 0.000100
Epoch [53] Batch [50/130] Loss: 0.0020 LR: 0.000100
Epoch [53] Batch [60/130] Loss: 0.0278 LR: 0.000100
Epoch [53] Batch [70/130] Loss: 0.0042 LR: 0.000100
Epoch [53] Batch [80/130] Loss: 0.0211 LR: 0.000100
Epoch [53] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [53] Batch [100/130] Loss: 0.0002 LR: 0.000100
Epoch [53] Batch [110/130] Loss: 0.0359 LR: 0.000100
Epoch [53] Batch [120/130] Loss: 1.2844 LR: 0.000100

Epoch [54/350] - Time: 8.25s
Train Loss: 0.1127 | Val Loss: 2.0472
Train Acc: 0.9750 | Val Acc: 0.7756
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.905    Count 5: 0.618  
  Count 6: 0.765    Count 7: 0.542    Count 8: 0.717    Count 9: 0.489    Count 10: 0.420  
Epoch [54] Batch [0/130] Loss: 0.0002 LR: 0.000100
Epoch [54] Batch [10/130] Loss: 0.0010 LR: 0.000100
Epoch [54] Batch [20/130] Loss: 0.0003 LR: 0.000100
Epoch [54] Batch [30/130] Loss: 0.0005 LR: 0.000100
Epoch [54] Batch [40/130] Loss: 0.0001 LR: 0.000100
Epoch [54] Batch [50/130] Loss: 0.0013 LR: 0.000100
Epoch [54] Batch [60/130] Loss: 0.0877 LR: 0.000100
Epoch [54] Batch [70/130] Loss: 0.0616 LR: 0.000100
Epoch [54] Batch [80/130] Loss: 0.0204 LR: 0.000100
Epoch [54] Batch [90/130] Loss: 0.0163 LR: 0.000100
Epoch [54] Batch [100/130] Loss: 0.0002 LR: 0.000100
Epoch [54] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [54] Batch [120/130] Loss: 0.0002 LR: 0.000100

Epoch [55/350] - Time: 8.23s
Train Loss: 0.1037 | Val Loss: 1.9915
Train Acc: 0.9783 | Val Acc: 0.8073
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.981    Count 5: 0.892  
  Count 6: 0.837    Count 7: 0.667    Count 8: 0.525    Count 9: 0.333    Count 10: 0.568  
Epoch [55] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [55] Batch [10/130] Loss: 0.1307 LR: 0.000100
Epoch [55] Batch [20/130] Loss: 0.0011 LR: 0.000100
Epoch [55] Batch [30/130] Loss: 0.0580 LR: 0.000100
Epoch [55] Batch [40/130] Loss: 0.0129 LR: 0.000100
Epoch [55] Batch [50/130] Loss: 0.0360 LR: 0.000100
Epoch [55] Batch [60/130] Loss: 0.0014 LR: 0.000100
Epoch [55] Batch [70/130] Loss: 0.0001 LR: 0.000100
Epoch [55] Batch [80/130] Loss: 0.2595 LR: 0.000100
Epoch [55] Batch [90/130] Loss: 0.0002 LR: 0.000100
Epoch [55] Batch [100/130] Loss: 0.0007 LR: 0.000100
Epoch [55] Batch [110/130] Loss: 0.0061 LR: 0.000100
Epoch [55] Batch [120/130] Loss: 0.0005 LR: 0.000100

Epoch [56/350] - Time: 8.30s
Train Loss: 0.0477 | Val Loss: 2.3967
Train Acc: 0.9841 | Val Acc: 0.7858
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.981    Count 5: 0.980  
  Count 6: 0.898    Count 7: 0.490    Count 8: 0.616    Count 9: 0.233    Count 10: 0.318  
Epoch [56] Batch [0/130] Loss: 0.0298 LR: 0.000100
Epoch [56] Batch [10/130] Loss: 0.0002 LR: 0.000100
Epoch [56] Batch [20/130] Loss: 0.0001 LR: 0.000100
Epoch [56] Batch [30/130] Loss: 0.0001 LR: 0.000100
Epoch [56] Batch [40/130] Loss: 0.0010 LR: 0.000100
Epoch [56] Batch [50/130] Loss: 0.0003 LR: 0.000100
Epoch [56] Batch [60/130] Loss: 0.5489 LR: 0.000100
Epoch [56] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [56] Batch [80/130] Loss: 0.0001 LR: 0.000100
Epoch [56] Batch [90/130] Loss: 0.0002 LR: 0.000100
Epoch [56] Batch [100/130] Loss: 0.0003 LR: 0.000100
Epoch [56] Batch [110/130] Loss: 0.0004 LR: 0.000100
Epoch [56] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [57/350] - Time: 8.20s
Train Loss: 0.0351 | Val Loss: 2.1534
Train Acc: 0.9908 | Val Acc: 0.8063
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.981    Count 5: 0.951  
  Count 6: 0.765    Count 7: 0.604    Count 8: 0.616    Count 9: 0.444    Count 10: 0.409  
Epoch [57] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [57] Batch [10/130] Loss: 0.0296 LR: 0.000100
Epoch [57] Batch [20/130] Loss: 0.0904 LR: 0.000100
Epoch [57] Batch [30/130] Loss: 0.0060 LR: 0.000100
Epoch [57] Batch [40/130] Loss: 0.0035 LR: 0.000100
Epoch [57] Batch [50/130] Loss: 0.0010 LR: 0.000100
Epoch [57] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [57] Batch [70/130] Loss: 0.0825 LR: 0.000100
Epoch [57] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [57] Batch [90/130] Loss: 0.2592 LR: 0.000100
Epoch [57] Batch [100/130] Loss: 0.0239 LR: 0.000100
Epoch [57] Batch [110/130] Loss: 0.0019 LR: 0.000100
Epoch [57] Batch [120/130] Loss: 0.1587 LR: 0.000100

Epoch [58/350] - Time: 8.31s
Train Loss: 0.0945 | Val Loss: 2.1374
Train Acc: 0.9764 | Val Acc: 0.7998
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 1.000    Count 3: 0.981    Count 4: 0.952    Count 5: 0.912  
  Count 6: 0.827    Count 7: 0.562    Count 8: 0.737    Count 9: 0.389    Count 10: 0.341  
Epoch [58] Batch [0/130] Loss: 0.0002 LR: 0.000100
Epoch [58] Batch [10/130] Loss: 0.0237 LR: 0.000100
Epoch [58] Batch [20/130] Loss: 0.0132 LR: 0.000100
Epoch [58] Batch [30/130] Loss: 0.0023 LR: 0.000100
Epoch [58] Batch [40/130] Loss: 0.0162 LR: 0.000100
Epoch [58] Batch [50/130] Loss: 0.0316 LR: 0.000100
Epoch [58] Batch [60/130] Loss: 0.3980 LR: 0.000100
Epoch [58] Batch [70/130] Loss: 0.0192 LR: 0.000100
Epoch [58] Batch [80/130] Loss: 0.0590 LR: 0.000100
Epoch [58] Batch [90/130] Loss: 0.0004 LR: 0.000100
Epoch [58] Batch [100/130] Loss: 0.0002 LR: 0.000100
Epoch [58] Batch [110/130] Loss: 0.2483 LR: 0.000100
Epoch [58] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [59/350] - Time: 8.50s
Train Loss: 0.0585 | Val Loss: 2.1270
Train Acc: 0.9841 | Val Acc: 0.7989
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.972    Count 4: 0.952    Count 5: 0.951  
  Count 6: 0.827    Count 7: 0.500    Count 8: 0.556    Count 9: 0.344    Count 10: 0.602  
Epoch [59] Batch [0/130] Loss: 0.0011 LR: 0.000100
Epoch [59] Batch [10/130] Loss: 0.0013 LR: 0.000100
Epoch [59] Batch [20/130] Loss: 0.0080 LR: 0.000100
Epoch [59] Batch [30/130] Loss: 0.0021 LR: 0.000100
Epoch [59] Batch [40/130] Loss: 0.0178 LR: 0.000100
Epoch [59] Batch [50/130] Loss: 0.0227 LR: 0.000100
Epoch [59] Batch [60/130] Loss: 0.0611 LR: 0.000100
Epoch [59] Batch [70/130] Loss: 0.0004 LR: 0.000100
Epoch [59] Batch [80/130] Loss: 0.0233 LR: 0.000100
Epoch [59] Batch [90/130] Loss: 0.0170 LR: 0.000100
Epoch [59] Batch [100/130] Loss: 0.0001 LR: 0.000100
Epoch [59] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [59] Batch [120/130] Loss: 0.0063 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [60/350] - Time: 8.26s
Train Loss: 0.0662 | Val Loss: 3.3041
Train Acc: 0.9774 | Val Acc: 0.7067
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.972    Count 4: 0.867    Count 5: 0.853  
  Count 6: 0.673    Count 7: 0.375    Count 8: 0.273    Count 9: 0.211    Count 10: 0.455  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/single_image_checkpoint_epoch_59.pth
Epoch [60] Batch [0/130] Loss: 0.6000 LR: 0.000100
Epoch [60] Batch [10/130] Loss: 0.0018 LR: 0.000100
Epoch [60] Batch [20/130] Loss: 0.0342 LR: 0.000100
Epoch [60] Batch [30/130] Loss: 0.0011 LR: 0.000100
Epoch [60] Batch [40/130] Loss: 0.0008 LR: 0.000100
Epoch [60] Batch [50/130] Loss: 0.0001 LR: 0.000100
Epoch [60] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [60] Batch [70/130] Loss: 0.0002 LR: 0.000100
Epoch [60] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [60] Batch [90/130] Loss: 0.0039 LR: 0.000100
Epoch [60] Batch [100/130] Loss: 0.1924 LR: 0.000100
Epoch [60] Batch [110/130] Loss: 0.0246 LR: 0.000100
Epoch [60] Batch [120/130] Loss: 0.0002 LR: 0.000100

Epoch [61/350] - Time: 8.20s
Train Loss: 0.0621 | Val Loss: 2.8892
Train Acc: 0.9836 | Val Acc: 0.7374
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.992    Count 3: 0.981    Count 4: 0.876    Count 5: 0.892  
  Count 6: 0.837    Count 7: 0.365    Count 8: 0.485    Count 9: 0.233    Count 10: 0.341  
Epoch [61] Batch [0/130] Loss: 0.4208 LR: 0.000100
Epoch [61] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [61] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [61] Batch [30/130] Loss: 0.0203 LR: 0.000100
Epoch [61] Batch [40/130] Loss: 0.0882 LR: 0.000100
Epoch [61] Batch [50/130] Loss: 0.1283 LR: 0.000100
Epoch [61] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [61] Batch [70/130] Loss: 0.0062 LR: 0.000100
Epoch [61] Batch [80/130] Loss: 0.0098 LR: 0.000100
Epoch [61] Batch [90/130] Loss: 0.0004 LR: 0.000100
Epoch [61] Batch [100/130] Loss: 0.0014 LR: 0.000100
Epoch [61] Batch [110/130] Loss: 0.0279 LR: 0.000100
Epoch [61] Batch [120/130] Loss: 0.0080 LR: 0.000100

Epoch [62/350] - Time: 8.35s
Train Loss: 0.0673 | Val Loss: 2.1890
Train Acc: 0.9860 | Val Acc: 0.8007
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.952    Count 5: 0.941  
  Count 6: 0.796    Count 7: 0.740    Count 8: 0.556    Count 9: 0.222    Count 10: 0.511  
Epoch [62] Batch [0/130] Loss: 0.0008 LR: 0.000100
Epoch [62] Batch [10/130] Loss: 0.0096 LR: 0.000100
Epoch [62] Batch [20/130] Loss: 0.0353 LR: 0.000100
Epoch [62] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [62] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [62] Batch [50/130] Loss: 0.0001 LR: 0.000100
Epoch [62] Batch [60/130] Loss: 0.0330 LR: 0.000100
Epoch [62] Batch [70/130] Loss: 0.0001 LR: 0.000100
Epoch [62] Batch [80/130] Loss: 0.9337 LR: 0.000100
Epoch [62] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [62] Batch [100/130] Loss: 0.0068 LR: 0.000100
Epoch [62] Batch [110/130] Loss: 0.0002 LR: 0.000100
Epoch [62] Batch [120/130] Loss: 0.0013 LR: 0.000100

Epoch [63/350] - Time: 8.35s
Train Loss: 0.0519 | Val Loss: 2.6941
Train Acc: 0.9831 | Val Acc: 0.7961
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.981    Count 4: 0.962    Count 5: 0.922  
  Count 6: 0.878    Count 7: 0.677    Count 8: 0.323    Count 9: 0.222    Count 10: 0.716  
Epoch [63] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [63] Batch [10/130] Loss: 0.0003 LR: 0.000100
Epoch [63] Batch [20/130] Loss: 0.0246 LR: 0.000100
Epoch [63] Batch [30/130] Loss: 0.0003 LR: 0.000100
Epoch [63] Batch [40/130] Loss: 0.0001 LR: 0.000100
Epoch [63] Batch [50/130] Loss: 0.0002 LR: 0.000100
Epoch [63] Batch [60/130] Loss: 0.1233 LR: 0.000100
Epoch [63] Batch [70/130] Loss: 0.0004 LR: 0.000100
Epoch [63] Batch [80/130] Loss: 0.0062 LR: 0.000100
Epoch [63] Batch [90/130] Loss: 0.0002 LR: 0.000100
Epoch [63] Batch [100/130] Loss: 0.0001 LR: 0.000100
Epoch [63] Batch [110/130] Loss: 0.9380 LR: 0.000100
Epoch [63] Batch [120/130] Loss: 0.2102 LR: 0.000100

Epoch [64/350] - Time: 8.37s
Train Loss: 0.0585 | Val Loss: 2.1839
Train Acc: 0.9851 | Val Acc: 0.8017
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.971    Count 5: 0.951  
  Count 6: 0.847    Count 7: 0.531    Count 8: 0.747    Count 9: 0.344    Count 10: 0.318  
Epoch [64] Batch [0/130] Loss: 0.0001 LR: 0.000100
Epoch [64] Batch [10/130] Loss: 0.0487 LR: 0.000100
Epoch [64] Batch [20/130] Loss: 0.0001 LR: 0.000100
Epoch [64] Batch [30/130] Loss: 0.0002 LR: 0.000100
Epoch [64] Batch [40/130] Loss: 0.0003 LR: 0.000100
Epoch [64] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [64] Batch [60/130] Loss: 0.2923 LR: 0.000100
Epoch [64] Batch [70/130] Loss: 0.0003 LR: 0.000100
Epoch [64] Batch [80/130] Loss: 0.0002 LR: 0.000100
Epoch [64] Batch [90/130] Loss: 0.0001 LR: 0.000100
Epoch [64] Batch [100/130] Loss: 0.2852 LR: 0.000100
Epoch [64] Batch [110/130] Loss: 0.0151 LR: 0.000100
Epoch [64] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [65/350] - Time: 8.22s
Train Loss: 0.0435 | Val Loss: 3.0912
Train Acc: 0.9865 | Val Acc: 0.7654
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.971    Count 5: 0.961  
  Count 6: 0.786    Count 7: 0.417    Count 8: 0.636    Count 9: 0.233    Count 10: 0.295  
Epoch [65] Batch [0/130] Loss: 0.0252 LR: 0.000100
Epoch [65] Batch [10/130] Loss: 0.0362 LR: 0.000100
Epoch [65] Batch [20/130] Loss: 0.0003 LR: 0.000100
Epoch [65] Batch [30/130] Loss: 0.0017 LR: 0.000100
Epoch [65] Batch [40/130] Loss: 0.0004 LR: 0.000100
Epoch [65] Batch [50/130] Loss: 0.0001 LR: 0.000100
Epoch [65] Batch [60/130] Loss: 0.0038 LR: 0.000100
Epoch [65] Batch [70/130] Loss: 0.0016 LR: 0.000100
Epoch [65] Batch [80/130] Loss: 0.0035 LR: 0.000100
Epoch [65] Batch [90/130] Loss: 0.0004 LR: 0.000100
Epoch [65] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [65] Batch [110/130] Loss: 0.0001 LR: 0.000100
Epoch [65] Batch [120/130] Loss: 0.0002 LR: 0.000100

Epoch [66/350] - Time: 8.24s
Train Loss: 0.0413 | Val Loss: 2.8014
Train Acc: 0.9884 | Val Acc: 0.7998
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 1.000    Count 3: 1.000    Count 4: 0.990    Count 5: 0.951  
  Count 6: 0.857    Count 7: 0.458    Count 8: 0.596    Count 9: 0.400    Count 10: 0.455  
Epoch [66] Batch [0/130] Loss: 0.0001 LR: 0.000100
Epoch [66] Batch [10/130] Loss: 0.0002 LR: 0.000100
Epoch [66] Batch [20/130] Loss: 0.0003 LR: 0.000100
Epoch [66] Batch [30/130] Loss: 0.0594 LR: 0.000100
Epoch [66] Batch [40/130] Loss: 0.0566 LR: 0.000100
Epoch [66] Batch [50/130] Loss: 0.0003 LR: 0.000100
Epoch [66] Batch [60/130] Loss: 0.0008 LR: 0.000100
Epoch [66] Batch [70/130] Loss: 0.0049 LR: 0.000100
Epoch [66] Batch [80/130] Loss: 0.0001 LR: 0.000100
Epoch [66] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [66] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [66] Batch [110/130] Loss: 0.3559 LR: 0.000100
Epoch [66] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [67/350] - Time: 8.71s
Train Loss: 0.0524 | Val Loss: 2.7010
Train Acc: 0.9846 | Val Acc: 0.7933
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 1.000    Count 3: 0.991    Count 4: 0.943    Count 5: 0.922  
  Count 6: 0.878    Count 7: 0.510    Count 8: 0.646    Count 9: 0.367    Count 10: 0.375  
Epoch [67] Batch [0/130] Loss: 0.0001 LR: 0.000100
Epoch [67] Batch [10/130] Loss: 0.0097 LR: 0.000100
Epoch [67] Batch [20/130] Loss: 0.0152 LR: 0.000100
Epoch [67] Batch [30/130] Loss: 0.1124 LR: 0.000100
Epoch [67] Batch [40/130] Loss: 0.0011 LR: 0.000100
Epoch [67] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [67] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [67] Batch [70/130] Loss: 0.0001 LR: 0.000100
Epoch [67] Batch [80/130] Loss: 0.0144 LR: 0.000100
Epoch [67] Batch [90/130] Loss: 0.0010 LR: 0.000100
Epoch [67] Batch [100/130] Loss: 0.0031 LR: 0.000100
Epoch [67] Batch [110/130] Loss: 0.1714 LR: 0.000100
Epoch [67] Batch [120/130] Loss: 0.0055 LR: 0.000100

Epoch [68/350] - Time: 8.17s
Train Loss: 0.0437 | Val Loss: 2.4810
Train Acc: 0.9889 | Val Acc: 0.8082
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.972    Count 4: 0.971    Count 5: 0.980  
  Count 6: 0.847    Count 7: 0.688    Count 8: 0.596    Count 9: 0.278    Count 10: 0.455  
Epoch [68] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [68] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [68] Batch [20/130] Loss: 0.0004 LR: 0.000100
Epoch [68] Batch [30/130] Loss: 0.1648 LR: 0.000100
Epoch [68] Batch [40/130] Loss: 0.0023 LR: 0.000100
Epoch [68] Batch [50/130] Loss: 0.0001 LR: 0.000100
Epoch [68] Batch [60/130] Loss: 0.0001 LR: 0.000100
Epoch [68] Batch [70/130] Loss: 0.4494 LR: 0.000100
Epoch [68] Batch [80/130] Loss: 0.0011 LR: 0.000100
Epoch [68] Batch [90/130] Loss: 0.0017 LR: 0.000100
Epoch [68] Batch [100/130] Loss: 0.0029 LR: 0.000100
Epoch [68] Batch [110/130] Loss: 0.0004 LR: 0.000100
Epoch [68] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [69/350] - Time: 8.30s
Train Loss: 0.0550 | Val Loss: 2.6383
Train Acc: 0.9865 | Val Acc: 0.7821
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.981    Count 4: 0.924    Count 5: 0.990  
  Count 6: 0.867    Count 7: 0.604    Count 8: 0.222    Count 9: 0.256    Count 10: 0.682  
Epoch [69] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [69] Batch [10/130] Loss: 0.0010 LR: 0.000100
Epoch [69] Batch [20/130] Loss: 0.0001 LR: 0.000100
Epoch [69] Batch [30/130] Loss: 0.0001 LR: 0.000100
Epoch [69] Batch [40/130] Loss: 0.0001 LR: 0.000100
Epoch [69] Batch [50/130] Loss: 0.0290 LR: 0.000100
Epoch [69] Batch [60/130] Loss: 0.0217 LR: 0.000100
Epoch [69] Batch [70/130] Loss: 0.0042 LR: 0.000100
Epoch [69] Batch [80/130] Loss: 0.2949 LR: 0.000100
Epoch [69] Batch [90/130] Loss: 0.0214 LR: 0.000100
Epoch [69] Batch [100/130] Loss: 0.0011 LR: 0.000100
Epoch [69] Batch [110/130] Loss: 0.0013 LR: 0.000100
Epoch [69] Batch [120/130] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [70/350] - Time: 8.30s
Train Loss: 0.0373 | Val Loss: 2.6784
Train Acc: 0.9899 | Val Acc: 0.8101
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 1.000    Count 3: 1.000    Count 4: 0.971    Count 5: 0.961  
  Count 6: 0.857    Count 7: 0.479    Count 8: 0.697    Count 9: 0.389    Count 10: 0.466  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/single_image_checkpoint_epoch_69.pth
Epoch [70] Batch [0/130] Loss: 0.0039 LR: 0.000100
Epoch [70] Batch [10/130] Loss: 0.0005 LR: 0.000100
Epoch [70] Batch [20/130] Loss: 0.1175 LR: 0.000100
Epoch [70] Batch [30/130] Loss: 0.4335 LR: 0.000100
Epoch [70] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [70] Batch [50/130] Loss: 0.3788 LR: 0.000100
Epoch [70] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [70] Batch [70/130] Loss: 0.0031 LR: 0.000100
Epoch [70] Batch [80/130] Loss: 0.0001 LR: 0.000100
Epoch [70] Batch [90/130] Loss: 0.0003 LR: 0.000100
Epoch [70] Batch [100/130] Loss: 0.0724 LR: 0.000100
Epoch [70] Batch [110/130] Loss: 0.0001 LR: 0.000100
Epoch [70] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [71/350] - Time: 8.27s
Train Loss: 0.0571 | Val Loss: 2.4721
Train Acc: 0.9855 | Val Acc: 0.7914
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.963    Count 4: 0.895    Count 5: 1.000  
  Count 6: 0.908    Count 7: 0.542    Count 8: 0.515    Count 9: 0.278    Count 10: 0.511  
Epoch [71] Batch [0/130] Loss: 0.0005 LR: 0.000100
Epoch [71] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [71] Batch [20/130] Loss: 0.0023 LR: 0.000100
Epoch [71] Batch [30/130] Loss: 0.2425 LR: 0.000100
Epoch [71] Batch [40/130] Loss: 0.0004 LR: 0.000100
Epoch [71] Batch [50/130] Loss: 0.0004 LR: 0.000100
Epoch [71] Batch [60/130] Loss: 0.0001 LR: 0.000100
Epoch [71] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [71] Batch [80/130] Loss: 0.0007 LR: 0.000100
Epoch [71] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [71] Batch [100/130] Loss: 0.0840 LR: 0.000100
Epoch [71] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [71] Batch [120/130] Loss: 0.2251 LR: 0.000100

Epoch [72/350] - Time: 8.28s
Train Loss: 0.0480 | Val Loss: 2.4326
Train Acc: 0.9851 | Val Acc: 0.8063
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.990    Count 5: 0.951  
  Count 6: 0.898    Count 7: 0.573    Count 8: 0.384    Count 9: 0.322    Count 10: 0.670  
Epoch [72] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [72] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [72] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [72] Batch [30/130] Loss: 0.0464 LR: 0.000100
Epoch [72] Batch [40/130] Loss: 0.0002 LR: 0.000100
Epoch [72] Batch [50/130] Loss: 0.0015 LR: 0.000100
Epoch [72] Batch [60/130] Loss: 0.0060 LR: 0.000100
Epoch [72] Batch [70/130] Loss: 0.0003 LR: 0.000100
Epoch [72] Batch [80/130] Loss: 0.1119 LR: 0.000100
Epoch [72] Batch [90/130] Loss: 0.0001 LR: 0.000100
Epoch [72] Batch [100/130] Loss: 0.0004 LR: 0.000100
Epoch [72] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [72] Batch [120/130] Loss: 0.0053 LR: 0.000100

Epoch [73/350] - Time: 8.17s
Train Loss: 0.0177 | Val Loss: 3.2706
Train Acc: 0.9942 | Val Acc: 0.7337
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.968    Count 3: 0.963    Count 4: 0.914    Count 5: 0.922  
  Count 6: 0.816    Count 7: 0.250    Count 8: 0.465    Count 9: 0.222    Count 10: 0.455  
Epoch [73] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [73] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [73] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [73] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [73] Batch [40/130] Loss: 0.0003 LR: 0.000100
Epoch [73] Batch [50/130] Loss: 0.0004 LR: 0.000100
Epoch [73] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [73] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [73] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [73] Batch [90/130] Loss: 0.1284 LR: 0.000100
Epoch [73] Batch [100/130] Loss: 0.0001 LR: 0.000100
Epoch [73] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [73] Batch [120/130] Loss: 0.0010 LR: 0.000100

Epoch [74/350] - Time: 8.22s
Train Loss: 0.0642 | Val Loss: 2.7000
Train Acc: 0.9870 | Val Acc: 0.8091
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.971    Count 5: 0.951  
  Count 6: 0.888    Count 7: 0.615    Count 8: 0.596    Count 9: 0.289    Count 10: 0.489  
Epoch [74] Batch [0/130] Loss: 0.0005 LR: 0.000100
Epoch [74] Batch [10/130] Loss: 0.0032 LR: 0.000100
Epoch [74] Batch [20/130] Loss: 0.0001 LR: 0.000100
Epoch [74] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [74] Batch [40/130] Loss: 0.0105 LR: 0.000100
Epoch [74] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [74] Batch [60/130] Loss: 0.0001 LR: 0.000100
Epoch [74] Batch [70/130] Loss: 0.2985 LR: 0.000100
Epoch [74] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [74] Batch [90/130] Loss: 0.0119 LR: 0.000100
Epoch [74] Batch [100/130] Loss: 0.0001 LR: 0.000100
Epoch [74] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [74] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [75/350] - Time: 8.27s
Train Loss: 0.0360 | Val Loss: 3.6655
Train Acc: 0.9913 | Val Acc: 0.7337
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.984    Count 3: 0.972    Count 4: 0.981    Count 5: 0.961  
  Count 6: 0.827    Count 7: 0.333    Count 8: 0.343    Count 9: 0.189    Count 10: 0.364  
Epoch [75] Batch [0/130] Loss: 0.0949 LR: 0.000100
Epoch [75] Batch [10/130] Loss: 0.2620 LR: 0.000100
Epoch [75] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [75] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [75] Batch [40/130] Loss: 0.0951 LR: 0.000100
Epoch [75] Batch [50/130] Loss: 0.0001 LR: 0.000100
Epoch [75] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [75] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [75] Batch [80/130] Loss: 0.0027 LR: 0.000100
Epoch [75] Batch [90/130] Loss: 0.3645 LR: 0.000100
Epoch [75] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [75] Batch [110/130] Loss: 0.0005 LR: 0.000100
Epoch [75] Batch [120/130] Loss: 0.0027 LR: 0.000100

Epoch [76/350] - Time: 9.24s
Train Loss: 0.0747 | Val Loss: 2.4778
Train Acc: 0.9798 | Val Acc: 0.7961
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.981    Count 5: 0.961  
  Count 6: 0.867    Count 7: 0.531    Count 8: 0.444    Count 9: 0.256    Count 10: 0.625  
Epoch [76] Batch [0/130] Loss: 0.0489 LR: 0.000100
Epoch [76] Batch [10/130] Loss: 0.2426 LR: 0.000100
Epoch [76] Batch [20/130] Loss: 0.0001 LR: 0.000100
Epoch [76] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [76] Batch [40/130] Loss: 0.0002 LR: 0.000100
Epoch [76] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [76] Batch [60/130] Loss: 0.0001 LR: 0.000100
Epoch [76] Batch [70/130] Loss: 0.0001 LR: 0.000100
Epoch [76] Batch [80/130] Loss: 0.0151 LR: 0.000100
Epoch [76] Batch [90/130] Loss: 0.1291 LR: 0.000100
Epoch [76] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [76] Batch [110/130] Loss: 0.0001 LR: 0.000100
Epoch [76] Batch [120/130] Loss: 0.0001 LR: 0.000100

Epoch [77/350] - Time: 8.25s
Train Loss: 0.0396 | Val Loss: 2.2182
Train Acc: 0.9908 | Val Acc: 0.8147
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.990    Count 5: 0.971  
  Count 6: 0.837    Count 7: 0.583    Count 8: 0.596    Count 9: 0.244    Count 10: 0.648  
Epoch [77] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [77] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [77] Batch [20/130] Loss: 0.0027 LR: 0.000100
Epoch [77] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [77] Batch [40/130] Loss: 0.4617 LR: 0.000100
Epoch [77] Batch [50/130] Loss: 0.0026 LR: 0.000100
Epoch [77] Batch [60/130] Loss: 0.0299 LR: 0.000100
Epoch [77] Batch [70/130] Loss: 0.0003 LR: 0.000100
Epoch [77] Batch [80/130] Loss: 0.3922 LR: 0.000100
Epoch [77] Batch [90/130] Loss: 0.8287 LR: 0.000100
Epoch [77] Batch [100/130] Loss: 0.0016 LR: 0.000100
Epoch [77] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [77] Batch [120/130] Loss: 0.0002 LR: 0.000100

Epoch [78/350] - Time: 8.30s
Train Loss: 0.0827 | Val Loss: 2.5587
Train Acc: 0.9778 | Val Acc: 0.8007
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.962    Count 5: 0.892  
  Count 6: 0.827    Count 7: 0.542    Count 8: 0.566    Count 9: 0.444    Count 10: 0.489  
Epoch [78] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [78] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [78] Batch [20/130] Loss: 0.0513 LR: 0.000100
Epoch [78] Batch [30/130] Loss: 0.4031 LR: 0.000100
Epoch [78] Batch [40/130] Loss: 0.0045 LR: 0.000100
Epoch [78] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [78] Batch [60/130] Loss: 0.0001 LR: 0.000100
Epoch [78] Batch [70/130] Loss: 0.3016 LR: 0.000100
Epoch [78] Batch [80/130] Loss: 0.3875 LR: 0.000100
Epoch [78] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [78] Batch [100/130] Loss: 0.0483 LR: 0.000100
Epoch [78] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [78] Batch [120/130] Loss: 0.0001 LR: 0.000100

Epoch [79/350] - Time: 8.19s
Train Loss: 0.0610 | Val Loss: 3.1090
Train Acc: 0.9836 | Val Acc: 0.7626
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.992    Count 3: 0.963    Count 4: 0.886    Count 5: 0.951  
  Count 6: 0.847    Count 7: 0.385    Count 8: 0.404    Count 9: 0.356    Count 10: 0.523  
Epoch [79] Batch [0/130] Loss: 0.0787 LR: 0.000100
Epoch [79] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [79] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [79] Batch [30/130] Loss: 0.0037 LR: 0.000100
Epoch [79] Batch [40/130] Loss: 0.0100 LR: 0.000100
Epoch [79] Batch [50/130] Loss: 0.0347 LR: 0.000100
Epoch [79] Batch [60/130] Loss: 0.0276 LR: 0.000100
Epoch [79] Batch [70/130] Loss: 0.0001 LR: 0.000100
Epoch [79] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [79] Batch [90/130] Loss: 0.0001 LR: 0.000100
Epoch [79] Batch [100/130] Loss: 0.0007 LR: 0.000100
Epoch [79] Batch [110/130] Loss: 0.0087 LR: 0.000100
Epoch [79] Batch [120/130] Loss: 0.4682 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [80/350] - Time: 8.28s
Train Loss: 0.0563 | Val Loss: 3.2336
Train Acc: 0.9889 | Val Acc: 0.7905
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 1.000    Count 3: 0.991    Count 4: 0.943    Count 5: 0.902  
  Count 6: 0.663    Count 7: 0.740    Count 8: 0.465    Count 9: 0.256    Count 10: 0.670  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/single_image_checkpoint_epoch_79.pth
Epoch [80] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [80] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [80] Batch [20/130] Loss: 0.0001 LR: 0.000100
Epoch [80] Batch [30/130] Loss: 0.0004 LR: 0.000100
Epoch [80] Batch [40/130] Loss: 0.0011 LR: 0.000100
Epoch [80] Batch [50/130] Loss: 0.0024 LR: 0.000100
Epoch [80] Batch [60/130] Loss: 0.0008 LR: 0.000100
Epoch [80] Batch [70/130] Loss: 0.0037 LR: 0.000100
Epoch [80] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [80] Batch [90/130] Loss: 0.0007 LR: 0.000100
Epoch [80] Batch [100/130] Loss: 0.0001 LR: 0.000100
Epoch [80] Batch [110/130] Loss: 0.0039 LR: 0.000100
Epoch [80] Batch [120/130] Loss: 0.0702 LR: 0.000100

Epoch [81/350] - Time: 8.17s
Train Loss: 0.0391 | Val Loss: 2.5064
Train Acc: 0.9880 | Val Acc: 0.7924
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.981    Count 5: 0.961  
  Count 6: 0.898    Count 7: 0.438    Count 8: 0.525    Count 9: 0.456    Count 10: 0.364  
Epoch [81] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [81] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [81] Batch [20/130] Loss: 0.0094 LR: 0.000100
Epoch [81] Batch [30/130] Loss: 0.0005 LR: 0.000100
Epoch [81] Batch [40/130] Loss: 0.1566 LR: 0.000100
Epoch [81] Batch [50/130] Loss: 0.4556 LR: 0.000100
Epoch [81] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [81] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [81] Batch [80/130] Loss: 0.0026 LR: 0.000100
Epoch [81] Batch [90/130] Loss: 0.0010 LR: 0.000100
Epoch [81] Batch [100/130] Loss: 0.0423 LR: 0.000100
Epoch [81] Batch [110/130] Loss: 0.0083 LR: 0.000100
Epoch [81] Batch [120/130] Loss: 0.0001 LR: 0.000100

Epoch [82/350] - Time: 8.18s
Train Loss: 0.0986 | Val Loss: 2.7237
Train Acc: 0.9803 | Val Acc: 0.8147
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.990    Count 5: 0.951  
  Count 6: 0.837    Count 7: 0.667    Count 8: 0.525    Count 9: 0.467    Count 10: 0.432  
Epoch [82] Batch [0/130] Loss: 0.0008 LR: 0.000100
Epoch [82] Batch [10/130] Loss: 0.0350 LR: 0.000100
Epoch [82] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [82] Batch [30/130] Loss: 0.0021 LR: 0.000100
Epoch [82] Batch [40/130] Loss: 0.0088 LR: 0.000100
Epoch [82] Batch [50/130] Loss: 0.0786 LR: 0.000100
Epoch [82] Batch [60/130] Loss: 0.0001 LR: 0.000100
Epoch [82] Batch [70/130] Loss: 0.0060 LR: 0.000100
Epoch [82] Batch [80/130] Loss: 0.0005 LR: 0.000100
Epoch [82] Batch [90/130] Loss: 0.0011 LR: 0.000100
Epoch [82] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [82] Batch [110/130] Loss: 0.0036 LR: 0.000100
Epoch [82] Batch [120/130] Loss: 0.0094 LR: 0.000100

Epoch [83/350] - Time: 8.24s
Train Loss: 0.0864 | Val Loss: 2.2279
Train Acc: 0.9817 | Val Acc: 0.8175
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.962    Count 5: 0.922  
  Count 6: 0.867    Count 7: 0.562    Count 8: 0.616    Count 9: 0.500    Count 10: 0.477  
Epoch [83] Batch [0/130] Loss: 0.1338 LR: 0.000100
Epoch [83] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [83] Batch [20/130] Loss: 0.1692 LR: 0.000100
Epoch [83] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [83] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [83] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [83] Batch [60/130] Loss: 0.0002 LR: 0.000100
Epoch [83] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [83] Batch [80/130] Loss: 0.0463 LR: 0.000100
Epoch [83] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [83] Batch [100/130] Loss: 0.0216 LR: 0.000100
Epoch [83] Batch [110/130] Loss: 0.0065 LR: 0.000100
Epoch [83] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [84/350] - Time: 8.57s
Train Loss: 0.0334 | Val Loss: 2.5071
Train Acc: 0.9913 | Val Acc: 0.8035
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.971    Count 5: 0.951  
  Count 6: 0.806    Count 7: 0.604    Count 8: 0.646    Count 9: 0.322    Count 10: 0.432  
Epoch [84] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [84] Batch [10/130] Loss: 0.0005 LR: 0.000100
Epoch [84] Batch [20/130] Loss: 0.0010 LR: 0.000100
Epoch [84] Batch [30/130] Loss: 0.2684 LR: 0.000100
Epoch [84] Batch [40/130] Loss: 0.3528 LR: 0.000100
Epoch [84] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [84] Batch [60/130] Loss: 0.0462 LR: 0.000100
Epoch [84] Batch [70/130] Loss: 0.0101 LR: 0.000100
Epoch [84] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [84] Batch [90/130] Loss: 0.0001 LR: 0.000100
Epoch [84] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [84] Batch [110/130] Loss: 0.2599 LR: 0.000100
Epoch [84] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [85/350] - Time: 8.36s
Train Loss: 0.0415 | Val Loss: 2.6038
Train Acc: 0.9889 | Val Acc: 0.8063
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.972    Count 4: 0.952    Count 5: 0.961  
  Count 6: 0.837    Count 7: 0.625    Count 8: 0.485    Count 9: 0.378    Count 10: 0.580  
Epoch [85] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [85] Batch [10/130] Loss: 0.0007 LR: 0.000100
Epoch [85] Batch [20/130] Loss: 0.0001 LR: 0.000100
Epoch [85] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [85] Batch [40/130] Loss: 0.0001 LR: 0.000100
Epoch [85] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [85] Batch [60/130] Loss: 0.0001 LR: 0.000100
Epoch [85] Batch [70/130] Loss: 0.0001 LR: 0.000100
Epoch [85] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [85] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [85] Batch [100/130] Loss: 0.0004 LR: 0.000100
Epoch [85] Batch [110/130] Loss: 0.0002 LR: 0.000100
Epoch [85] Batch [120/130] Loss: 0.0006 LR: 0.000100

Epoch [86/350] - Time: 8.24s
Train Loss: 0.0128 | Val Loss: 2.8346
Train Acc: 0.9971 | Val Acc: 0.8045
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.943    Count 5: 0.912  
  Count 6: 0.724    Count 7: 0.604    Count 8: 0.576    Count 9: 0.444    Count 10: 0.568  
Epoch [86] Batch [0/130] Loss: 0.3449 LR: 0.000100
Epoch [86] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [86] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [86] Batch [30/130] Loss: 0.0323 LR: 0.000100
Epoch [86] Batch [40/130] Loss: 0.0001 LR: 0.000100
Epoch [86] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [86] Batch [60/130] Loss: 0.0836 LR: 0.000100
Epoch [86] Batch [70/130] Loss: 0.0003 LR: 0.000100
Epoch [86] Batch [80/130] Loss: 0.0009 LR: 0.000100
Epoch [86] Batch [90/130] Loss: 0.0002 LR: 0.000100
Epoch [86] Batch [100/130] Loss: 0.0006 LR: 0.000100
Epoch [86] Batch [110/130] Loss: 0.0018 LR: 0.000100
Epoch [86] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [87/350] - Time: 8.26s
Train Loss: 0.0531 | Val Loss: 2.7161
Train Acc: 0.9865 | Val Acc: 0.8035
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.981    Count 5: 0.971  
  Count 6: 0.847    Count 7: 0.510    Count 8: 0.535    Count 9: 0.322    Count 10: 0.591  
Epoch [87] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [87] Batch [10/130] Loss: 0.0003 LR: 0.000100
Epoch [87] Batch [20/130] Loss: 0.0007 LR: 0.000100
Epoch [87] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [87] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [87] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [87] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [87] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [87] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [87] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [87] Batch [100/130] Loss: 0.0001 LR: 0.000100
Epoch [87] Batch [110/130] Loss: 0.3617 LR: 0.000100
Epoch [87] Batch [120/130] Loss: 0.0001 LR: 0.000100

Epoch [88/350] - Time: 8.13s
Train Loss: 0.0114 | Val Loss: 3.1961
Train Acc: 0.9971 | Val Acc: 0.8007
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.981    Count 5: 0.990  
  Count 6: 0.918    Count 7: 0.552    Count 8: 0.535    Count 9: 0.233    Count 10: 0.500  
Epoch [88] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [88] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [88] Batch [20/130] Loss: 0.0029 LR: 0.000100
Epoch [88] Batch [30/130] Loss: 0.0004 LR: 0.000100
Epoch [88] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [88] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [88] Batch [60/130] Loss: 0.3872 LR: 0.000100
Epoch [88] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [88] Batch [80/130] Loss: 0.0002 LR: 0.000100
Epoch [88] Batch [90/130] Loss: 0.0466 LR: 0.000100
Epoch [88] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [88] Batch [110/130] Loss: 0.0065 LR: 0.000100
Epoch [88] Batch [120/130] Loss: 0.0694 LR: 0.000100

Epoch [89/350] - Time: 8.14s
Train Loss: 0.0375 | Val Loss: 2.9984
Train Acc: 0.9933 | Val Acc: 0.8119
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.981    Count 4: 1.000    Count 5: 0.912  
  Count 6: 0.867    Count 7: 0.479    Count 8: 0.687    Count 9: 0.478    Count 10: 0.432  
Epoch [89] Batch [0/130] Loss: 0.0001 LR: 0.000100
Epoch [89] Batch [10/130] Loss: 0.2538 LR: 0.000100
Epoch [89] Batch [20/130] Loss: 0.3818 LR: 0.000100
Epoch [89] Batch [30/130] Loss: 0.0036 LR: 0.000100
Epoch [89] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [89] Batch [50/130] Loss: 0.0001 LR: 0.000100
Epoch [89] Batch [60/130] Loss: 0.0001 LR: 0.000100
Epoch [89] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [89] Batch [80/130] Loss: 0.6662 LR: 0.000100
Epoch [89] Batch [90/130] Loss: 0.0002 LR: 0.000100
Epoch [89] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [89] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [89] Batch [120/130] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [90/350] - Time: 8.41s
Train Loss: 0.0796 | Val Loss: 3.6566
Train Acc: 0.9822 | Val Acc: 0.7821
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.981    Count 4: 0.905    Count 5: 0.784  
  Count 6: 0.765    Count 7: 0.615    Count 8: 0.606    Count 9: 0.433    Count 10: 0.432  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/single_image_checkpoint_epoch_89.pth
Epoch [90] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [90] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [90] Batch [20/130] Loss: 0.0003 LR: 0.000100
Epoch [90] Batch [30/130] Loss: 0.0004 LR: 0.000100
Epoch [90] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [90] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [90] Batch [60/130] Loss: 0.0583 LR: 0.000100
Epoch [90] Batch [70/130] Loss: 0.0001 LR: 0.000100
Epoch [90] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [90] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [90] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [90] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [90] Batch [120/130] Loss: 0.1132 LR: 0.000100

Epoch [91/350] - Time: 8.40s
Train Loss: 0.0165 | Val Loss: 3.5836
Train Acc: 0.9947 | Val Acc: 0.7467
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.988    Count 2: 1.000    Count 3: 0.991    Count 4: 0.952    Count 5: 0.912  
  Count 6: 0.827    Count 7: 0.292    Count 8: 0.535    Count 9: 0.200    Count 10: 0.409  
Epoch [91] Batch [0/130] Loss: 0.0004 LR: 0.000100
Epoch [91] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [91] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [91] Batch [30/130] Loss: 0.0011 LR: 0.000100
Epoch [91] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [91] Batch [50/130] Loss: 0.0001 LR: 0.000100
Epoch [91] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [91] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [91] Batch [80/130] Loss: 0.0113 LR: 0.000100
Epoch [91] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [91] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [91] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [91] Batch [120/130] Loss: 0.0105 LR: 0.000100

Epoch [92/350] - Time: 8.93s
Train Loss: 0.0348 | Val Loss: 3.5566
Train Acc: 0.9947 | Val Acc: 0.7775
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.990    Count 5: 0.912  
  Count 6: 0.837    Count 7: 0.646    Count 8: 0.232    Count 9: 0.156    Count 10: 0.705  
Epoch [92] Batch [0/130] Loss: 0.3337 LR: 0.000100
Epoch [92] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [92] Batch [20/130] Loss: 0.1098 LR: 0.000100
Epoch [92] Batch [30/130] Loss: 0.8641 LR: 0.000100
Epoch [92] Batch [40/130] Loss: 0.0127 LR: 0.000100
Epoch [92] Batch [50/130] Loss: 0.0001 LR: 0.000100
Epoch [92] Batch [60/130] Loss: 0.0017 LR: 0.000100
Epoch [92] Batch [70/130] Loss: 0.0037 LR: 0.000100
Epoch [92] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [92] Batch [90/130] Loss: 0.0003 LR: 0.000100
Epoch [92] Batch [100/130] Loss: 0.0001 LR: 0.000100
Epoch [92] Batch [110/130] Loss: 0.0930 LR: 0.000100
Epoch [92] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [93/350] - Time: 8.38s
Train Loss: 0.0712 | Val Loss: 2.9960
Train Acc: 0.9870 | Val Acc: 0.8063
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.981    Count 5: 0.882  
  Count 6: 0.837    Count 7: 0.677    Count 8: 0.586    Count 9: 0.400    Count 10: 0.409  
Epoch [93] Batch [0/130] Loss: 0.2959 LR: 0.000100
Epoch [93] Batch [10/130] Loss: 0.0001 LR: 0.000100
Epoch [93] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [93] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [93] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [93] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [93] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [93] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [93] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [93] Batch [90/130] Loss: 0.0103 LR: 0.000100
Epoch [93] Batch [100/130] Loss: 0.0003 LR: 0.000100
Epoch [93] Batch [110/130] Loss: 0.0002 LR: 0.000100
Epoch [93] Batch [120/130] Loss: 0.0003 LR: 0.000100

Epoch [94/350] - Time: 8.34s
Train Loss: 0.0409 | Val Loss: 2.9510
Train Acc: 0.9918 | Val Acc: 0.8156
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 1.000    Count 3: 0.991    Count 4: 0.981    Count 5: 0.892  
  Count 6: 0.786    Count 7: 0.667    Count 8: 0.535    Count 9: 0.378    Count 10: 0.682  
Epoch [94] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [94] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [94] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [94] Batch [30/130] Loss: 0.0013 LR: 0.000100
Epoch [94] Batch [40/130] Loss: 0.0003 LR: 0.000100
Epoch [94] Batch [50/130] Loss: 0.0022 LR: 0.000100
Epoch [94] Batch [60/130] Loss: 0.0002 LR: 0.000100
Epoch [94] Batch [70/130] Loss: 0.5225 LR: 0.000100
Epoch [94] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [94] Batch [90/130] Loss: 0.0263 LR: 0.000100
Epoch [94] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [94] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [94] Batch [120/130] Loss: 0.1872 LR: 0.000100

Epoch [95/350] - Time: 8.27s
Train Loss: 0.0534 | Val Loss: 2.7712
Train Acc: 0.9875 | Val Acc: 0.7933
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.981    Count 4: 0.962    Count 5: 0.971  
  Count 6: 0.888    Count 7: 0.469    Count 8: 0.525    Count 9: 0.322    Count 10: 0.511  
Epoch [95] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [95] Batch [10/130] Loss: 0.0001 LR: 0.000100
Epoch [95] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [95] Batch [30/130] Loss: 0.6351 LR: 0.000100
Epoch [95] Batch [40/130] Loss: 0.0028 LR: 0.000100
Epoch [95] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [95] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [95] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [95] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [95] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [95] Batch [100/130] Loss: 0.0150 LR: 0.000100
Epoch [95] Batch [110/130] Loss: 0.0021 LR: 0.000100
Epoch [95] Batch [120/130] Loss: 0.0002 LR: 0.000100

Epoch [96/350] - Time: 8.17s
Train Loss: 0.0422 | Val Loss: 4.2704
Train Acc: 0.9899 | Val Acc: 0.6527
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.984    Count 3: 0.602    Count 4: 0.562    Count 5: 0.559  
  Count 6: 0.796    Count 7: 0.417    Count 8: 0.364    Count 9: 0.333    Count 10: 0.568  
Epoch [96] Batch [0/130] Loss: 0.4093 LR: 0.000100
Epoch [96] Batch [10/130] Loss: 0.0046 LR: 0.000100
Epoch [96] Batch [20/130] Loss: 0.2928 LR: 0.000100
Epoch [96] Batch [30/130] Loss: 0.0001 LR: 0.000100
Epoch [96] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [96] Batch [50/130] Loss: 0.0162 LR: 0.000100
Epoch [96] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [96] Batch [70/130] Loss: 0.0001 LR: 0.000100
Epoch [96] Batch [80/130] Loss: 0.0001 LR: 0.000100
Epoch [96] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [96] Batch [100/130] Loss: 0.0003 LR: 0.000100
Epoch [96] Batch [110/130] Loss: 0.0002 LR: 0.000100
Epoch [96] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [97/350] - Time: 8.22s
Train Loss: 0.0785 | Val Loss: 4.2764
Train Acc: 0.9831 | Val Acc: 0.7477
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.990    Count 5: 0.971  
  Count 6: 0.827    Count 7: 0.333    Count 8: 0.182    Count 9: 0.300    Count 10: 0.523  
Epoch [97] Batch [0/130] Loss: 0.0125 LR: 0.000100
Epoch [97] Batch [10/130] Loss: 0.0009 LR: 0.000100
Epoch [97] Batch [20/130] Loss: 0.0916 LR: 0.000100
Epoch [97] Batch [30/130] Loss: 0.0004 LR: 0.000100
Epoch [97] Batch [40/130] Loss: 0.0012 LR: 0.000100
Epoch [97] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [97] Batch [60/130] Loss: 1.3917 LR: 0.000100
Epoch [97] Batch [70/130] Loss: 0.0001 LR: 0.000100
Epoch [97] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [97] Batch [90/130] Loss: 0.0068 LR: 0.000100
Epoch [97] Batch [100/130] Loss: 0.0003 LR: 0.000100
Epoch [97] Batch [110/130] Loss: 0.0002 LR: 0.000100
Epoch [97] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [98/350] - Time: 8.06s
Train Loss: 0.0255 | Val Loss: 3.2256
Train Acc: 0.9933 | Val Acc: 0.7924
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.981    Count 5: 0.971  
  Count 6: 0.908    Count 7: 0.573    Count 8: 0.333    Count 9: 0.267    Count 10: 0.602  
Epoch [98] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [98] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [98] Batch [20/130] Loss: 0.0004 LR: 0.000100
Epoch [98] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [98] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [98] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [98] Batch [60/130] Loss: 0.0005 LR: 0.000100
Epoch [98] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [98] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [98] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [98] Batch [100/130] Loss: 0.1133 LR: 0.000100
Epoch [98] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [98] Batch [120/130] Loss: 0.0014 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [99/350] - Time: 8.32s
Train Loss: 0.0154 | Val Loss: 2.8792
Train Acc: 0.9966 | Val Acc: 0.8389
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.990    Count 5: 0.922  
  Count 6: 0.847    Count 7: 0.646    Count 8: 0.646    Count 9: 0.456    Count 10: 0.648  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/best_single_image_model.pth
  → New best model! Validation accuracy: 0.8389
新的最佳模型! 验证准确率: 0.8389
Epoch [99] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [99] Batch [10/130] Loss: 0.2339 LR: 0.000100
Epoch [99] Batch [20/130] Loss: 0.0008 LR: 0.000100
Epoch [99] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [99] Batch [40/130] Loss: 0.4717 LR: 0.000100
Epoch [99] Batch [50/130] Loss: 0.0018 LR: 0.000100
Epoch [99] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [99] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [99] Batch [80/130] Loss: 0.0008 LR: 0.000100
Epoch [99] Batch [90/130] Loss: 0.3843 LR: 0.000100
Epoch [99] Batch [100/130] Loss: 0.0104 LR: 0.000100
Epoch [99] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [99] Batch [120/130] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [100/350] - Time: 8.63s
Train Loss: 0.0560 | Val Loss: 3.0603
Train Acc: 0.9841 | Val Acc: 0.8138
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 0.892  
  Count 6: 0.857    Count 7: 0.656    Count 8: 0.576    Count 9: 0.356    Count 10: 0.534  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/single_image_checkpoint_epoch_99.pth
Epoch [100] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [100] Batch [10/130] Loss: 0.0001 LR: 0.000100
Epoch [100] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [100] Batch [30/130] Loss: 0.1203 LR: 0.000100
Epoch [100] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [100] Batch [50/130] Loss: 0.0016 LR: 0.000100
Epoch [100] Batch [60/130] Loss: 0.0316 LR: 0.000100
Epoch [100] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [100] Batch [80/130] Loss: 0.0004 LR: 0.000100
Epoch [100] Batch [90/130] Loss: 0.0002 LR: 0.000100
Epoch [100] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [100] Batch [110/130] Loss: 0.3021 LR: 0.000100
Epoch [100] Batch [120/130] Loss: 0.0001 LR: 0.000100

Epoch [101/350] - Time: 8.11s
Train Loss: 0.0940 | Val Loss: 3.1569
Train Acc: 0.9803 | Val Acc: 0.8110
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.962    Count 5: 0.941  
  Count 6: 0.796    Count 7: 0.833    Count 8: 0.364    Count 9: 0.300    Count 10: 0.648  
Epoch [101] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [101] Batch [10/130] Loss: 0.0006 LR: 0.000100
Epoch [101] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [101] Batch [30/130] Loss: 0.1676 LR: 0.000100
Epoch [101] Batch [40/130] Loss: 0.0007 LR: 0.000100
Epoch [101] Batch [50/130] Loss: 0.0003 LR: 0.000100
Epoch [101] Batch [60/130] Loss: 0.0928 LR: 0.000100
Epoch [101] Batch [70/130] Loss: 0.0001 LR: 0.000100
Epoch [101] Batch [80/130] Loss: 0.0001 LR: 0.000100
Epoch [101] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [101] Batch [100/130] Loss: 0.0015 LR: 0.000100
Epoch [101] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [101] Batch [120/130] Loss: 0.4643 LR: 0.000100

Epoch [102/350] - Time: 8.23s
Train Loss: 0.1555 | Val Loss: 5.1679
Train Acc: 0.9769 | Val Acc: 0.6331
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.952    Count 3: 0.833    Count 4: 0.657    Count 5: 0.147  
  Count 6: 0.480    Count 7: 0.458    Count 8: 0.525    Count 9: 0.489    Count 10: 0.432  
Epoch [102] Batch [0/130] Loss: 1.6301 LR: 0.000100
Epoch [102] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [102] Batch [20/130] Loss: 0.0001 LR: 0.000100
Epoch [102] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [102] Batch [40/130] Loss: 0.0044 LR: 0.000100
Epoch [102] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [102] Batch [60/130] Loss: 0.0026 LR: 0.000100
Epoch [102] Batch [70/130] Loss: 0.0001 LR: 0.000100
Epoch [102] Batch [80/130] Loss: 0.2690 LR: 0.000100
Epoch [102] Batch [90/130] Loss: 0.0459 LR: 0.000100
Epoch [102] Batch [100/130] Loss: 0.0001 LR: 0.000100
Epoch [102] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [102] Batch [120/130] Loss: 0.0001 LR: 0.000100

Epoch [103/350] - Time: 8.17s
Train Loss: 0.0813 | Val Loss: 4.1508
Train Acc: 0.9846 | Val Acc: 0.7402
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.990    Count 5: 0.961  
  Count 6: 0.898    Count 7: 0.177    Count 8: 0.465    Count 9: 0.256    Count 10: 0.261  
Epoch [103] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [103] Batch [10/130] Loss: 0.2642 LR: 0.000100
Epoch [103] Batch [20/130] Loss: 0.0037 LR: 0.000100
Epoch [103] Batch [30/130] Loss: 0.0001 LR: 0.000100
Epoch [103] Batch [40/130] Loss: 0.0001 LR: 0.000100
Epoch [103] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [103] Batch [60/130] Loss: 0.0312 LR: 0.000100
Epoch [103] Batch [70/130] Loss: 0.0008 LR: 0.000100
Epoch [103] Batch [80/130] Loss: 0.3642 LR: 0.000100
Epoch [103] Batch [90/130] Loss: 0.1801 LR: 0.000100
Epoch [103] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [103] Batch [110/130] Loss: 0.0001 LR: 0.000100
Epoch [103] Batch [120/130] Loss: 0.0067 LR: 0.000100

Epoch [104/350] - Time: 8.45s
Train Loss: 0.0486 | Val Loss: 4.0659
Train Acc: 0.9870 | Val Acc: 0.7598
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.912  
  Count 6: 0.878    Count 7: 0.583    Count 8: 0.242    Count 9: 0.211    Count 10: 0.420  
Epoch [104] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [104] Batch [10/130] Loss: 0.0079 LR: 0.000100
Epoch [104] Batch [20/130] Loss: 0.0777 LR: 0.000100
Epoch [104] Batch [30/130] Loss: 0.0001 LR: 0.000100
Epoch [104] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [104] Batch [50/130] Loss: 0.0154 LR: 0.000100
Epoch [104] Batch [60/130] Loss: 0.0001 LR: 0.000100
Epoch [104] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [104] Batch [80/130] Loss: 0.0024 LR: 0.000100
Epoch [104] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [104] Batch [100/130] Loss: 0.9048 LR: 0.000100
Epoch [104] Batch [110/130] Loss: 0.0415 LR: 0.000100
Epoch [104] Batch [120/130] Loss: 0.0008 LR: 0.000100

Epoch [105/350] - Time: 8.28s
Train Loss: 0.0151 | Val Loss: 3.4870
Train Acc: 0.9961 | Val Acc: 0.7998
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.992    Count 3: 1.000    Count 4: 0.981    Count 5: 0.902  
  Count 6: 0.816    Count 7: 0.521    Count 8: 0.576    Count 9: 0.467    Count 10: 0.455  
Epoch [105] Batch [0/130] Loss: 0.0010 LR: 0.000100
Epoch [105] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [105] Batch [20/130] Loss: 0.4195 LR: 0.000100
Epoch [105] Batch [30/130] Loss: 0.5505 LR: 0.000100
Epoch [105] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [105] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [105] Batch [60/130] Loss: 0.0011 LR: 0.000100
Epoch [105] Batch [70/130] Loss: 0.3214 LR: 0.000100
Epoch [105] Batch [80/130] Loss: 0.1609 LR: 0.000100
Epoch [105] Batch [90/130] Loss: 0.0005 LR: 0.000100
Epoch [105] Batch [100/130] Loss: 0.0006 LR: 0.000100
Epoch [105] Batch [110/130] Loss: 0.0007 LR: 0.000100
Epoch [105] Batch [120/130] Loss: 0.0891 LR: 0.000100

Epoch [106/350] - Time: 8.15s
Train Loss: 0.0412 | Val Loss: 3.5510
Train Acc: 0.9880 | Val Acc: 0.8082
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.833  
  Count 6: 0.724    Count 7: 0.667    Count 8: 0.566    Count 9: 0.322    Count 10: 0.705  
Epoch [106] Batch [0/130] Loss: 0.0002 LR: 0.000100
Epoch [106] Batch [10/130] Loss: 0.0571 LR: 0.000100
Epoch [106] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [106] Batch [30/130] Loss: 0.0014 LR: 0.000100
Epoch [106] Batch [40/130] Loss: 0.0016 LR: 0.000100
Epoch [106] Batch [50/130] Loss: 0.0006 LR: 0.000100
Epoch [106] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [106] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [106] Batch [80/130] Loss: 0.0005 LR: 0.000100
Epoch [106] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [106] Batch [100/130] Loss: 0.0775 LR: 0.000100
Epoch [106] Batch [110/130] Loss: 0.0066 LR: 0.000100
Epoch [106] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [107/350] - Time: 8.40s
Train Loss: 0.1127 | Val Loss: 3.3053
Train Acc: 0.9803 | Val Acc: 0.7644
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.981    Count 5: 0.922  
  Count 6: 0.888    Count 7: 0.531    Count 8: 0.404    Count 9: 0.244    Count 10: 0.330  
Epoch [107] Batch [0/130] Loss: 0.0001 LR: 0.000100
Epoch [107] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [107] Batch [20/130] Loss: 0.0496 LR: 0.000100
Epoch [107] Batch [30/130] Loss: 0.0003 LR: 0.000100
Epoch [107] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [107] Batch [50/130] Loss: 0.0001 LR: 0.000100
Epoch [107] Batch [60/130] Loss: 0.0002 LR: 0.000100
Epoch [107] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [107] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [107] Batch [90/130] Loss: 0.0001 LR: 0.000100
Epoch [107] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [107] Batch [110/130] Loss: 0.0005 LR: 0.000100
Epoch [107] Batch [120/130] Loss: 0.0001 LR: 0.000100

Epoch [108/350] - Time: 8.19s
Train Loss: 0.0315 | Val Loss: 3.0519
Train Acc: 0.9908 | Val Acc: 0.8166
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.990    Count 5: 0.980  
  Count 6: 0.918    Count 7: 0.531    Count 8: 0.737    Count 9: 0.344    Count 10: 0.364  
Epoch [108] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [108] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [108] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [108] Batch [30/130] Loss: 0.0112 LR: 0.000100
Epoch [108] Batch [40/130] Loss: 0.0013 LR: 0.000100
Epoch [108] Batch [50/130] Loss: 0.8395 LR: 0.000100
Epoch [108] Batch [60/130] Loss: 0.0065 LR: 0.000100
Epoch [108] Batch [70/130] Loss: 0.3697 LR: 0.000100
Epoch [108] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [108] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [108] Batch [100/130] Loss: 0.8162 LR: 0.000100
Epoch [108] Batch [110/130] Loss: 0.0008 LR: 0.000100
Epoch [108] Batch [120/130] Loss: 0.0001 LR: 0.000100

Epoch [109/350] - Time: 8.94s
Train Loss: 0.0517 | Val Loss: 3.0620
Train Acc: 0.9884 | Val Acc: 0.8250
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.931  
  Count 6: 0.867    Count 7: 0.656    Count 8: 0.495    Count 9: 0.333    Count 10: 0.716  
Epoch [109] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [109] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [109] Batch [20/130] Loss: 0.0001 LR: 0.000100
Epoch [109] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [109] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [109] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [109] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [109] Batch [70/130] Loss: 0.0002 LR: 0.000100
Epoch [109] Batch [80/130] Loss: 0.0006 LR: 0.000100
Epoch [109] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [109] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [109] Batch [110/130] Loss: 0.0016 LR: 0.000100
Epoch [109] Batch [120/130] Loss: 0.0001 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [110/350] - Time: 8.26s
Train Loss: 0.0149 | Val Loss: 3.2706
Train Acc: 0.9957 | Val Acc: 0.8082
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.971    Count 5: 0.922  
  Count 6: 0.898    Count 7: 0.594    Count 8: 0.465    Count 9: 0.389    Count 10: 0.580  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/single_image_checkpoint_epoch_109.pth
Epoch [110] Batch [0/130] Loss: 0.0001 LR: 0.000100
Epoch [110] Batch [10/130] Loss: 0.0001 LR: 0.000100
Epoch [110] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [110] Batch [30/130] Loss: 0.0002 LR: 0.000100
Epoch [110] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [110] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [110] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [110] Batch [70/130] Loss: 0.0987 LR: 0.000100
Epoch [110] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [110] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [110] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [110] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [110] Batch [120/130] Loss: 0.1457 LR: 0.000100

Epoch [111/350] - Time: 8.33s
Train Loss: 0.0237 | Val Loss: 3.0920
Train Acc: 0.9957 | Val Acc: 0.8222
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.981    Count 5: 0.912  
  Count 6: 0.827    Count 7: 0.688    Count 8: 0.616    Count 9: 0.367    Count 10: 0.568  
Epoch [111] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [111] Batch [10/130] Loss: 0.0439 LR: 0.000100
Epoch [111] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [111] Batch [30/130] Loss: 0.0001 LR: 0.000100
Epoch [111] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [111] Batch [50/130] Loss: 0.0002 LR: 0.000100
Epoch [111] Batch [60/130] Loss: 0.0001 LR: 0.000100
Epoch [111] Batch [70/130] Loss: 0.0783 LR: 0.000100
Epoch [111] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [111] Batch [90/130] Loss: 0.3566 LR: 0.000100
Epoch [111] Batch [100/130] Loss: 0.0007 LR: 0.000100
Epoch [111] Batch [110/130] Loss: 0.0022 LR: 0.000100
Epoch [111] Batch [120/130] Loss: 0.0006 LR: 0.000100

Epoch [112/350] - Time: 8.14s
Train Loss: 0.0551 | Val Loss: 4.4367
Train Acc: 0.9880 | Val Acc: 0.7439
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.990    Count 5: 0.971  
  Count 6: 0.847    Count 7: 0.438    Count 8: 0.232    Count 9: 0.244    Count 10: 0.352  
Epoch [112] Batch [0/130] Loss: 0.0036 LR: 0.000100
Epoch [112] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [112] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [112] Batch [30/130] Loss: 0.2162 LR: 0.000100
Epoch [112] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [112] Batch [50/130] Loss: 0.0056 LR: 0.000100
Epoch [112] Batch [60/130] Loss: 0.0001 LR: 0.000100
Epoch [112] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [112] Batch [80/130] Loss: 0.0271 LR: 0.000100
Epoch [112] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [112] Batch [100/130] Loss: 0.0002 LR: 0.000100
Epoch [112] Batch [110/130] Loss: 0.1833 LR: 0.000100
Epoch [112] Batch [120/130] Loss: 0.0281 LR: 0.000100

Epoch [113/350] - Time: 8.19s
Train Loss: 0.0501 | Val Loss: 3.7722
Train Acc: 0.9870 | Val Acc: 0.7709
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.972    Count 4: 0.952    Count 5: 0.951  
  Count 6: 0.847    Count 7: 0.531    Count 8: 0.354    Count 9: 0.267    Count 10: 0.511  
Epoch [113] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [113] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [113] Batch [20/130] Loss: 0.0015 LR: 0.000100
Epoch [113] Batch [30/130] Loss: 0.0017 LR: 0.000100
Epoch [113] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [113] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [113] Batch [60/130] Loss: 0.0029 LR: 0.000100
Epoch [113] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [113] Batch [80/130] Loss: 0.0002 LR: 0.000100
Epoch [113] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [113] Batch [100/130] Loss: 0.0006 LR: 0.000100
Epoch [113] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [113] Batch [120/130] Loss: 0.0037 LR: 0.000100

Epoch [114/350] - Time: 8.35s
Train Loss: 0.0623 | Val Loss: 3.4000
Train Acc: 0.9904 | Val Acc: 0.7719
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.972    Count 4: 0.933    Count 5: 0.804  
  Count 6: 0.520    Count 7: 0.625    Count 8: 0.475    Count 9: 0.400    Count 10: 0.705  
Epoch [114] Batch [0/130] Loss: 0.0009 LR: 0.000100
Epoch [114] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [114] Batch [20/130] Loss: 0.0001 LR: 0.000100
Epoch [114] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [114] Batch [40/130] Loss: 0.0003 LR: 0.000100
Epoch [114] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [114] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [114] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [114] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [114] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [114] Batch [100/130] Loss: 0.0001 LR: 0.000100
Epoch [114] Batch [110/130] Loss: 0.7257 LR: 0.000100
Epoch [114] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [115/350] - Time: 8.20s
Train Loss: 0.0317 | Val Loss: 3.4972
Train Acc: 0.9933 | Val Acc: 0.7896
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.981    Count 4: 0.962    Count 5: 0.902  
  Count 6: 0.796    Count 7: 0.604    Count 8: 0.525    Count 9: 0.289    Count 10: 0.534  
Epoch [115] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [115] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [115] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [115] Batch [30/130] Loss: 0.0874 LR: 0.000100
Epoch [115] Batch [40/130] Loss: 0.5180 LR: 0.000100
Epoch [115] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [115] Batch [60/130] Loss: 0.0713 LR: 0.000100
Epoch [115] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [115] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [115] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [115] Batch [100/130] Loss: 0.0535 LR: 0.000100
Epoch [115] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [115] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [116/350] - Time: 8.31s
Train Loss: 0.0194 | Val Loss: 4.2278
Train Acc: 0.9952 | Val Acc: 0.7775
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.941  
  Count 6: 0.929    Count 7: 0.344    Count 8: 0.677    Count 9: 0.333    Count 10: 0.193  
Epoch [116] Batch [0/130] Loss: 0.0450 LR: 0.000100
Epoch [116] Batch [10/130] Loss: 0.9891 LR: 0.000100
Epoch [116] Batch [20/130] Loss: 0.2773 LR: 0.000100
Epoch [116] Batch [30/130] Loss: 0.0250 LR: 0.000100
Epoch [116] Batch [40/130] Loss: 0.0014 LR: 0.000100
Epoch [116] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [116] Batch [60/130] Loss: 0.0559 LR: 0.000100
Epoch [116] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [116] Batch [80/130] Loss: 0.0129 LR: 0.000100
Epoch [116] Batch [90/130] Loss: 0.0002 LR: 0.000100
Epoch [116] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [116] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [116] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [117/350] - Time: 8.59s
Train Loss: 0.0748 | Val Loss: 3.1179
Train Acc: 0.9836 | Val Acc: 0.7868
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 1.000    Count 3: 1.000    Count 4: 0.952    Count 5: 0.951  
  Count 6: 0.918    Count 7: 0.573    Count 8: 0.485    Count 9: 0.233    Count 10: 0.443  
Epoch [117] Batch [0/130] Loss: 0.0002 LR: 0.000100
Epoch [117] Batch [10/130] Loss: 0.1802 LR: 0.000100
Epoch [117] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [117] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [117] Batch [40/130] Loss: 0.0019 LR: 0.000100
Epoch [117] Batch [50/130] Loss: 0.0218 LR: 0.000100
Epoch [117] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [117] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [117] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [117] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [117] Batch [100/130] Loss: 0.0001 LR: 0.000100
Epoch [117] Batch [110/130] Loss: 0.1923 LR: 0.000100
Epoch [117] Batch [120/130] Loss: 0.0016 LR: 0.000100

Epoch [118/350] - Time: 8.23s
Train Loss: 0.0583 | Val Loss: 3.4333
Train Acc: 0.9875 | Val Acc: 0.7970
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.933    Count 5: 0.814  
  Count 6: 0.755    Count 7: 0.729    Count 8: 0.465    Count 9: 0.344    Count 10: 0.670  
Epoch [118] Batch [0/130] Loss: 1.0511 LR: 0.000100
Epoch [118] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [118] Batch [20/130] Loss: 0.0001 LR: 0.000100
Epoch [118] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [118] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [118] Batch [50/130] Loss: 0.3745 LR: 0.000100
Epoch [118] Batch [60/130] Loss: 0.0007 LR: 0.000100
Epoch [118] Batch [70/130] Loss: 0.3208 LR: 0.000100
Epoch [118] Batch [80/130] Loss: 0.2540 LR: 0.000100
Epoch [118] Batch [90/130] Loss: 0.0003 LR: 0.000100
Epoch [118] Batch [100/130] Loss: 0.0003 LR: 0.000100
Epoch [118] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [118] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [119/350] - Time: 8.31s
Train Loss: 0.0509 | Val Loss: 3.6647
Train Acc: 0.9884 | Val Acc: 0.8026
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.962    Count 5: 0.980  
  Count 6: 0.857    Count 7: 0.562    Count 8: 0.616    Count 9: 0.344    Count 10: 0.398  
Epoch [119] Batch [0/130] Loss: 0.0005 LR: 0.000100
Epoch [119] Batch [10/130] Loss: 0.0003 LR: 0.000100
Epoch [119] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [119] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [119] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [119] Batch [50/130] Loss: 0.0001 LR: 0.000100
Epoch [119] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [119] Batch [70/130] Loss: 0.0014 LR: 0.000100
Epoch [119] Batch [80/130] Loss: 0.0364 LR: 0.000100
Epoch [119] Batch [90/130] Loss: 0.0066 LR: 0.000100
Epoch [119] Batch [100/130] Loss: 0.0332 LR: 0.000100
Epoch [119] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [119] Batch [120/130] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [120/350] - Time: 8.20s
Train Loss: 0.0528 | Val Loss: 3.4198
Train Acc: 0.9899 | Val Acc: 0.8045
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 0.912  
  Count 6: 0.816    Count 7: 0.635    Count 8: 0.485    Count 9: 0.333    Count 10: 0.591  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/single_image_checkpoint_epoch_119.pth
Epoch [120] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [120] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [120] Batch [20/130] Loss: 0.0001 LR: 0.000100
Epoch [120] Batch [30/130] Loss: 0.0040 LR: 0.000100
Epoch [120] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [120] Batch [50/130] Loss: 0.0028 LR: 0.000100
Epoch [120] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [120] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [120] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [120] Batch [90/130] Loss: 0.3207 LR: 0.000100
Epoch [120] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [120] Batch [110/130] Loss: 0.0040 LR: 0.000100
Epoch [120] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [121/350] - Time: 8.14s
Train Loss: 0.0150 | Val Loss: 3.1909
Train Acc: 0.9952 | Val Acc: 0.8156
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.971    Count 5: 0.892  
  Count 6: 0.786    Count 7: 0.729    Count 8: 0.475    Count 9: 0.356    Count 10: 0.705  
Epoch [121] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [121] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [121] Batch [20/130] Loss: 0.0003 LR: 0.000100
Epoch [121] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [121] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [121] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [121] Batch [60/130] Loss: 0.0003 LR: 0.000100
Epoch [121] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [121] Batch [80/130] Loss: 0.0003 LR: 0.000100
Epoch [121] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [121] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [121] Batch [110/130] Loss: 0.0002 LR: 0.000100
Epoch [121] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [122/350] - Time: 8.22s
Train Loss: 0.0207 | Val Loss: 3.6745
Train Acc: 0.9947 | Val Acc: 0.7849
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.963    Count 4: 0.905    Count 5: 0.980  
  Count 6: 0.888    Count 7: 0.604    Count 8: 0.434    Count 9: 0.233    Count 10: 0.534  
Epoch [122] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [122] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [122] Batch [20/130] Loss: 0.3186 LR: 0.000100
Epoch [122] Batch [30/130] Loss: 0.0002 LR: 0.000100
Epoch [122] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [122] Batch [50/130] Loss: 0.4299 LR: 0.000100
Epoch [122] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [122] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [122] Batch [80/130] Loss: 0.0986 LR: 0.000100
Epoch [122] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [122] Batch [100/130] Loss: 0.0001 LR: 0.000100
Epoch [122] Batch [110/130] Loss: 0.0001 LR: 0.000100
Epoch [122] Batch [120/130] Loss: 0.0104 LR: 0.000100

Epoch [123/350] - Time: 8.24s
Train Loss: 0.0357 | Val Loss: 3.9693
Train Acc: 0.9913 | Val Acc: 0.7709
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.981    Count 5: 0.990  
  Count 6: 0.918    Count 7: 0.438    Count 8: 0.384    Count 9: 0.189    Count 10: 0.477  
Epoch [123] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [123] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [123] Batch [20/130] Loss: 0.0002 LR: 0.000100
Epoch [123] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [123] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [123] Batch [50/130] Loss: 0.0009 LR: 0.000100
Epoch [123] Batch [60/130] Loss: 0.0006 LR: 0.000100
Epoch [123] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [123] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [123] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [123] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [123] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [123] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [124/350] - Time: 8.24s
Train Loss: 0.0296 | Val Loss: 3.5679
Train Acc: 0.9942 | Val Acc: 0.8007
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.990    Count 5: 0.961  
  Count 6: 0.959    Count 7: 0.562    Count 8: 0.404    Count 9: 0.222    Count 10: 0.614  
Epoch [124] Batch [0/130] Loss: 0.6993 LR: 0.000100
Epoch [124] Batch [10/130] Loss: 0.0001 LR: 0.000100
Epoch [124] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [124] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [124] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [124] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [124] Batch [60/130] Loss: 0.2112 LR: 0.000100
Epoch [124] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [124] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [124] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [124] Batch [100/130] Loss: 0.3737 LR: 0.000100
Epoch [124] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [124] Batch [120/130] Loss: 0.0009 LR: 0.000100

Epoch [125/350] - Time: 8.57s
Train Loss: 0.0266 | Val Loss: 4.1811
Train Acc: 0.9957 | Val Acc: 0.7523
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.981    Count 5: 0.961  
  Count 6: 0.908    Count 7: 0.438    Count 8: 0.404    Count 9: 0.133    Count 10: 0.318  
Epoch [125] Batch [0/130] Loss: 0.3063 LR: 0.000100
Epoch [125] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [125] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [125] Batch [30/130] Loss: 0.0161 LR: 0.000100
Epoch [125] Batch [40/130] Loss: 0.4109 LR: 0.000100
Epoch [125] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [125] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [125] Batch [70/130] Loss: 0.0006 LR: 0.000100
Epoch [125] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [125] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [125] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [125] Batch [110/130] Loss: 0.0823 LR: 0.000100
Epoch [125] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [126/350] - Time: 8.27s
Train Loss: 0.0417 | Val Loss: 3.8641
Train Acc: 0.9923 | Val Acc: 0.7747
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.952    Count 5: 0.951  
  Count 6: 0.980    Count 7: 0.385    Count 8: 0.465    Count 9: 0.178    Count 10: 0.500  
Epoch [126] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [126] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [126] Batch [20/130] Loss: 0.0018 LR: 0.000100
Epoch [126] Batch [30/130] Loss: 0.0002 LR: 0.000100
Epoch [126] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [126] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [126] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [126] Batch [70/130] Loss: 0.0002 LR: 0.000100
Epoch [126] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [126] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [126] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [126] Batch [110/130] Loss: 0.6837 LR: 0.000100
Epoch [126] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [127/350] - Time: 8.30s
Train Loss: 0.0449 | Val Loss: 2.8764
Train Acc: 0.9933 | Val Acc: 0.7961
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.981    Count 5: 0.980  
  Count 6: 0.918    Count 7: 0.573    Count 8: 0.697    Count 9: 0.111    Count 10: 0.364  
Epoch [127] Batch [0/130] Loss: 0.0034 LR: 0.000100
Epoch [127] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [127] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [127] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [127] Batch [40/130] Loss: 0.0091 LR: 0.000100
Epoch [127] Batch [50/130] Loss: 0.0004 LR: 0.000100
Epoch [127] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [127] Batch [70/130] Loss: 0.0006 LR: 0.000100
Epoch [127] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [127] Batch [90/130] Loss: 0.0002 LR: 0.000100
Epoch [127] Batch [100/130] Loss: 0.0009 LR: 0.000100
Epoch [127] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [127] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [128/350] - Time: 8.27s
Train Loss: 0.0339 | Val Loss: 3.9261
Train Acc: 0.9918 | Val Acc: 0.7719
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.981    Count 5: 0.941  
  Count 6: 0.898    Count 7: 0.542    Count 8: 0.222    Count 9: 0.311    Count 10: 0.500  
Epoch [128] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [128] Batch [10/130] Loss: 0.0001 LR: 0.000100
Epoch [128] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [128] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [128] Batch [40/130] Loss: 0.0002 LR: 0.000100
Epoch [128] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [128] Batch [60/130] Loss: 0.0006 LR: 0.000100
Epoch [128] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [128] Batch [80/130] Loss: 0.0205 LR: 0.000100
Epoch [128] Batch [90/130] Loss: 0.0001 LR: 0.000100
Epoch [128] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [128] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [128] Batch [120/130] Loss: 0.0002 LR: 0.000100

Epoch [129/350] - Time: 8.29s
Train Loss: 0.0591 | Val Loss: 2.9797
Train Acc: 0.9875 | Val Acc: 0.8128
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.962    Count 5: 0.971  
  Count 6: 0.857    Count 7: 0.635    Count 8: 0.384    Count 9: 0.367    Count 10: 0.693  
Epoch [129] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [129] Batch [10/130] Loss: 0.0949 LR: 0.000100
Epoch [129] Batch [20/130] Loss: 0.0710 LR: 0.000100
Epoch [129] Batch [30/130] Loss: 0.3334 LR: 0.000100
Epoch [129] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [129] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [129] Batch [60/130] Loss: 0.1983 LR: 0.000100
Epoch [129] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [129] Batch [80/130] Loss: 0.0015 LR: 0.000100
Epoch [129] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [129] Batch [100/130] Loss: 0.0103 LR: 0.000100
Epoch [129] Batch [110/130] Loss: 0.0001 LR: 0.000100
Epoch [129] Batch [120/130] Loss: 0.0009 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [130/350] - Time: 8.13s
Train Loss: 0.0622 | Val Loss: 2.9560
Train Acc: 0.9870 | Val Acc: 0.7989
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.981    Count 4: 0.952    Count 5: 0.863  
  Count 6: 0.724    Count 7: 0.688    Count 8: 0.485    Count 9: 0.444    Count 10: 0.580  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/single_image_checkpoint_epoch_129.pth
Epoch [130] Batch [0/130] Loss: 0.0601 LR: 0.000100
Epoch [130] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [130] Batch [20/130] Loss: 0.9185 LR: 0.000100
Epoch [130] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [130] Batch [40/130] Loss: 0.0449 LR: 0.000100
Epoch [130] Batch [50/130] Loss: 0.0033 LR: 0.000100
Epoch [130] Batch [60/130] Loss: 0.2161 LR: 0.000100
Epoch [130] Batch [70/130] Loss: 0.0121 LR: 0.000100
Epoch [130] Batch [80/130] Loss: 0.4508 LR: 0.000100
Epoch [130] Batch [90/130] Loss: 0.0027 LR: 0.000100
Epoch [130] Batch [100/130] Loss: 0.0023 LR: 0.000100
Epoch [130] Batch [110/130] Loss: 0.0001 LR: 0.000100
Epoch [130] Batch [120/130] Loss: 0.0750 LR: 0.000100

Epoch [131/350] - Time: 8.23s
Train Loss: 0.0612 | Val Loss: 2.7024
Train Acc: 0.9870 | Val Acc: 0.8073
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.990    Count 5: 0.892  
  Count 6: 0.837    Count 7: 0.688    Count 8: 0.596    Count 9: 0.322    Count 10: 0.455  
Epoch [131] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [131] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [131] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [131] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [131] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [131] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [131] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [131] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [131] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [131] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [131] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [131] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [131] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [132/350] - Time: 8.32s
Train Loss: 0.0196 | Val Loss: 3.4157
Train Acc: 0.9957 | Val Acc: 0.7933
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.990  
  Count 6: 0.939    Count 7: 0.458    Count 8: 0.566    Count 9: 0.278    Count 10: 0.375  
Epoch [132] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [132] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [132] Batch [20/130] Loss: 0.0217 LR: 0.000100
Epoch [132] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [132] Batch [40/130] Loss: 0.0002 LR: 0.000100
Epoch [132] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [132] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [132] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [132] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [132] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [132] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [132] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [132] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [133/350] - Time: 8.51s
Train Loss: 0.0059 | Val Loss: 2.9363
Train Acc: 0.9976 | Val Acc: 0.8287
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.990  
  Count 6: 0.878    Count 7: 0.594    Count 8: 0.657    Count 9: 0.444    Count 10: 0.455  
Epoch [133] Batch [0/130] Loss: 0.0017 LR: 0.000100
Epoch [133] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [133] Batch [20/130] Loss: 0.0002 LR: 0.000100
Epoch [133] Batch [30/130] Loss: 0.0016 LR: 0.000100
Epoch [133] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [133] Batch [50/130] Loss: 0.0096 LR: 0.000100
Epoch [133] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [133] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [133] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [133] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [133] Batch [100/130] Loss: 0.0001 LR: 0.000100
Epoch [133] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [133] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [134/350] - Time: 8.42s
Train Loss: 0.0093 | Val Loss: 3.0678
Train Acc: 0.9961 | Val Acc: 0.8119
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.981    Count 4: 0.971    Count 5: 0.882  
  Count 6: 0.816    Count 7: 0.625    Count 8: 0.606    Count 9: 0.333    Count 10: 0.636  
Epoch [134] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [134] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [134] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [134] Batch [30/130] Loss: 0.0745 LR: 0.000100
Epoch [134] Batch [40/130] Loss: 0.0011 LR: 0.000100
Epoch [134] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [134] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [134] Batch [70/130] Loss: 0.0007 LR: 0.000100
Epoch [134] Batch [80/130] Loss: 0.0041 LR: 0.000100
Epoch [134] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [134] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [134] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [134] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [135/350] - Time: 8.22s
Train Loss: 0.0332 | Val Loss: 3.3439
Train Acc: 0.9933 | Val Acc: 0.8147
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.981    Count 5: 1.000  
  Count 6: 0.908    Count 7: 0.615    Count 8: 0.475    Count 9: 0.389    Count 10: 0.500  
Epoch [135] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [135] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [135] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [135] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [135] Batch [40/130] Loss: 0.0010 LR: 0.000100
Epoch [135] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [135] Batch [60/130] Loss: 0.8779 LR: 0.000100
Epoch [135] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [135] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [135] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [135] Batch [100/130] Loss: 0.0010 LR: 0.000100
Epoch [135] Batch [110/130] Loss: 0.0001 LR: 0.000100
Epoch [135] Batch [120/130] Loss: 0.0025 LR: 0.000100

Epoch [136/350] - Time: 8.15s
Train Loss: 0.0836 | Val Loss: 4.1246
Train Acc: 0.9870 | Val Acc: 0.7672
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.971  
  Count 6: 0.959    Count 7: 0.448    Count 8: 0.212    Count 9: 0.200    Count 10: 0.545  
Epoch [136] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [136] Batch [10/130] Loss: 0.4276 LR: 0.000100
Epoch [136] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [136] Batch [30/130] Loss: 0.0009 LR: 0.000100
Epoch [136] Batch [40/130] Loss: 0.0004 LR: 0.000100
Epoch [136] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [136] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [136] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [136] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [136] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [136] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [136] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [136] Batch [120/130] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [137/350] - Time: 8.17s
Train Loss: 0.0214 | Val Loss: 2.3497
Train Acc: 0.9971 | Val Acc: 0.8399
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.988    Count 2: 0.992    Count 3: 1.000    Count 4: 1.000    Count 5: 0.922  
  Count 6: 0.949    Count 7: 0.604    Count 8: 0.707    Count 9: 0.489    Count 10: 0.511  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/best_single_image_model.pth
  → New best model! Validation accuracy: 0.8399
新的最佳模型! 验证准确率: 0.8399
Epoch [137] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [137] Batch [10/130] Loss: 0.0018 LR: 0.000100
Epoch [137] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [137] Batch [30/130] Loss: 0.0225 LR: 0.000100
Epoch [137] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [137] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [137] Batch [60/130] Loss: 0.1067 LR: 0.000100
Epoch [137] Batch [70/130] Loss: 0.0002 LR: 0.000100
Epoch [137] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [137] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [137] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [137] Batch [110/130] Loss: 0.0461 LR: 0.000100
Epoch [137] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [138/350] - Time: 8.26s
Train Loss: 0.0187 | Val Loss: 2.9064
Train Acc: 0.9923 | Val Acc: 0.8054
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.981    Count 5: 0.961  
  Count 6: 0.939    Count 7: 0.615    Count 8: 0.313    Count 9: 0.289    Count 10: 0.682  
Epoch [138] Batch [0/130] Loss: 0.5578 LR: 0.000100
Epoch [138] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [138] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [138] Batch [30/130] Loss: 0.0019 LR: 0.000100
Epoch [138] Batch [40/130] Loss: 0.0926 LR: 0.000100
Epoch [138] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [138] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [138] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [138] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [138] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [138] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [138] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [138] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [139/350] - Time: 8.23s
Train Loss: 0.0225 | Val Loss: 3.2239
Train Acc: 0.9947 | Val Acc: 0.8073
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.980  
  Count 6: 0.898    Count 7: 0.490    Count 8: 0.475    Count 9: 0.356    Count 10: 0.591  
Epoch [139] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [139] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [139] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [139] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [139] Batch [40/130] Loss: 0.2511 LR: 0.000100
Epoch [139] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [139] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [139] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [139] Batch [80/130] Loss: 0.5048 LR: 0.000100
Epoch [139] Batch [90/130] Loss: 0.0001 LR: 0.000100
Epoch [139] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [139] Batch [110/130] Loss: 0.0003 LR: 0.000100
Epoch [139] Batch [120/130] Loss: 0.0011 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [140/350] - Time: 8.29s
Train Loss: 0.0440 | Val Loss: 4.9674
Train Acc: 0.9889 | Val Acc: 0.7672
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.990    Count 5: 0.971  
  Count 6: 0.888    Count 7: 0.604    Count 8: 0.091    Count 9: 0.122    Count 10: 0.682  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/single_image_checkpoint_epoch_139.pth
Epoch [140] Batch [0/130] Loss: 0.1031 LR: 0.000100
Epoch [140] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [140] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [140] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [140] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [140] Batch [50/130] Loss: 0.2676 LR: 0.000100
Epoch [140] Batch [60/130] Loss: 0.0001 LR: 0.000100
Epoch [140] Batch [70/130] Loss: 0.0009 LR: 0.000100
Epoch [140] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [140] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [140] Batch [100/130] Loss: 0.0001 LR: 0.000100
Epoch [140] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [140] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [141/350] - Time: 8.39s
Train Loss: 0.0709 | Val Loss: 3.6056
Train Acc: 0.9889 | Val Acc: 0.7672
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.876    Count 5: 0.686  
  Count 6: 0.459    Count 7: 0.646    Count 8: 0.606    Count 9: 0.422    Count 10: 0.693  
Epoch [141] Batch [0/130] Loss: 0.9326 LR: 0.000100
Epoch [141] Batch [10/130] Loss: 0.0138 LR: 0.000100
Epoch [141] Batch [20/130] Loss: 0.0072 LR: 0.000100
Epoch [141] Batch [30/130] Loss: 0.0004 LR: 0.000100
Epoch [141] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [141] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [141] Batch [60/130] Loss: 0.0540 LR: 0.000100
Epoch [141] Batch [70/130] Loss: 0.0543 LR: 0.000100
Epoch [141] Batch [80/130] Loss: 0.5265 LR: 0.000100
Epoch [141] Batch [90/130] Loss: 0.0469 LR: 0.000100
Epoch [141] Batch [100/130] Loss: 0.0015 LR: 0.000100
Epoch [141] Batch [110/130] Loss: 0.0024 LR: 0.000100
Epoch [141] Batch [120/130] Loss: 0.0001 LR: 0.000100

Epoch [142/350] - Time: 9.04s
Train Loss: 0.0925 | Val Loss: 3.7466
Train Acc: 0.9783 | Val Acc: 0.7821
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 1.000    Count 3: 0.972    Count 4: 0.924    Count 5: 0.725  
  Count 6: 0.663    Count 7: 0.677    Count 8: 0.556    Count 9: 0.278    Count 10: 0.761  
Epoch [142] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [142] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [142] Batch [20/130] Loss: 0.0003 LR: 0.000100
Epoch [142] Batch [30/130] Loss: 0.4909 LR: 0.000100
Epoch [142] Batch [40/130] Loss: 0.2559 LR: 0.000100
Epoch [142] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [142] Batch [60/130] Loss: 0.0004 LR: 0.000100
Epoch [142] Batch [70/130] Loss: 0.0001 LR: 0.000100
Epoch [142] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [142] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [142] Batch [100/130] Loss: 0.0002 LR: 0.000100
Epoch [142] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [142] Batch [120/130] Loss: 0.0003 LR: 0.000100

Epoch [143/350] - Time: 8.13s
Train Loss: 0.0433 | Val Loss: 2.9401
Train Acc: 0.9899 | Val Acc: 0.8110
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.971    Count 5: 0.863  
  Count 6: 0.796    Count 7: 0.688    Count 8: 0.606    Count 9: 0.433    Count 10: 0.477  
Epoch [143] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [143] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [143] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [143] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [143] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [143] Batch [50/130] Loss: 0.0002 LR: 0.000100
Epoch [143] Batch [60/130] Loss: 0.0001 LR: 0.000100
Epoch [143] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [143] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [143] Batch [90/130] Loss: 0.0005 LR: 0.000100
Epoch [143] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [143] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [143] Batch [120/130] Loss: 0.0006 LR: 0.000100

Epoch [144/350] - Time: 8.34s
Train Loss: 0.0101 | Val Loss: 3.8707
Train Acc: 0.9966 | Val Acc: 0.7570
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.981    Count 5: 0.971  
  Count 6: 0.918    Count 7: 0.479    Count 8: 0.394    Count 9: 0.122    Count 10: 0.330  
Epoch [144] Batch [0/130] Loss: 0.0001 LR: 0.000100
Epoch [144] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [144] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [144] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [144] Batch [40/130] Loss: 0.0025 LR: 0.000100
Epoch [144] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [144] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [144] Batch [70/130] Loss: 0.0001 LR: 0.000100
Epoch [144] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [144] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [144] Batch [100/130] Loss: 0.0094 LR: 0.000100
Epoch [144] Batch [110/130] Loss: 0.0003 LR: 0.000100
Epoch [144] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [145/350] - Time: 8.17s
Train Loss: 0.0162 | Val Loss: 3.1385
Train Acc: 0.9976 | Val Acc: 0.8091
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.980  
  Count 6: 0.898    Count 7: 0.604    Count 8: 0.535    Count 9: 0.322    Count 10: 0.455  
Epoch [145] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [145] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [145] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [145] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [145] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [145] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [145] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [145] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [145] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [145] Batch [90/130] Loss: 0.0005 LR: 0.000100
Epoch [145] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [145] Batch [110/130] Loss: 0.0001 LR: 0.000100
Epoch [145] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [146/350] - Time: 8.27s
Train Loss: 0.0038 | Val Loss: 3.1114
Train Acc: 0.9995 | Val Acc: 0.8156
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.990    Count 5: 0.922  
  Count 6: 0.867    Count 7: 0.583    Count 8: 0.657    Count 9: 0.422    Count 10: 0.443  
Epoch [146] Batch [0/130] Loss: 0.0001 LR: 0.000100
Epoch [146] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [146] Batch [20/130] Loss: 0.0002 LR: 0.000100
Epoch [146] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [146] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [146] Batch [50/130] Loss: 0.0001 LR: 0.000100
Epoch [146] Batch [60/130] Loss: 0.0014 LR: 0.000100
Epoch [146] Batch [70/130] Loss: 0.7061 LR: 0.000100
Epoch [146] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [146] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [146] Batch [100/130] Loss: 0.0037 LR: 0.000100
Epoch [146] Batch [110/130] Loss: 0.5546 LR: 0.000100
Epoch [146] Batch [120/130] Loss: 0.0004 LR: 0.000100

Epoch [147/350] - Time: 8.21s
Train Loss: 0.0745 | Val Loss: 3.0373
Train Acc: 0.9855 | Val Acc: 0.8035
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.962    Count 5: 0.990  
  Count 6: 0.939    Count 7: 0.552    Count 8: 0.596    Count 9: 0.211    Count 10: 0.477  
Epoch [147] Batch [0/130] Loss: 0.0003 LR: 0.000100
Epoch [147] Batch [10/130] Loss: 0.1964 LR: 0.000100
Epoch [147] Batch [20/130] Loss: 0.6728 LR: 0.000100
Epoch [147] Batch [30/130] Loss: 0.0003 LR: 0.000100
Epoch [147] Batch [40/130] Loss: 0.4548 LR: 0.000100
Epoch [147] Batch [50/130] Loss: 0.0036 LR: 0.000100
Epoch [147] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [147] Batch [70/130] Loss: 0.0653 LR: 0.000100
Epoch [147] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [147] Batch [90/130] Loss: 0.0786 LR: 0.000100
Epoch [147] Batch [100/130] Loss: 0.0009 LR: 0.000100
Epoch [147] Batch [110/130] Loss: 0.0002 LR: 0.000100
Epoch [147] Batch [120/130] Loss: 0.0082 LR: 0.000100

Epoch [148/350] - Time: 8.43s
Train Loss: 0.0364 | Val Loss: 3.1133
Train Acc: 0.9913 | Val Acc: 0.8110
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.924    Count 5: 0.892  
  Count 6: 0.776    Count 7: 0.583    Count 8: 0.727    Count 9: 0.478    Count 10: 0.455  
Epoch [148] Batch [0/130] Loss: 0.0107 LR: 0.000100
Epoch [148] Batch [10/130] Loss: 0.0001 LR: 0.000100
Epoch [148] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [148] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [148] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [148] Batch [50/130] Loss: 0.0003 LR: 0.000100
Epoch [148] Batch [60/130] Loss: 0.0056 LR: 0.000100
Epoch [148] Batch [70/130] Loss: 0.0076 LR: 0.000100
Epoch [148] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [148] Batch [90/130] Loss: 0.0078 LR: 0.000100
Epoch [148] Batch [100/130] Loss: 0.2890 LR: 0.000100
Epoch [148] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [148] Batch [120/130] Loss: 0.0925 LR: 0.000100

Epoch [149/350] - Time: 8.19s
Train Loss: 0.0345 | Val Loss: 2.6132
Train Acc: 0.9942 | Val Acc: 0.8147
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.990    Count 5: 0.902  
  Count 6: 0.827    Count 7: 0.615    Count 8: 0.545    Count 9: 0.422    Count 10: 0.591  
Epoch [149] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [149] Batch [10/130] Loss: 0.0039 LR: 0.000100
Epoch [149] Batch [20/130] Loss: 0.0016 LR: 0.000100
Epoch [149] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [149] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [149] Batch [50/130] Loss: 0.0001 LR: 0.000100
Epoch [149] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [149] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [149] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [149] Batch [90/130] Loss: 0.2564 LR: 0.000100
Epoch [149] Batch [100/130] Loss: 0.0001 LR: 0.000100
Epoch [149] Batch [110/130] Loss: 0.0830 LR: 0.000100
Epoch [149] Batch [120/130] Loss: 0.0003 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [150/350] - Time: 8.43s
Train Loss: 0.0806 | Val Loss: 3.1746
Train Acc: 0.9865 | Val Acc: 0.7998
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.961  
  Count 6: 0.908    Count 7: 0.625    Count 8: 0.424    Count 9: 0.211    Count 10: 0.568  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/single_image_checkpoint_epoch_149.pth
Epoch [150] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [150] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [150] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [150] Batch [30/130] Loss: 0.0005 LR: 0.000100
Epoch [150] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [150] Batch [50/130] Loss: 0.0012 LR: 0.000100
Epoch [150] Batch [60/130] Loss: 0.0001 LR: 0.000100
Epoch [150] Batch [70/130] Loss: 0.0026 LR: 0.000100
Epoch [150] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [150] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [150] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [150] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [150] Batch [120/130] Loss: 0.0001 LR: 0.000100

Epoch [151/350] - Time: 8.24s
Train Loss: 0.0255 | Val Loss: 3.6584
Train Acc: 0.9947 | Val Acc: 0.8045
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.984    Count 3: 1.000    Count 4: 1.000    Count 5: 0.990  
  Count 6: 0.918    Count 7: 0.646    Count 8: 0.374    Count 9: 0.133    Count 10: 0.716  
Epoch [151] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [151] Batch [10/130] Loss: 0.0777 LR: 0.000100
Epoch [151] Batch [20/130] Loss: 0.0027 LR: 0.000100
Epoch [151] Batch [30/130] Loss: 0.0008 LR: 0.000100
Epoch [151] Batch [40/130] Loss: 0.0003 LR: 0.000100
Epoch [151] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [151] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [151] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [151] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [151] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [151] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [151] Batch [110/130] Loss: 0.0004 LR: 0.000100
Epoch [151] Batch [120/130] Loss: 0.0257 LR: 0.000100

Epoch [152/350] - Time: 8.28s
Train Loss: 0.0227 | Val Loss: 3.6239
Train Acc: 0.9952 | Val Acc: 0.7914
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.988    Count 2: 1.000    Count 3: 1.000    Count 4: 0.962    Count 5: 0.892  
  Count 6: 0.837    Count 7: 0.656    Count 8: 0.495    Count 9: 0.211    Count 10: 0.580  
Epoch [152] Batch [0/130] Loss: 0.0006 LR: 0.000100
Epoch [152] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [152] Batch [20/130] Loss: 0.0008 LR: 0.000100
Epoch [152] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [152] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [152] Batch [50/130] Loss: 0.0002 LR: 0.000100
Epoch [152] Batch [60/130] Loss: 0.1189 LR: 0.000100
Epoch [152] Batch [70/130] Loss: 0.0001 LR: 0.000100
Epoch [152] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [152] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [152] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [152] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [152] Batch [120/130] Loss: 0.0066 LR: 0.000100

Epoch [153/350] - Time: 8.19s
Train Loss: 0.0350 | Val Loss: 3.2255
Train Acc: 0.9913 | Val Acc: 0.8203
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.971  
  Count 6: 0.939    Count 7: 0.552    Count 8: 0.475    Count 9: 0.311    Count 10: 0.693  
Epoch [153] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [153] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [153] Batch [20/130] Loss: 0.1326 LR: 0.000100
Epoch [153] Batch [30/130] Loss: 0.3413 LR: 0.000100
Epoch [153] Batch [40/130] Loss: 0.0002 LR: 0.000100
Epoch [153] Batch [50/130] Loss: 0.0014 LR: 0.000100
Epoch [153] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [153] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [153] Batch [80/130] Loss: 0.1476 LR: 0.000100
Epoch [153] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [153] Batch [100/130] Loss: 0.0001 LR: 0.000100
Epoch [153] Batch [110/130] Loss: 0.0058 LR: 0.000100
Epoch [153] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [154/350] - Time: 8.16s
Train Loss: 0.0805 | Val Loss: 3.3982
Train Acc: 0.9855 | Val Acc: 0.8082
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.990    Count 5: 0.892  
  Count 6: 0.786    Count 7: 0.625    Count 8: 0.475    Count 9: 0.489    Count 10: 0.557  
Epoch [154] Batch [0/130] Loss: 0.0366 LR: 0.000100
Epoch [154] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [154] Batch [20/130] Loss: 0.0061 LR: 0.000100
Epoch [154] Batch [30/130] Loss: 0.0001 LR: 0.000100
Epoch [154] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [154] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [154] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [154] Batch [70/130] Loss: 0.0004 LR: 0.000100
Epoch [154] Batch [80/130] Loss: 0.9036 LR: 0.000100
Epoch [154] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [154] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [154] Batch [110/130] Loss: 0.0001 LR: 0.000100
Epoch [154] Batch [120/130] Loss: 0.0002 LR: 0.000100

Epoch [155/350] - Time: 8.35s
Train Loss: 0.0408 | Val Loss: 3.7122
Train Acc: 0.9913 | Val Acc: 0.7970
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.962    Count 5: 0.824  
  Count 6: 0.755    Count 7: 0.625    Count 8: 0.535    Count 9: 0.389    Count 10: 0.602  
Epoch [155] Batch [0/130] Loss: 0.0103 LR: 0.000100
Epoch [155] Batch [10/130] Loss: 0.0006 LR: 0.000100
Epoch [155] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [155] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [155] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [155] Batch [50/130] Loss: 0.3311 LR: 0.000100
Epoch [155] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [155] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [155] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [155] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [155] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [155] Batch [110/130] Loss: 0.0113 LR: 0.000100
Epoch [155] Batch [120/130] Loss: 0.1122 LR: 0.000100

Epoch [156/350] - Time: 8.19s
Train Loss: 0.0303 | Val Loss: 3.7087
Train Acc: 0.9947 | Val Acc: 0.7998
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.990  
  Count 6: 0.949    Count 7: 0.448    Count 8: 0.495    Count 9: 0.322    Count 10: 0.489  
Epoch [156] Batch [0/130] Loss: 0.0002 LR: 0.000100
Epoch [156] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [156] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [156] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [156] Batch [40/130] Loss: 0.0514 LR: 0.000100
Epoch [156] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [156] Batch [60/130] Loss: 0.0013 LR: 0.000100
Epoch [156] Batch [70/130] Loss: 0.0005 LR: 0.000100
Epoch [156] Batch [80/130] Loss: 0.0026 LR: 0.000100
Epoch [156] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [156] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [156] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [156] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [157/350] - Time: 8.22s
Train Loss: 0.0094 | Val Loss: 3.3176
Train Acc: 0.9966 | Val Acc: 0.7989
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.990    Count 5: 0.961  
  Count 6: 0.888    Count 7: 0.510    Count 8: 0.586    Count 9: 0.411    Count 10: 0.330  
Epoch [157] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [157] Batch [10/130] Loss: 0.0004 LR: 0.000100
Epoch [157] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [157] Batch [30/130] Loss: 0.5382 LR: 0.000100
Epoch [157] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [157] Batch [50/130] Loss: 0.0004 LR: 0.000100
Epoch [157] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [157] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [157] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [157] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [157] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [157] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [157] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [158/350] - Time: 8.48s
Train Loss: 0.0112 | Val Loss: 3.4759
Train Acc: 0.9976 | Val Acc: 0.7980
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.981    Count 5: 0.951  
  Count 6: 0.867    Count 7: 0.740    Count 8: 0.333    Count 9: 0.256    Count 10: 0.568  
Epoch [158] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [158] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [158] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [158] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [158] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [158] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [158] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [158] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [158] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [158] Batch [90/130] Loss: 0.0001 LR: 0.000100
Epoch [158] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [158] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [158] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [159/350] - Time: 8.13s
Train Loss: 0.0093 | Val Loss: 3.1641
Train Acc: 0.9990 | Val Acc: 0.8119
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.990    Count 5: 0.931  
  Count 6: 0.878    Count 7: 0.542    Count 8: 0.475    Count 9: 0.411    Count 10: 0.625  
Epoch [159] Batch [0/130] Loss: 0.0002 LR: 0.000100
Epoch [159] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [159] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [159] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [159] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [159] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [159] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [159] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [159] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [159] Batch [90/130] Loss: 0.0001 LR: 0.000100
Epoch [159] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [159] Batch [110/130] Loss: 0.0010 LR: 0.000100
Epoch [159] Batch [120/130] Loss: 0.0003 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [160/350] - Time: 8.20s
Train Loss: 0.0688 | Val Loss: 2.8181
Train Acc: 0.9904 | Val Acc: 0.8203
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.943    Count 5: 0.863  
  Count 6: 0.796    Count 7: 0.802    Count 8: 0.495    Count 9: 0.367    Count 10: 0.693  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/single_image_checkpoint_epoch_159.pth
Epoch [160] Batch [0/130] Loss: 0.5841 LR: 0.000100
Epoch [160] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [160] Batch [20/130] Loss: 0.0049 LR: 0.000100
Epoch [160] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [160] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [160] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [160] Batch [60/130] Loss: 0.0454 LR: 0.000100
Epoch [160] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [160] Batch [80/130] Loss: 0.0195 LR: 0.000100
Epoch [160] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [160] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [160] Batch [110/130] Loss: 0.1822 LR: 0.000100
Epoch [160] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [161/350] - Time: 8.11s
Train Loss: 0.0392 | Val Loss: 3.4284
Train Acc: 0.9918 | Val Acc: 0.8101
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 0.941  
  Count 6: 0.898    Count 7: 0.531    Count 8: 0.586    Count 9: 0.389    Count 10: 0.477  
Epoch [161] Batch [0/130] Loss: 1.4091 LR: 0.000100
Epoch [161] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [161] Batch [20/130] Loss: 0.0013 LR: 0.000100
Epoch [161] Batch [30/130] Loss: 0.0002 LR: 0.000100
Epoch [161] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [161] Batch [50/130] Loss: 0.3746 LR: 0.000100
Epoch [161] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [161] Batch [70/130] Loss: 0.0039 LR: 0.000100
Epoch [161] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [161] Batch [90/130] Loss: 0.0001 LR: 0.000100
Epoch [161] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [161] Batch [110/130] Loss: 0.0001 LR: 0.000100
Epoch [161] Batch [120/130] Loss: 0.3500 LR: 0.000100

Epoch [162/350] - Time: 8.19s
Train Loss: 0.0503 | Val Loss: 2.5818
Train Acc: 0.9923 | Val Acc: 0.8231
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.981    Count 4: 0.952    Count 5: 0.961  
  Count 6: 0.888    Count 7: 0.573    Count 8: 0.606    Count 9: 0.333    Count 10: 0.682  
Epoch [162] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [162] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [162] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [162] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [162] Batch [40/130] Loss: 0.0181 LR: 0.000100
Epoch [162] Batch [50/130] Loss: 0.0001 LR: 0.000100
Epoch [162] Batch [60/130] Loss: 0.0101 LR: 0.000100
Epoch [162] Batch [70/130] Loss: 0.0022 LR: 0.000100
Epoch [162] Batch [80/130] Loss: 0.0004 LR: 0.000100
Epoch [162] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [162] Batch [100/130] Loss: 0.0006 LR: 0.000100
Epoch [162] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [162] Batch [120/130] Loss: 0.0003 LR: 0.000100

Epoch [163/350] - Time: 8.17s
Train Loss: 0.0810 | Val Loss: 3.0462
Train Acc: 0.9851 | Val Acc: 0.7980
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.941  
  Count 6: 0.878    Count 7: 0.594    Count 8: 0.465    Count 9: 0.333    Count 10: 0.466  
Epoch [163] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [163] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [163] Batch [20/130] Loss: 0.0003 LR: 0.000100
Epoch [163] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [163] Batch [40/130] Loss: 0.0005 LR: 0.000100
Epoch [163] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [163] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [163] Batch [70/130] Loss: 0.0007 LR: 0.000100
Epoch [163] Batch [80/130] Loss: 0.0193 LR: 0.000100
Epoch [163] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [163] Batch [100/130] Loss: 0.0007 LR: 0.000100
Epoch [163] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [163] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [164/350] - Time: 8.33s
Train Loss: 0.0384 | Val Loss: 3.0192
Train Acc: 0.9908 | Val Acc: 0.8147
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.990    Count 5: 0.941  
  Count 6: 0.908    Count 7: 0.562    Count 8: 0.657    Count 9: 0.433    Count 10: 0.364  
Epoch [164] Batch [0/130] Loss: 0.0004 LR: 0.000100
Epoch [164] Batch [10/130] Loss: 0.0002 LR: 0.000100
Epoch [164] Batch [20/130] Loss: 0.0005 LR: 0.000100
Epoch [164] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [164] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [164] Batch [50/130] Loss: 0.0001 LR: 0.000100
Epoch [164] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [164] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [164] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [164] Batch [90/130] Loss: 0.1734 LR: 0.000100
Epoch [164] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [164] Batch [110/130] Loss: 0.2703 LR: 0.000100
Epoch [164] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [165/350] - Time: 8.17s
Train Loss: 0.0326 | Val Loss: 2.7570
Train Acc: 0.9913 | Val Acc: 0.8343
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.922  
  Count 6: 0.888    Count 7: 0.688    Count 8: 0.606    Count 9: 0.344    Count 10: 0.659  
Epoch [165] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [165] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [165] Batch [20/130] Loss: 0.0036 LR: 0.000100
Epoch [165] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [165] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [165] Batch [50/130] Loss: 0.0027 LR: 0.000100
Epoch [165] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [165] Batch [70/130] Loss: 0.0002 LR: 0.000100
Epoch [165] Batch [80/130] Loss: 0.4539 LR: 0.000100
Epoch [165] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [165] Batch [100/130] Loss: 0.0002 LR: 0.000100
Epoch [165] Batch [110/130] Loss: 0.5006 LR: 0.000100
Epoch [165] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [166/350] - Time: 8.58s
Train Loss: 0.0193 | Val Loss: 3.1195
Train Acc: 0.9952 | Val Acc: 0.8054
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.971    Count 5: 0.922  
  Count 6: 0.816    Count 7: 0.552    Count 8: 0.626    Count 9: 0.467    Count 10: 0.409  
Epoch [166] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [166] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [166] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [166] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [166] Batch [40/130] Loss: 0.1631 LR: 0.000100
Epoch [166] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [166] Batch [60/130] Loss: 0.0011 LR: 0.000100
Epoch [166] Batch [70/130] Loss: 0.0460 LR: 0.000100
Epoch [166] Batch [80/130] Loss: 0.0564 LR: 0.000100
Epoch [166] Batch [90/130] Loss: 0.0001 LR: 0.000100
Epoch [166] Batch [100/130] Loss: 0.0102 LR: 0.000100
Epoch [166] Batch [110/130] Loss: 0.0037 LR: 0.000100
Epoch [166] Batch [120/130] Loss: 0.0001 LR: 0.000100

Epoch [167/350] - Time: 8.19s
Train Loss: 0.0880 | Val Loss: 3.4367
Train Acc: 0.9846 | Val Acc: 0.7914
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.992    Count 3: 1.000    Count 4: 0.981    Count 5: 0.971  
  Count 6: 0.908    Count 7: 0.354    Count 8: 0.687    Count 9: 0.300    Count 10: 0.398  
Epoch [167] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [167] Batch [10/130] Loss: 0.2223 LR: 0.000100
Epoch [167] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [167] Batch [30/130] Loss: 0.1603 LR: 0.000100
Epoch [167] Batch [40/130] Loss: 0.3906 LR: 0.000100
Epoch [167] Batch [50/130] Loss: 0.4036 LR: 0.000100
Epoch [167] Batch [60/130] Loss: 0.0001 LR: 0.000100
Epoch [167] Batch [70/130] Loss: 0.0013 LR: 0.000100
Epoch [167] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [167] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [167] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [167] Batch [110/130] Loss: 0.0006 LR: 0.000100
Epoch [167] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [168/350] - Time: 8.18s
Train Loss: 0.0520 | Val Loss: 2.9432
Train Acc: 0.9880 | Val Acc: 0.8063
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 1.000    Count 3: 0.991    Count 4: 0.971    Count 5: 0.941  
  Count 6: 0.878    Count 7: 0.604    Count 8: 0.586    Count 9: 0.278    Count 10: 0.534  
Epoch [168] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [168] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [168] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [168] Batch [30/130] Loss: 0.0003 LR: 0.000100
Epoch [168] Batch [40/130] Loss: 0.0073 LR: 0.000100
Epoch [168] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [168] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [168] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [168] Batch [80/130] Loss: 0.0001 LR: 0.000100
Epoch [168] Batch [90/130] Loss: 0.0005 LR: 0.000100
Epoch [168] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [168] Batch [110/130] Loss: 0.5582 LR: 0.000100
Epoch [168] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [169/350] - Time: 8.17s
Train Loss: 0.0276 | Val Loss: 3.0274
Train Acc: 0.9942 | Val Acc: 0.8026
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 1.000    Count 3: 1.000    Count 4: 0.990    Count 5: 0.922  
  Count 6: 0.939    Count 7: 0.583    Count 8: 0.515    Count 9: 0.256    Count 10: 0.534  
Epoch [169] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [169] Batch [10/130] Loss: 0.0043 LR: 0.000100
Epoch [169] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [169] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [169] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [169] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [169] Batch [60/130] Loss: 0.1331 LR: 0.000100
Epoch [169] Batch [70/130] Loss: 0.0472 LR: 0.000100
Epoch [169] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [169] Batch [90/130] Loss: 0.0080 LR: 0.000100
Epoch [169] Batch [100/130] Loss: 0.2580 LR: 0.000100
Epoch [169] Batch [110/130] Loss: 0.0001 LR: 0.000100
Epoch [169] Batch [120/130] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [170/350] - Time: 8.31s
Train Loss: 0.0212 | Val Loss: 3.3747
Train Acc: 0.9928 | Val Acc: 0.8119
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 1.000    Count 3: 0.991    Count 4: 0.990    Count 5: 0.873  
  Count 6: 0.816    Count 7: 0.656    Count 8: 0.556    Count 9: 0.278    Count 10: 0.705  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/single_image_checkpoint_epoch_169.pth
Epoch [170] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [170] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [170] Batch [20/130] Loss: 0.0001 LR: 0.000100
Epoch [170] Batch [30/130] Loss: 0.0037 LR: 0.000100
Epoch [170] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [170] Batch [50/130] Loss: 0.0003 LR: 0.000100
Epoch [170] Batch [60/130] Loss: 0.0005 LR: 0.000100
Epoch [170] Batch [70/130] Loss: 0.0006 LR: 0.000100
Epoch [170] Batch [80/130] Loss: 0.0001 LR: 0.000100
Epoch [170] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [170] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [170] Batch [110/130] Loss: 0.0001 LR: 0.000100
Epoch [170] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [171/350] - Time: 8.21s
Train Loss: 0.0370 | Val Loss: 2.9705
Train Acc: 0.9942 | Val Acc: 0.8138
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.981    Count 5: 0.941  
  Count 6: 0.888    Count 7: 0.625    Count 8: 0.566    Count 9: 0.411    Count 10: 0.443  
Epoch [171] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [171] Batch [10/130] Loss: 0.0001 LR: 0.000100
Epoch [171] Batch [20/130] Loss: 0.0001 LR: 0.000100
Epoch [171] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [171] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [171] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [171] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [171] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [171] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [171] Batch [90/130] Loss: 0.0001 LR: 0.000100
Epoch [171] Batch [100/130] Loss: 0.0050 LR: 0.000100
Epoch [171] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [171] Batch [120/130] Loss: 0.0008 LR: 0.000100

Epoch [172/350] - Time: 8.09s
Train Loss: 0.0019 | Val Loss: 3.3748
Train Acc: 0.9990 | Val Acc: 0.7840
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 1.000    Count 3: 0.981    Count 4: 0.971    Count 5: 0.824  
  Count 6: 0.714    Count 7: 0.615    Count 8: 0.495    Count 9: 0.367    Count 10: 0.591  
Epoch [172] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [172] Batch [10/130] Loss: 0.0010 LR: 0.000100
Epoch [172] Batch [20/130] Loss: 0.0001 LR: 0.000100
Epoch [172] Batch [30/130] Loss: 0.0019 LR: 0.000100
Epoch [172] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [172] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [172] Batch [60/130] Loss: 0.0348 LR: 0.000100
Epoch [172] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [172] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [172] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [172] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [172] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [172] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [173/350] - Time: 8.13s
Train Loss: 0.0252 | Val Loss: 3.3053
Train Acc: 0.9937 | Val Acc: 0.7831
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.990    Count 5: 0.980  
  Count 6: 0.949    Count 7: 0.552    Count 8: 0.404    Count 9: 0.244    Count 10: 0.375  
Epoch [173] Batch [0/130] Loss: 0.0001 LR: 0.000100
Epoch [173] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [173] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [173] Batch [30/130] Loss: 0.1697 LR: 0.000100
Epoch [173] Batch [40/130] Loss: 0.3666 LR: 0.000100
Epoch [173] Batch [50/130] Loss: 0.0001 LR: 0.000100
Epoch [173] Batch [60/130] Loss: 0.0001 LR: 0.000100
Epoch [173] Batch [70/130] Loss: 0.0002 LR: 0.000100
Epoch [173] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [173] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [173] Batch [100/130] Loss: 0.0001 LR: 0.000100
Epoch [173] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [173] Batch [120/130] Loss: 0.0001 LR: 0.000100

Epoch [174/350] - Time: 8.21s
Train Loss: 0.0190 | Val Loss: 3.4459
Train Acc: 0.9952 | Val Acc: 0.8017
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 0.912  
  Count 6: 0.878    Count 7: 0.594    Count 8: 0.495    Count 9: 0.278    Count 10: 0.580  
Epoch [174] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [174] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [174] Batch [20/130] Loss: 0.0006 LR: 0.000100
Epoch [174] Batch [30/130] Loss: 0.0684 LR: 0.000100
Epoch [174] Batch [40/130] Loss: 0.0001 LR: 0.000100
Epoch [174] Batch [50/130] Loss: 0.0003 LR: 0.000100
Epoch [174] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [174] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [174] Batch [80/130] Loss: 0.0001 LR: 0.000100
Epoch [174] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [174] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [174] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [174] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [175/350] - Time: 8.64s
Train Loss: 0.0149 | Val Loss: 3.2782
Train Acc: 0.9937 | Val Acc: 0.8026
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.990    Count 5: 0.971  
  Count 6: 0.929    Count 7: 0.729    Count 8: 0.263    Count 9: 0.167    Count 10: 0.693  
Epoch [175] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [175] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [175] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [175] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [175] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [175] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [175] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [175] Batch [70/130] Loss: 0.3128 LR: 0.000100
Epoch [175] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [175] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [175] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [175] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [175] Batch [120/130] Loss: 0.0013 LR: 0.000100

Epoch [176/350] - Time: 8.30s
Train Loss: 0.0306 | Val Loss: 3.5075
Train Acc: 0.9937 | Val Acc: 0.7868
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.952    Count 5: 1.000  
  Count 6: 0.980    Count 7: 0.490    Count 8: 0.343    Count 9: 0.267    Count 10: 0.523  
Epoch [176] Batch [0/130] Loss: 0.0364 LR: 0.000100
Epoch [176] Batch [10/130] Loss: 0.0001 LR: 0.000100
Epoch [176] Batch [20/130] Loss: 0.2211 LR: 0.000100
Epoch [176] Batch [30/130] Loss: 0.0001 LR: 0.000100
Epoch [176] Batch [40/130] Loss: 0.0017 LR: 0.000100
Epoch [176] Batch [50/130] Loss: 0.0013 LR: 0.000100
Epoch [176] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [176] Batch [70/130] Loss: 0.0003 LR: 0.000100
Epoch [176] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [176] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [176] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [176] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [176] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [177/350] - Time: 8.26s
Train Loss: 0.0355 | Val Loss: 3.2706
Train Acc: 0.9918 | Val Acc: 0.7886
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.981    Count 5: 0.941  
  Count 6: 0.918    Count 7: 0.479    Count 8: 0.475    Count 9: 0.444    Count 10: 0.330  
Epoch [177] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [177] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [177] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [177] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [177] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [177] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [177] Batch [60/130] Loss: 0.0001 LR: 0.000100
Epoch [177] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [177] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [177] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [177] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [177] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [177] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [178/350] - Time: 8.37s
Train Loss: 0.0037 | Val Loss: 3.3905
Train Acc: 0.9990 | Val Acc: 0.8026
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.971    Count 5: 0.951  
  Count 6: 0.878    Count 7: 0.625    Count 8: 0.495    Count 9: 0.156    Count 10: 0.659  
Epoch [178] Batch [0/130] Loss: 0.0031 LR: 0.000100
Epoch [178] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [178] Batch [20/130] Loss: 0.0001 LR: 0.000100
Epoch [178] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [178] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [178] Batch [50/130] Loss: 0.0010 LR: 0.000100
Epoch [178] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [178] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [178] Batch [80/130] Loss: 0.0001 LR: 0.000100
Epoch [178] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [178] Batch [100/130] Loss: 0.0001 LR: 0.000100
Epoch [178] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [178] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [179/350] - Time: 8.29s
Train Loss: 0.0149 | Val Loss: 2.7560
Train Acc: 0.9981 | Val Acc: 0.8175
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.971    Count 5: 0.902  
  Count 6: 0.867    Count 7: 0.708    Count 8: 0.586    Count 9: 0.422    Count 10: 0.443  
Epoch [179] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [179] Batch [10/130] Loss: 0.0080 LR: 0.000100
Epoch [179] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [179] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [179] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [179] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [179] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [179] Batch [70/130] Loss: 0.0001 LR: 0.000100
Epoch [179] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [179] Batch [90/130] Loss: 0.0339 LR: 0.000100
Epoch [179] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [179] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [179] Batch [120/130] Loss: 0.0520 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [180/350] - Time: 8.31s
Train Loss: 0.0382 | Val Loss: 2.8260
Train Acc: 0.9928 | Val Acc: 0.8147
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.981    Count 5: 0.912  
  Count 6: 0.857    Count 7: 0.635    Count 8: 0.434    Count 9: 0.456    Count 10: 0.614  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/single_image_checkpoint_epoch_179.pth
Epoch [180] Batch [0/130] Loss: 0.0007 LR: 0.000100
Epoch [180] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [180] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [180] Batch [30/130] Loss: 0.0002 LR: 0.000100
Epoch [180] Batch [40/130] Loss: 0.0001 LR: 0.000100
Epoch [180] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [180] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [180] Batch [70/130] Loss: 0.0001 LR: 0.000100
Epoch [180] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [180] Batch [90/130] Loss: 0.0001 LR: 0.000100
Epoch [180] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [180] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [180] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [181/350] - Time: 8.29s
Train Loss: 0.0165 | Val Loss: 3.6517
Train Acc: 0.9976 | Val Acc: 0.7933
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.981    Count 5: 0.931  
  Count 6: 0.847    Count 7: 0.552    Count 8: 0.505    Count 9: 0.356    Count 10: 0.455  
Epoch [181] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [181] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [181] Batch [20/130] Loss: 0.0001 LR: 0.000100
Epoch [181] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [181] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [181] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [181] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [181] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [181] Batch [80/130] Loss: 0.0001 LR: 0.000100
Epoch [181] Batch [90/130] Loss: 0.0050 LR: 0.000100
Epoch [181] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [181] Batch [110/130] Loss: 0.2510 LR: 0.000100
Epoch [181] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [182/350] - Time: 8.22s
Train Loss: 0.0071 | Val Loss: 3.5295
Train Acc: 0.9981 | Val Acc: 0.7989
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.943    Count 5: 0.902  
  Count 6: 0.847    Count 7: 0.573    Count 8: 0.576    Count 9: 0.400    Count 10: 0.455  
Epoch [182] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [182] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [182] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [182] Batch [30/130] Loss: 0.0001 LR: 0.000100
Epoch [182] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [182] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [182] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [182] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [182] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [182] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [182] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [182] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [182] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [183/350] - Time: 8.58s
Train Loss: 0.0179 | Val Loss: 4.5806
Train Acc: 0.9957 | Val Acc: 0.7365
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.971    Count 5: 0.882  
  Count 6: 0.827    Count 7: 0.250    Count 8: 0.455    Count 9: 0.156    Count 10: 0.443  
Epoch [183] Batch [0/130] Loss: 0.3754 LR: 0.000100
Epoch [183] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [183] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [183] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [183] Batch [40/130] Loss: 0.2300 LR: 0.000100
Epoch [183] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [183] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [183] Batch [70/130] Loss: 0.0001 LR: 0.000100
Epoch [183] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [183] Batch [90/130] Loss: 0.0004 LR: 0.000100
Epoch [183] Batch [100/130] Loss: 0.0045 LR: 0.000100
Epoch [183] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [183] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [184/350] - Time: 8.15s
Train Loss: 0.0356 | Val Loss: 3.4309
Train Acc: 0.9942 | Val Acc: 0.8073
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.990    Count 5: 0.980  
  Count 6: 0.949    Count 7: 0.396    Count 8: 0.596    Count 9: 0.289    Count 10: 0.580  
Epoch [184] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [184] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [184] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [184] Batch [30/130] Loss: 0.0001 LR: 0.000100
Epoch [184] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [184] Batch [50/130] Loss: 0.0005 LR: 0.000100
Epoch [184] Batch [60/130] Loss: 0.0107 LR: 0.000100
Epoch [184] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [184] Batch [80/130] Loss: 0.0001 LR: 0.000100
Epoch [184] Batch [90/130] Loss: 0.3228 LR: 0.000100
Epoch [184] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [184] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [184] Batch [120/130] Loss: 0.0001 LR: 0.000100

Epoch [185/350] - Time: 8.23s
Train Loss: 0.0430 | Val Loss: 3.0497
Train Acc: 0.9942 | Val Acc: 0.8035
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.902  
  Count 6: 0.857    Count 7: 0.625    Count 8: 0.515    Count 9: 0.278    Count 10: 0.568  
Epoch [185] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [185] Batch [10/130] Loss: 0.0025 LR: 0.000100
Epoch [185] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [185] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [185] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [185] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [185] Batch [60/130] Loss: 0.0001 LR: 0.000100
Epoch [185] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [185] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [185] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [185] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [185] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [185] Batch [120/130] Loss: 0.0841 LR: 0.000100

Epoch [186/350] - Time: 8.21s
Train Loss: 0.0145 | Val Loss: 3.7651
Train Acc: 0.9952 | Val Acc: 0.7924
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.952    Count 5: 0.833  
  Count 6: 0.786    Count 7: 0.583    Count 8: 0.465    Count 9: 0.567    Count 10: 0.455  
Epoch [186] Batch [0/130] Loss: 0.0003 LR: 0.000100
Epoch [186] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [186] Batch [20/130] Loss: 0.0014 LR: 0.000100
Epoch [186] Batch [30/130] Loss: 0.4143 LR: 0.000100
Epoch [186] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [186] Batch [50/130] Loss: 0.0016 LR: 0.000100
Epoch [186] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [186] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [186] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [186] Batch [90/130] Loss: 0.0008 LR: 0.000100
Epoch [186] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [186] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [186] Batch [120/130] Loss: 0.0003 LR: 0.000100

Epoch [187/350] - Time: 8.31s
Train Loss: 0.0923 | Val Loss: 4.1380
Train Acc: 0.9822 | Val Acc: 0.7831
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.972    Count 4: 0.943    Count 5: 0.765  
  Count 6: 0.735    Count 7: 0.875    Count 8: 0.253    Count 9: 0.233    Count 10: 0.784  
Epoch [187] Batch [0/130] Loss: 0.0006 LR: 0.000100
Epoch [187] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [187] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [187] Batch [30/130] Loss: 0.0031 LR: 0.000100
Epoch [187] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [187] Batch [50/130] Loss: 0.0183 LR: 0.000100
Epoch [187] Batch [60/130] Loss: 0.0001 LR: 0.000100
Epoch [187] Batch [70/130] Loss: 0.0002 LR: 0.000100
Epoch [187] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [187] Batch [90/130] Loss: 0.0323 LR: 0.000100
Epoch [187] Batch [100/130] Loss: 0.0005 LR: 0.000100
Epoch [187] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [187] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [188/350] - Time: 8.48s
Train Loss: 0.0128 | Val Loss: 3.0183
Train Acc: 0.9990 | Val Acc: 0.8194
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.902  
  Count 6: 0.857    Count 7: 0.688    Count 8: 0.485    Count 9: 0.300    Count 10: 0.705  
Epoch [188] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [188] Batch [10/130] Loss: 0.0001 LR: 0.000100
Epoch [188] Batch [20/130] Loss: 0.1848 LR: 0.000100
Epoch [188] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [188] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [188] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [188] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [188] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [188] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [188] Batch [90/130] Loss: 0.1789 LR: 0.000100
Epoch [188] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [188] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [188] Batch [120/130] Loss: 0.3182 LR: 0.000100

Epoch [189/350] - Time: 8.25s
Train Loss: 0.0271 | Val Loss: 3.6844
Train Acc: 0.9923 | Val Acc: 0.7765
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.962    Count 5: 0.873  
  Count 6: 0.714    Count 7: 0.615    Count 8: 0.424    Count 9: 0.289    Count 10: 0.580  
Epoch [189] Batch [0/130] Loss: 0.0001 LR: 0.000100
Epoch [189] Batch [10/130] Loss: 0.0002 LR: 0.000100
Epoch [189] Batch [20/130] Loss: 0.0001 LR: 0.000100
Epoch [189] Batch [30/130] Loss: 0.1193 LR: 0.000100
Epoch [189] Batch [40/130] Loss: 0.1274 LR: 0.000100
Epoch [189] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [189] Batch [60/130] Loss: 0.1305 LR: 0.000100
Epoch [189] Batch [70/130] Loss: 0.0001 LR: 0.000100
Epoch [189] Batch [80/130] Loss: 0.0002 LR: 0.000100
Epoch [189] Batch [90/130] Loss: 0.1691 LR: 0.000100
Epoch [189] Batch [100/130] Loss: 0.0003 LR: 0.000100
Epoch [189] Batch [110/130] Loss: 0.0002 LR: 0.000100
Epoch [189] Batch [120/130] Loss: 0.0106 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [190/350] - Time: 8.36s
Train Loss: 0.0413 | Val Loss: 4.1832
Train Acc: 0.9884 | Val Acc: 0.7598
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.971    Count 5: 0.902  
  Count 6: 0.929    Count 7: 0.469    Count 8: 0.182    Count 9: 0.200    Count 10: 0.614  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/single_image_checkpoint_epoch_189.pth
Epoch [190] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [190] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [190] Batch [20/130] Loss: 0.0264 LR: 0.000100
Epoch [190] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [190] Batch [40/130] Loss: 0.0006 LR: 0.000100
Epoch [190] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [190] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [190] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [190] Batch [80/130] Loss: 0.0560 LR: 0.000100
Epoch [190] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [190] Batch [100/130] Loss: 0.2533 LR: 0.000100
Epoch [190] Batch [110/130] Loss: 1.1427 LR: 0.000100
Epoch [190] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [191/350] - Time: 8.49s
Train Loss: 0.0408 | Val Loss: 3.7148
Train Acc: 0.9933 | Val Acc: 0.7942
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.981    Count 4: 0.952    Count 5: 0.931  
  Count 6: 0.827    Count 7: 0.635    Count 8: 0.364    Count 9: 0.256    Count 10: 0.716  
Epoch [191] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [191] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [191] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [191] Batch [30/130] Loss: 0.0004 LR: 0.000100
Epoch [191] Batch [40/130] Loss: 0.0258 LR: 0.000100
Epoch [191] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [191] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [191] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [191] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [191] Batch [90/130] Loss: 0.3389 LR: 0.000100
Epoch [191] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [191] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [191] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [192/350] - Time: 8.22s
Train Loss: 0.0181 | Val Loss: 3.9904
Train Acc: 0.9947 | Val Acc: 0.7765
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.981    Count 5: 0.863  
  Count 6: 0.786    Count 7: 0.646    Count 8: 0.394    Count 9: 0.189    Count 10: 0.591  
Epoch [192] Batch [0/130] Loss: 0.0001 LR: 0.000100
Epoch [192] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [192] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [192] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [192] Batch [40/130] Loss: 0.0166 LR: 0.000100
Epoch [192] Batch [50/130] Loss: 0.0194 LR: 0.000100
Epoch [192] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [192] Batch [70/130] Loss: 0.0001 LR: 0.000100
Epoch [192] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [192] Batch [90/130] Loss: 0.0012 LR: 0.000100
Epoch [192] Batch [100/130] Loss: 0.0168 LR: 0.000100
Epoch [192] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [192] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [193/350] - Time: 8.29s
Train Loss: 0.0280 | Val Loss: 3.2395
Train Acc: 0.9937 | Val Acc: 0.8101
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.971    Count 5: 0.902  
  Count 6: 0.796    Count 7: 0.562    Count 8: 0.717    Count 9: 0.444    Count 10: 0.420  
Epoch [193] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [193] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [193] Batch [20/130] Loss: 0.0004 LR: 0.000100
Epoch [193] Batch [30/130] Loss: 0.0001 LR: 0.000100
Epoch [193] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [193] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [193] Batch [60/130] Loss: 0.0001 LR: 0.000100
Epoch [193] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [193] Batch [80/130] Loss: 0.0107 LR: 0.000100
Epoch [193] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [193] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [193] Batch [110/130] Loss: 0.0004 LR: 0.000100
Epoch [193] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [194/350] - Time: 8.20s
Train Loss: 0.0131 | Val Loss: 3.7197
Train Acc: 0.9971 | Val Acc: 0.8119
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.980  
  Count 6: 0.939    Count 7: 0.573    Count 8: 0.434    Count 9: 0.233    Count 10: 0.682  
Epoch [194] Batch [0/130] Loss: 0.0407 LR: 0.000100
Epoch [194] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [194] Batch [20/130] Loss: 0.0053 LR: 0.000100
Epoch [194] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [194] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [194] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [194] Batch [60/130] Loss: 0.0012 LR: 0.000100
Epoch [194] Batch [70/130] Loss: 0.0003 LR: 0.000100
Epoch [194] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [194] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [194] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [194] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [194] Batch [120/130] Loss: 0.4265 LR: 0.000100

Epoch [195/350] - Time: 8.27s
Train Loss: 0.0196 | Val Loss: 3.0014
Train Acc: 0.9966 | Val Acc: 0.8063
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.962    Count 5: 0.824  
  Count 6: 0.878    Count 7: 0.531    Count 8: 0.646    Count 9: 0.456    Count 10: 0.500  
Epoch [195] Batch [0/130] Loss: 0.0001 LR: 0.000100
Epoch [195] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [195] Batch [20/130] Loss: 1.1091 LR: 0.000100
Epoch [195] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [195] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [195] Batch [50/130] Loss: 0.0125 LR: 0.000100
Epoch [195] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [195] Batch [70/130] Loss: 0.4104 LR: 0.000100
Epoch [195] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [195] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [195] Batch [100/130] Loss: 0.0032 LR: 0.000100
Epoch [195] Batch [110/130] Loss: 0.0001 LR: 0.000100
Epoch [195] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [196/350] - Time: 8.38s
Train Loss: 0.0278 | Val Loss: 4.0136
Train Acc: 0.9971 | Val Acc: 0.7784
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 0.922  
  Count 6: 0.908    Count 7: 0.354    Count 8: 0.596    Count 9: 0.300    Count 10: 0.375  
Epoch [196] Batch [0/130] Loss: 0.0059 LR: 0.000100
Epoch [196] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [196] Batch [20/130] Loss: 0.0127 LR: 0.000100
Epoch [196] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [196] Batch [40/130] Loss: 0.0045 LR: 0.000100
Epoch [196] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [196] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [196] Batch [70/130] Loss: 0.0001 LR: 0.000100
Epoch [196] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [196] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [196] Batch [100/130] Loss: 0.2140 LR: 0.000100
Epoch [196] Batch [110/130] Loss: 0.0001 LR: 0.000100
Epoch [196] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [197/350] - Time: 8.24s
Train Loss: 0.0300 | Val Loss: 4.0544
Train Acc: 0.9928 | Val Acc: 0.7691
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.954    Count 4: 0.867    Count 5: 0.686  
  Count 6: 0.837    Count 7: 0.469    Count 8: 0.697    Count 9: 0.444    Count 10: 0.432  
Epoch [197] Batch [0/130] Loss: 0.2646 LR: 0.000100
Epoch [197] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [197] Batch [20/130] Loss: 0.1403 LR: 0.000100
Epoch [197] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [197] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [197] Batch [50/130] Loss: 0.0019 LR: 0.000100
Epoch [197] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [197] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [197] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [197] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [197] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [197] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [197] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [198/350] - Time: 8.16s
Train Loss: 0.0202 | Val Loss: 3.7606
Train Acc: 0.9942 | Val Acc: 0.7831
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.981    Count 4: 0.962    Count 5: 0.853  
  Count 6: 0.776    Count 7: 0.677    Count 8: 0.333    Count 9: 0.378    Count 10: 0.580  
Epoch [198] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [198] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [198] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [198] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [198] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [198] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [198] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [198] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [198] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [198] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [198] Batch [100/130] Loss: 0.0001 LR: 0.000100
Epoch [198] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [198] Batch [120/130] Loss: 0.0647 LR: 0.000100

Epoch [199/350] - Time: 8.80s
Train Loss: 0.0230 | Val Loss: 4.0925
Train Acc: 0.9947 | Val Acc: 0.7654
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 1.000    Count 3: 0.991    Count 4: 0.990    Count 5: 0.971  
  Count 6: 0.929    Count 7: 0.604    Count 8: 0.212    Count 9: 0.233    Count 10: 0.386  
Epoch [199] Batch [0/130] Loss: 0.1477 LR: 0.000100
Epoch [199] Batch [10/130] Loss: 0.0019 LR: 0.000100
Epoch [199] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [199] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [199] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [199] Batch [50/130] Loss: 0.0059 LR: 0.000100
Epoch [199] Batch [60/130] Loss: 0.0002 LR: 0.000100
Epoch [199] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [199] Batch [80/130] Loss: 0.0001 LR: 0.000100
Epoch [199] Batch [90/130] Loss: 1.8691 LR: 0.000100
Epoch [199] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [199] Batch [110/130] Loss: 0.0001 LR: 0.000100
Epoch [199] Batch [120/130] Loss: 0.0001 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [200/350] - Time: 8.26s
Train Loss: 0.1441 | Val Loss: 3.2832
Train Acc: 0.9750 | Val Acc: 0.7905
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 0.992    Count 3: 0.991    Count 4: 0.914    Count 5: 0.853  
  Count 6: 0.898    Count 7: 0.646    Count 8: 0.707    Count 9: 0.156    Count 10: 0.443  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/single_image_checkpoint_epoch_199.pth
Epoch [200] Batch [0/130] Loss: 0.0001 LR: 0.000100
Epoch [200] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [200] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [200] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [200] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [200] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [200] Batch [60/130] Loss: 0.0001 LR: 0.000100
Epoch [200] Batch [70/130] Loss: 0.0116 LR: 0.000100
Epoch [200] Batch [80/130] Loss: 0.0130 LR: 0.000100
Epoch [200] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [200] Batch [100/130] Loss: 0.0001 LR: 0.000100
Epoch [200] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [200] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [201/350] - Time: 8.29s
Train Loss: 0.0445 | Val Loss: 3.5959
Train Acc: 0.9942 | Val Acc: 0.7905
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.992    Count 3: 1.000    Count 4: 0.990    Count 5: 0.931  
  Count 6: 0.847    Count 7: 0.500    Count 8: 0.444    Count 9: 0.322    Count 10: 0.580  
Epoch [201] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [201] Batch [10/130] Loss: 0.0112 LR: 0.000100
Epoch [201] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [201] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [201] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [201] Batch [50/130] Loss: 0.0002 LR: 0.000100
Epoch [201] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [201] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [201] Batch [80/130] Loss: 0.0017 LR: 0.000100
Epoch [201] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [201] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [201] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [201] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [202/350] - Time: 8.15s
Train Loss: 0.0155 | Val Loss: 3.4990
Train Acc: 0.9966 | Val Acc: 0.8110
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.981    Count 5: 0.931  
  Count 6: 0.857    Count 7: 0.490    Count 8: 0.626    Count 9: 0.489    Count 10: 0.455  
Epoch [202] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [202] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [202] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [202] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [202] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [202] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [202] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [202] Batch [70/130] Loss: 0.0441 LR: 0.000100
Epoch [202] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [202] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [202] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [202] Batch [110/130] Loss: 0.0027 LR: 0.000100
Epoch [202] Batch [120/130] Loss: 0.0007 LR: 0.000100

Epoch [203/350] - Time: 8.20s
Train Loss: 0.0087 | Val Loss: 4.2545
Train Acc: 0.9971 | Val Acc: 0.7877
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.971    Count 5: 0.902  
  Count 6: 0.694    Count 7: 0.562    Count 8: 0.495    Count 9: 0.400    Count 10: 0.557  
Epoch [203] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [203] Batch [10/130] Loss: 0.0003 LR: 0.000100
Epoch [203] Batch [20/130] Loss: 0.0001 LR: 0.000100
Epoch [203] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [203] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [203] Batch [50/130] Loss: 0.0001 LR: 0.000100
Epoch [203] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [203] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [203] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [203] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [203] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [203] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [203] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [204/350] - Time: 8.25s
Train Loss: 0.0250 | Val Loss: 3.7008
Train Acc: 0.9947 | Val Acc: 0.8054
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.990    Count 5: 0.971  
  Count 6: 0.898    Count 7: 0.510    Count 8: 0.525    Count 9: 0.367    Count 10: 0.500  
Epoch [204] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [204] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [204] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [204] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [204] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [204] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [204] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [204] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [204] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [204] Batch [90/130] Loss: 0.0004 LR: 0.000100
Epoch [204] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [204] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [204] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [205/350] - Time: 8.14s
Train Loss: 0.0071 | Val Loss: 2.9606
Train Acc: 0.9986 | Val Acc: 0.8091
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.981    Count 5: 0.892  
  Count 6: 0.888    Count 7: 0.542    Count 8: 0.596    Count 9: 0.433    Count 10: 0.477  
Epoch [205] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [205] Batch [10/130] Loss: 0.0001 LR: 0.000100
Epoch [205] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [205] Batch [30/130] Loss: 0.0003 LR: 0.000100
Epoch [205] Batch [40/130] Loss: 0.0006 LR: 0.000100
Epoch [205] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [205] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [205] Batch [70/130] Loss: 0.0200 LR: 0.000100
Epoch [205] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [205] Batch [90/130] Loss: 0.0001 LR: 0.000100
Epoch [205] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [205] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [205] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [206/350] - Time: 8.32s
Train Loss: 0.0340 | Val Loss: 3.2646
Train Acc: 0.9952 | Val Acc: 0.8222
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.990    Count 5: 0.931  
  Count 6: 0.898    Count 7: 0.573    Count 8: 0.677    Count 9: 0.356    Count 10: 0.534  
Epoch [206] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [206] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [206] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [206] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [206] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [206] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [206] Batch [60/130] Loss: 0.0461 LR: 0.000100
Epoch [206] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [206] Batch [80/130] Loss: 0.0001 LR: 0.000100
Epoch [206] Batch [90/130] Loss: 0.0005 LR: 0.000100
Epoch [206] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [206] Batch [110/130] Loss: 0.0001 LR: 0.000100
Epoch [206] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [207/350] - Time: 8.18s
Train Loss: 0.0386 | Val Loss: 3.6839
Train Acc: 0.9937 | Val Acc: 0.7607
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.988    Count 2: 1.000    Count 3: 0.991    Count 4: 0.962    Count 5: 0.912  
  Count 6: 0.776    Count 7: 0.510    Count 8: 0.323    Count 9: 0.222    Count 10: 0.602  
Epoch [207] Batch [0/130] Loss: 0.0266 LR: 0.000100
Epoch [207] Batch [10/130] Loss: 0.0043 LR: 0.000100
Epoch [207] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [207] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [207] Batch [40/130] Loss: 0.1901 LR: 0.000100
Epoch [207] Batch [50/130] Loss: 0.3090 LR: 0.000100
Epoch [207] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [207] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [207] Batch [80/130] Loss: 0.0090 LR: 0.000100
Epoch [207] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [207] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [207] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [207] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [208/350] - Time: 8.64s
Train Loss: 0.0219 | Val Loss: 4.3569
Train Acc: 0.9942 | Val Acc: 0.7877
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.971    Count 5: 0.863  
  Count 6: 0.837    Count 7: 0.604    Count 8: 0.323    Count 9: 0.344    Count 10: 0.648  
Epoch [208] Batch [0/130] Loss: 0.0214 LR: 0.000100
Epoch [208] Batch [10/130] Loss: 0.0132 LR: 0.000100
Epoch [208] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [208] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [208] Batch [40/130] Loss: 0.0002 LR: 0.000100
Epoch [208] Batch [50/130] Loss: 0.0006 LR: 0.000100
Epoch [208] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [208] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [208] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [208] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [208] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [208] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [208] Batch [120/130] Loss: 0.0032 LR: 0.000100

Epoch [209/350] - Time: 8.11s
Train Loss: 0.0503 | Val Loss: 5.0274
Train Acc: 0.9933 | Val Acc: 0.7197
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.971    Count 5: 0.922  
  Count 6: 0.663    Count 7: 0.260    Count 8: 0.354    Count 9: 0.189    Count 10: 0.455  
Epoch [209] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [209] Batch [10/130] Loss: 0.0009 LR: 0.000100
Epoch [209] Batch [20/130] Loss: 0.0792 LR: 0.000100
Epoch [209] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [209] Batch [40/130] Loss: 0.0410 LR: 0.000100
Epoch [209] Batch [50/130] Loss: 0.0001 LR: 0.000100
Epoch [209] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [209] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [209] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [209] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [209] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [209] Batch [110/130] Loss: 0.0001 LR: 0.000100
Epoch [209] Batch [120/130] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [210/350] - Time: 8.24s
Train Loss: 0.0644 | Val Loss: 3.3116
Train Acc: 0.9875 | Val Acc: 0.8147
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.962    Count 5: 0.863  
  Count 6: 0.878    Count 7: 0.698    Count 8: 0.495    Count 9: 0.289    Count 10: 0.705  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/single_image_checkpoint_epoch_209.pth
Epoch [210] Batch [0/130] Loss: 0.0013 LR: 0.000100
Epoch [210] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [210] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [210] Batch [30/130] Loss: 0.0003 LR: 0.000100
Epoch [210] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [210] Batch [50/130] Loss: 0.0002 LR: 0.000100
Epoch [210] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [210] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [210] Batch [80/130] Loss: 0.0001 LR: 0.000100
Epoch [210] Batch [90/130] Loss: 0.0001 LR: 0.000100
Epoch [210] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [210] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [210] Batch [120/130] Loss: 0.0001 LR: 0.000100

Epoch [211/350] - Time: 8.27s
Train Loss: 0.0307 | Val Loss: 3.4463
Train Acc: 0.9942 | Val Acc: 0.8045
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.941  
  Count 6: 0.918    Count 7: 0.479    Count 8: 0.636    Count 9: 0.256    Count 10: 0.511  
Epoch [211] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [211] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [211] Batch [20/130] Loss: 0.0001 LR: 0.000100
Epoch [211] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [211] Batch [40/130] Loss: 0.0003 LR: 0.000100
Epoch [211] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [211] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [211] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [211] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [211] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [211] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [211] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [211] Batch [120/130] Loss: 0.5357 LR: 0.000100

Epoch [212/350] - Time: 8.32s
Train Loss: 0.0343 | Val Loss: 3.6832
Train Acc: 0.9942 | Val Acc: 0.8035
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.971    Count 5: 0.941  
  Count 6: 0.847    Count 7: 0.635    Count 8: 0.465    Count 9: 0.233    Count 10: 0.659  
Epoch [212] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [212] Batch [10/130] Loss: 0.0008 LR: 0.000100
Epoch [212] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [212] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [212] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [212] Batch [50/130] Loss: 0.0002 LR: 0.000100
Epoch [212] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [212] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [212] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [212] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [212] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [212] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [212] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [213/350] - Time: 8.28s
Train Loss: 0.0084 | Val Loss: 3.3917
Train Acc: 0.9976 | Val Acc: 0.7970
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.981    Count 4: 0.971    Count 5: 0.931  
  Count 6: 0.908    Count 7: 0.490    Count 8: 0.677    Count 9: 0.322    Count 10: 0.375  
Epoch [213] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [213] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [213] Batch [20/130] Loss: 0.0001 LR: 0.000100
Epoch [213] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [213] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [213] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [213] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [213] Batch [70/130] Loss: 0.0006 LR: 0.000100
Epoch [213] Batch [80/130] Loss: 0.0002 LR: 0.000100
Epoch [213] Batch [90/130] Loss: 0.0539 LR: 0.000100
Epoch [213] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [213] Batch [110/130] Loss: 0.3859 LR: 0.000100
Epoch [213] Batch [120/130] Loss: 0.4179 LR: 0.000100

Epoch [214/350] - Time: 8.23s
Train Loss: 0.0313 | Val Loss: 3.1990
Train Acc: 0.9918 | Val Acc: 0.8073
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.981    Count 5: 0.941  
  Count 6: 0.918    Count 7: 0.531    Count 8: 0.505    Count 9: 0.311    Count 10: 0.614  
Epoch [214] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [214] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [214] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [214] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [214] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [214] Batch [50/130] Loss: 0.0001 LR: 0.000100
Epoch [214] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [214] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [214] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [214] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [214] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [214] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [214] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [215/350] - Time: 8.34s
Train Loss: 0.0055 | Val Loss: 3.5286
Train Acc: 0.9986 | Val Acc: 0.8147
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.971    Count 5: 0.882  
  Count 6: 0.837    Count 7: 0.656    Count 8: 0.566    Count 9: 0.289    Count 10: 0.693  
Epoch [215] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [215] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [215] Batch [20/130] Loss: 0.5279 LR: 0.000100
Epoch [215] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [215] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [215] Batch [50/130] Loss: 0.0050 LR: 0.000100
Epoch [215] Batch [60/130] Loss: 0.0002 LR: 0.000100
Epoch [215] Batch [70/130] Loss: 0.0001 LR: 0.000100
Epoch [215] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [215] Batch [90/130] Loss: 0.0001 LR: 0.000100
Epoch [215] Batch [100/130] Loss: 0.0046 LR: 0.000100
Epoch [215] Batch [110/130] Loss: 0.0032 LR: 0.000100
Epoch [215] Batch [120/130] Loss: 0.0009 LR: 0.000100

Epoch [216/350] - Time: 8.59s
Train Loss: 0.0372 | Val Loss: 3.2769
Train Acc: 0.9904 | Val Acc: 0.8175
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.992    Count 3: 1.000    Count 4: 0.962    Count 5: 0.912  
  Count 6: 0.867    Count 7: 0.406    Count 8: 0.687    Count 9: 0.567    Count 10: 0.523  
Epoch [216] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [216] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [216] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [216] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [216] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [216] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [216] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [216] Batch [70/130] Loss: 0.0017 LR: 0.000100
Epoch [216] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [216] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [216] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [216] Batch [110/130] Loss: 0.0001 LR: 0.000100
Epoch [216] Batch [120/130] Loss: 0.0005 LR: 0.000100

Epoch [217/350] - Time: 8.09s
Train Loss: 0.0345 | Val Loss: 3.5120
Train Acc: 0.9947 | Val Acc: 0.7952
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.981    Count 5: 0.931  
  Count 6: 0.847    Count 7: 0.542    Count 8: 0.525    Count 9: 0.344    Count 10: 0.477  
Epoch [217] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [217] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [217] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [217] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [217] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [217] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [217] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [217] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [217] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [217] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [217] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [217] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [217] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [218/350] - Time: 8.19s
Train Loss: 0.0213 | Val Loss: 4.3425
Train Acc: 0.9957 | Val Acc: 0.7598
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.980  
  Count 6: 0.847    Count 7: 0.292    Count 8: 0.465    Count 9: 0.233    Count 10: 0.420  
Epoch [218] Batch [0/130] Loss: 0.0001 LR: 0.000100
Epoch [218] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [218] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [218] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [218] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [218] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [218] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [218] Batch [70/130] Loss: 0.0001 LR: 0.000100
Epoch [218] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [218] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [218] Batch [100/130] Loss: 0.0005 LR: 0.000100
Epoch [218] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [218] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [219/350] - Time: 8.09s
Train Loss: 0.0129 | Val Loss: 3.9390
Train Acc: 0.9971 | Val Acc: 0.7831
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.981    Count 5: 0.961  
  Count 6: 0.898    Count 7: 0.594    Count 8: 0.374    Count 9: 0.111    Count 10: 0.591  
Epoch [219] Batch [0/130] Loss: 0.4417 LR: 0.000100
Epoch [219] Batch [10/130] Loss: 0.0743 LR: 0.000100
Epoch [219] Batch [20/130] Loss: 0.2225 LR: 0.000100
Epoch [219] Batch [30/130] Loss: 0.0019 LR: 0.000100
Epoch [219] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [219] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [219] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [219] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [219] Batch [80/130] Loss: 0.0001 LR: 0.000100
Epoch [219] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [219] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [219] Batch [110/130] Loss: 0.2906 LR: 0.000100
Epoch [219] Batch [120/130] Loss: 0.0001 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [220/350] - Time: 8.21s
Train Loss: 0.0291 | Val Loss: 3.9014
Train Acc: 0.9923 | Val Acc: 0.7812
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.990    Count 5: 0.961  
  Count 6: 0.918    Count 7: 0.583    Count 8: 0.293    Count 9: 0.222    Count 10: 0.523  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/single_image_checkpoint_epoch_219.pth
Epoch [220] Batch [0/130] Loss: 0.0005 LR: 0.000100
Epoch [220] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [220] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [220] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [220] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [220] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [220] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [220] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [220] Batch [80/130] Loss: 0.0001 LR: 0.000100
Epoch [220] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [220] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [220] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [220] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [221/350] - Time: 8.13s
Train Loss: 0.0056 | Val Loss: 3.8303
Train Acc: 0.9986 | Val Acc: 0.7970
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.981    Count 5: 0.922  
  Count 6: 0.918    Count 7: 0.531    Count 8: 0.455    Count 9: 0.356    Count 10: 0.511  
Epoch [221] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [221] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [221] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [221] Batch [30/130] Loss: 0.0053 LR: 0.000100
Epoch [221] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [221] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [221] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [221] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [221] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [221] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [221] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [221] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [221] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [222/350] - Time: 8.17s
Train Loss: 0.0177 | Val Loss: 2.7954
Train Acc: 0.9961 | Val Acc: 0.8324
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.971    Count 5: 0.951  
  Count 6: 0.867    Count 7: 0.760    Count 8: 0.535    Count 9: 0.400    Count 10: 0.591  
Epoch [222] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [222] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [222] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [222] Batch [30/130] Loss: 0.0001 LR: 0.000100
Epoch [222] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [222] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [222] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [222] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [222] Batch [80/130] Loss: 0.0064 LR: 0.000100
Epoch [222] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [222] Batch [100/130] Loss: 0.0002 LR: 0.000100
Epoch [222] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [222] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [223/350] - Time: 8.26s
Train Loss: 0.0083 | Val Loss: 2.6453
Train Acc: 0.9986 | Val Acc: 0.8399
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.971    Count 5: 0.931  
  Count 6: 0.898    Count 7: 0.750    Count 8: 0.697    Count 9: 0.233    Count 10: 0.670  
Epoch [223] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [223] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [223] Batch [20/130] Loss: 0.2518 LR: 0.000100
Epoch [223] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [223] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [223] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [223] Batch [60/130] Loss: 0.0014 LR: 0.000100
Epoch [223] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [223] Batch [80/130] Loss: 1.0410 LR: 0.000100
Epoch [223] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [223] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [223] Batch [110/130] Loss: 0.0065 LR: 0.000100
Epoch [223] Batch [120/130] Loss: 0.0024 LR: 0.000100

Epoch [224/350] - Time: 8.61s
Train Loss: 0.0268 | Val Loss: 4.0593
Train Acc: 0.9957 | Val Acc: 0.7877
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.971    Count 5: 0.980  
  Count 6: 0.939    Count 7: 0.375    Count 8: 0.545    Count 9: 0.167    Count 10: 0.580  
Epoch [224] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [224] Batch [10/130] Loss: 0.1511 LR: 0.000100
Epoch [224] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [224] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [224] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [224] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [224] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [224] Batch [70/130] Loss: 0.0031 LR: 0.000100
Epoch [224] Batch [80/130] Loss: 0.9840 LR: 0.000100
Epoch [224] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [224] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [224] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [224] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [225/350] - Time: 8.21s
Train Loss: 0.0300 | Val Loss: 3.5599
Train Acc: 0.9947 | Val Acc: 0.8073
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.981    Count 5: 0.961  
  Count 6: 0.898    Count 7: 0.500    Count 8: 0.566    Count 9: 0.333    Count 10: 0.545  
Epoch [225] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [225] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [225] Batch [20/130] Loss: 0.1611 LR: 0.000100
Epoch [225] Batch [30/130] Loss: 0.0339 LR: 0.000100
Epoch [225] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [225] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [225] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [225] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [225] Batch [80/130] Loss: 0.0004 LR: 0.000100
Epoch [225] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [225] Batch [100/130] Loss: 0.0010 LR: 0.000100
Epoch [225] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [225] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [226/350] - Time: 8.18s
Train Loss: 0.0371 | Val Loss: 3.8531
Train Acc: 0.9928 | Val Acc: 0.8017
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.971    Count 5: 0.922  
  Count 6: 0.867    Count 7: 0.635    Count 8: 0.465    Count 9: 0.189    Count 10: 0.682  
Epoch [226] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [226] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [226] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [226] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [226] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [226] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [226] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [226] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [226] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [226] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [226] Batch [100/130] Loss: 0.0002 LR: 0.000100
Epoch [226] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [226] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [227/350] - Time: 8.23s
Train Loss: 0.0087 | Val Loss: 4.1160
Train Acc: 0.9990 | Val Acc: 0.8082
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.971    Count 5: 0.912  
  Count 6: 0.867    Count 7: 0.573    Count 8: 0.515    Count 9: 0.289    Count 10: 0.682  
Epoch [227] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [227] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [227] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [227] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [227] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [227] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [227] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [227] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [227] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [227] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [227] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [227] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [227] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [228/350] - Time: 8.30s
Train Loss: 0.0061 | Val Loss: 3.6140
Train Acc: 0.9976 | Val Acc: 0.8007
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.990    Count 5: 0.912  
  Count 6: 0.827    Count 7: 0.677    Count 8: 0.394    Count 9: 0.233    Count 10: 0.693  
Epoch [228] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [228] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [228] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [228] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [228] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [228] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [228] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [228] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [228] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [228] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [228] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [228] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [228] Batch [120/130] Loss: 0.0003 LR: 0.000100

Epoch [229/350] - Time: 8.29s
Train Loss: 0.0055 | Val Loss: 3.4656
Train Acc: 0.9986 | Val Acc: 0.8063
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.981    Count 5: 0.922  
  Count 6: 0.847    Count 7: 0.531    Count 8: 0.515    Count 9: 0.400    Count 10: 0.591  
Epoch [229] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [229] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [229] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [229] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [229] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [229] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [229] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [229] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [229] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [229] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [229] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [229] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [229] Batch [120/130] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [230/350] - Time: 8.11s
Train Loss: 0.0034 | Val Loss: 3.7779
Train Acc: 0.9986 | Val Acc: 0.7998
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 1.000  
  Count 6: 0.827    Count 7: 0.542    Count 8: 0.333    Count 9: 0.433    Count 10: 0.580  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/single_image_checkpoint_epoch_229.pth
Epoch [230] Batch [0/130] Loss: 0.0002 LR: 0.000100
Epoch [230] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [230] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [230] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [230] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [230] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [230] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [230] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [230] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [230] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [230] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [230] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [230] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [231/350] - Time: 8.34s
Train Loss: 0.0305 | Val Loss: 4.4296
Train Acc: 0.9928 | Val Acc: 0.7337
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.981    Count 4: 0.933    Count 5: 0.941  
  Count 6: 0.796    Count 7: 0.281    Count 8: 0.253    Count 9: 0.267    Count 10: 0.523  
Epoch [231] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [231] Batch [10/130] Loss: 0.3896 LR: 0.000100
Epoch [231] Batch [20/130] Loss: 0.0001 LR: 0.000100
Epoch [231] Batch [30/130] Loss: 0.0004 LR: 0.000100
Epoch [231] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [231] Batch [50/130] Loss: 0.0183 LR: 0.000100
Epoch [231] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [231] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [231] Batch [80/130] Loss: 0.0138 LR: 0.000100
Epoch [231] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [231] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [231] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [231] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [232/350] - Time: 8.26s
Train Loss: 0.0412 | Val Loss: 4.1439
Train Acc: 0.9923 | Val Acc: 0.7905
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.971    Count 5: 0.980  
  Count 6: 0.949    Count 7: 0.604    Count 8: 0.333    Count 9: 0.200    Count 10: 0.568  
Epoch [232] Batch [0/130] Loss: 0.0070 LR: 0.000100
Epoch [232] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [232] Batch [20/130] Loss: 0.0557 LR: 0.000100
Epoch [232] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [232] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [232] Batch [50/130] Loss: 0.0001 LR: 0.000100
Epoch [232] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [232] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [232] Batch [80/130] Loss: 0.0004 LR: 0.000100
Epoch [232] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [232] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [232] Batch [110/130] Loss: 0.0003 LR: 0.000100
Epoch [232] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [233/350] - Time: 9.21s
Train Loss: 0.0358 | Val Loss: 3.6862
Train Acc: 0.9933 | Val Acc: 0.8073
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.962    Count 5: 0.882  
  Count 6: 0.827    Count 7: 0.604    Count 8: 0.505    Count 9: 0.422    Count 10: 0.614  
Epoch [233] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [233] Batch [10/130] Loss: 0.0003 LR: 0.000100
Epoch [233] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [233] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [233] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [233] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [233] Batch [60/130] Loss: 0.0704 LR: 0.000100
Epoch [233] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [233] Batch [80/130] Loss: 0.0001 LR: 0.000100
Epoch [233] Batch [90/130] Loss: 0.0301 LR: 0.000100
Epoch [233] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [233] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [233] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [234/350] - Time: 8.24s
Train Loss: 0.0218 | Val Loss: 3.7720
Train Acc: 0.9957 | Val Acc: 0.7849
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.981    Count 4: 0.867    Count 5: 0.775  
  Count 6: 0.684    Count 7: 0.583    Count 8: 0.525    Count 9: 0.533    Count 10: 0.636  
Epoch [234] Batch [0/130] Loss: 0.1969 LR: 0.000100
Epoch [234] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [234] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [234] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [234] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [234] Batch [50/130] Loss: 0.0072 LR: 0.000100
Epoch [234] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [234] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [234] Batch [80/130] Loss: 0.0014 LR: 0.000100
Epoch [234] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [234] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [234] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [234] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [235/350] - Time: 8.18s
Train Loss: 0.0436 | Val Loss: 3.7364
Train Acc: 0.9894 | Val Acc: 0.7970
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.992    Count 3: 0.991    Count 4: 0.971    Count 5: 0.971  
  Count 6: 0.878    Count 7: 0.573    Count 8: 0.556    Count 9: 0.256    Count 10: 0.477  
Epoch [235] Batch [0/130] Loss: 0.0003 LR: 0.000100
Epoch [235] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [235] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [235] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [235] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [235] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [235] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [235] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [235] Batch [80/130] Loss: 0.0314 LR: 0.000100
Epoch [235] Batch [90/130] Loss: 0.0071 LR: 0.000100
Epoch [235] Batch [100/130] Loss: 0.0127 LR: 0.000100
Epoch [235] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [235] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [236/350] - Time: 8.35s
Train Loss: 0.0116 | Val Loss: 3.8787
Train Acc: 0.9971 | Val Acc: 0.7886
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.943    Count 5: 0.755  
  Count 6: 0.765    Count 7: 0.542    Count 8: 0.677    Count 9: 0.433    Count 10: 0.477  
Epoch [236] Batch [0/130] Loss: 0.0001 LR: 0.000100
Epoch [236] Batch [10/130] Loss: 0.0744 LR: 0.000100
Epoch [236] Batch [20/130] Loss: 0.0842 LR: 0.000100
Epoch [236] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [236] Batch [40/130] Loss: 0.0031 LR: 0.000100
Epoch [236] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [236] Batch [60/130] Loss: 0.0382 LR: 0.000100
Epoch [236] Batch [70/130] Loss: 0.0001 LR: 0.000100
Epoch [236] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [236] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [236] Batch [100/130] Loss: 0.1408 LR: 0.000100
Epoch [236] Batch [110/130] Loss: 0.0003 LR: 0.000100
Epoch [236] Batch [120/130] Loss: 0.0003 LR: 0.000100

Epoch [237/350] - Time: 8.25s
Train Loss: 0.0511 | Val Loss: 4.2819
Train Acc: 0.9913 | Val Acc: 0.7719
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.981    Count 4: 0.857    Count 5: 0.784  
  Count 6: 0.653    Count 7: 0.688    Count 8: 0.424    Count 9: 0.367    Count 10: 0.682  
Epoch [237] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [237] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [237] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [237] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [237] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [237] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [237] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [237] Batch [70/130] Loss: 0.0013 LR: 0.000100
Epoch [237] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [237] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [237] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [237] Batch [110/130] Loss: 0.0011 LR: 0.000100
Epoch [237] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [238/350] - Time: 8.35s
Train Loss: 0.0456 | Val Loss: 3.4950
Train Acc: 0.9923 | Val Acc: 0.7961
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 1.000    Count 3: 1.000    Count 4: 0.971    Count 5: 0.882  
  Count 6: 0.857    Count 7: 0.531    Count 8: 0.586    Count 9: 0.400    Count 10: 0.443  
Epoch [238] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [238] Batch [10/130] Loss: 0.0004 LR: 0.000100
Epoch [238] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [238] Batch [30/130] Loss: 0.0007 LR: 0.000100
Epoch [238] Batch [40/130] Loss: 1.0087 LR: 0.000100
Epoch [238] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [238] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [238] Batch [70/130] Loss: 0.2458 LR: 0.000100
Epoch [238] Batch [80/130] Loss: 0.0008 LR: 0.000100
Epoch [238] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [238] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [238] Batch [110/130] Loss: 0.0401 LR: 0.000100
Epoch [238] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [239/350] - Time: 8.46s
Train Loss: 0.0523 | Val Loss: 3.6527
Train Acc: 0.9918 | Val Acc: 0.8101
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.992    Count 3: 1.000    Count 4: 0.971    Count 5: 0.912  
  Count 6: 0.867    Count 7: 0.594    Count 8: 0.525    Count 9: 0.378    Count 10: 0.591  
Epoch [239] Batch [0/130] Loss: 0.0411 LR: 0.000100
Epoch [239] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [239] Batch [20/130] Loss: 0.0265 LR: 0.000100
Epoch [239] Batch [30/130] Loss: 0.0021 LR: 0.000100
Epoch [239] Batch [40/130] Loss: 0.0090 LR: 0.000100
Epoch [239] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [239] Batch [60/130] Loss: 0.0001 LR: 0.000100
Epoch [239] Batch [70/130] Loss: 0.0001 LR: 0.000100
Epoch [239] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [239] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [239] Batch [100/130] Loss: 0.0032 LR: 0.000100
Epoch [239] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [239] Batch [120/130] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [240/350] - Time: 8.38s
Train Loss: 0.0357 | Val Loss: 3.7552
Train Acc: 0.9923 | Val Acc: 0.7989
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.976    Count 3: 1.000    Count 4: 0.952    Count 5: 0.882  
  Count 6: 0.745    Count 7: 0.573    Count 8: 0.556    Count 9: 0.400    Count 10: 0.636  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/single_image_checkpoint_epoch_239.pth
Epoch [240] Batch [0/130] Loss: 0.0001 LR: 0.000100
Epoch [240] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [240] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [240] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [240] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [240] Batch [50/130] Loss: 0.0008 LR: 0.000100
Epoch [240] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [240] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [240] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [240] Batch [90/130] Loss: 0.0002 LR: 0.000100
Epoch [240] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [240] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [240] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [241/350] - Time: 8.43s
Train Loss: 0.0078 | Val Loss: 3.8593
Train Acc: 0.9976 | Val Acc: 0.7980
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.990    Count 5: 0.941  
  Count 6: 0.837    Count 7: 0.698    Count 8: 0.313    Count 9: 0.267    Count 10: 0.648  
Epoch [241] Batch [0/130] Loss: 0.0005 LR: 0.000100
Epoch [241] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [241] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [241] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [241] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [241] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [241] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [241] Batch [70/130] Loss: 0.2916 LR: 0.000100
Epoch [241] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [241] Batch [90/130] Loss: 0.0518 LR: 0.000100
Epoch [241] Batch [100/130] Loss: 0.0002 LR: 0.000100
Epoch [241] Batch [110/130] Loss: 0.0987 LR: 0.000100
Epoch [241] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [242/350] - Time: 8.25s
Train Loss: 0.0307 | Val Loss: 4.7829
Train Acc: 0.9889 | Val Acc: 0.7281
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.988    Count 2: 1.000    Count 3: 0.991    Count 4: 0.971    Count 5: 0.971  
  Count 6: 0.918    Count 7: 0.365    Count 8: 0.162    Count 9: 0.167    Count 10: 0.364  
Epoch [242] Batch [0/130] Loss: 0.0034 LR: 0.000100
Epoch [242] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [242] Batch [20/130] Loss: 0.0001 LR: 0.000100
Epoch [242] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [242] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [242] Batch [50/130] Loss: 0.1374 LR: 0.000100
Epoch [242] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [242] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [242] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [242] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [242] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [242] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [242] Batch [120/130] Loss: 0.8456 LR: 0.000100

Epoch [243/350] - Time: 8.38s
Train Loss: 0.0291 | Val Loss: 3.5023
Train Acc: 0.9947 | Val Acc: 0.7970
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.943    Count 5: 0.794  
  Count 6: 0.867    Count 7: 0.615    Count 8: 0.475    Count 9: 0.367    Count 10: 0.636  
Epoch [243] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [243] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [243] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [243] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [243] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [243] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [243] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [243] Batch [70/130] Loss: 0.0092 LR: 0.000100
Epoch [243] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [243] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [243] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [243] Batch [110/130] Loss: 0.0005 LR: 0.000100
Epoch [243] Batch [120/130] Loss: 0.5729 LR: 0.000100

Epoch [244/350] - Time: 8.21s
Train Loss: 0.0118 | Val Loss: 3.5254
Train Acc: 0.9976 | Val Acc: 0.7644
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.981    Count 4: 0.924    Count 5: 0.882  
  Count 6: 0.827    Count 7: 0.302    Count 8: 0.465    Count 9: 0.422    Count 10: 0.523  
Epoch [244] Batch [0/130] Loss: 0.1848 LR: 0.000100
Epoch [244] Batch [10/130] Loss: 0.0005 LR: 0.000100
Epoch [244] Batch [20/130] Loss: 0.0003 LR: 0.000100
Epoch [244] Batch [30/130] Loss: 0.0002 LR: 0.000100
Epoch [244] Batch [40/130] Loss: 0.0793 LR: 0.000100
Epoch [244] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [244] Batch [60/130] Loss: 0.0066 LR: 0.000100
Epoch [244] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [244] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [244] Batch [90/130] Loss: 0.0670 LR: 0.000100
Epoch [244] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [244] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [244] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [245/350] - Time: 8.17s
Train Loss: 0.0342 | Val Loss: 3.3919
Train Acc: 0.9937 | Val Acc: 0.7952
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 0.902  
  Count 6: 0.847    Count 7: 0.469    Count 8: 0.495    Count 9: 0.400    Count 10: 0.557  
Epoch [245] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [245] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [245] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [245] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [245] Batch [40/130] Loss: 0.0001 LR: 0.000100
Epoch [245] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [245] Batch [60/130] Loss: 0.0105 LR: 0.000100
Epoch [245] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [245] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [245] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [245] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [245] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [245] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [246/350] - Time: 8.19s
Train Loss: 0.0186 | Val Loss: 3.4301
Train Acc: 0.9986 | Val Acc: 0.8063
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.990    Count 5: 0.951  
  Count 6: 0.847    Count 7: 0.542    Count 8: 0.424    Count 9: 0.411    Count 10: 0.625  
Epoch [246] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [246] Batch [10/130] Loss: 0.0001 LR: 0.000100
Epoch [246] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [246] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [246] Batch [40/130] Loss: 0.1675 LR: 0.000100
Epoch [246] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [246] Batch [60/130] Loss: 0.0002 LR: 0.000100
Epoch [246] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [246] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [246] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [246] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [246] Batch [110/130] Loss: 0.0001 LR: 0.000100
Epoch [246] Batch [120/130] Loss: 0.0517 LR: 0.000100

Epoch [247/350] - Time: 8.45s
Train Loss: 0.0109 | Val Loss: 3.2212
Train Acc: 0.9971 | Val Acc: 0.8073
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.902  
  Count 6: 0.867    Count 7: 0.594    Count 8: 0.545    Count 9: 0.211    Count 10: 0.670  
Epoch [247] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [247] Batch [10/130] Loss: 0.0005 LR: 0.000100
Epoch [247] Batch [20/130] Loss: 0.0006 LR: 0.000100
Epoch [247] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [247] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [247] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [247] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [247] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [247] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [247] Batch [90/130] Loss: 0.0001 LR: 0.000100
Epoch [247] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [247] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [247] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [248/350] - Time: 8.20s
Train Loss: 0.0055 | Val Loss: 3.0340
Train Acc: 0.9986 | Val Acc: 0.8156
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.981    Count 5: 0.902  
  Count 6: 0.837    Count 7: 0.688    Count 8: 0.465    Count 9: 0.356    Count 10: 0.670  
Epoch [248] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [248] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [248] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [248] Batch [30/130] Loss: 0.0001 LR: 0.000100
Epoch [248] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [248] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [248] Batch [60/130] Loss: 0.0001 LR: 0.000100
Epoch [248] Batch [70/130] Loss: 0.0002 LR: 0.000100
Epoch [248] Batch [80/130] Loss: 0.0001 LR: 0.000100
Epoch [248] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [248] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [248] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [248] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [249/350] - Time: 8.50s
Train Loss: 0.0057 | Val Loss: 3.8291
Train Acc: 0.9976 | Val Acc: 0.7784
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.981    Count 4: 0.943    Count 5: 0.794  
  Count 6: 0.796    Count 7: 0.656    Count 8: 0.455    Count 9: 0.233    Count 10: 0.625  
Epoch [249] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [249] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [249] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [249] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [249] Batch [40/130] Loss: 0.0001 LR: 0.000100
Epoch [249] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [249] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [249] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [249] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [249] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [249] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [249] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [249] Batch [120/130] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [250/350] - Time: 8.21s
Train Loss: 0.0116 | Val Loss: 3.7924
Train Acc: 0.9981 | Val Acc: 0.7877
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.992    Count 3: 0.991    Count 4: 0.981    Count 5: 0.971  
  Count 6: 0.898    Count 7: 0.521    Count 8: 0.465    Count 9: 0.244    Count 10: 0.500  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/single_image_checkpoint_epoch_249.pth
Epoch [250] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [250] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [250] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [250] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [250] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [250] Batch [50/130] Loss: 0.0001 LR: 0.000100
Epoch [250] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [250] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [250] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [250] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [250] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [250] Batch [110/130] Loss: 0.0003 LR: 0.000100
Epoch [250] Batch [120/130] Loss: 0.0001 LR: 0.000100

Epoch [251/350] - Time: 8.17s
Train Loss: 0.0135 | Val Loss: 3.3480
Train Acc: 0.9957 | Val Acc: 0.7989
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.902  
  Count 6: 0.857    Count 7: 0.438    Count 8: 0.566    Count 9: 0.500    Count 10: 0.432  
Epoch [251] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [251] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [251] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [251] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [251] Batch [40/130] Loss: 0.0382 LR: 0.000100
Epoch [251] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [251] Batch [60/130] Loss: 0.0001 LR: 0.000100
Epoch [251] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [251] Batch [80/130] Loss: 0.0004 LR: 0.000100
Epoch [251] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [251] Batch [100/130] Loss: 0.0011 LR: 0.000100
Epoch [251] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [251] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [252/350] - Time: 8.28s
Train Loss: 0.0227 | Val Loss: 3.1996
Train Acc: 0.9966 | Val Acc: 0.8082
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.931  
  Count 6: 0.867    Count 7: 0.573    Count 8: 0.455    Count 9: 0.433    Count 10: 0.557  
Epoch [252] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [252] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [252] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [252] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [252] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [252] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [252] Batch [60/130] Loss: 0.0002 LR: 0.000100
Epoch [252] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [252] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [252] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [252] Batch [100/130] Loss: 0.0001 LR: 0.000100
Epoch [252] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [252] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [253/350] - Time: 8.12s
Train Loss: 0.0039 | Val Loss: 3.2225
Train Acc: 0.9995 | Val Acc: 0.8091
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.912  
  Count 6: 0.867    Count 7: 0.594    Count 8: 0.434    Count 9: 0.378    Count 10: 0.636  
Epoch [253] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [253] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [253] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [253] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [253] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [253] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [253] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [253] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [253] Batch [80/130] Loss: 0.0064 LR: 0.000100
Epoch [253] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [253] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [253] Batch [110/130] Loss: 0.0002 LR: 0.000100
Epoch [253] Batch [120/130] Loss: 0.0023 LR: 0.000100

Epoch [254/350] - Time: 8.24s
Train Loss: 0.0338 | Val Loss: 3.9507
Train Acc: 0.9937 | Val Acc: 0.7728
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 0.882  
  Count 6: 0.867    Count 7: 0.552    Count 8: 0.172    Count 9: 0.244    Count 10: 0.716  
Epoch [254] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [254] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [254] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [254] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [254] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [254] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [254] Batch [60/130] Loss: 0.0005 LR: 0.000100
Epoch [254] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [254] Batch [80/130] Loss: 0.0011 LR: 0.000100
Epoch [254] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [254] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [254] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [254] Batch [120/130] Loss: 0.0046 LR: 0.000100

Epoch [255/350] - Time: 8.37s
Train Loss: 0.0117 | Val Loss: 2.9927
Train Acc: 0.9966 | Val Acc: 0.8175
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.990    Count 5: 0.931  
  Count 6: 0.878    Count 7: 0.406    Count 8: 0.778    Count 9: 0.522    Count 10: 0.386  
Epoch [255] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [255] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [255] Batch [20/130] Loss: 0.7073 LR: 0.000100
Epoch [255] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [255] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [255] Batch [50/130] Loss: 0.0014 LR: 0.000100
Epoch [255] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [255] Batch [70/130] Loss: 0.0017 LR: 0.000100
Epoch [255] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [255] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [255] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [255] Batch [110/130] Loss: 0.6850 LR: 0.000100
Epoch [255] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [256/350] - Time: 8.35s
Train Loss: 0.0380 | Val Loss: 2.8670
Train Acc: 0.9942 | Val Acc: 0.8250
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.971    Count 5: 0.892  
  Count 6: 0.847    Count 7: 0.635    Count 8: 0.586    Count 9: 0.411    Count 10: 0.670  
Epoch [256] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [256] Batch [10/130] Loss: 0.0098 LR: 0.000100
Epoch [256] Batch [20/130] Loss: 0.0001 LR: 0.000100
Epoch [256] Batch [30/130] Loss: 0.7927 LR: 0.000100
Epoch [256] Batch [40/130] Loss: 0.0058 LR: 0.000100
Epoch [256] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [256] Batch [60/130] Loss: 0.0001 LR: 0.000100
Epoch [256] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [256] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [256] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [256] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [256] Batch [110/130] Loss: 0.0001 LR: 0.000100
Epoch [256] Batch [120/130] Loss: 0.0002 LR: 0.000100

Epoch [257/350] - Time: 8.60s
Train Loss: 0.0720 | Val Loss: 3.3788
Train Acc: 0.9855 | Val Acc: 0.8045
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.992    Count 3: 1.000    Count 4: 0.981    Count 5: 0.882  
  Count 6: 0.857    Count 7: 0.604    Count 8: 0.455    Count 9: 0.300    Count 10: 0.705  
Epoch [257] Batch [0/130] Loss: 0.0014 LR: 0.000100
Epoch [257] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [257] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [257] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [257] Batch [40/130] Loss: 0.0001 LR: 0.000100
Epoch [257] Batch [50/130] Loss: 0.1503 LR: 0.000100
Epoch [257] Batch [60/130] Loss: 0.0002 LR: 0.000100
Epoch [257] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [257] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [257] Batch [90/130] Loss: 0.0005 LR: 0.000100
Epoch [257] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [257] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [257] Batch [120/130] Loss: 0.0021 LR: 0.000100

Epoch [258/350] - Time: 8.17s
Train Loss: 0.0200 | Val Loss: 4.1335
Train Acc: 0.9947 | Val Acc: 0.7654
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.992    Count 3: 0.991    Count 4: 0.981    Count 5: 0.961  
  Count 6: 0.918    Count 7: 0.562    Count 8: 0.172    Count 9: 0.111    Count 10: 0.636  
Epoch [258] Batch [0/130] Loss: 0.0720 LR: 0.000100
Epoch [258] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [258] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [258] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [258] Batch [40/130] Loss: 0.0001 LR: 0.000100
Epoch [258] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [258] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [258] Batch [70/130] Loss: 0.0001 LR: 0.000100
Epoch [258] Batch [80/130] Loss: 0.1580 LR: 0.000100
Epoch [258] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [258] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [258] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [258] Batch [120/130] Loss: 0.0001 LR: 0.000100

Epoch [259/350] - Time: 8.29s
Train Loss: 0.0402 | Val Loss: 3.1919
Train Acc: 0.9918 | Val Acc: 0.8045
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.971    Count 5: 0.843  
  Count 6: 0.837    Count 7: 0.667    Count 8: 0.394    Count 9: 0.311    Count 10: 0.761  
Epoch [259] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [259] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [259] Batch [20/130] Loss: 0.5147 LR: 0.000100
Epoch [259] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [259] Batch [40/130] Loss: 0.0008 LR: 0.000100
Epoch [259] Batch [50/130] Loss: 0.0004 LR: 0.000100
Epoch [259] Batch [60/130] Loss: 0.0703 LR: 0.000100
Epoch [259] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [259] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [259] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [259] Batch [100/130] Loss: 0.0058 LR: 0.000100
Epoch [259] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [259] Batch [120/130] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [260/350] - Time: 8.21s
Train Loss: 0.0244 | Val Loss: 3.7730
Train Acc: 0.9933 | Val Acc: 0.7793
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.992    Count 3: 1.000    Count 4: 0.962    Count 5: 0.892  
  Count 6: 0.847    Count 7: 0.573    Count 8: 0.434    Count 9: 0.256    Count 10: 0.523  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/single_image_checkpoint_epoch_259.pth
Epoch [260] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [260] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [260] Batch [20/130] Loss: 0.0001 LR: 0.000100
Epoch [260] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [260] Batch [40/130] Loss: 0.0282 LR: 0.000100
Epoch [260] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [260] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [260] Batch [70/130] Loss: 0.0298 LR: 0.000100
Epoch [260] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [260] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [260] Batch [100/130] Loss: 0.0024 LR: 0.000100
Epoch [260] Batch [110/130] Loss: 0.0004 LR: 0.000100
Epoch [260] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [261/350] - Time: 8.05s
Train Loss: 0.0294 | Val Loss: 3.0537
Train Acc: 0.9957 | Val Acc: 0.8045
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 1.000    Count 3: 0.991    Count 4: 0.952    Count 5: 0.882  
  Count 6: 0.827    Count 7: 0.594    Count 8: 0.636    Count 9: 0.189    Count 10: 0.705  
Epoch [261] Batch [0/130] Loss: 0.0010 LR: 0.000100
Epoch [261] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [261] Batch [20/130] Loss: 0.0001 LR: 0.000100
Epoch [261] Batch [30/130] Loss: 0.0002 LR: 0.000100
Epoch [261] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [261] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [261] Batch [60/130] Loss: 0.0002 LR: 0.000100
Epoch [261] Batch [70/130] Loss: 0.2588 LR: 0.000100
Epoch [261] Batch [80/130] Loss: 0.0001 LR: 0.000100
Epoch [261] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [261] Batch [100/130] Loss: 0.0001 LR: 0.000100
Epoch [261] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [261] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [262/350] - Time: 8.18s
Train Loss: 0.0193 | Val Loss: 3.4686
Train Acc: 0.9957 | Val Acc: 0.8082
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.962    Count 5: 0.873  
  Count 6: 0.837    Count 7: 0.552    Count 8: 0.596    Count 9: 0.456    Count 10: 0.534  
Epoch [262] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [262] Batch [10/130] Loss: 0.0008 LR: 0.000100
Epoch [262] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [262] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [262] Batch [40/130] Loss: 0.0001 LR: 0.000100
Epoch [262] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [262] Batch [60/130] Loss: 0.0010 LR: 0.000100
Epoch [262] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [262] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [262] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [262] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [262] Batch [110/130] Loss: 0.0001 LR: 0.000100
Epoch [262] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [263/350] - Time: 8.11s
Train Loss: 0.0012 | Val Loss: 3.4944
Train Acc: 0.9995 | Val Acc: 0.8091
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 0.902  
  Count 6: 0.878    Count 7: 0.594    Count 8: 0.434    Count 9: 0.422    Count 10: 0.602  
Epoch [263] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [263] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [263] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [263] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [263] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [263] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [263] Batch [60/130] Loss: 0.0580 LR: 0.000100
Epoch [263] Batch [70/130] Loss: 0.0010 LR: 0.000100
Epoch [263] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [263] Batch [90/130] Loss: 0.0023 LR: 0.000100
Epoch [263] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [263] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [263] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [264/350] - Time: 8.23s
Train Loss: 0.0023 | Val Loss: 3.6112
Train Acc: 0.9986 | Val Acc: 0.7868
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.952    Count 5: 0.902  
  Count 6: 0.786    Count 7: 0.656    Count 8: 0.414    Count 9: 0.267    Count 10: 0.602  
Epoch [264] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [264] Batch [10/130] Loss: 0.0001 LR: 0.000100
Epoch [264] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [264] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [264] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [264] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [264] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [264] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [264] Batch [80/130] Loss: 0.0380 LR: 0.000100
Epoch [264] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [264] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [264] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [264] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [265/350] - Time: 8.22s
Train Loss: 0.0167 | Val Loss: 3.1790
Train Acc: 0.9971 | Val Acc: 0.8240
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.990    Count 5: 0.892  
  Count 6: 0.888    Count 7: 0.771    Count 8: 0.465    Count 9: 0.311    Count 10: 0.670  
Epoch [265] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [265] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [265] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [265] Batch [30/130] Loss: 0.0001 LR: 0.000100
Epoch [265] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [265] Batch [50/130] Loss: 0.0035 LR: 0.000100
Epoch [265] Batch [60/130] Loss: 0.0009 LR: 0.000100
Epoch [265] Batch [70/130] Loss: 0.0021 LR: 0.000100
Epoch [265] Batch [80/130] Loss: 0.3433 LR: 0.000100
Epoch [265] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [265] Batch [100/130] Loss: 0.1038 LR: 0.000100
Epoch [265] Batch [110/130] Loss: 0.0002 LR: 0.000100
Epoch [265] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [266/350] - Time: 8.57s
Train Loss: 0.0414 | Val Loss: 3.6324
Train Acc: 0.9899 | Val Acc: 0.7812
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.952    Count 5: 0.863  
  Count 6: 0.837    Count 7: 0.500    Count 8: 0.525    Count 9: 0.411    Count 10: 0.409  
Epoch [266] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [266] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [266] Batch [20/130] Loss: 0.0002 LR: 0.000100
Epoch [266] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [266] Batch [40/130] Loss: 0.0099 LR: 0.000100
Epoch [266] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [266] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [266] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [266] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [266] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [266] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [266] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [266] Batch [120/130] Loss: 0.0021 LR: 0.000100

Epoch [267/350] - Time: 8.24s
Train Loss: 0.0045 | Val Loss: 3.3705
Train Acc: 0.9986 | Val Acc: 0.8073
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 1.000    Count 3: 1.000    Count 4: 0.971    Count 5: 0.892  
  Count 6: 0.816    Count 7: 0.646    Count 8: 0.485    Count 9: 0.456    Count 10: 0.545  
Epoch [267] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [267] Batch [10/130] Loss: 0.0001 LR: 0.000100
Epoch [267] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [267] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [267] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [267] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [267] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [267] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [267] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [267] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [267] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [267] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [267] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [268/350] - Time: 8.25s
Train Loss: 0.0000 | Val Loss: 3.3878
Train Acc: 1.0000 | Val Acc: 0.8147
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.912  
  Count 6: 0.827    Count 7: 0.708    Count 8: 0.384    Count 9: 0.400    Count 10: 0.659  
Epoch [268] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [268] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [268] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [268] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [268] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [268] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [268] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [268] Batch [70/130] Loss: 0.1799 LR: 0.000100
Epoch [268] Batch [80/130] Loss: 0.0007 LR: 0.000100
Epoch [268] Batch [90/130] Loss: 0.0787 LR: 0.000100
Epoch [268] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [268] Batch [110/130] Loss: 0.0018 LR: 0.000100
Epoch [268] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [269/350] - Time: 8.26s
Train Loss: 0.0312 | Val Loss: 3.9576
Train Acc: 0.9933 | Val Acc: 0.7821
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.962    Count 5: 0.804  
  Count 6: 0.694    Count 7: 0.625    Count 8: 0.354    Count 9: 0.422    Count 10: 0.693  
Epoch [269] Batch [0/130] Loss: 0.0001 LR: 0.000100
Epoch [269] Batch [10/130] Loss: 0.0001 LR: 0.000100
Epoch [269] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [269] Batch [30/130] Loss: 0.0125 LR: 0.000100
Epoch [269] Batch [40/130] Loss: 0.4143 LR: 0.000100
Epoch [269] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [269] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [269] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [269] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [269] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [269] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [269] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [269] Batch [120/130] Loss: 0.0097 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [270/350] - Time: 8.42s
Train Loss: 0.0042 | Val Loss: 4.0967
Train Acc: 0.9990 | Val Acc: 0.8035
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.804  
  Count 6: 0.816    Count 7: 0.708    Count 8: 0.414    Count 9: 0.456    Count 10: 0.568  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/single_image_checkpoint_epoch_269.pth
Epoch [270] Batch [0/130] Loss: 0.0804 LR: 0.000100
Epoch [270] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [270] Batch [20/130] Loss: 0.0003 LR: 0.000100
Epoch [270] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [270] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [270] Batch [50/130] Loss: 0.0001 LR: 0.000100
Epoch [270] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [270] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [270] Batch [80/130] Loss: 0.1069 LR: 0.000100
Epoch [270] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [270] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [270] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [270] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [271/350] - Time: 8.51s
Train Loss: 0.0053 | Val Loss: 3.4096
Train Acc: 0.9976 | Val Acc: 0.8082
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.922  
  Count 6: 0.949    Count 7: 0.510    Count 8: 0.495    Count 9: 0.289    Count 10: 0.648  
Epoch [271] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [271] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [271] Batch [20/130] Loss: 0.0002 LR: 0.000100
Epoch [271] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [271] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [271] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [271] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [271] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [271] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [271] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [271] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [271] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [271] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [272/350] - Time: 8.17s
Train Loss: 0.0007 | Val Loss: 3.3960
Train Acc: 0.9995 | Val Acc: 0.8147
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.922  
  Count 6: 0.867    Count 7: 0.615    Count 8: 0.444    Count 9: 0.333    Count 10: 0.705  
Epoch [272] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [272] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [272] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [272] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [272] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [272] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [272] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [272] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [272] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [272] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [272] Batch [100/130] Loss: 0.0001 LR: 0.000100
Epoch [272] Batch [110/130] Loss: 0.5971 LR: 0.000100
Epoch [272] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [273/350] - Time: 8.29s
Train Loss: 0.0140 | Val Loss: 4.2909
Train Acc: 0.9961 | Val Acc: 0.7607
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.962    Count 5: 0.951  
  Count 6: 0.929    Count 7: 0.344    Count 8: 0.404    Count 9: 0.211    Count 10: 0.455  
Epoch [273] Batch [0/130] Loss: 0.0001 LR: 0.000100
Epoch [273] Batch [10/130] Loss: 0.0001 LR: 0.000100
Epoch [273] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [273] Batch [30/130] Loss: 0.0001 LR: 0.000100
Epoch [273] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [273] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [273] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [273] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [273] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [273] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [273] Batch [100/130] Loss: 0.0047 LR: 0.000100
Epoch [273] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [273] Batch [120/130] Loss: 0.0004 LR: 0.000100

Epoch [274/350] - Time: 8.46s
Train Loss: 0.0249 | Val Loss: 3.3333
Train Acc: 0.9937 | Val Acc: 0.8035
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.962    Count 5: 0.922  
  Count 6: 0.929    Count 7: 0.719    Count 8: 0.354    Count 9: 0.311    Count 10: 0.568  
Epoch [274] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [274] Batch [10/130] Loss: 0.0213 LR: 0.000100
Epoch [274] Batch [20/130] Loss: 0.0007 LR: 0.000100
Epoch [274] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [274] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [274] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [274] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [274] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [274] Batch [80/130] Loss: 0.4275 LR: 0.000100
Epoch [274] Batch [90/130] Loss: 0.0755 LR: 0.000100
Epoch [274] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [274] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [274] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [275/350] - Time: 8.34s
Train Loss: 0.0181 | Val Loss: 4.3635
Train Acc: 0.9961 | Val Acc: 0.7672
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.962    Count 5: 0.892  
  Count 6: 0.867    Count 7: 0.562    Count 8: 0.202    Count 9: 0.244    Count 10: 0.636  
Epoch [275] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [275] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [275] Batch [20/130] Loss: 0.0001 LR: 0.000100
Epoch [275] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [275] Batch [40/130] Loss: 0.8315 LR: 0.000100
Epoch [275] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [275] Batch [60/130] Loss: 0.0059 LR: 0.000100
Epoch [275] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [275] Batch [80/130] Loss: 0.0005 LR: 0.000100
Epoch [275] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [275] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [275] Batch [110/130] Loss: 0.0300 LR: 0.000100
Epoch [275] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [276/350] - Time: 8.15s
Train Loss: 0.0445 | Val Loss: 4.4149
Train Acc: 0.9928 | Val Acc: 0.7728
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.962    Count 5: 0.853  
  Count 6: 0.837    Count 7: 0.656    Count 8: 0.081    Count 9: 0.267    Count 10: 0.784  
Epoch [276] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [276] Batch [10/130] Loss: 0.0002 LR: 0.000100
Epoch [276] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [276] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [276] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [276] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [276] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [276] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [276] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [276] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [276] Batch [100/130] Loss: 0.0003 LR: 0.000100
Epoch [276] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [276] Batch [120/130] Loss: 0.0001 LR: 0.000100

Epoch [277/350] - Time: 8.16s
Train Loss: 0.0027 | Val Loss: 3.3919
Train Acc: 0.9986 | Val Acc: 0.8119
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.873  
  Count 6: 0.816    Count 7: 0.604    Count 8: 0.424    Count 9: 0.433    Count 10: 0.716  
Epoch [277] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [277] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [277] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [277] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [277] Batch [40/130] Loss: 0.0020 LR: 0.000100
Epoch [277] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [277] Batch [60/130] Loss: 0.0290 LR: 0.000100
Epoch [277] Batch [70/130] Loss: 0.0002 LR: 0.000100
Epoch [277] Batch [80/130] Loss: 0.2721 LR: 0.000100
Epoch [277] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [277] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [277] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [277] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [278/350] - Time: 8.34s
Train Loss: 0.0073 | Val Loss: 4.1319
Train Acc: 0.9976 | Val Acc: 0.7812
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 1.000    Count 3: 1.000    Count 4: 0.952    Count 5: 0.843  
  Count 6: 0.765    Count 7: 0.552    Count 8: 0.414    Count 9: 0.433    Count 10: 0.568  
Epoch [278] Batch [0/130] Loss: 0.0501 LR: 0.000100
Epoch [278] Batch [10/130] Loss: 0.0003 LR: 0.000100
Epoch [278] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [278] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [278] Batch [40/130] Loss: 0.0008 LR: 0.000100
Epoch [278] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [278] Batch [60/130] Loss: 0.0001 LR: 0.000100
Epoch [278] Batch [70/130] Loss: 0.0205 LR: 0.000100
Epoch [278] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [278] Batch [90/130] Loss: 0.7587 LR: 0.000100
Epoch [278] Batch [100/130] Loss: 0.0003 LR: 0.000100
Epoch [278] Batch [110/130] Loss: 0.1856 LR: 0.000100
Epoch [278] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [279/350] - Time: 8.35s
Train Loss: 0.0186 | Val Loss: 3.8469
Train Acc: 0.9961 | Val Acc: 0.7914
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.941  
  Count 6: 0.898    Count 7: 0.531    Count 8: 0.364    Count 9: 0.411    Count 10: 0.477  
Epoch [279] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [279] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [279] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [279] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [279] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [279] Batch [50/130] Loss: 0.0004 LR: 0.000100
Epoch [279] Batch [60/130] Loss: 0.0002 LR: 0.000100
Epoch [279] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [279] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [279] Batch [90/130] Loss: 0.0001 LR: 0.000100
Epoch [279] Batch [100/130] Loss: 0.1059 LR: 0.000100
Epoch [279] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [279] Batch [120/130] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [280/350] - Time: 8.18s
Train Loss: 0.0111 | Val Loss: 4.1839
Train Acc: 0.9976 | Val Acc: 0.7858
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.990    Count 5: 0.941  
  Count 6: 0.898    Count 7: 0.292    Count 8: 0.606    Count 9: 0.433    Count 10: 0.375  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/single_image_checkpoint_epoch_279.pth
Epoch [280] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [280] Batch [10/130] Loss: 0.0001 LR: 0.000100
Epoch [280] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [280] Batch [30/130] Loss: 0.2688 LR: 0.000100
Epoch [280] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [280] Batch [50/130] Loss: 0.0001 LR: 0.000100
Epoch [280] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [280] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [280] Batch [80/130] Loss: 0.0001 LR: 0.000100
Epoch [280] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [280] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [280] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [280] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [281/350] - Time: 8.30s
Train Loss: 0.0317 | Val Loss: 3.5836
Train Acc: 0.9933 | Val Acc: 0.7924
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.981    Count 5: 0.971  
  Count 6: 0.949    Count 7: 0.479    Count 8: 0.404    Count 9: 0.211    Count 10: 0.625  
Epoch [281] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [281] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [281] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [281] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [281] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [281] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [281] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [281] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [281] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [281] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [281] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [281] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [281] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [282/350] - Time: 8.62s
Train Loss: 0.0000 | Val Loss: 3.8410
Train Acc: 1.0000 | Val Acc: 0.7942
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.951  
  Count 6: 0.959    Count 7: 0.396    Count 8: 0.616    Count 9: 0.356    Count 10: 0.341  
Epoch [282] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [282] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [282] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [282] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [282] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [282] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [282] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [282] Batch [70/130] Loss: 0.0001 LR: 0.000100
Epoch [282] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [282] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [282] Batch [100/130] Loss: 0.0004 LR: 0.000100
Epoch [282] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [282] Batch [120/130] Loss: 0.0148 LR: 0.000100

Epoch [283/350] - Time: 8.32s
Train Loss: 0.0161 | Val Loss: 3.5380
Train Acc: 0.9976 | Val Acc: 0.8203
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 0.994    Count 2: 1.000    Count 3: 1.000    Count 4: 0.981    Count 5: 0.912  
  Count 6: 0.918    Count 7: 0.531    Count 8: 0.535    Count 9: 0.378    Count 10: 0.705  
Epoch [283] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [283] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [283] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [283] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [283] Batch [40/130] Loss: 0.0001 LR: 0.000100
Epoch [283] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [283] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [283] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [283] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [283] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [283] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [283] Batch [110/130] Loss: 0.0002 LR: 0.000100
Epoch [283] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [284/350] - Time: 8.33s
Train Loss: 0.0359 | Val Loss: 3.2368
Train Acc: 0.9933 | Val Acc: 0.8138
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.922  
  Count 6: 0.857    Count 7: 0.552    Count 8: 0.465    Count 9: 0.444    Count 10: 0.636  
Epoch [284] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [284] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [284] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [284] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [284] Batch [40/130] Loss: 0.0082 LR: 0.000100
Epoch [284] Batch [50/130] Loss: 0.5501 LR: 0.000100
Epoch [284] Batch [60/130] Loss: 0.0351 LR: 0.000100
Epoch [284] Batch [70/130] Loss: 0.0001 LR: 0.000100
Epoch [284] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [284] Batch [90/130] Loss: 0.0055 LR: 0.000100
Epoch [284] Batch [100/130] Loss: 0.0056 LR: 0.000100
Epoch [284] Batch [110/130] Loss: 0.0258 LR: 0.000100
Epoch [284] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [285/350] - Time: 8.11s
Train Loss: 0.0436 | Val Loss: 3.8271
Train Acc: 0.9942 | Val Acc: 0.7728
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.992    Count 3: 0.981    Count 4: 0.990    Count 5: 0.971  
  Count 6: 0.918    Count 7: 0.406    Count 8: 0.202    Count 9: 0.322    Count 10: 0.636  
Epoch [285] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [285] Batch [10/130] Loss: 0.0002 LR: 0.000100
Epoch [285] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [285] Batch [30/130] Loss: 0.0001 LR: 0.000100
Epoch [285] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [285] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [285] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [285] Batch [70/130] Loss: 0.0001 LR: 0.000100
Epoch [285] Batch [80/130] Loss: 0.0004 LR: 0.000100
Epoch [285] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [285] Batch [100/130] Loss: 0.0446 LR: 0.000100
Epoch [285] Batch [110/130] Loss: 0.0001 LR: 0.000100
Epoch [285] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [286/350] - Time: 8.34s
Train Loss: 0.0138 | Val Loss: 3.6101
Train Acc: 0.9947 | Val Acc: 0.7914
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.971    Count 5: 0.951  
  Count 6: 0.847    Count 7: 0.604    Count 8: 0.485    Count 9: 0.133    Count 10: 0.614  
Epoch [286] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [286] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [286] Batch [20/130] Loss: 0.0007 LR: 0.000100
Epoch [286] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [286] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [286] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [286] Batch [60/130] Loss: 0.0001 LR: 0.000100
Epoch [286] Batch [70/130] Loss: 0.0007 LR: 0.000100
Epoch [286] Batch [80/130] Loss: 0.0001 LR: 0.000100
Epoch [286] Batch [90/130] Loss: 0.0009 LR: 0.000100
Epoch [286] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [286] Batch [110/130] Loss: 0.0964 LR: 0.000100
Epoch [286] Batch [120/130] Loss: 0.0001 LR: 0.000100

Epoch [287/350] - Time: 8.19s
Train Loss: 0.0323 | Val Loss: 3.2375
Train Acc: 0.9937 | Val Acc: 0.8147
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.961  
  Count 6: 0.980    Count 7: 0.417    Count 8: 0.626    Count 9: 0.244    Count 10: 0.636  
Epoch [287] Batch [0/130] Loss: 0.0001 LR: 0.000100
Epoch [287] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [287] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [287] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [287] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [287] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [287] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [287] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [287] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [287] Batch [90/130] Loss: 0.0001 LR: 0.000100
Epoch [287] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [287] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [287] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [288/350] - Time: 8.27s
Train Loss: 0.0150 | Val Loss: 3.4793
Train Acc: 0.9976 | Val Acc: 0.7989
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.981    Count 5: 0.961  
  Count 6: 0.867    Count 7: 0.552    Count 8: 0.404    Count 9: 0.300    Count 10: 0.636  
Epoch [288] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [288] Batch [10/130] Loss: 0.0038 LR: 0.000100
Epoch [288] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [288] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [288] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [288] Batch [50/130] Loss: 0.0584 LR: 0.000100
Epoch [288] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [288] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [288] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [288] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [288] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [288] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [288] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [289/350] - Time: 8.24s
Train Loss: 0.0612 | Val Loss: 3.3253
Train Acc: 0.9889 | Val Acc: 0.8017
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.981    Count 4: 0.943    Count 5: 0.961  
  Count 6: 0.949    Count 7: 0.469    Count 8: 0.444    Count 9: 0.356    Count 10: 0.636  
Epoch [289] Batch [0/130] Loss: 0.0137 LR: 0.000100
Epoch [289] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [289] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [289] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [289] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [289] Batch [50/130] Loss: 0.0001 LR: 0.000100
Epoch [289] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [289] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [289] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [289] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [289] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [289] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [289] Batch [120/130] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [290/350] - Time: 8.53s
Train Loss: 0.0028 | Val Loss: 3.2679
Train Acc: 0.9986 | Val Acc: 0.8054
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 0.922  
  Count 6: 0.847    Count 7: 0.552    Count 8: 0.394    Count 9: 0.389    Count 10: 0.693  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/single_image_checkpoint_epoch_289.pth
Epoch [290] Batch [0/130] Loss: 0.0003 LR: 0.000100
Epoch [290] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [290] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [290] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [290] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [290] Batch [50/130] Loss: 0.0008 LR: 0.000100
Epoch [290] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [290] Batch [70/130] Loss: 0.0001 LR: 0.000100
Epoch [290] Batch [80/130] Loss: 0.0002 LR: 0.000100
Epoch [290] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [290] Batch [100/130] Loss: 0.0035 LR: 0.000100
Epoch [290] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [290] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [291/350] - Time: 8.12s
Train Loss: 0.0080 | Val Loss: 3.8132
Train Acc: 0.9976 | Val Acc: 0.7858
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.951  
  Count 6: 0.959    Count 7: 0.406    Count 8: 0.384    Count 9: 0.267    Count 10: 0.580  
Epoch [291] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [291] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [291] Batch [20/130] Loss: 0.0001 LR: 0.000100
Epoch [291] Batch [30/130] Loss: 0.0011 LR: 0.000100
Epoch [291] Batch [40/130] Loss: 0.0001 LR: 0.000100
Epoch [291] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [291] Batch [60/130] Loss: 0.0002 LR: 0.000100
Epoch [291] Batch [70/130] Loss: 0.0004 LR: 0.000100
Epoch [291] Batch [80/130] Loss: 0.2836 LR: 0.000100
Epoch [291] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [291] Batch [100/130] Loss: 0.7211 LR: 0.000100
Epoch [291] Batch [110/130] Loss: 0.0002 LR: 0.000100
Epoch [291] Batch [120/130] Loss: 0.5931 LR: 0.000100

Epoch [292/350] - Time: 8.21s
Train Loss: 0.0299 | Val Loss: 3.6471
Train Acc: 0.9942 | Val Acc: 0.7719
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.943    Count 5: 0.618  
  Count 6: 0.724    Count 7: 0.542    Count 8: 0.586    Count 9: 0.444    Count 10: 0.568  
Epoch [292] Batch [0/130] Loss: 0.0001 LR: 0.000100
Epoch [292] Batch [10/130] Loss: 0.0020 LR: 0.000100
Epoch [292] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [292] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [292] Batch [40/130] Loss: 0.1697 LR: 0.000100
Epoch [292] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [292] Batch [60/130] Loss: 0.0023 LR: 0.000100
Epoch [292] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [292] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [292] Batch [90/130] Loss: 0.0520 LR: 0.000100
Epoch [292] Batch [100/130] Loss: 0.0014 LR: 0.000100
Epoch [292] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [292] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [293/350] - Time: 8.32s
Train Loss: 0.0090 | Val Loss: 3.5470
Train Acc: 0.9981 | Val Acc: 0.7970
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.971    Count 5: 0.941  
  Count 6: 0.898    Count 7: 0.479    Count 8: 0.566    Count 9: 0.289    Count 10: 0.523  
Epoch [293] Batch [0/130] Loss: 0.0186 LR: 0.000100
Epoch [293] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [293] Batch [20/130] Loss: 0.0001 LR: 0.000100
Epoch [293] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [293] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [293] Batch [50/130] Loss: 0.0473 LR: 0.000100
Epoch [293] Batch [60/130] Loss: 0.0054 LR: 0.000100
Epoch [293] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [293] Batch [80/130] Loss: 0.0006 LR: 0.000100
Epoch [293] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [293] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [293] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [293] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [294/350] - Time: 8.24s
Train Loss: 0.0091 | Val Loss: 3.8376
Train Acc: 0.9966 | Val Acc: 0.7849
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.962    Count 5: 0.882  
  Count 6: 0.847    Count 7: 0.573    Count 8: 0.374    Count 9: 0.433    Count 10: 0.489  
Epoch [294] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [294] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [294] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [294] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [294] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [294] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [294] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [294] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [294] Batch [80/130] Loss: 0.0001 LR: 0.000100
Epoch [294] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [294] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [294] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [294] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [295/350] - Time: 8.21s
Train Loss: 0.0305 | Val Loss: 4.5179
Train Acc: 0.9937 | Val Acc: 0.7700
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.943    Count 5: 0.990  
  Count 6: 0.857    Count 7: 0.469    Count 8: 0.192    Count 9: 0.300    Count 10: 0.648  
Epoch [295] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [295] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [295] Batch [20/130] Loss: 0.0001 LR: 0.000100
Epoch [295] Batch [30/130] Loss: 0.0338 LR: 0.000100
Epoch [295] Batch [40/130] Loss: 0.0006 LR: 0.000100
Epoch [295] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [295] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [295] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [295] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [295] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [295] Batch [100/130] Loss: 0.0022 LR: 0.000100
Epoch [295] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [295] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [296/350] - Time: 8.18s
Train Loss: 0.0155 | Val Loss: 3.5580
Train Acc: 0.9981 | Val Acc: 0.7952
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.933    Count 5: 0.784  
  Count 6: 0.796    Count 7: 0.625    Count 8: 0.475    Count 9: 0.378    Count 10: 0.693  
Epoch [296] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [296] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [296] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [296] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [296] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [296] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [296] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [296] Batch [70/130] Loss: 0.0073 LR: 0.000100
Epoch [296] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [296] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [296] Batch [100/130] Loss: 0.0848 LR: 0.000100
Epoch [296] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [296] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [297/350] - Time: 8.29s
Train Loss: 0.0024 | Val Loss: 3.3544
Train Acc: 0.9990 | Val Acc: 0.8101
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.990    Count 5: 0.990  
  Count 6: 0.929    Count 7: 0.438    Count 8: 0.606    Count 9: 0.289    Count 10: 0.568  
Epoch [297] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [297] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [297] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [297] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [297] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [297] Batch [50/130] Loss: 0.0014 LR: 0.000100
Epoch [297] Batch [60/130] Loss: 0.0761 LR: 0.000100
Epoch [297] Batch [70/130] Loss: 0.1606 LR: 0.000100
Epoch [297] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [297] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [297] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [297] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [297] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [298/350] - Time: 8.21s
Train Loss: 0.0028 | Val Loss: 4.3202
Train Acc: 0.9990 | Val Acc: 0.7719
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.992    Count 3: 0.991    Count 4: 0.981    Count 5: 0.990  
  Count 6: 0.908    Count 7: 0.500    Count 8: 0.212    Count 9: 0.200    Count 10: 0.625  
Epoch [298] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [298] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [298] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [298] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [298] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [298] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [298] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [298] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [298] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [298] Batch [90/130] Loss: 0.0001 LR: 0.000100
Epoch [298] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [298] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [298] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [299/350] - Time: 8.66s
Train Loss: 0.0180 | Val Loss: 3.9100
Train Acc: 0.9966 | Val Acc: 0.8119
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.933    Count 5: 1.000  
  Count 6: 0.878    Count 7: 0.615    Count 8: 0.485    Count 9: 0.267    Count 10: 0.682  
Epoch [299] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [299] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [299] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [299] Batch [30/130] Loss: 0.1679 LR: 0.000100
Epoch [299] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [299] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [299] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [299] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [299] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [299] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [299] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [299] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [299] Batch [120/130] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [300/350] - Time: 8.15s
Train Loss: 0.0154 | Val Loss: 3.8032
Train Acc: 0.9971 | Val Acc: 0.8101
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.990    Count 5: 0.990  
  Count 6: 0.939    Count 7: 0.615    Count 8: 0.455    Count 9: 0.233    Count 10: 0.591  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/single_image_checkpoint_epoch_299.pth
Epoch [300] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [300] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [300] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [300] Batch [30/130] Loss: 0.0825 LR: 0.000100
Epoch [300] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [300] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [300] Batch [60/130] Loss: 0.0001 LR: 0.000100
Epoch [300] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [300] Batch [80/130] Loss: 0.0767 LR: 0.000100
Epoch [300] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [300] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [300] Batch [110/130] Loss: 0.1470 LR: 0.000100
Epoch [300] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [301/350] - Time: 8.51s
Train Loss: 0.0344 | Val Loss: 3.5591
Train Acc: 0.9933 | Val Acc: 0.8035
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.971    Count 5: 0.902  
  Count 6: 0.847    Count 7: 0.646    Count 8: 0.333    Count 9: 0.356    Count 10: 0.716  
Epoch [301] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [301] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [301] Batch [20/130] Loss: 0.0011 LR: 0.000100
Epoch [301] Batch [30/130] Loss: 0.0114 LR: 0.000100
Epoch [301] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [301] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [301] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [301] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [301] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [301] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [301] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [301] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [301] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [302/350] - Time: 8.27s
Train Loss: 0.0130 | Val Loss: 3.3654
Train Acc: 0.9976 | Val Acc: 0.8119
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.990    Count 5: 0.980  
  Count 6: 0.827    Count 7: 0.656    Count 8: 0.444    Count 9: 0.356    Count 10: 0.602  
Epoch [302] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [302] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [302] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [302] Batch [30/130] Loss: 0.0001 LR: 0.000100
Epoch [302] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [302] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [302] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [302] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [302] Batch [80/130] Loss: 0.0001 LR: 0.000100
Epoch [302] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [302] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [302] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [302] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [303/350] - Time: 8.35s
Train Loss: 0.0068 | Val Loss: 3.6174
Train Acc: 0.9981 | Val Acc: 0.7998
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.941  
  Count 6: 0.847    Count 7: 0.531    Count 8: 0.535    Count 9: 0.333    Count 10: 0.511  
Epoch [303] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [303] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [303] Batch [20/130] Loss: 0.1776 LR: 0.000100
Epoch [303] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [303] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [303] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [303] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [303] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [303] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [303] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [303] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [303] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [303] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [304/350] - Time: 8.39s
Train Loss: 0.0017 | Val Loss: 3.5440
Train Acc: 0.9995 | Val Acc: 0.8091
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.922  
  Count 6: 0.929    Count 7: 0.552    Count 8: 0.434    Count 9: 0.367    Count 10: 0.614  
Epoch [304] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [304] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [304] Batch [20/130] Loss: 0.2330 LR: 0.000100
Epoch [304] Batch [30/130] Loss: 0.0201 LR: 0.000100
Epoch [304] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [304] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [304] Batch [60/130] Loss: 0.0002 LR: 0.000100
Epoch [304] Batch [70/130] Loss: 0.9953 LR: 0.000100
Epoch [304] Batch [80/130] Loss: 0.0002 LR: 0.000100
Epoch [304] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [304] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [304] Batch [110/130] Loss: 0.0001 LR: 0.000100
Epoch [304] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [305/350] - Time: 8.39s
Train Loss: 0.0284 | Val Loss: 4.3946
Train Acc: 0.9928 | Val Acc: 0.7849
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.952    Count 5: 0.833  
  Count 6: 0.827    Count 7: 0.625    Count 8: 0.384    Count 9: 0.433    Count 10: 0.500  
Epoch [305] Batch [0/130] Loss: 0.0004 LR: 0.000100
Epoch [305] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [305] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [305] Batch [30/130] Loss: 0.0871 LR: 0.000100
Epoch [305] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [305] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [305] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [305] Batch [70/130] Loss: 0.0005 LR: 0.000100
Epoch [305] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [305] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [305] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [305] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [305] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [306/350] - Time: 8.18s
Train Loss: 0.0084 | Val Loss: 3.0592
Train Acc: 0.9976 | Val Acc: 0.8082
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.933    Count 5: 0.863  
  Count 6: 0.847    Count 7: 0.667    Count 8: 0.475    Count 9: 0.433    Count 10: 0.602  
Epoch [306] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [306] Batch [10/130] Loss: 0.0001 LR: 0.000100
Epoch [306] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [306] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [306] Batch [40/130] Loss: 0.0065 LR: 0.000100
Epoch [306] Batch [50/130] Loss: 0.0002 LR: 0.000100
Epoch [306] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [306] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [306] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [306] Batch [90/130] Loss: 0.0011 LR: 0.000100
Epoch [306] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [306] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [306] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [307/350] - Time: 8.72s
Train Loss: 0.0215 | Val Loss: 3.8206
Train Acc: 0.9957 | Val Acc: 0.7970
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.990    Count 5: 0.971  
  Count 6: 0.796    Count 7: 0.552    Count 8: 0.475    Count 9: 0.289    Count 10: 0.602  
Epoch [307] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [307] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [307] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [307] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [307] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [307] Batch [50/130] Loss: 0.0015 LR: 0.000100
Epoch [307] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [307] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [307] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [307] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [307] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [307] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [307] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [308/350] - Time: 8.54s
Train Loss: 0.0019 | Val Loss: 4.1980
Train Acc: 0.9990 | Val Acc: 0.7737
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.990    Count 5: 0.971  
  Count 6: 0.939    Count 7: 0.438    Count 8: 0.303    Count 9: 0.178    Count 10: 0.602  
Epoch [308] Batch [0/130] Loss: 0.0001 LR: 0.000100
Epoch [308] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [308] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [308] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [308] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [308] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [308] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [308] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [308] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [308] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [308] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [308] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [308] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [309/350] - Time: 8.28s
Train Loss: 0.0124 | Val Loss: 4.1047
Train Acc: 0.9971 | Val Acc: 0.7849
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.943    Count 5: 0.833  
  Count 6: 0.816    Count 7: 0.542    Count 8: 0.444    Count 9: 0.367    Count 10: 0.614  
Epoch [309] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [309] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [309] Batch [20/130] Loss: 0.4717 LR: 0.000100
Epoch [309] Batch [30/130] Loss: 0.3094 LR: 0.000100
Epoch [309] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [309] Batch [50/130] Loss: 0.0003 LR: 0.000100
Epoch [309] Batch [60/130] Loss: 1.0252 LR: 0.000100
Epoch [309] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [309] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [309] Batch [90/130] Loss: 0.0049 LR: 0.000100
Epoch [309] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [309] Batch [110/130] Loss: 0.9694 LR: 0.000100
Epoch [309] Batch [120/130] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [310/350] - Time: 8.31s
Train Loss: 0.0808 | Val Loss: 3.7773
Train Acc: 0.9841 | Val Acc: 0.8063
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.990    Count 5: 0.931  
  Count 6: 0.878    Count 7: 0.500    Count 8: 0.556    Count 9: 0.467    Count 10: 0.455  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/single_image_checkpoint_epoch_309.pth
Epoch [310] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [310] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [310] Batch [20/130] Loss: 0.0001 LR: 0.000100
Epoch [310] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [310] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [310] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [310] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [310] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [310] Batch [80/130] Loss: 0.0116 LR: 0.000100
Epoch [310] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [310] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [310] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [310] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [311/350] - Time: 8.39s
Train Loss: 0.0210 | Val Loss: 3.8098
Train Acc: 0.9976 | Val Acc: 0.7654
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.943    Count 5: 0.931  
  Count 6: 0.908    Count 7: 0.510    Count 8: 0.202    Count 9: 0.289    Count 10: 0.557  
Epoch [311] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [311] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [311] Batch [20/130] Loss: 0.1591 LR: 0.000100
Epoch [311] Batch [30/130] Loss: 0.0001 LR: 0.000100
Epoch [311] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [311] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [311] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [311] Batch [70/130] Loss: 0.0001 LR: 0.000100
Epoch [311] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [311] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [311] Batch [100/130] Loss: 0.0027 LR: 0.000100
Epoch [311] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [311] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [312/350] - Time: 8.18s
Train Loss: 0.0146 | Val Loss: 3.6650
Train Acc: 0.9961 | Val Acc: 0.7961
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.990    Count 5: 0.951  
  Count 6: 0.827    Count 7: 0.479    Count 8: 0.485    Count 9: 0.489    Count 10: 0.443  
Epoch [312] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [312] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [312] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [312] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [312] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [312] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [312] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [312] Batch [70/130] Loss: 0.0001 LR: 0.000100
Epoch [312] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [312] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [312] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [312] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [312] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [313/350] - Time: 8.36s
Train Loss: 0.0012 | Val Loss: 3.5848
Train Acc: 0.9990 | Val Acc: 0.7952
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 0.941  
  Count 6: 0.837    Count 7: 0.500    Count 8: 0.485    Count 9: 0.444    Count 10: 0.455  
Epoch [313] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [313] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [313] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [313] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [313] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [313] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [313] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [313] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [313] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [313] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [313] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [313] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [313] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [314/350] - Time: 8.26s
Train Loss: 0.0052 | Val Loss: 3.4053
Train Acc: 0.9981 | Val Acc: 0.7952
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.981    Count 5: 0.843  
  Count 6: 0.908    Count 7: 0.427    Count 8: 0.535    Count 9: 0.389    Count 10: 0.580  
Epoch [314] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [314] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [314] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [314] Batch [30/130] Loss: 0.0016 LR: 0.000100
Epoch [314] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [314] Batch [50/130] Loss: 0.0001 LR: 0.000100
Epoch [314] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [314] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [314] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [314] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [314] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [314] Batch [110/130] Loss: 0.1563 LR: 0.000100
Epoch [314] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [315/350] - Time: 8.45s
Train Loss: 0.0392 | Val Loss: 3.4977
Train Acc: 0.9884 | Val Acc: 0.7914
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.933    Count 5: 0.980  
  Count 6: 0.929    Count 7: 0.427    Count 8: 0.465    Count 9: 0.233    Count 10: 0.659  
Epoch [315] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [315] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [315] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [315] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [315] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [315] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [315] Batch [60/130] Loss: 0.3631 LR: 0.000100
Epoch [315] Batch [70/130] Loss: 0.0019 LR: 0.000100
Epoch [315] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [315] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [315] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [315] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [315] Batch [120/130] Loss: 0.0014 LR: 0.000100

Epoch [316/350] - Time: 8.43s
Train Loss: 0.0406 | Val Loss: 3.5867
Train Acc: 0.9904 | Val Acc: 0.7868
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.941  
  Count 6: 0.898    Count 7: 0.521    Count 8: 0.354    Count 9: 0.244    Count 10: 0.602  
Epoch [316] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [316] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [316] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [316] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [316] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [316] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [316] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [316] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [316] Batch [80/130] Loss: 0.0002 LR: 0.000100
Epoch [316] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [316] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [316] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [316] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [317/350] - Time: 8.15s
Train Loss: 0.0151 | Val Loss: 4.2542
Train Acc: 0.9937 | Val Acc: 0.7868
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.931  
  Count 6: 0.857    Count 7: 0.406    Count 8: 0.566    Count 9: 0.467    Count 10: 0.318  
Epoch [317] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [317] Batch [10/130] Loss: 0.0002 LR: 0.000100
Epoch [317] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [317] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [317] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [317] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [317] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [317] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [317] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [317] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [317] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [317] Batch [110/130] Loss: 0.0039 LR: 0.000100
Epoch [317] Batch [120/130] Loss: 0.1340 LR: 0.000100

Epoch [318/350] - Time: 8.29s
Train Loss: 0.0208 | Val Loss: 3.1490
Train Acc: 0.9937 | Val Acc: 0.8007
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.931  
  Count 6: 0.908    Count 7: 0.490    Count 8: 0.556    Count 9: 0.444    Count 10: 0.375  
Epoch [318] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [318] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [318] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [318] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [318] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [318] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [318] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [318] Batch [70/130] Loss: 0.0001 LR: 0.000100
Epoch [318] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [318] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [318] Batch [100/130] Loss: 0.0001 LR: 0.000100
Epoch [318] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [318] Batch [120/130] Loss: 0.0001 LR: 0.000100

Epoch [319/350] - Time: 8.17s
Train Loss: 0.0088 | Val Loss: 3.7155
Train Acc: 0.9986 | Val Acc: 0.7942
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.981    Count 5: 0.863  
  Count 6: 0.857    Count 7: 0.521    Count 8: 0.424    Count 9: 0.400    Count 10: 0.625  
Epoch [319] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [319] Batch [10/130] Loss: 0.0001 LR: 0.000100
Epoch [319] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [319] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [319] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [319] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [319] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [319] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [319] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [319] Batch [90/130] Loss: 0.0001 LR: 0.000100
Epoch [319] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [319] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [319] Batch [120/130] Loss: 0.0001 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [320/350] - Time: 8.21s
Train Loss: 0.0051 | Val Loss: 3.7598
Train Acc: 0.9995 | Val Acc: 0.7952
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 0.882  
  Count 6: 0.867    Count 7: 0.562    Count 8: 0.424    Count 9: 0.389    Count 10: 0.545  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/single_image_checkpoint_epoch_319.pth
Epoch [320] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [320] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [320] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [320] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [320] Batch [40/130] Loss: 0.0001 LR: 0.000100
Epoch [320] Batch [50/130] Loss: 0.0005 LR: 0.000100
Epoch [320] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [320] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [320] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [320] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [320] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [320] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [320] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [321/350] - Time: 8.29s
Train Loss: 0.0056 | Val Loss: 4.0091
Train Acc: 0.9990 | Val Acc: 0.7914
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.981    Count 4: 0.990    Count 5: 0.951  
  Count 6: 0.918    Count 7: 0.448    Count 8: 0.475    Count 9: 0.233    Count 10: 0.614  
Epoch [321] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [321] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [321] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [321] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [321] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [321] Batch [50/130] Loss: 0.0258 LR: 0.000100
Epoch [321] Batch [60/130] Loss: 0.0001 LR: 0.000100
Epoch [321] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [321] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [321] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [321] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [321] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [321] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [322/350] - Time: 8.17s
Train Loss: 0.0002 | Val Loss: 4.2021
Train Acc: 1.0000 | Val Acc: 0.7970
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.990    Count 5: 0.941  
  Count 6: 0.888    Count 7: 0.500    Count 8: 0.444    Count 9: 0.278    Count 10: 0.636  
Epoch [322] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [322] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [322] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [322] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [322] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [322] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [322] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [322] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [322] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [322] Batch [90/130] Loss: 0.0001 LR: 0.000100
Epoch [322] Batch [100/130] Loss: 0.0002 LR: 0.000100
Epoch [322] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [322] Batch [120/130] Loss: 0.0001 LR: 0.000100

Epoch [323/350] - Time: 8.79s
Train Loss: 0.0016 | Val Loss: 4.6811
Train Acc: 0.9990 | Val Acc: 0.7682
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 0.931  
  Count 6: 0.888    Count 7: 0.521    Count 8: 0.263    Count 9: 0.289    Count 10: 0.466  
Epoch [323] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [323] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [323] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [323] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [323] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [323] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [323] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [323] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [323] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [323] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [323] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [323] Batch [110/130] Loss: 0.4183 LR: 0.000100
Epoch [323] Batch [120/130] Loss: 0.0002 LR: 0.000100

Epoch [324/350] - Time: 8.22s
Train Loss: 0.0049 | Val Loss: 4.4076
Train Acc: 0.9990 | Val Acc: 0.7765
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.981    Count 5: 0.931  
  Count 6: 0.929    Count 7: 0.417    Count 8: 0.515    Count 9: 0.311    Count 10: 0.341  
Epoch [324] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [324] Batch [10/130] Loss: 0.0003 LR: 0.000100
Epoch [324] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [324] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [324] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [324] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [324] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [324] Batch [70/130] Loss: 0.0001 LR: 0.000100
Epoch [324] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [324] Batch [90/130] Loss: 0.4116 LR: 0.000100
Epoch [324] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [324] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [324] Batch [120/130] Loss: 0.0104 LR: 0.000100

Epoch [325/350] - Time: 8.41s
Train Loss: 0.0094 | Val Loss: 3.8355
Train Acc: 0.9976 | Val Acc: 0.8063
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.990    Count 5: 0.814  
  Count 6: 0.827    Count 7: 0.708    Count 8: 0.394    Count 9: 0.367    Count 10: 0.705  
Epoch [325] Batch [0/130] Loss: 0.0117 LR: 0.000100
Epoch [325] Batch [10/130] Loss: 0.1415 LR: 0.000100
Epoch [325] Batch [20/130] Loss: 0.2011 LR: 0.000100
Epoch [325] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [325] Batch [40/130] Loss: 0.2875 LR: 0.000100
Epoch [325] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [325] Batch [60/130] Loss: 0.2182 LR: 0.000100
Epoch [325] Batch [70/130] Loss: 0.0005 LR: 0.000100
Epoch [325] Batch [80/130] Loss: 0.0011 LR: 0.000100
Epoch [325] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [325] Batch [100/130] Loss: 0.0001 LR: 0.000100
Epoch [325] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [325] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [326/350] - Time: 8.17s
Train Loss: 0.0343 | Val Loss: 4.1830
Train Acc: 0.9923 | Val Acc: 0.7719
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.943    Count 5: 0.784  
  Count 6: 0.663    Count 7: 0.573    Count 8: 0.444    Count 9: 0.411    Count 10: 0.602  
Epoch [326] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [326] Batch [10/130] Loss: 0.0004 LR: 0.000100
Epoch [326] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [326] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [326] Batch [40/130] Loss: 0.0084 LR: 0.000100
Epoch [326] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [326] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [326] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [326] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [326] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [326] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [326] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [326] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [327/350] - Time: 8.10s
Train Loss: 0.0142 | Val Loss: 4.0194
Train Acc: 0.9976 | Val Acc: 0.7821
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.943    Count 5: 0.745  
  Count 6: 0.796    Count 7: 0.542    Count 8: 0.505    Count 9: 0.356    Count 10: 0.648  
Epoch [327] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [327] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [327] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [327] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [327] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [327] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [327] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [327] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [327] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [327] Batch [90/130] Loss: 0.0001 LR: 0.000100
Epoch [327] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [327] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [327] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [328/350] - Time: 8.19s
Train Loss: 0.0000 | Val Loss: 3.6879
Train Acc: 1.0000 | Val Acc: 0.7896
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.962    Count 5: 0.843  
  Count 6: 0.847    Count 7: 0.552    Count 8: 0.485    Count 9: 0.278    Count 10: 0.636  
Epoch [328] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [328] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [328] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [328] Batch [30/130] Loss: 0.0001 LR: 0.000100
Epoch [328] Batch [40/130] Loss: 0.0003 LR: 0.000100
Epoch [328] Batch [50/130] Loss: 0.0002 LR: 0.000100
Epoch [328] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [328] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [328] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [328] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [328] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [328] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [328] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [329/350] - Time: 8.20s
Train Loss: 0.0050 | Val Loss: 4.3614
Train Acc: 0.9986 | Val Acc: 0.7775
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.922  
  Count 6: 0.918    Count 7: 0.479    Count 8: 0.545    Count 9: 0.256    Count 10: 0.307  
Epoch [329] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [329] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [329] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [329] Batch [30/130] Loss: 0.0006 LR: 0.000100
Epoch [329] Batch [40/130] Loss: 0.0130 LR: 0.000100
Epoch [329] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [329] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [329] Batch [70/130] Loss: 0.0217 LR: 0.000100
Epoch [329] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [329] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [329] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [329] Batch [110/130] Loss: 0.0089 LR: 0.000100
Epoch [329] Batch [120/130] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [330/350] - Time: 8.20s
Train Loss: 0.0756 | Val Loss: 4.9500
Train Acc: 0.9865 | Val Acc: 0.7635
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.971    Count 5: 0.716  
  Count 6: 0.633    Count 7: 0.562    Count 8: 0.394    Count 9: 0.422    Count 10: 0.648  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/single_image_checkpoint_epoch_329.pth
Epoch [330] Batch [0/130] Loss: 0.0010 LR: 0.000100
Epoch [330] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [330] Batch [20/130] Loss: 0.0002 LR: 0.000100
Epoch [330] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [330] Batch [40/130] Loss: 0.0047 LR: 0.000100
Epoch [330] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [330] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [330] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [330] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [330] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [330] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [330] Batch [110/130] Loss: 0.0001 LR: 0.000100
Epoch [330] Batch [120/130] Loss: 0.0055 LR: 0.000100

Epoch [331/350] - Time: 8.22s
Train Loss: 0.0157 | Val Loss: 4.1191
Train Acc: 0.9976 | Val Acc: 0.7654
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.924    Count 5: 0.961  
  Count 6: 0.847    Count 7: 0.552    Count 8: 0.172    Count 9: 0.278    Count 10: 0.614  
Epoch [331] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [331] Batch [10/130] Loss: 0.0006 LR: 0.000100
Epoch [331] Batch [20/130] Loss: 0.0002 LR: 0.000100
Epoch [331] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [331] Batch [40/130] Loss: 0.0002 LR: 0.000100
Epoch [331] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [331] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [331] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [331] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [331] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [331] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [331] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [331] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [332/350] - Time: 8.63s
Train Loss: 0.0036 | Val Loss: 4.0503
Train Acc: 0.9995 | Val Acc: 0.7728
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.951  
  Count 6: 0.918    Count 7: 0.479    Count 8: 0.232    Count 9: 0.200    Count 10: 0.625  
Epoch [332] Batch [0/130] Loss: 0.0239 LR: 0.000100
Epoch [332] Batch [10/130] Loss: 0.1726 LR: 0.000100
Epoch [332] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [332] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [332] Batch [40/130] Loss: 0.0004 LR: 0.000100
Epoch [332] Batch [50/130] Loss: 0.0135 LR: 0.000100
Epoch [332] Batch [60/130] Loss: 0.0734 LR: 0.000100
Epoch [332] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [332] Batch [80/130] Loss: 0.0001 LR: 0.000100
Epoch [332] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [332] Batch [100/130] Loss: 0.2299 LR: 0.000100
Epoch [332] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [332] Batch [120/130] Loss: 0.0168 LR: 0.000100

Epoch [333/350] - Time: 8.17s
Train Loss: 0.0216 | Val Loss: 4.0319
Train Acc: 0.9923 | Val Acc: 0.7961
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.882  
  Count 6: 0.776    Count 7: 0.510    Count 8: 0.556    Count 9: 0.389    Count 10: 0.557  
Epoch [333] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [333] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [333] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [333] Batch [30/130] Loss: 0.0001 LR: 0.000100
Epoch [333] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [333] Batch [50/130] Loss: 0.1087 LR: 0.000100
Epoch [333] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [333] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [333] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [333] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [333] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [333] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [333] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [334/350] - Time: 8.10s
Train Loss: 0.0425 | Val Loss: 3.0778
Train Acc: 0.9937 | Val Acc: 0.8184
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.990    Count 5: 0.941  
  Count 6: 0.878    Count 7: 0.552    Count 8: 0.566    Count 9: 0.411    Count 10: 0.580  
Epoch [334] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [334] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [334] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [334] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [334] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [334] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [334] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [334] Batch [70/130] Loss: 0.0174 LR: 0.000100
Epoch [334] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [334] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [334] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [334] Batch [110/130] Loss: 0.0010 LR: 0.000100
Epoch [334] Batch [120/130] Loss: 0.0001 LR: 0.000100

Epoch [335/350] - Time: 8.19s
Train Loss: 0.0231 | Val Loss: 2.6834
Train Acc: 0.9961 | Val Acc: 0.8268
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.931  
  Count 6: 0.878    Count 7: 0.573    Count 8: 0.677    Count 9: 0.400    Count 10: 0.545  
Epoch [335] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [335] Batch [10/130] Loss: 0.0001 LR: 0.000100
Epoch [335] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [335] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [335] Batch [40/130] Loss: 0.0016 LR: 0.000100
Epoch [335] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [335] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [335] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [335] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [335] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [335] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [335] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [335] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [336/350] - Time: 8.32s
Train Loss: 0.0141 | Val Loss: 3.6540
Train Acc: 0.9971 | Val Acc: 0.7747
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.990  
  Count 6: 0.857    Count 7: 0.542    Count 8: 0.293    Count 9: 0.122    Count 10: 0.614  
Epoch [336] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [336] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [336] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [336] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [336] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [336] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [336] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [336] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [336] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [336] Batch [90/130] Loss: 0.0001 LR: 0.000100
Epoch [336] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [336] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [336] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [337/350] - Time: 8.23s
Train Loss: 0.0005 | Val Loss: 3.2756
Train Acc: 0.9995 | Val Acc: 0.8045
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.941  
  Count 6: 0.837    Count 7: 0.562    Count 8: 0.525    Count 9: 0.333    Count 10: 0.557  
Epoch [337] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [337] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [337] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [337] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [337] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [337] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [337] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [337] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [337] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [337] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [337] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [337] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [337] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [338/350] - Time: 8.36s
Train Loss: 0.0000 | Val Loss: 3.2996
Train Acc: 1.0000 | Val Acc: 0.7989
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.941  
  Count 6: 0.847    Count 7: 0.562    Count 8: 0.475    Count 9: 0.311    Count 10: 0.557  
Epoch [338] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [338] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [338] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [338] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [338] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [338] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [338] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [338] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [338] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [338] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [338] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [338] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [338] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [339/350] - Time: 8.17s
Train Loss: 0.0000 | Val Loss: 3.4585
Train Acc: 1.0000 | Val Acc: 0.7924
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.951  
  Count 6: 0.857    Count 7: 0.542    Count 8: 0.465    Count 9: 0.278    Count 10: 0.523  
Epoch [339] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [339] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [339] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [339] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [339] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [339] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [339] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [339] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [339] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [339] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [339] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [339] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [339] Batch [120/130] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [340/350] - Time: 8.72s
Train Loss: 0.0000 | Val Loss: 3.6864
Train Acc: 1.0000 | Val Acc: 0.7831
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.922  
  Count 6: 0.847    Count 7: 0.583    Count 8: 0.374    Count 9: 0.222    Count 10: 0.568  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/single_image_checkpoint_epoch_339.pth
Epoch [340] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [340] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [340] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [340] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [340] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [340] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [340] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [340] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [340] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [340] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [340] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [340] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [340] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [341/350] - Time: 8.14s
Train Loss: 0.0000 | Val Loss: 3.7503
Train Acc: 1.0000 | Val Acc: 0.7858
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.902  
  Count 6: 0.837    Count 7: 0.562    Count 8: 0.364    Count 9: 0.300    Count 10: 0.591  
Epoch [341] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [341] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [341] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [341] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [341] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [341] Batch [50/130] Loss: 0.0001 LR: 0.000100
Epoch [341] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [341] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [341] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [341] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [341] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [341] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [341] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [342/350] - Time: 8.40s
Train Loss: 0.0000 | Val Loss: 3.5436
Train Acc: 1.0000 | Val Acc: 0.7933
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.912  
  Count 6: 0.837    Count 7: 0.552    Count 8: 0.424    Count 9: 0.300    Count 10: 0.614  
Epoch [342] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [342] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [342] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [342] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [342] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [342] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [342] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [342] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [342] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [342] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [342] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [342] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [342] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [343/350] - Time: 8.20s
Train Loss: 0.0000 | Val Loss: 3.5632
Train Acc: 1.0000 | Val Acc: 0.7924
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 1.000    Count 5: 0.931  
  Count 6: 0.867    Count 7: 0.531    Count 8: 0.414    Count 9: 0.289    Count 10: 0.591  
Epoch [343] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [343] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [343] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [343] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [343] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [343] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [343] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [343] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [343] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [343] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [343] Batch [100/130] Loss: 0.0221 LR: 0.000100
Epoch [343] Batch [110/130] Loss: 0.0026 LR: 0.000100
Epoch [343] Batch [120/130] Loss: 0.0343 LR: 0.000100

Epoch [344/350] - Time: 8.11s
Train Loss: 0.0184 | Val Loss: 3.9751
Train Acc: 0.9961 | Val Acc: 0.7626
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 0.984    Count 3: 1.000    Count 4: 0.990    Count 5: 0.951  
  Count 6: 0.867    Count 7: 0.375    Count 8: 0.394    Count 9: 0.189    Count 10: 0.534  
Epoch [344] Batch [0/130] Loss: 0.0003 LR: 0.000100
Epoch [344] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [344] Batch [20/130] Loss: 0.0001 LR: 0.000100
Epoch [344] Batch [30/130] Loss: 0.0012 LR: 0.000100
Epoch [344] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [344] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [344] Batch [60/130] Loss: 0.0010 LR: 0.000100
Epoch [344] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [344] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [344] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [344] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [344] Batch [110/130] Loss: 0.5137 LR: 0.000100
Epoch [344] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [345/350] - Time: 8.20s
Train Loss: 0.0601 | Val Loss: 2.5345
Train Acc: 0.9884 | Val Acc: 0.8194
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.990    Count 5: 0.961  
  Count 6: 0.939    Count 7: 0.458    Count 8: 0.646    Count 9: 0.611    Count 10: 0.318  
Epoch [345] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [345] Batch [10/130] Loss: 0.0954 LR: 0.000100
Epoch [345] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [345] Batch [30/130] Loss: 0.0001 LR: 0.000100
Epoch [345] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [345] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [345] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [345] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [345] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [345] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [345] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [345] Batch [110/130] Loss: 0.0021 LR: 0.000100
Epoch [345] Batch [120/130] Loss: 0.3801 LR: 0.000100

Epoch [346/350] - Time: 8.26s
Train Loss: 0.0283 | Val Loss: 3.3404
Train Acc: 0.9947 | Val Acc: 0.7989
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 1.000    Count 4: 0.990    Count 5: 0.941  
  Count 6: 0.898    Count 7: 0.552    Count 8: 0.394    Count 9: 0.256    Count 10: 0.670  
Epoch [346] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [346] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [346] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [346] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [346] Batch [40/130] Loss: 0.1481 LR: 0.000100
Epoch [346] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [346] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [346] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [346] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [346] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [346] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [346] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [346] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [347/350] - Time: 8.34s
Train Loss: 0.0203 | Val Loss: 3.1122
Train Acc: 0.9947 | Val Acc: 0.8045
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.981    Count 4: 0.981    Count 5: 0.922  
  Count 6: 0.837    Count 7: 0.531    Count 8: 0.485    Count 9: 0.378    Count 10: 0.659  
Epoch [347] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [347] Batch [10/130] Loss: 0.0001 LR: 0.000100
Epoch [347] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [347] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [347] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [347] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [347] Batch [60/130] Loss: 0.0024 LR: 0.000100
Epoch [347] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [347] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [347] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [347] Batch [100/130] Loss: 0.0002 LR: 0.000100
Epoch [347] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [347] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [348/350] - Time: 8.68s
Train Loss: 0.0078 | Val Loss: 3.4561
Train Acc: 0.9981 | Val Acc: 0.7952
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 0.952    Count 5: 0.716  
  Count 6: 0.806    Count 7: 0.625    Count 8: 0.596    Count 9: 0.511    Count 10: 0.477  
Epoch [348] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [348] Batch [10/130] Loss: 0.6127 LR: 0.000100
Epoch [348] Batch [20/130] Loss: 0.0002 LR: 0.000100
Epoch [348] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [348] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [348] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [348] Batch [60/130] Loss: 0.0001 LR: 0.000100
Epoch [348] Batch [70/130] Loss: 0.0005 LR: 0.000100
Epoch [348] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [348] Batch [90/130] Loss: 0.0048 LR: 0.000100
Epoch [348] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [348] Batch [110/130] Loss: 0.0000 LR: 0.000100
Epoch [348] Batch [120/130] Loss: 0.0000 LR: 0.000100

Epoch [349/350] - Time: 8.14s
Train Loss: 0.0156 | Val Loss: 2.7995
Train Acc: 0.9976 | Val Acc: 0.8277
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 0.931  
  Count 6: 0.939    Count 7: 0.521    Count 8: 0.687    Count 9: 0.322    Count 10: 0.625  
Epoch [349] Batch [0/130] Loss: 0.0000 LR: 0.000100
Epoch [349] Batch [10/130] Loss: 0.0000 LR: 0.000100
Epoch [349] Batch [20/130] Loss: 0.0000 LR: 0.000100
Epoch [349] Batch [30/130] Loss: 0.0000 LR: 0.000100
Epoch [349] Batch [40/130] Loss: 0.0000 LR: 0.000100
Epoch [349] Batch [50/130] Loss: 0.0000 LR: 0.000100
Epoch [349] Batch [60/130] Loss: 0.0000 LR: 0.000100
Epoch [349] Batch [70/130] Loss: 0.0000 LR: 0.000100
Epoch [349] Batch [80/130] Loss: 0.0000 LR: 0.000100
Epoch [349] Batch [90/130] Loss: 0.0000 LR: 0.000100
Epoch [349] Batch [100/130] Loss: 0.0000 LR: 0.000100
Epoch [349] Batch [110/130] Loss: 0.0003 LR: 0.000100
Epoch [349] Batch [120/130] Loss: 0.0000 LR: 0.000100
/net/scratch/k09562zs/Ball_counting_CNN/Train_single_image.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  test_checkpoint = torch.load(temp_path, map_location='cpu')

Epoch [350/350] - Time: 8.25s
Train Loss: 0.0075 | Val Loss: 2.7695
Train Acc: 0.9986 | Val Acc: 0.8194
Learning Rate: 0.000100
Per-class validation accuracy:
  Count 1: 1.000    Count 2: 1.000    Count 3: 0.991    Count 4: 1.000    Count 5: 0.912  
  Count 6: 0.969    Count 7: 0.594    Count 8: 0.566    Count 9: 0.344    Count 10: 0.545  
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/single_image_checkpoint_epoch_349.pth
✓ Checkpoint saved and validated: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/final_single_image_model.pth

训练完成!
最佳验证准确率: 0.8399
最佳验证损失: 2.3497
最终混淆矩阵保存到: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/final_confusion_matrix.png
详细报告保存到: ./scratch/Ball_counting_CNN/Final_result/Visual_CNN_only/50data/check_points/classification_report.txt

训练成功完成！
程序结束。
=== 完成 ===
结束时间: Sun 20 Jul 01:34:23 BST 2025
