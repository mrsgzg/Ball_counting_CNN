"""
å¹¶è¡Œæ¶ˆèå®éªŒä¸»è„šæœ¬
æ”¯æŒåœ¨å•ä¸ªA100 GPUä¸ŠåŒæ—¶è¿è¡Œå¤šä¸ªå®éªŒï¼ˆé€šè¿‡CUDA MPSæˆ–å¤šè¿›ç¨‹ï¼‰
"""

import torch
import torch.multiprocessing as mp
import argparse
import os
import time
import json
import pandas as pd
import numpy as np
from datetime import datetime
from pathlib import Path
import subprocess
import signal
import sys

# è®¾ç½®å¤šè¿›ç¨‹å¯åŠ¨æ–¹æ³•
try:
    mp.set_start_method('spawn', force=True)
except RuntimeError:
    pass


# ==================== æ¶ˆèå®éªŒé…ç½® ====================
ABLATION_CONFIGS = {
    'full_model': {
        'name': 'Full Model',
        'description': 'Complete model with all components',
        'model_variant': None,  # ä½¿ç”¨åŸå§‹æ¨¡å‹
        'data_wrapper': None,
        'loss_modifications': {}
    },
    
    'no_forward_model': {
        'name': 'No Forward Model',
        'description': 'Remove motion prediction (forward model)',
        'model_variant': 'no_forward_model',
        'data_wrapper': None,
        'loss_modifications': {
            'embodiment_loss_weight': 0.0  # æ²¡æœ‰motion loss
        }
    },
    
    'no_attention': {
        'name': 'No Spatial Attention',
        'description': 'Replace attention with global average pooling',
        'model_variant': 'no_attention',
        'data_wrapper': None,
        'loss_modifications': {}
    },
    
    'late_fusion': {
        'name': 'Late Fusion',
        'description': 'Fuse vision and joints after LSTM',
        'model_variant': 'late_fusion',
        'data_wrapper': None,
        'loss_modifications': {}
    },
    
    'shuffled_batch': {
        'name': 'Shuffled Batch',
        'description': 'Shuffle vision-joint pairing across samples',
        'model_variant': None,
        'data_wrapper': 'shuffled_batch',
        'loss_modifications': {}
    },
    
    'shuffled_temporal': {
        'name': 'Shuffled Temporal',
        'description': 'Shuffle temporal order within sequences',
        'model_variant': None,
        'data_wrapper': 'shuffled_temporal',
        'loss_modifications': {}
    }
}


def run_single_ablation_experiment(
    ablation_type,
    model_type,
    seed,
    data_config,
    base_config,
    save_dir,
    gpu_id,
    process_id
):
    """
    è¿è¡Œå•ä¸ªæ¶ˆèå®éªŒ
    
    Args:
        ablation_type: æ¶ˆèç±»å‹
        model_type: è§†è§‰ç¼–ç å™¨ç±»å‹ ('baseline', 'alexnet_pretrain', 'alexnet_no_pretrain')
        seed: éšæœºç§å­
        data_config: æ•°æ®é…ç½®
        base_config: åŸºç¡€é…ç½®
        save_dir: ä¿å­˜ç›®å½•
        gpu_id: GPU ID
        process_id: è¿›ç¨‹IDï¼ˆç”¨äºæ—¥å¿—ï¼‰
    """
    try:
        # è®¾ç½®CUDAè®¾å¤‡
        os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)
        device = torch.device('cuda:0')  # å› ä¸ºCUDA_VISIBLE_DEVICESå·²è®¾ç½®ï¼Œæ‰€ä»¥æ€»æ˜¯0
        
        # å¯¼å…¥å¿…è¦çš„æ¨¡å—
        from complete_alexnet_embody_experiment import EmbodiedTrainer
        from DataLoader_embodiment import get_ball_counting_data_loaders
        from DataLoader_embodiment_ablation import wrap_dataloader
        from Model_alexnet_embodiment_ablation import create_ablation_model
        
        # è®¾ç½®éšæœºç§å­
        torch.manual_seed(seed)
        np.random.seed(seed)
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        
        # åˆ›å»ºå®éªŒç›®å½•
        experiment_name = f"{ablation_type}_{model_type}_seed{seed}"
        experiment_dir = os.path.join(save_dir, experiment_name)
        log_dir = os.path.join(experiment_dir, 'tensorboard_logs')
        checkpoint_dir = os.path.join(experiment_dir, 'checkpoints')
        
        os.makedirs(experiment_dir, exist_ok=True)
        os.makedirs(log_dir, exist_ok=True)
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        print(f"[Process {process_id}] Starting: {experiment_name} on GPU {gpu_id}")
        
        # è·å–æ¶ˆèé…ç½®
        ablation_config = ABLATION_CONFIGS[ablation_type]
        
        # æ„å»ºå®Œæ•´é…ç½®
        config = base_config.copy()
        config['model_type'] = model_type
        config['seed'] = seed
        config['ablation_type'] = ablation_type
        config['ablation_name'] = ablation_config['name']
        
        # åº”ç”¨æŸå¤±ä¿®æ”¹
        config.update(ablation_config['loss_modifications'])
        
        # ä¿å­˜é…ç½®
        config_path = os.path.join(experiment_dir, 'config.json')
        with open(config_path, 'w') as f:
            json.dump(config, f, indent=4)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader, val_loader, normalizer = get_ball_counting_data_loaders(
            train_csv_path=data_config['train_csv'],
            val_csv_path=data_config['val_csv'],
            data_root=data_config['data_root'],
            batch_size=config['batch_size'],
            sequence_length=config['sequence_length'],
            normalize=config['normalize'],
            num_workers=config['num_workers'],
            image_mode=config['image_mode']
        )
        
        # åº”ç”¨æ•°æ®åŒ…è£…ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if ablation_config['data_wrapper'] is not None:
            print(f"[Process {process_id}] Applying data wrapper: {ablation_config['data_wrapper']}")
            train_loader = wrap_dataloader(train_loader, ablation_config['data_wrapper'], seed=seed)
            val_loader = wrap_dataloader(val_loader, ablation_config['data_wrapper'], seed=seed)
        
        # åˆ›å»ºæ¨¡å‹
        if ablation_config['model_variant'] is not None:
            # ä½¿ç”¨æ¶ˆèå˜ä½“
            model = create_ablation_model(config, ablation_config['model_variant'])
        else:
            # ä½¿ç”¨åŸå§‹æ¨¡å‹
            from Model_alexnet_embodiment import create_model
            model = create_model(config, model_type=model_type)
        
        model = model.to(device)
        
        # ç»Ÿè®¡å‚æ•°
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"[Process {process_id}] Model parameters: {total_params:,} (trainable: {trainable_params:,})")
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = EmbodiedTrainer(
            model=model,
            train_loader=train_loader,
            val_loader=val_loader,
            config=config,
            device=device,
            log_dir=log_dir,
            checkpoint_dir=checkpoint_dir
        )
        
        # å¼€å§‹è®­ç»ƒ
        start_time = time.time()
        print(f"[Process {process_id}] Training started: {experiment_name}")
        
        history = trainer.train(num_epochs=config['total_epochs'])
        
        training_time = time.time() - start_time
        
        # æ”¶é›†ç»“æœ
        final_metrics = history[-1] if history else {}
        result = {
            'ablation_type': ablation_type,
            'ablation_name': ablation_config['name'],
            'model_type': model_type,
            'seed': seed,
            'best_val_accuracy': trainer.best_val_accuracy,
            'best_val_loss': trainer.best_val_loss,
            'final_val_accuracy': final_metrics.get('val_count_acc', 0.0),
            'final_val_final_accuracy': final_metrics.get('val_final_acc', 0.0),
            'final_val_true_final_accuracy': final_metrics.get('val_true_final_acc', 0.0),
            'final_joint_mse': final_metrics.get('joint_mse', 0.0) if 'joint_mse' in final_metrics else None,
            'total_epochs': config['total_epochs'],
            'training_time_hours': training_time / 3600,
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'experiment_dir': experiment_dir,
            'gpu_id': gpu_id,
            'process_id': process_id
        }
        
        # ä¿å­˜ç»“æœ
        result_file = os.path.join(experiment_dir, 'result.json')
        with open(result_file, 'w') as f:
            json.dump(result, f, indent=4)
        
        print(f"[Process {process_id}] Completed: {experiment_name}")
        print(f"[Process {process_id}] Best accuracy: {trainer.best_val_accuracy:.4f}")
        print(f"[Process {process_id}] Training time: {training_time/3600:.2f} hours")
        
        return result
        
    except Exception as e:
        print(f"[Process {process_id}] ERROR in {ablation_type}_{model_type}_seed{seed}: {str(e)}")
        import traceback
        traceback.print_exc()
        return None


def worker_process(task_queue, result_queue, gpu_id, worker_id, data_config, base_config, save_dir):
    """
    å·¥ä½œè¿›ç¨‹ï¼šä»é˜Ÿåˆ—è·å–ä»»åŠ¡å¹¶æ‰§è¡Œ
    
    Args:
        task_queue: ä»»åŠ¡é˜Ÿåˆ—
        result_queue: ç»“æœé˜Ÿåˆ—
        gpu_id: GPU ID
        worker_id: å·¥ä½œè¿›ç¨‹ID
        data_config: æ•°æ®é…ç½®
        base_config: åŸºç¡€é…ç½®
        save_dir: ä¿å­˜ç›®å½•
    """
    print(f"[Worker {worker_id}] Started on GPU {gpu_id}")
    
    while True:
        try:
            # ä»é˜Ÿåˆ—è·å–ä»»åŠ¡
            task = task_queue.get(timeout=1)
            
            if task is None:  # ç»ˆæ­¢ä¿¡å·
                print(f"[Worker {worker_id}] Received termination signal")
                break
            
            ablation_type, model_type, seed = task
            
            # æ‰§è¡Œå®éªŒ
            result = run_single_ablation_experiment(
                ablation_type=ablation_type,
                model_type=model_type,
                seed=seed,
                data_config=data_config,
                base_config=base_config,
                save_dir=save_dir,
                gpu_id=gpu_id,
                process_id=worker_id
            )
            
            # å°†ç»“æœæ”¾å…¥ç»“æœé˜Ÿåˆ—
            if result is not None:
                result_queue.put(result)
            
        except Exception as e:
            if "Empty" not in str(e):
                print(f"[Worker {worker_id}] Error: {e}")
                import traceback
                traceback.print_exc()
    
    print(f"[Worker {worker_id}] Finished")


def main():
    parser = argparse.ArgumentParser(description='å¹¶è¡Œæ¶ˆèå®éªŒ - A100ä¼˜åŒ–ç‰ˆ')
    
    # æ•°æ®è·¯å¾„
    parser.add_argument('--data_root', type=str, 
                       default='/mnt/iusers01/fatpou01/compsci01/k09562zs/scratch/Ball_counting_CNN/ball_data_collection')
    parser.add_argument('--train_csv', type=str,
                       default='scratch/Ball_counting_CNN/Tools_script/ball_counting_dataset_train.csv')
    parser.add_argument('--val_csv', type=str,
                       default='scratch/Ball_counting_CNN/Tools_script/ball_counting_dataset_val.csv')
    
    # å®éªŒå‚æ•°
    parser.add_argument('--total_epochs', type=int, default=100,
                       help='è®­ç»ƒæ€»epochæ•°')
    parser.add_argument('--ablations', nargs='+', 
                       default=['no_forward_model', 'no_attention', 
                               'late_fusion', 'shuffled_batch', 'shuffled_temporal'],
                       choices=list(ABLATION_CONFIGS.keys()),
                       help='è¦è¿è¡Œçš„æ¶ˆèå®éªŒ')
    parser.add_argument('--model_types', nargs='+',
                       default=['alexnet_no_pretrain'],
                       choices=['baseline', 'alexnet_no_pretrain', 'alexnet_pretrain'],
                       help='è§†è§‰ç¼–ç å™¨ç±»å‹')
    parser.add_argument('--seeds', nargs='+', type=int,
                       default=[2048, 4096, 9999],
                       help='éšæœºç§å­åˆ—è¡¨')
    
    # å¹¶è¡Œé…ç½®
    parser.add_argument('--num_parallel', type=int, default=3,
                       help='åŒæ—¶è¿è¡Œçš„å®éªŒæ•°é‡ï¼ˆA100å¯ä»¥è·‘3ä¸ªï¼‰')
    parser.add_argument('--gpu_id', type=int, default=0,
                       help='ä½¿ç”¨çš„GPU ID')
    
    # ä¿å­˜é…ç½®
    parser.add_argument('--save_dir', type=str, default='./ablation_experiments',
                       help='ç»“æœä¿å­˜ç›®å½•')
    parser.add_argument('--experiment_name', type=str, default=None,
                       help='å®éªŒåç§°ï¼ˆå¯é€‰ï¼‰')
    
    args = parser.parse_args()
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    experiment_name = args.experiment_name or f'ablation_parallel_{timestamp}'
    save_dir = os.path.join(args.save_dir, experiment_name)
    os.makedirs(save_dir, exist_ok=True)
    
    # æ•°æ®é…ç½®
    data_config = {
        'data_root': args.data_root,
        'train_csv': args.train_csv,
        'val_csv': args.val_csv
    }
    
    # åŸºç¡€é…ç½®
    base_config = {
        'total_epochs': args.total_epochs,
        'batch_size': 16,
        'sequence_length': 11,
        'learning_rate': 1e-4,
        'image_mode': 'rgb',
        'num_workers': 4,
        'save_checkpoints': True,
        'save_every': 10,
        'print_every': 10,
        'model_config': {
            'cnn_layers': 3,
            'cnn_channels': [64, 128, 256],
            'lstm_layers': 2,
            'lstm_hidden_size': 512,
            'feature_dim': 256,
            'joint_dim': 7,
            'dropout': 0.1,
            'use_fovea_bias': True
        },
        'adam_betas': (0.9, 0.999),
        'weight_decay': 1e-5,
        'grad_clip_norm': 1.0,
        'scheduler_type': 'cosine',
        'normalize': True,
        'embodiment_loss_weight': 0.3,
        'attention_loss_weight': 0.1
    }
    
    # ç”Ÿæˆæ‰€æœ‰ä»»åŠ¡
    tasks = []
    for ablation in args.ablations:
        for model_type in args.model_types:
            for seed in args.seeds:
                tasks.append((ablation, model_type, seed))
    
    total_experiments = len(tasks)
    
    print("\n" + "="*80)
    print("ğŸš€ å¹¶è¡Œæ¶ˆèå®éªŒå¯åŠ¨")
    print("="*80)
    print(f"æ¶ˆèç±»å‹: {args.ablations}")
    print(f"æ¨¡å‹ç±»å‹: {args.model_types}")
    print(f"éšæœºç§å­: {args.seeds}")
    print(f"æ€»å®éªŒæ•°: {total_experiments}")
    print(f"å¹¶è¡Œæ•°é‡: {args.num_parallel}")
    print(f"GPU ID: {args.gpu_id}")
    print(f"æ¯ä¸ªå®éªŒepochs: {args.total_epochs}")
    print(f"ä¿å­˜ç›®å½•: {save_dir}")
    print("="*80 + "\n")
    
    # ä¿å­˜å®éªŒé…ç½®
    experiment_config = {
        'timestamp': timestamp,
        'ablations': args.ablations,
        'model_types': args.model_types,
        'seeds': args.seeds,
        'total_experiments': total_experiments,
        'num_parallel': args.num_parallel,
        'gpu_id': args.gpu_id,
        'total_epochs': args.total_epochs,
        'data_config': data_config,
        'base_config': base_config
    }
    
    config_file = os.path.join(save_dir, 'experiment_config.json')
    with open(config_file, 'w') as f:
        json.dump(experiment_config, f, indent=4)
    
    # åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—å’Œç»“æœé˜Ÿåˆ—
    task_queue = mp.Queue()
    result_queue = mp.Queue()
    
    # å°†æ‰€æœ‰ä»»åŠ¡æ”¾å…¥é˜Ÿåˆ—
    for task in tasks:
        task_queue.put(task)
    
    # æ·»åŠ ç»ˆæ­¢ä¿¡å·
    for _ in range(args.num_parallel):
        task_queue.put(None)
    
    # å¯åŠ¨å·¥ä½œè¿›ç¨‹
    processes = []
    for i in range(args.num_parallel):
        p = mp.Process(
            target=worker_process,
            args=(task_queue, result_queue, args.gpu_id, i, data_config, base_config, save_dir)
        )
        p.start()
        processes.append(p)
        print(f"âœ“ å¯åŠ¨å·¥ä½œè¿›ç¨‹ {i}")
    
    # æ”¶é›†ç»“æœ
    all_results = []
    results_file = os.path.join(save_dir, 'all_results.csv')
    
    start_time = time.time()
    completed = 0
    
    print("\n" + "="*80)
    print("ğŸ“Š å®éªŒè¿›åº¦ç›‘æ§")
    print("="*80)
    
    # å®æ—¶æ”¶é›†ç»“æœ
    while completed < total_experiments:
        try:
            result = result_queue.get(timeout=10)
            completed += 1
            all_results.append(result)
            
            elapsed = time.time() - start_time
            avg_time = elapsed / completed
            remaining = avg_time * (total_experiments - completed)
            
            print(f"\n[{completed}/{total_experiments}] å®Œæˆå®éªŒ:")
            print(f"  æ¶ˆè: {result['ablation_name']}")
            print(f"  æ¨¡å‹: {result['model_type']}")
            print(f"  ç§å­: {result['seed']}")
            print(f"  æœ€ä½³å‡†ç¡®ç‡: {result['best_val_accuracy']:.4f}")
            print(f"  è®­ç»ƒæ—¶é—´: {result['training_time_hours']:.2f}h")
            print(f"  å·²ç”¨æ—¶: {elapsed/3600:.1f}h, é¢„è®¡å‰©ä½™: {remaining/3600:.1f}h")
            
            # å®æ—¶ä¿å­˜ç»“æœ
            results_df = pd.DataFrame(all_results)
            results_df.to_csv(results_file, index=False)
            
        except Exception as e:
            if "Empty" not in str(e):
                print(f"âš ï¸  ç»“æœé˜Ÿåˆ—é”™è¯¯: {e}")
    
    # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹ç»“æŸ
    print("\nç­‰å¾…æ‰€æœ‰è¿›ç¨‹ç»“æŸ...")
    for i, p in enumerate(processes):
        p.join()
        print(f"âœ“ è¿›ç¨‹ {i} å·²ç»“æŸ")
    
    total_time = time.time() - start_time
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    print("\n" + "="*80)
    print("ğŸ“Š ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š")
    print("="*80)
    
    results_df = pd.DataFrame(all_results)
    
    # æŒ‰æ¶ˆèç±»å‹åˆ†ç»„ç»Ÿè®¡
    summary = results_df.groupby(['ablation_type', 'model_type']).agg({
        'best_val_accuracy': ['mean', 'std', 'min', 'max'],
        'final_val_true_final_accuracy': ['mean', 'std'],
        'training_time_hours': ['mean', 'sum']
    }).round(4)
    
    summary_file = os.path.join(save_dir, 'summary_statistics.csv')
    summary.to_csv(summary_file)
    
    print("\nç»Ÿè®¡æ‘˜è¦:")
    print(summary)
    
    # ç”ŸæˆMarkdownæŠ¥å‘Š
    report_content = f"""# æ¶ˆèå®éªŒæŠ¥å‘Š

ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
å®éªŒåç§°: {experiment_name}

## å®éªŒé…ç½®

- **æ¶ˆèç±»å‹**: {', '.join(args.ablations)}
- **æ¨¡å‹ç±»å‹**: {', '.join(args.model_types)}
- **éšæœºç§å­**: {args.seeds}
- **æ€»å®éªŒæ•°**: {total_experiments}
- **å¹¶è¡Œæ•°é‡**: {args.num_parallel}
- **è®­ç»ƒepochs**: {args.total_epochs}
- **æ€»è€—æ—¶**: {total_time/3600:.2f} å°æ—¶

## æ¶ˆèç±»å‹è¯´æ˜

"""
    
    for ablation, config in ABLATION_CONFIGS.items():
        if ablation in args.ablations:
            report_content += f"### {config['name']}\n{config['description']}\n\n"
    
    report_content += """## å®éªŒç»“æœ

### æŒ‰æ¶ˆèç±»å‹ç»Ÿè®¡

| æ¶ˆèç±»å‹ | æ¨¡å‹ | å¹³å‡å‡†ç¡®ç‡ | æ ‡å‡†å·® | æœ€å°å€¼ | æœ€å¤§å€¼ |
|---------|------|-----------|--------|--------|--------|
"""
    
    for (ablation, model), row in results_df.groupby(['ablation_type', 'model_type']):
        mean_acc = row['best_val_accuracy'].mean()
        std_acc = row['best_val_accuracy'].std()
        min_acc = row['best_val_accuracy'].min()
        max_acc = row['best_val_accuracy'].max()
        abl_name = ABLATION_CONFIGS[ablation]['name']
        report_content += f"| {abl_name} | {model} | {mean_acc:.4f} | {std_acc:.4f} | {min_acc:.4f} | {max_acc:.4f} |\n"
    
    report_content += f"""

## æ–‡ä»¶è¯´æ˜

- `all_results.csv`: æ‰€æœ‰å®éªŒçš„è¯¦ç»†ç»“æœ
- `summary_statistics.csv`: ç»Ÿè®¡æ‘˜è¦
- `experiment_config.json`: å®éªŒé…ç½®
- å„å®éªŒç›®å½•åŒ…å«:
  - `config.json`: å®éªŒé…ç½®
  - `result.json`: å®éªŒç»“æœ
  - `checkpoints/`: æ¨¡å‹æ£€æŸ¥ç‚¹
  - `tensorboard_logs/`: TensorBoardæ—¥å¿—

## æŸ¥çœ‹TensorBoard

```bash
tensorboard --logdir {save_dir}
```

## ä¸»è¦å‘ç°

TODO: æ ¹æ®å®éªŒç»“æœå¡«å†™å…³é”®å‘ç°

"""
    
    report_file = os.path.join(save_dir, 'REPORT.md')
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report_content)
    
    print("\n" + "="*80)
    print("ğŸ‰ æ‰€æœ‰å®éªŒå®Œæˆï¼")
    print("="*80)
    print(f"â±ï¸  æ€»è€—æ—¶: {total_time/3600:.1f} å°æ—¶")
    print(f"ğŸ“Š ç»“æœæ–‡ä»¶: {results_file}")
    print(f"ğŸ“ˆ ç»Ÿè®¡æ‘˜è¦: {summary_file}")
    print(f"ğŸ“‹ æŠ¥å‘Š: {report_file}")
    print(f"ğŸ’¾ æ‰€æœ‰æ–‡ä»¶: {save_dir}")
    print("="*80 + "\n")


if __name__ == "__main__":
    main()